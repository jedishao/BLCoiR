<bugrepository name="druid">
    <bug id="6139" opendate="2018-08-09 00:00:00" fixdate="2018-08-11 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>Race in testCheckpointForUnknownTaskGroup() of KafkaSupervisorTest.
            </summary>
            <description>The stacktrace is:
                2018-08-10T00:03:11,557 ERROR [KafkaSupervisor-testDS]
                io.druid.indexing.kafka.supervisor.KafkaSupervisor - KafkaSupervisor[testDS] failed to handle notice:
                {class=io.druid.indexing.kafka.supervisor.KafkaSupervisor, exceptionType=class
                io.druid.java.util.common.ISE, exceptionMessage=WTH?! cannot find taskGroup [0] among all taskGroups
                [{}], noticeClass=CheckpointNotice}
                io.druid.java.util.common.ISE: WTH?! cannot find taskGroup [0] among all taskGroups [{}]
                at
                io.druid.indexing.kafka.supervisor.KafkaSupervisor$CheckpointNotice.isValidTaskGroup(KafkaSupervisor.java:686)
                ~[classes/:?]
                at io.druid.indexing.kafka.supervisor.KafkaSupervisor$CheckpointNotice.handle(KafkaSupervisor.java:638)
                ~[classes/:?]
                at io.druid.indexing.kafka.supervisor.KafkaSupervisor$2.run(KafkaSupervisor.java:363) [classes/:?]
                at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_161]
                at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_161]
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_161]
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_161]
                at java.lang.Thread.run(Thread.java:748) [?:1.8.0_161]

                The corresponding code is:
                Assert.assertNotNull(serviceEmitter.getStackTrace());
                So, the issue is, serviceEmitter.getStackTrace() can return null when this check is called, so the test
                should wait for the stacktrace to be set.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
<file type="M">org.apache.druid.indexing.kafka.supervisor.KafkaSupervisorTest.java</file>
        </fixedFiles>
    </bug>
    <bug id="2991" opendate="2016-05-19 00:00:00" fixdate="2016-05-26 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>Race condition in IncrementalIndex's facts.
            </summary>
            <description>In OnheapIncrementalIndex::addToFacts(...), when we get a new fact, we will first add the
                mapping from key to fact's index, then we do aggregate. It will cause query saw some unaggregated
                metrics with initial value, and sometimes may cause some weird situations. Like if we use LongMax for
                build but use LongSum for query, then the result may add LongMax's initial value which is
                LONG.MIN_VALUE.
                BTW, do we need to deal with this:
                final Integer prev = facts.putIfAbsent(key, rowIndex);
                if (null == prev) {
                numEntries.incrementAndGet();
                } else {
                // We lost a race
                aggs = concurrentGet(prev);
                // Free up the misfire
                concurrentRemove(rowIndex);
                // This is expected to occur ~80% of the time in the worst scenarios
                }

                I don't find any multi-thread adds for IncremetalIndex.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.druid.segment.incremental.OnheapIncrementalIndex.java</file>
            <file type="M">org.apache.druid.segment.incremental.OnheapIncrementalIndexTest.java</file>
        </fixedFiles>
    </bug>
    <bug id="3593" opendate="2016-10-19 00:00:00" fixdate="2022-02-25 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>[QTL] Deadlock in offheap cache.
            </summary>
            <description>=============================
                "NamespaceExtractionCacheManager-0":
                waiting for ownable synchronizer 0x00000004f0550030, (a
                java.util.concurrent.locks.ReentrantLock$NonfairSync),
                which is held by "qtp1930842682-69"
                "qtp1930842682-69":
                waiting to lock monitor 0x00007fa3d4009bd8 (object 0x00000004c38b04a8, a
                java.util.concurrent.atomic.AtomicBoolean),
                which is held by "NamespaceExtractionCacheManager-0"

                Java stack information for the threads listed above:
                ===================================================
                "NamespaceExtractionCacheManager-0":
                at sun.misc.Unsafe.park(Native Method)
                - parking to wait for#0x00000004f0550030> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
                at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
                at
                java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
                at
                java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:870)
                at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1199)
                at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:209)
                at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:285)
                at
                io.druid.server.lookup.namespace.cache.OffHeapNamespaceExtractionCacheManager.swapAndClearCache(OffHeapNamespaceExtractionCacheManager.java:112)
                at
                io.druid.server.lookup.namespace.cache.NamespaceExtractionCacheManager$2.run(NamespaceExtractionCacheManager.java:173)
                - locked#0x00000004c38b04a8> (a java.util.concurrent.atomic.AtomicBoolean)
                at
                io.druid.server.lookup.namespace.cache.NamespaceExtractionCacheManager$4.run(NamespaceExtractionCacheManager.java:368)
                at
                com.google.common.util.concurrent.MoreExecutors$ScheduledListeningDecorator$NeverSuccessfulListenableFutureTask.run(MoreExecutors.java:582)
                at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
                at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
                at
                java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
                at
                java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
                at java.lang.Thread.run(Thread.java:745)
                "qtp1930842682-69":
                at
                io.druid.server.lookup.namespace.cache.NamespaceExtractionCacheManager.removeNamespaceLocalMetadata(NamespaceExtractionCacheManager.java:304)
                - waiting to lock#0x00000004c38b04a8> (a java.util.concurrent.atomic.AtomicBoolean)
                at
                io.druid.server.lookup.namespace.cache.NamespaceExtractionCacheManager.delete(NamespaceExtractionCacheManager.java:447)
                at
                io.druid.server.lookup.namespace.cache.OffHeapNamespaceExtractionCacheManager.delete(OffHeapNamespaceExtractionCacheManager.java:139)
                at
                io.druid.server.lookup.namespace.cache.NamespaceExtractionCacheManager.scheduleAndWait(NamespaceExtractionCacheManager.java:254)
                at io.druid.query.lookup.NamespaceLookupExtractorFactory.start(NamespaceLookupExtractorFactory.java:111)
                at io.druid.query.lookup.LookupReferencesManager.updateIfNew(LookupReferencesManager.java:235)
                at io.druid.query.lookup.LookupListeningResource$2.post(LookupModule.java:129)
                at
                io.druid.server.listener.resource.AbstractListenerHandler.handlePOSTAll(AbstractListenerHandler.java:107)
                at
                io.druid.server.listener.resource.ListenerResource.serviceAnnouncementPOSTAll(ListenerResource.java:92)
                at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
                at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
                at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
                at java.lang.reflect.Method.invoke(Method.java:498)
                at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
                at
                com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)
                at
                com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
                at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)
                at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
                at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
                at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
                at
                com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)
                at
                com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)
                at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)
                at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)
                at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)
                at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)
                at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)
                at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
                at com.google.inject.servlet.ServletDefinition.doServiceImpl(ServletDefinition.java:286)
                at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:276)
                at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:181)
                at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91)
                at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:85)
                at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:120)
                at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:135)
                at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1652)
                at org.eclipse.jetty.servlets.UserAgentFilter.doFilter(UserAgentFilter.java:83)
                at org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:364)
                at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1652)
                at org.eclipse.jetty.servlets.QoSFilter.doFilter(QoSFilter.java:200)
                at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1652)
                at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585)
                at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:221)
                at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1125)
                at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515)
                at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)
                at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1059)
                at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
                at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52)
                at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97)
                at org.eclipse.jetty.server.Server.handle(Server.java:497)
                at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:310)
                at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:248)
                at org.eclipse.jetty.io.AbstractConnection$2.run(AbstractConnection.java:540)
                at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:620)
                at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:540)
                at java.lang.Thread.run(Thread.java:745)

                Found 1 deadlock.

            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
<file type="M">org.apache.druid.server.lookup.namespace.cache.NamespaceExtractionCacheManager.java</file>
<file type="M">org.apache.druid.server.lookup.namespace.cache.OffHeapNamespaceExtractionCacheManager.java</file>
<file type="M">org.apache.druid.server.lookup.namespace.cache.OnHeapNamespaceExtractionCacheManager.java</file>
<file type="M">org.apache.druid.server.lookup.namespace.cache.NamespaceExtractionCacheManagerExecutorsTest.java</file>
<file type="M">org.apache.druid.query.lookup.KafkaLookupExtractorFactory.java</file>
<file type="M">org.apache.druid.query.lookup.KafkaLookupExtractorFactoryTest.java</file>
<file type="M">org.apache.druid.query.lookup.NamespaceLookupExtractorFactory.java</file>
<file type="M">org.apache.druid.query.lookup.NamespaceLookupIntrospectHandler.java</file>
<file type="M">org.apache.druid.query.lookup.namespace.ExtractionNamespaceCacheFactory.java</file>
<file type="M">org.apache.druid.server.lookup.namespace.JDBCExtractionNamespaceCacheFactory.java</file>
<file type="M">org.apache.druid.server.lookup.namespace.NamespaceExtractionModule.java</file>
<file type="M">org.apache.druid.server.lookup.namespace.StaticMapExtractionNamespaceCacheFactory.java</file>
<file type="M">org.apache.druid.server.lookup.namespace.cache.CacheHandler.java</file>
<file type="M">org.apache.druid.server.lookup.namespace.cache.CacheProxy.java</file>
<file type="M">org.apache.druid.server.lookup.namespace.cache.CacheScheduler.java</file>
<file type="M">org.apache.druid.server.lookup.namespace.cache.NamespaceExtractionCacheManager.java</file>
<file type="M">org.apache.druid.server.lookup.namespace.cache.OffHeapNamespaceExtractionCacheManager.java</file>
<file type="M">org.apache.druid.server.lookup.namespace.cache.OnHeapNamespaceExtractionCacheManager.java</file>
<file type="M">org.apache.druid.server.lookup.namespace.cache.UpdateCounter.java</file>
<file type="M">org.apache.druid.query.lookup.NamespaceLookupExtractorFactoryTest.java</file>
<file type="M">org.apache.druid.server.lookup.namespace.NamespacedExtractorModuleTest.java</file>
<file type="M">org.apache.druid.server.lookup.namespace.StaticMapExtractionNamespaceCacheFactoryTest.java</file>
<file type="M">org.apache.druid.server.lookup.namespace.URIExtractionNamespaceCacheFactoryTest.java</file>
<file type="M">org.apache.druid.server.lookup.namespace.cache.JDBCExtractionNamespaceTest.java</file>
<file type="M">org.apache.druid.server.lookup.namespace.cache.NamespaceExtractionCacheManagerExecutorsTest.java</file>
<file type="M">org.apache.druid.server.lookup.namespace.cache.NamespaceExtractionCacheManagersTest.java</file>
<file type="M">org.apache.druid.server.lookup.namespace.cache.OffHeapNamespaceExtractionCacheManagerTest.java</file>
        </fixedFiles>
    </bug>
    <bug id="6826" opendate="2019-01-09 00:00:00" fixdate="2019-04-12 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>Concurrency bug in DruidSchema.refreshSegmentsForDataSource().
            </summary>
            <description>segmentMap is created outside of lock, so by the time of SegmentMetadataHolder holder =
                dataSourceSegments.get(segment); segment could be already removed. It could cause NPE below.
                FYI @surekhasaharan @gianm
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.druid.sql.calcite.schema.DruidSchema.java</file>
            <file type="M">org.apache.druid.sql.calcite.schema.DruidSchemaTest.java</file>
        </fixedFiles>
    </bug>
    <bug id="2842" opendate="2016-04-15 00:00:00" fixdate="2016-04-28 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>NPE in RTR with multiple threads for task assignment.
            </summary>
            <description>Multiple threads in task assignment causes NPE.
                Seems to have been introduced by #2521
                2016-04-15T09:56:11,335 ERROR [rtr-pending-tasks-runner-2] io.druid.indexing.overlord.RemoteTaskRunner -
                Exception while trying to assign task: {class=io.druid.indexing.overlord.RemoteTask
                Runner, exceptionType=class java.lang.NullPointerException, exceptionMessage=task, taskId=#TASKID>}
                java.lang.NullPointerException: task
                at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:229)
                ~[druid-selfcontained-7aaf2b3.jar:7aaf2b3]
                at io.druid.indexing.overlord.RemoteTaskRunner.tryAssignTask(RemoteTaskRunner.java:638)
                ~[druid-selfcontained-7aaf2b3.jar:7aaf2b3]
                at io.druid.indexing.overlord.RemoteTaskRunner.access$1400(RemoteTaskRunner.java:116)
                ~[druid-selfcontained-7aaf2b3.jar:7aaf2b3]
                at io.druid.indexing.overlord.RemoteTaskRunner$3.call(RemoteTaskRunner.java:568)
                [druid-selfcontained-7aaf2b3.jar:7aaf2b3]
                at io.druid.indexing.overlord.RemoteTaskRunner$3.call(RemoteTaskRunner.java:556)
                [druid-selfcontained-7aaf2b3.jar:7aaf2b3]
                at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_60]
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_60]
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_60]
                at java.lang.Thread.run(Thread.java:745) [?:1.8.0_60]
                2016-04-15T09:56:11,336 ERROR [rtr-pending-tasks-runner-2] io.druid.indexing.overlord.RemoteTaskRunner -
                Exception in running pending tasks: {class=io.druid.indexing.overlord.RemoteTaskRun
                ner, exceptionType=class java.lang.NullPointerException, exceptionMessage=taskRunnerWorkItem}
                java.lang.NullPointerException: taskRunnerWorkItem
                at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:229)
                ~[druid-selfcontained-7aaf2b3.jar:7aaf2b3]
                at io.druid.indexing.overlord.RemoteTaskRunner.taskComplete(RemoteTaskRunner.java:1048)
                ~[druid-selfcontained-7aaf2b3.jar:7aaf2b3]
                at io.druid.indexing.overlord.RemoteTaskRunner.access$1500(RemoteTaskRunner.java:116)
                ~[druid-selfcontained-7aaf2b3.jar:7aaf2b3]
                at io.druid.indexing.overlord.RemoteTaskRunner$3.call(RemoteTaskRunner.java:577)
                [druid-selfcontained-7aaf2b3.jar:7aaf2b3]
                at io.druid.indexing.overlord.RemoteTaskRunner$3.call(RemoteTaskRunner.java:556)
                [druid-selfcontained-7aaf2b3.jar:7aaf2b3]
                at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_60]
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_60]
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_60]
                at java.lang.Thread.run(Thread.java:745) [?:1.8.0_60]

            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.druid.indexing.overlord.RemoteTaskRunner.java</file>
<file type="M">org.apache.druid.indexing.overlord.RemoteTaskRunnerRunPendingTasksConcurrencyTest.java</file>
            <file type="M">org.apache.druid.indexing.overlord.RemoteTaskRunnerTest.java</file>
            <file type="M">org.apache.druid.indexing.overlord.RemoteTaskRunnerTestUtils.java</file>
        </fixedFiles>
    </bug>
    <bug id="6201" opendate="2018-08-21 00:00:00" fixdate="2018-08-25 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>Deadlock on overlord with HttpRemoteTaskRunner.
            </summary>
            <description>We noticed deadlocks on the overlord in our test cluster in HttpRemoteTaskRunner:
                "hrtr-pending-tasks-runner-0" #193 daemon prio=5 os_prio=0 tid=0x00007fc5a8020000 nid=0x72ec waiting for
                monitor entry [0x00007fc5644b4000]
                java.lang.Thread.State: BLOCKED (on object monitor)
                at io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner.runTaskOnWorker(HttpRemoteTaskRunner.java:379)
                - waiting to lock#0x00000000c1544c10> (a java.lang.Object)
                at
                io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner.lambda$addPendingTaskToExecutor$7(HttpRemoteTaskRunner.java:1005)
                at io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner$$Lambda$133/748833484.run(Unknown Source)
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
                at java.lang.Thread.run(Thread.java:748)

                java.lang.Thread.State: BLOCKED (on object monitor)
                at io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner.getKnownTasks(HttpRemoteTaskRunner.java:1111)
                - waiting to lock#0x00000000c1544c10> (a java.lang.Object)
                at io.druid.indexing.overlord.TaskQueue.manage(TaskQueue.java:232)
                at io.druid.indexing.overlord.TaskQueue.access$000(TaskQueue.java:69)
                at io.druid.indexing.overlord.TaskQueue$1.run(TaskQueue.java:136)
                at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
                at java.util.concurrent.FutureTask.run(FutureTask.java:266)
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
                at java.lang.Thread.run(Thread.java:748)

                "HttpRemoteTaskRunner-worker-sync-4" #116 daemon prio=5 os_prio=0 tid=0x00007fc588018800 nid=0x724c
                waiting for monitor entry [0x00007fc569801000]
                java.lang.Thread.State: BLOCKED (on object monitor)
                at io.druid.server.coordination.ChangeRequestHttpSyncer.stop(ChangeRequestHttpSyncer.java:138)
                - waiting to lock#0x00000000c1544980> (a io.druid.concurrent.LifecycleLock)
                at io.druid.indexing.overlord.hrtr.WorkerHolder.stop(WorkerHolder.java:333)
                at io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner.removeWorker(HttpRemoteTaskRunner.java:532)
                - locked#0x00000000c1540300> (a java.util.concurrent.ConcurrentHashMap)
                at
                io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner.lambda$scheduleSyncMonitoring$2(HttpRemoteTaskRunner.java:642)
                - locked#0x00000000c1540300> (a java.util.concurrent.ConcurrentHashMap)
                at io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner$$Lambda$90/1851975649.run(Unknown Source)
                at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
                at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
                at
                java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
                at
                java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
                at java.lang.Thread.run(Thread.java:748)

                java.lang.Thread.State: BLOCKED (on object monitor)
                at
                io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner.taskAddedOrUpdated(HttpRemoteTaskRunner.java:1174)
                - waiting to lock#0x00000000c1544c10> (a java.lang.Object)
                at io.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner$$Lambda$87/935105358.taskAddedOrUpdated(Unknown
                Source)
                at io.druid.indexing.overlord.hrtr.WorkerHolder$2.notifyListener(WorkerHolder.java:449)
                at io.druid.indexing.overlord.hrtr.WorkerHolder$2.deltaSync(WorkerHolder.java:442)
                at io.druid.server.coordination.ChangeRequestHttpSyncer$1.onSuccess(ChangeRequestHttpSyncer.java:269)
                - locked#0x00000000c1543b50> (a io.druid.concurrent.LifecycleLock)
                at io.druid.server.coordination.ChangeRequestHttpSyncer$1.onSuccess(ChangeRequestHttpSyncer.java:225)
                at com.google.common.util.concurrent.Futures$4.run(Futures.java:1181)
                at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
                at java.util.concurrent.FutureTask.run(FutureTask.java:266)
                at
                java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
                at
                java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
                at java.lang.Thread.run(Thread.java:748)
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner.java</file>
        </fixedFiles>
    </bug>
    <bug id="4984" opendate="2017-10-20 00:00:00" fixdate="2017-10-20 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>Callback race on start in cachingCost balancer.
            </summary>
            <description>CachingCostBalancerStrategyFactory registers callbacks in its start() method, which is
                problematic since any callbacks that happen between start of ServerInventoryView and
                BalancerStrategyFactory will be missed. Inventory view callbacks should generally be registered in
                constructors, not in start methods.
                I discovered this when trying to test the cachingCost balancer on a cluster that has a small number of
                segments, and found that it missed the initialization callback (happened too fast) so it refused to
                create itself.
                cc: @dgolitsyn @leventov
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.druid.server.coordinator.CachingCostBalancerStrategyFactory.java</file>
        </fixedFiles>
    </bug>
    <bug id="9292" opendate="2020-01-30 00:00:00" fixdate="2020-02-05 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>CoordinatorRuleManager.rules doesn't need to store ConcurrentHashMap.
            </summary>
            <description>CoordinatorRuleManager didn't have a race condition prior to #6898, actually, because the
                ConcurrentHashMap is never modified after assigning into AtomicReference. So the proper fix was (and the
                proper action now is) to replace ConcurrentHashMap with simple HashMap.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.druid.server.router.CoordinatorRuleManager.java</file>
        </fixedFiles>
    </bug>
    <bug id="3393" opendate="2016-08-24 00:00:00" fixdate="2022-02-25 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>should handle channel disconnected better in NettyHttpClient or Druid.
            </summary>
            <description>I'm facing such a race condition:
                Broker runs a query, finally calls the httpClient.go() in DirectDruidClient, the httpClient is instance
                of NettyHttpClient.
                Inside the NettyHttpClient.go() method, after the channel is taken from the pool and verified in good
                shape and ready to fire the query, then the historical side reaches the idle time and disconnect this
                channel.
                Then handler.channelDisconnected callback is called, and finally the query is failed in broker.
                This exception occurs hundreds of times in our druid cluster (it is a busy cluster).
                This channel never has a chance to actually connect to the historical to get a single byte back, thus ,I
                think, the NettyHttpClient.go() should retry internally one time to get a new channel to finish the "go"
                instead of set the retVal future to fail.
                The same thing can happen when channel.write(httpRequest).addListener(...) in NettyHttpClient. the
                channel to write can be closed by historical server after it is taken from the pool channel, thus it
                also should be retried.
                here is the trace:
                2016-08-24T14:03:29,029 ERROR
                [qtp1369854401-202[timeseries_eternal_olap_click_da76287c-4eec-4c84-a8b8-2e12ea92aa05]]
                io.druid.server.QueryResource - Exception handling request: {class=io.druid.server.QueryResource,
                exceptionType=class com.metamx.common.RE, exceptionMessage=Failure getting results
                from[http://hdppic0101.et2.tbsite.net:8083/druid/v2/] because of
                [org.jboss.netty.channel.ChannelException: Channel disconnected], exception=com.metamx.common.RE:
                Failure getting results from[http://hdppic0101.et2.tbsite.net:8083/druid/v2/] because of
                [org.jboss.netty.channel.ChannelException: Channel disconnected],
                query=TimeseriesQuery{dataSource='eternal_olap_click',
                querySegmentSpec=LegacySegmentSpec{intervals=[2016-08-24T00:00:00.000+08:00/2016-08-24T13:54:00.000+08:00]},
                descending=false, dimFilter=(scene_tag = sort_type^C_coefp ## bu_src = taobao_topic ## item_id =
                tm6048738 ## exper_token_str = wl_topic_sort ## action_type = click),
                granularity='PeriodGranularity{period=PT1M, timeZone=+08:00, origin=null}',
                aggregatorSpecs=[DoubleSumAggregatorFactory{fieldName='ipv', name='ipv'}], postAggregatorSpecs=[],
                context={queryId=da76287c-4eec-4c84-a8b8-2e12ea92aa05, timeout=30000}}, peer=10.197.16.234}
                com.metamx.common.RE: Failure getting results from[http://hdppic0101.et2.tbsite.net:8083/druid/v2/]
                because of [org.jboss.netty.channel.ChannelException: Channel disconnected]
                at io.druid.client.DirectDruidClient$JsonParserIterator.init(DirectDruidClient.java:498)
                ~[druid-server-0.9.0.jar:0.9.0]
                at io.druid.client.DirectDruidClient$JsonParserIterator.hasNext(DirectDruidClient.java:442)
                ~[druid-server-0.9.0.jar:0.9.0]
                at com.metamx.common.guava.BaseSequence.makeYielder(BaseSequence.java:103) ~[java-util-0.27.7.jar:?]
                at com.metamx.common.guava.BaseSequence.toYielder(BaseSequence.java:81) ~[java-util-0.27.7.jar:?]
                at com.metamx.common.guava.MappedSequence.toYielder(MappedSequence.java:46) ~[java-util-0.27.7.jar:?]
                at com.metamx.common.guava.MergeSequence$2.accumulate(MergeSequence.java:66) ~[java-util-0.27.7.jar:?]
                at com.metamx.common.guava.MergeSequence$2.accumulate(MergeSequence.java:62) ~[java-util-0.27.7.jar:?]
                at com.metamx.common.guava.YieldingAccumulators$1.accumulate(YieldingAccumulators.java:32)
                ~[java-util-0.27.7.jar:?]
                at com.metamx.common.guava.BaseSequence.makeYielder(BaseSequence.java:104) ~[java-util-0.27.7.jar:?]
                at com.metamx.common.guava.BaseSequence.toYielder(BaseSequence.java:81) ~[java-util-0.27.7.jar:?]
                at com.metamx.common.guava.BaseSequence.accumulate(BaseSequence.java:67) ~[java-util-0.27.7.jar:?]
                at com.metamx.common.guava.MergeSequence.toYielder(MergeSequence.java:59) ~[java-util-0.27.7.jar:?]
                at com.metamx.common.guava.LazySequence.toYielder(LazySequence.java:43) ~[java-util-0.27.7.jar:?]
                at io.druid.query.RetryQueryRunner$1.toYielder(RetryQueryRunner.java:105)
                ~[druid-processing-0.9.0.jar:0.9.0]
                at io.druid.common.guava.CombiningSequence.toYielder(CombiningSequence.java:79)
                ~[druid-common-0.9.0.jar:0.9.0]
                at com.metamx.common.guava.MappedSequence.toYielder(MappedSequence.java:46) ~[java-util-0.27.7.jar:?]
                at io.druid.query.CPUTimeMetricQueryRunner$1.toYielder(CPUTimeMetricQueryRunner.java:93)
                ~[druid-processing-0.9.0.jar:0.9.0]
                at com.metamx.common.guava.Sequences$1.toYielder(Sequences.java:98) ~[java-util-0.27.7.jar:?]
                at io.druid.server.QueryResource.doPost(QueryResource.java:170) [druid-server-0.9.0.jar:0.9.0]
                at sun.reflect.GeneratedMethodAccessor52.invoke(Unknown Source) ~[?:?]
                at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_66]
                at java.lang.reflect.Method.invoke(Method.java:497) ~[?:1.8.0_66]
                at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
                [jersey-server-1.19.jar:1.19]
                at
                com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)
                [jersey-server-1.19.jar:1.19]
                at
                com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
                [jersey-server-1.19.jar:1.19]
                at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)
                [jersey-server-1.19.jar:1.19]
                at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
                [jersey-server-1.19.jar:1.19]
                at
                com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)
                [jersey-server-1.19.jar:1.19]
                at
                com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
                [jersey-server-1.19.jar:1.19]
                at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)
                [jersey-server-1.19.jar:1.19]
                at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
                [jersey-server-1.19.jar:1.19]
                at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
                [jersey-server-1.19.jar:1.19]
                at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
                [jersey-server-1.19.jar:1.19]
                at
                com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)
                [jersey-server-1.19.jar:1.19]
                at
                com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)
                [jersey-server-1.19.jar:1.19]
                at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)
                [jersey-server-1.19.jar:1.19]
                at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)
                [jersey-server-1.19.jar:1.19]
                at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)
                [jersey-servlet-1.19.jar:1.19]
                at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)
                [jersey-servlet-1.19.jar:1.19]
                at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)
                [jersey-servlet-1.19.jar:1.19]
                at javax.servlet.http.HttpServlet.service(HttpServlet.java:790) [javax.servlet-api-3.1.0.jar:3.1.0]
                at com.google.inject.servlet.ServletDefinition.doServiceImpl(ServletDefinition.java:278)
                [guice-servlet-4.0-beta.jar:?]
                at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:268)
                [guice-servlet-4.0-beta.jar:?]
                at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:180)
                [guice-servlet-4.0-beta.jar:?]
                at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:93)
                [guice-servlet-4.0-beta.jar:?]
                at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:85)
                [guice-servlet-4.0-beta.jar:?]
                at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:120)
                [guice-servlet-4.0-beta.jar:?]
                at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:132) [guice-servlet-4.0-beta.jar:?]
                at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:129) [guice-servlet-4.0-beta.jar:?]
                at com.google.inject.servlet.GuiceFilter$Context.call(GuiceFilter.java:206)
                [guice-servlet-4.0-beta.jar:?]
                at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:129) [guice-servlet-4.0-beta.jar:?]
                at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1668)
                [jetty-servlet-9.3.6.v20151106.jar:9.3.6.v20151106]
                at org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:45)
                [jetty-servlets-9.3.6.v20151106.jar:9.3.6.v20151106]
                at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1668)
                [jetty-servlet-9.3.6.v20151106.jar:9.3.6.v20151106]
                at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:581)
                [jetty-servlet-9.3.6.v20151106.jar:9.3.6.v20151106]
                at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:224)
                [jetty-server-9.3.6.v20151106.jar:9.3.6.v20151106]
                at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1158)
                [jetty-server-9.3.6.v20151106.jar:9.3.6.v20151106]
                at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:511)
                [jetty-servlet-9.3.6.v20151106.jar:9.3.6.v20151106]
                at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)
                [jetty-server-9.3.6.v20151106.jar:9.3.6.v20151106]
                at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1090)
                [jetty-server-9.3.6.v20151106.jar:9.3.6.v20151106]
                at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
                [jetty-server-9.3.6.v20151106.jar:9.3.6.v20151106]
                at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52)
                [jetty-server-9.3.6.v20151106.jar:9.3.6.v20151106]
                at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:119)
                [jetty-server-9.3.6.v20151106.jar:9.3.6.v20151106]
                at org.eclipse.jetty.server.Server.handle(Server.java:517)
                [jetty-server-9.3.6.v20151106.jar:9.3.6.v20151106]
                at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:308)
                [jetty-server-9.3.6.v20151106.jar:9.3.6.v20151106]
                at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:242)
                [jetty-server-9.3.6.v20151106.jar:9.3.6.v20151106]
                at
                org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceAndRun(ExecuteProduceConsume.java:213)
                [jetty-util-9.3.6.v20151106.jar:9.3.6.v20151106]
                at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:147)
                [jetty-util-9.3.6.v20151106.jar:9.3.6.v20151106]
                at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:654)
                [jetty-util-9.3.6.v20151106.jar:9.3.6.v20151106]
                at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
                [jetty-util-9.3.6.v20151106.jar:9.3.6.v20151106]
                at java.lang.Thread.run(Thread.java:756) [?:1.8.0_66]
                Caused by: java.util.concurrent.ExecutionException: org.jboss.netty.channel.ChannelException: Channel
                disconnected
                at com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:299)
                ~[guava-16.0.1.jar:?]
                at com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:286)
                ~[guava-16.0.1.jar:?]
                at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116) ~[guava-16.0.1.jar:?]
                at io.druid.client.DirectDruidClient$JsonParserIterator.init(DirectDruidClient.java:479)
                ~[druid-server-0.9.0.jar:0.9.0]
                ... 69 more
                Caused by: org.jboss.netty.channel.ChannelException: Channel disconnected
                at com.metamx.http.client.NettyHttpClient$1.channelDisconnected(NettyHttpClient.java:311)
                ~[http-client-1.0.4.jar:?]
                at
                org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:102)
                ~[netty-3.10.4.Final.jar:?]
                at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
                ~[netty-3.10.4.Final.jar:?]
                at
                org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
                ~[netty-3.10.4.Final.jar:?]
                at
                org.jboss.netty.channel.SimpleChannelUpstreamHandler.channelDisconnected(SimpleChannelUpstreamHandler.java:208)
                ~[netty-3.10.4.Final.jar:?]
                at
                org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:102)
                ~[netty-3.10.4.Final.jar:?]
                at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
                ~[netty-3.10.4.Final.jar:?]
                at
                org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
                ~[netty-3.10.4.Final.jar:?]
                at
                org.jboss.netty.channel.SimpleChannelUpstreamHandler.channelDisconnected(SimpleChannelUpstreamHandler.java:208)
                ~[netty-3.10.4.Final.jar:?]
                at
                org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:102)
                ~[netty-3.10.4.Final.jar:?]
                at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
                ~[netty-3.10.4.Final.jar:?]
                at
                org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
                ~[netty-3.10.4.Final.jar:?]
                at org.jboss.netty.handler.codec.replay.ReplayingDecoder.cleanup(ReplayingDecoder.java:570)
                ~[netty-3.10.4.Final.jar:?]
                at org.jboss.netty.handler.codec.frame.FrameDecoder.channelDisconnected(FrameDecoder.java:365)
                ~[netty-3.10.4.Final.jar:?]
                at
                org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:102)
                ~[netty-3.10.4.Final.jar:?]
                at org.jboss.netty.handler.codec.http.HttpClientCodec.handleUpstream(HttpClientCodec.java:92)
                ~[netty-3.10.4.Final.jar:?]
                at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
                ~[netty-3.10.4.Final.jar:?]
                at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
                ~[netty-3.10.4.Final.jar:?]
                at org.jboss.netty.channel.Channels.fireChannelDisconnected(Channels.java:396)
                ~[netty-3.10.4.Final.jar:?]
                at org.jboss.netty.channel.socket.nio.AbstractNioWorker.close(AbstractNioWorker.java:360)
                ~[netty-3.10.4.Final.jar:?]
                at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:93) ~[netty-3.10.4.Final.jar:?]
                at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
                ~[netty-3.10.4.Final.jar:?]
                at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
                ~[netty-3.10.4.Final.jar:?]
                at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
                ~[netty-3.10.4.Final.jar:?]
                at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178) ~[netty-3.10.4.Final.jar:?]
                at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
                ~[netty-3.10.4.Final.jar:?]
                at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
                ~[netty-3.10.4.Final.jar:?]
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[?:1.8.0_66]
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[?:1.8.0_66]

            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.druid.java.util.http.client.HttpClientConfig.java</file>
            <file type="M">org.apache.druid.java.util.client.HttpClientInit.java</file>
            <file type="M">org.apache.druid.java.util.client.pool.ResourcePool.java</file>
            <file type="M">org.apache.druid.java.util.client.pool.ResourcePoolConfig.java</file>
            <file type="M">org.apache.druid.java.util.client.pool.ResourcePoolTest.java</file>
        </fixedFiles>
    </bug>
    <bug id="3608" opendate="2016-10-25 00:00:00" fixdate="2017-03-17 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>IngestSegmentFirehoseFactory race between tasks.
            </summary>
            <description>IngestSegmentFirehoseFactory uses a fixed task id "reindex" when creating the toolbox factory
                that it uses to get segments. This creates a race condition where two tasks could actually download the
                same segment at the same time into the same directory, and one will get clobbered and fail.
                One situation where this can happen easily is if you're reindexing a datasource into two datasources
                (maybe reducing to two different levels of grain). The tasks will proceed simultaneously, since there's
                no cross locking, but they'll be downloading the same segments for the input datasource.
                One fix is having it actually use the real task id (which would need to get plumbed in). This has the
                advantage of putting the task files all together, and using the existing mechanisms for cleaning up task
                work directories.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.druid.indexing.common.task.IndexTask.java</file>
            <file type="M">org.apache.druid.indexing.firehose.IngestSegmentFirehoseFactory.java</file>
        </fixedFiles>
    </bug>
    <bug id="6028" opendate="2018-07-20 00:00:00" fixdate="2018-08-03 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>Error in SqlMetadataRuleManagerTest.
            </summary>
            <description>Tests run: 4, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 22.619 sec
                FAILURE! - in io.druid.metadata.SQLMetadataRuleManagerTest
                testMultipleStopAndStart(io.druid.metadata.SQLMetadataRuleManagerTest) Time elapsed: 20.698 sec
                ERROR!
                org.skife.jdbi.v2.exceptions.CallbackFailedException:
                org.skife.jdbi.v2.exceptions.UnableToExecuteStatementException:
                java.sql.SQLTransactionRollbackException: A lock could not be obtained due to a deadlock, cycle of locks
                and waiters is:
                Lock : ROW, SYSCOLUMNS, (5,12)
                Waiting XID : {237, X} , APP, DROP TABLE druidTestc0dd9b47c4df453daeba2aca4613783e_rules
                Granted XID : {217, S}
                Lock : TABLE, DRUIDTESTC0DD9B47C4DF453DAEBA2ACA4613783E_RULES, Tablelock
                Waiting XID : {217, IS} , APP, SELECT r.dataSource, r.payload FROM
                druidTestc0dd9b47c4df453daeba2aca4613783e_rules r INNER JOIN(SELECT dataSource, max(version) as version
                FROM druidTestc0dd9b47c4df453daeba2aca4613783e_rules GROUP BY dataSource) ds ON r.datasource =
                ds.datasource and r.version = ds.version
                Granted XID : {237, X}
                . The selected victim is XID : 237. [statement:"DROP TABLE
                druidTestc0dd9b47c4df453daeba2aca4613783e_rules", located:"DROP TABLE
                druidTestc0dd9b47c4df453daeba2aca4613783e_rules", rewritten:"DROP TABLE
                druidTestc0dd9b47c4df453daeba2aca4613783e_rules", arguments:{ positional:{}, named:{}, finder:[]}]
                at io.druid.metadata.SQLMetadataRuleManagerTest.dropTable(SQLMetadataRuleManagerTest.java:209)
                at io.druid.metadata.SQLMetadataRuleManagerTest.cleanup(SQLMetadataRuleManagerTest.java:204)
                Caused by: org.skife.jdbi.v2.exceptions.UnableToExecuteStatementException:
                java.sql.SQLTransactionRollbackException: A lock could not be obtained due to a deadlock, cycle of locks
                and waiters is:
                Lock : ROW, SYSCOLUMNS, (5,12)
                Waiting XID : {237, X} , APP, DROP TABLE druidTestc0dd9b47c4df453daeba2aca4613783e_rules
                Granted XID : {217, S}
                Lock : TABLE, DRUIDTESTC0DD9B47C4DF453DAEBA2ACA4613783E_RULES, Tablelock
                Waiting XID : {217, IS} , APP, SELECT r.dataSource, r.payload FROM
                druidTestc0dd9b47c4df453daeba2aca4613783e_rules r INNER JOIN(SELECT dataSource, max(version) as version
                FROM druidTestc0dd9b47c4df453daeba2aca4613783e_rules GROUP BY dataSource) ds ON r.datasource =
                ds.datasource and r.version = ds.version
                Granted XID : {237, X}
                . The selected victim is XID : 237. [statement:"DROP TABLE
                druidTestc0dd9b47c4df453daeba2aca4613783e_rules", located:"DROP TABLE
                druidTestc0dd9b47c4df453daeba2aca4613783e_rules", rewritten:"DROP TABLE
                druidTestc0dd9b47c4df453daeba2aca4613783e_rules", arguments:{ positional:{}, named:{}, finder:[]}]
                at io.druid.metadata.SQLMetadataRuleManagerTest.dropTable(SQLMetadataRuleManagerTest.java:209)
                at io.druid.metadata.SQLMetadataRuleManagerTest.cleanup(SQLMetadataRuleManagerTest.java:204)
                Caused by: java.sql.SQLTransactionRollbackException:
                A lock could not be obtained due to a deadlock, cycle of locks and waiters is:
                Lock : ROW, SYSCOLUMNS, (5,12)
                Waiting XID : {237, X} , APP, DROP TABLE druidTestc0dd9b47c4df453daeba2aca4613783e_rules
                Granted XID : {217, S}
                Lock : TABLE, DRUIDTESTC0DD9B47C4DF453DAEBA2ACA4613783E_RULES, Tablelock
                Waiting XID : {217, IS} , APP, SELECT r.dataSource, r.payload FROM
                druidTestc0dd9b47c4df453daeba2aca4613783e_rules r INNER JOIN(SELECT dataSource, max(version) as version
                FROM druidTestc0dd9b47c4df453daeba2aca4613783e_rules GROUP BY dataSource) ds ON r.datasource =
                ds.datasource and r.version = ds.version
                Granted XID : {237, X}
                . The selected victim is XID : 237.
                at io.druid.metadata.SQLMetadataRuleManagerTest.dropTable(SQLMetadataRuleManagerTest.java:209)
                at io.druid.metadata.SQLMetadataRuleManagerTest.cleanup(SQLMetadataRuleManagerTest.java:204)
                Caused by: org.apache.derby.iapi.error.StandardException:
                A lock could not be obtained due to a deadlock, cycle of locks and waiters is:
                Lock : ROW, SYSCOLUMNS, (5,12)
                Waiting XID : {237, X} , APP, DROP TABLE druidTestc0dd9b47c4df453daeba2aca4613783e_rules
                Granted XID : {217, S}
                Lock : TABLE, DRUIDTESTC0DD9B47C4DF453DAEBA2ACA4613783E_RULES, Tablelock
                Waiting XID : {217, IS} , APP, SELECT r.dataSource, r.payload FROM
                druidTestc0dd9b47c4df453daeba2aca4613783e_rules r INNER JOIN(SELECT dataSource, max(version) as version
                FROM druidTestc0dd9b47c4df453daeba2aca4613783e_rules GROUP BY dataSource) ds ON r.datasource =
                ds.datasource and r.version = ds.version
                Granted XID : {237, X}
                . The selected victim is XID : 237.
                at io.druid.metadata.SQLMetadataRuleManagerTest.dropTable(SQLMetadataRuleManagerTest.java:209)
                at io.druid.metadata.SQLMetadataRuleManagerTest.cleanup(SQLMetadataRuleManagerTest.java:204)

            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.druid.metadata.SQLMetadataRuleManager.java</file>
            <file type="M">org.apache.druid.metadata.SQLMetadataSegmentManager.java</file>
            <file type="M">org.apache.druid.metadata.SQLMetadataSegmentManagerTest.java</file>
        </fixedFiles>
    </bug>
    <bug id="3409" opendate="2016-08-30 00:00:00" fixdate="2022-02-25 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>groupBy: Better spilling to disk in v2.
            </summary>
            <description>Currently when groupBy v2 spills to disk, the spilling is done by a processing thread while it
                holds a lock on a slice of the query's merge buffer. This can take a while (potentially many seconds).
                This is all well and good, but it means that in very short order, all other processing tasks for the
                same will be blocked on the same lock.
                This jams things up in general:

                Other tasks for the same query cannot make progress, since odds are that within a small number of rows,
                they'll all be blocked waiting to lock the currently-spilling slice.
                It's easy for these tasks to fill up the processing pool, meaning that even other unrelated queries
                cannot make progress.

                By default, spilling to disk is disabled, so this won't happen (the query will just fail when it runs
                out of memory instead of spilling). But it would still be good to do something better here.
                Some possibilities for parallelizing spilling:

                Have tasks avoid slices that are currently spilling. e.g. if there are 16 slices, and two are spilling,
                then instead of using hash % 16 to choose a slice, use hash % 14 and skip over the two spilling slices.
                Or consistentHash(hash, 14). This will lead to imperfect merging, but that's fine; the rows will be
                sorted so we can combine them later while iterating, with a combining merging iterable.
                When any slice fills up, opportunistically use all other processing threads involved in the query to
                spill all slices.
                Split each buffer slice in half, such that one half can be spilling to disk in processing thread X while
                the other half is used for aggregation in all other processing threads. This should improve things a bit
                but may lead us to the same problem after a round of spilling, if spilling is slower than aggregating
                (which it probably is).

                Some possibilities for maintaining QoS for other queries:

                Suspend processing tasks for a query when it's waiting for a spill to finish, so other queries can
                continue working. This would involve re-doing work for the suspended query (they'd have to lose their
                accumulated buffers and start over) but would improve latency for other queries.
                Isolate thread pools / hardware that allows spilling vs does not allow spilling.

            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.druid.query.groupby.epinephelinae.ConcurrentGrouper.java</file>
            <file type="M">org.apache.druid.query.groupby.epinephelinae.GroupByQueryEngineV2.java</file>
            <file type="M">org.apache.druid.query.groupby.epinephelinae.RowBasedGrouperHelper.java</file>
            <file type="M">org.apache.druid.query.groupby.epinephelinae.SpillingGrouper.java</file>
        </fixedFiles>
    </bug>
    <bug id="3772" opendate="2016-12-12 00:00:00" fixdate="2021-09-05 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>Race condition in KafkaLookupExtractorFactory.
            </summary>
            <description>#3403 (comment)
                FYI @gvsmirnov: just creating an issue in order not to forget about it
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
<file type="M">org.apache.druid.query.lookup.KafkaLookupExtractorFactory.java</file>
        </fixedFiles>
    </bug>
    <bug id="1210" opendate="2015-03-17 00:00:00" fixdate="2015-04-07 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>race condition in coordinator.
            </summary>
            <description>the coordniator node stopped responding (and so did the broker).
                The coordinator logs say 'Crazy race condition'.
                Druid 0.6.160
                logs of the coordinator:


                2015-03-17 05:35:19,607 INFO [Master-PeonExec--0] com.metamx.emitter.core.LoggingEmitter - Event
                [{"feed":"alerts","timestamp":"2015-03-17T05:35:19.588Z","service":"coordinator","host":"191.237.157.96:8080","severity":"component-failure","description":"Crazy
                race condition!
                server[/druidprod/base/loadQueue/158.85.8.18:8080]","data":{"class":"io.druid.server.coordinator.LoadQueuePeon"}}]


                2015-03-17 05:35:19,580 ERROR [Master-PeonExec--0] io.druid.server.coordinator.LoadQueuePeon - Crazy
                race condition! server[/druidprod/base/loadQueue/158.85.8.18:8080]:
                {class=io.druid.server.coordinator.LoadQueuePeon}


                2015-03-17 05:35:18,594 INFO [CoordinatorLeader-0] io.druid.curator.discovery.CuratorServiceAnnouncer -
                Unannouncing service[DruidNode{serviceName='coordinator', host='191.237.157.96:8080', port=8080}]


                2015-03-17 05:35:18,554 INFO [main-EventThread]
                org.apache.curator.framework.state.ConnectionStateManager - State change: RECONNECTED


                2015-03-17 05:35:18,553 INFO [main-SendThread(104.130.162.16:2181)] org.apache.zookeeper.ClientCnxn -
                Session establishment complete on server 104.130.162.16/104.130.162.16:2181, sessionid =
                0x24bde5fbb1f0007, negotiated timeout = 30000


                2015-03-17 05:35:18,541 INFO [main-SendThread(104.130.162.16:2181)] org.apache.zookeeper.ClientCnxn -
                Socket connection established to 104.130.162.16/104.130.162.16:2181, initiating session


                2015-03-17 05:35:18,532 INFO [main-SendThread(104.130.162.16:2181)] org.apache.zookeeper.ClientCnxn -
                Opening socket connection to server 104.130.162.16/104.130.162.16:2181. Will not attempt to authenticate
                using SASL (unknown error)


                2015-03-17 05:35:18,480 INFO [CoordinatorLeader-0] io.druid.server.coordinator.DruidCoordinator - I am
                no longer the leader...


                2015-03-17 05:35:18,472 INFO [main-EventThread]
                org.apache.curator.framework.state.ConnectionStateManager - State change: SUSPENDED


                2015-03-17 05:35:18,356 INFO [main-SendThread(104.130.16.203:2181)] org.apache.zookeeper.ClientCnxn -
                Client session timed out, have not heard from server in 20008ms for sessionid 0x24bde5fbb1f0007, closing
                socket connection and attempting reconnect


                2015-03-17 05:35:07,878 INFO [Coordinator-Exec--0] com.metamx.emitter.core.LoggingEmitter - Event
                [{"feed":"metrics","timestamp":"2015-03-17T05:35:07.878Z","service":"coordinator","host":"191.237.157.96:8080","metric":"coordinator/segment/count","value":2852,"user1":"d

            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.druid.server.coordinator.LoadQueuePeon.java</file>
        </fixedFiles>
    </bug>
    <bug id="3600" opendate="2016-10-20 00:00:00" fixdate="2017-07-30 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>Possible race condition while updating DataSource Metadata [Kafka Indexing Service].
            </summary>
            <description>We have Kafka indexing service running currently with no replication and task count equivalent
                to number of kafka partitions. One of the tasks failed with this exception -
                com.metamx.common.ISE: Transaction failure publishing segments, aborting
                at io.druid.indexing.kafka.KafkaIndexTask.run(KafkaIndexTask.java:524)
                ~[druid-kafka-indexing-service-0.9.3-1476736930-8d59341-1004.jar:0.9.3-1476736930-8d59341-1004]
                at
                io.druid.indexing.overlord.ThreadPoolTaskRunner$ThreadPoolTaskRunnerCallable.call(ThreadPoolTaskRunner.java:436)
                [druid-indexing-service-0.9.3-1476736930-8d59341-1004.jar:0.9.3-1476736930-8d59341-1004]
                at
                io.druid.indexing.overlord.ThreadPoolTaskRunner$ThreadPoolTaskRunnerCallable.call(ThreadPoolTaskRunner.java:408)
                [druid-indexing-service-0.9.3-1476736930-8d59341-1004.jar:0.9.3-1476736930-8d59341-1004]
                at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_60]
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_60]
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_60]
                at java.lang.Thread.run(Thread.java:745) [?:1.8.0_60]
                2016-10-19T08:23:19,572 INFO [task-runner-0-priority-0] io.druid.indexing.overlord.TaskRunnerUtils -
                Task [index_kafka_v2_metrics_cluster_966f1c1b7a001cf_menkjigj] status changed to [FAILED].
                2016-10-19T08:23:19,575 INFO [task-runner-0-priority-0]
                io.druid.indexing.worker.executor.ExecutorLifecycle - Task completed with status: {
                "id" : "index_kafka_v2_metrics_cluster_966f1c1b7a001cf_menkjigj",
                "status" : "FAILED",
                "duration" : 11637165
                }

                Earlier in the task log I see this -
                2016-10-19T08:23:19,300 INFO [task-runner-0-priority-0]
                io.druid.indexing.common.actions.RemoteTaskActionClient - Submitting action for
                task[index_kafka_v2_metrics_cluster_966f1c1b7a001cf_menkjigj] to overlord[/druid/indexer/v1/action]:
                SegmentInsertAction{segments=[DataSegment{size=357264367, shardSpec=NumberedShardSpec{partitionNum=36,
                partitions=0}, metrics=[], dimensions=[], version='2016-10-19T05:00:01.567Z', loadSpec={type=hdfs,
                path=}, interval=2016-10-19T05:00:00.000Z/2016-10-19T06:00:00.000Z, dataSource='v2_metrics_cluster',
                binaryVersion='9'}, DataSegment{size=328688502, shardSpec=NumberedShardSpec{partitionNum=5,
                partitions=0}, metrics=[], dimensions=[], version='2016-10-19T07:00:00.975Z', loadSpec={type=hdfs,
                path=}, interval=2016-10-19T07:00:00.000Z/2016-10-19T08:00:00.000Z, dataSource='v2_metrics_cluster',
                binaryVersion='9'}, DataSegment{size=55419587, shardSpec=NumberedShardSpec{partitionNum=3,
                partitions=0}, metrics=[], dimensions=[], version='2016-10-19T08:00:00.443Z', loadSpec={type=hdfs,
                path=}, interval=2016-10-19T08:00:00.000Z/2016-10-19T09:00:00.000Z, dataSource='v2_metrics_cluster',
                binaryVersion='9'}, DataSegment{size=396175234, shardSpec=NumberedShardSpec{partitionNum=1,
                partitions=0}, metrics=[], dimensions=[], version='2016-10-19T06:00:00.545Z', loadSpec={type=hdfs,
                path=}, interval=2016-10-19T06:00:00.000Z/2016-10-19T07:00:00.000Z, dataSource='v2_metrics_cluster',
                binaryVersion='9'}],
                startMetadata=KafkaDataSourceMetadata{kafkaPartitions=KafkaPartitions{topic='mx-cluster-production-metrics',
                partitionOffsetMap={8=158229668}}},
                endMetadata=KafkaDataSourceMetadata{kafkaPartitions=KafkaPartitions{topic='mx-cluster-production-metrics',
                partitionOffsetMap={8=202436580}}}}
                2016-10-19T08:23:19,318 INFO [task-runner-0-priority-0]
                com.metamx.http.client.pool.ChannelResourceFactory - Generating:
                2016-10-19T08:23:19,411 INFO [task-runner-0-priority-0]
                io.druid.segment.realtime.appenderator.FiniteAppenderatorDriver - Transaction failure while publishing
                segments, checking if someone else beat us to it.
                2016-10-19T08:23:19,414 INFO [task-runner-0-priority-0]
                io.druid.indexing.common.actions.RemoteTaskActionClient - Performing action for
                task[index_kafka_v2_metrics_cluster_966f1c1b7a001cf_menkjigj]:
                SegmentListUsedAction{dataSource='v2_metrics_cluster',
                intervals=[2016-10-19T05:00:00.000Z/2016-10-19T09:00:00.000Z]}
                2016-10-19T08:23:19,419 INFO [task-runner-0-priority-0]
                io.druid.indexing.common.actions.RemoteTaskActionClient - Submitting action for
                task[index_kafka_v2_metrics_cluster_966f1c1b7a001cf_menkjigj] to overlord[/druid/indexer/v1/action]:
                SegmentListUsedAction{dataSource='v2_metrics_cluster',
                intervals=[2016-10-19T05:00:00.000Z/2016-10-19T09:00:00.000Z]}
                2016-10-19T08:23:19,422 INFO [task-runner-0-priority-0]
                com.metamx.http.client.pool.ChannelResourceFactory - Generating:
                2016-10-19T08:23:19,513 WARN [task-runner-0-priority-0]
                io.druid.segment.realtime.appenderator.FiniteAppenderatorDriver - Our segments don't exist, giving up.
                2016-10-19T08:23:19,524 INFO [task-runner-0-priority-0]
                io.druid.segment.realtime.appenderator.AppenderatorImpl - Shutting down...

                Meanwhile this was in the overlord log -
                2016-10-19T08:23:19,303 INFO [qtp370356001-171] io.druid.indexing.common.actions.LocalTaskActionClient -
                Performing action for task[index_kafka_v2_metrics_cluster_a47237b6077ddb0_opnaifmo]:
                SegmentInsertAction{segments=[DataSegment{size=396764861, shardSpec=NumberedShardSpec{partitionNum=12,
                partitions=0}, metrics=[], dimensions=[], version='2016-10-19T06:00:00.545Z', loadSpec={type=hdfs,
                path=}, interval=2016-10-19T06:00:00.000Z/2016-10-19T07:00:00.000Z, dataSource='v2_metrics_cluster',
                binaryVersion='9'}, DataSegment{size=327245790, shardSpec=NumberedShardSpec{partitionNum=7,
                partitions=0}, metrics=[], dimensions=[], version='2016-10-19T07:00:00.975Z', loadSpec={type=hdfs,
                path=}, interval=2016-10-19T07:00:00.000Z/2016-10-19T08:00:00.000Z, dataSource='v2_metrics_cluster',
                binaryVersion='9'}, DataSegment{size=55834521, shardSpec=NumberedShardSpec{partitionNum=20,
                partitions=0}, metrics=[], dimensions=[], version='2016-10-19T08:00:00.443Z', loadSpec={type=hdfs,
                path=}, interval=2016-10-19T08:00:00.000Z/2016-10-19T09:00:00.000Z, dataSource='v2_metrics_cluster',
                binaryVersion='9'}, DataSegment{size=357963459, shardSpec=NumberedShardSpec{partitionNum=33,
                partitions=0}, metrics=[], dimensions=[], version='2016-10-19T05:00:01.567Z', loadSpec={type=hdfs,
                path=}, interval=2016-10-19T05:00:00.000Z/2016-10-19T06:00:00.000Z, dataSource='v2_metrics_cluster',
                binaryVersion='9'}],
                startMetadata=KafkaDataSourceMetadata{kafkaPartitions=KafkaPartitions{topic='mx-cluster-production-metrics',
                partitionOffsetMap={0=176009748}}},
                endMetadata=KafkaDataSourceMetadata{kafkaPartitions=KafkaPartitions{topic='mx-cluster-production-metrics',
                partitionOffsetMap={0=220414028}}}}
                2016-10-19T08:23:19,321 INFO [qtp370356001-152] io.druid.indexing.common.actions.LocalTaskActionClient -
                Performing action for task[index_kafka_v2_metrics_cluster_966f1c1b7a001cf_menkjigj]:
                SegmentInsertAction{segments=[DataSegment{size=357264367, shardSpec=NumberedShardSpec{partitionNum=36,
                partitions=0}, metrics=[], dimensions=[], version='2016-10-19T05:00:01.567Z', loadSpec={type=hdfs,
                path=}, interval=2016-10-19T05:00:00.000Z/2016-10-19T06:00:00.000Z, dataSource='v2_metrics_cluster',
                binaryVersion='9'}, DataSegment{size=328688502, shardSpec=NumberedShardSpec{partitionNum=5,
                partitions=0}, metrics=[], dimensions=[], version='2016-10-19T07:00:00.975Z', loadSpec={type=hdfs,
                path=}, interval=2016-10-19T07:00:00.000Z/2016-10-19T08:00:00.000Z, dataSource='v2_metrics_cluster',
                binaryVersion='9'}, DataSegment{size=55419587, shardSpec=NumberedShardSpec{partitionNum=3,
                partitions=0}, metrics=[], dimensions=[], version='2016-10-19T08:00:00.443Z', loadSpec={type=hdfs,
                path=}, interval=2016-10-19T08:00:00.000Z/2016-10-19T09:00:00.000Z, dataSource='v2_metrics_cluster',
                binaryVersion='9'}, DataSegment{size=396175234, shardSpec=NumberedShardSpec{partitionNum=1,
                partitions=0}, metrics=[], dimensions=[], version='2016-10-19T06:00:00.545Z', loadSpec={type=hdfs,
                path=}, interval=2016-10-19T06:00:00.000Z/2016-10-19T07:00:00.000Z, dataSource='v2_metrics_cluster',
                binaryVersion='9'}],
                startMetadata=KafkaDataSourceMetadata{kafkaPartitions=KafkaPartitions{topic='mx-cluster-production-metrics',
                partitionOffsetMap={8=158229668}}},
                endMetadata=KafkaDataSourceMetadata{kafkaPartitions=KafkaPartitions{topic='mx-cluster-production-metrics',
                partitionOffsetMap={8=202436580}}}}
                2016-10-19T08:23:19,327 INFO [qtp370356001-171] io.druid.metadata.IndexerSQLMetadataStorageCoordinator -
                Updated metadata
                from[KafkaDataSourceMetadata{kafkaPartitions=KafkaPartitions{topic='mx-cluster-production-metrics',
                partitionOffsetMap={0=176009748, 1=220256750, 2=175667526, 3=202804552, 4=158505772, 5=158445369,
                6=158347444, 7=202503474, 8=158229668, 9=158129698, 10=158076757, 11=157993918, 12=157748351,
                13=201366328, 14=157333088, 15=157251399, 16=157198909, 17=157139279, 18=157074283, 19=156992007,
                20=156273109, 21=155401237, 22=155088780, 23=154850255, 24=154639521, 25=154498798, 26=154345135,
                27=154142700, 28=154004983, 29=153935016}}}]
                to[KafkaDataSourceMetadata{kafkaPartitions=KafkaPartitions{topic='mx-cluster-production-metrics',
                partitionOffsetMap={0=220414028, 1=220256750, 2=175667526, 3=202804552, 4=158505772, 5=158445369,
                6=158347444, 7=202503474, 8=158229668, 9=158129698, 10=158076757, 11=157993918, 12=157748351,
                13=201366328, 14=157333088, 15=157251399, 16=157198909, 17=157139279, 18=157074283, 19=156992007,
                20=156273109, 21=155401237, 22=155088780, 23=154850255, 24=154639521, 25=154498798, 26=154345135,
                27=154142700, 28=154004983, 29=153935016}}}].
                2016-10-19T08:23:19,342 INFO [qtp370356001-171] io.druid.metadata.IndexerSQLMetadataStorageCoordinator -
                Published segment
                [v2_metrics_cluster_2016-10-19T06:00:00.000Z_2016-10-19T07:00:00.000Z_2016-10-19T06:00:00.545Z_12] to DB
                2016-10-19T08:23:19,357 INFO [qtp370356001-171] io.druid.metadata.IndexerSQLMetadataStorageCoordinator -
                Published segment
                [v2_metrics_cluster_2016-10-19T07:00:00.000Z_2016-10-19T08:00:00.000Z_2016-10-19T07:00:00.975Z_7] to DB
                2016-10-19T08:23:19,372 INFO [qtp370356001-171] io.druid.metadata.IndexerSQLMetadataStorageCoordinator -
                Published segment
                [v2_metrics_cluster_2016-10-19T08:00:00.000Z_2016-10-19T09:00:00.000Z_2016-10-19T08:00:00.443Z_20] to DB
                2016-10-19T08:23:19,387 INFO [qtp370356001-171] io.druid.metadata.IndexerSQLMetadataStorageCoordinator -
                Published segment
                [v2_metrics_cluster_2016-10-19T05:00:00.000Z_2016-10-19T06:00:00.000Z_2016-10-19T05:00:01.567Z_33] to DB
                2016-10-19T08:23:19,392 INFO [qtp370356001-152] io.druid.metadata.IndexerSQLMetadataStorageCoordinator -
                Not updating metadata, compare-and-swap failure.
                2016-10-19T08:23:19,426 INFO [qtp370356001-112] io.druid.indexing.common.actions.LocalTaskActionClient -
                Performing action for task[index_kafka_v2_metrics_cluster_966f1c1b7a001cf_menkjigj]:
                SegmentListUsedAction{dataSource='v2_metrics_cluster',
                intervals=[2016-10-19T05:00:00.000Z/2016-10-19T09:00:00.000Z]}
                2016-10-19T08:23:21,085 INFO [Curator-PathChildrenCache-0] io.druid.indexing.overlord.RemoteTaskRunner -
                Worker[] wrote FAILED status for task [index_kafka_v2_metrics_cluster_966f1c1b7a001cf_menkjigj] on
                [TaskLocation{host='', port=}]
                2016-10-19T08:23:21,085 INFO [Curator-PathChildrenCache-0] io.druid.indexing.overlord.RemoteTaskRunner -
                Worker[] completed task[index_kafka_v2_metrics_cluster_966f1c1b7a001cf_menkjigj] with status[FAILED]
                2016-10-19T08:23:21,085 INFO [Curator-PathChildrenCache-0] io.druid.indexing.overlord.TaskQueue -
                Received FAILED status for task: index_kafka_v2_metrics_cluster_966f1c1b7a001cf_menkjigj

                From the logs it seems like that two threads qtp370356001-171 and qtp370356001-152 wanted to update the
                datasource metadata. Initially, both threads read the commit_metadata_payload from DB and then
                qtp370356001-171 updated the metadata successfully whereas qtp370356001-152 failed as the old commit
                metadata it read earlier is not as same as commit metadata in DB now. All this happens in
                updateDataSourceMetadataWithHandle method of IndexerSQLMetadataStorageCoordinator.java class.
                One way to solve this issue is to synchronize access to updateDataSourceMetadataWithHandle method but
                better way would be to retry in case of such failures. However, retryTransaction does not retry in case
                of RuntimeException which would be thrown in this case here -
                https://github.com/druid-io/druid/blob/master/server/src/main/java/io/druid/metadata/IndexerSQLMetadataStorageCoordinator.java#L342
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.druid.metadata.IndexerSQLMetadataStorageCoordinator.java</file>
            <file type="M">org.apache.druid.metadata.RetryTransactionException.java</file>
            <file type="M">org.apache.druid.metadata.SQLMetadataConnector.java</file>
            <file type="M">org.apache.druid.metadata.IndexerSQLMetadataStorageCoordinatorTest.java</file>
        </fixedFiles>
    </bug>
    <bug id="5338" opendate="2018-02-02 00:00:00" fixdate="2018-02-14 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>Potential bug in HttpPostEmitter causing high CPU usage.
            </summary>
            <description>After upgrading to 0.11.0, some of the deployments are facing high CPU usage issue.
                After taking a thread dump, it was a suspicion that emitter thread might be causing it.
                Thread 65688: (state = IN_JAVA)
                - com.metamx.emitter.core.HttpPostEmitter.emitAndReturnBatch(com.metamx.emitter.core.Event) @bci=111,
                line=249 (Compiled frame; information may be imprecise)
                - com.metamx.emitter.core.HttpPostEmitter.emit(com.metamx.emitter.core.Event) @bci=2, line=214 (Compiled
                frame)
                - com.metamx.emitter.core.ComposingEmitter.emit(com.metamx.emitter.core.Event) @bci=31, line=57
                (Compiled frame)
                - com.metamx.emitter.service.ServiceEmitter.emit(com.metamx.emitter.core.Event) @bci=5, line=72
                (Compiled frame)
                - com.metamx.emitter.service.ServiceEmitter.emit(com.metamx.emitter.service.ServiceEventBuilder) @bci=9,
                line=77 (Compiled frame)

                To verify the issue, we added an executor in DruidCoordinator class which just keeps on emitting events
                in while(true) loop like this -
                while (true) {
                emitter.emit(ServiceMetricEvent.builder().setDimension("dataSource", "try").build("test", 10));
                }

                We found that after some time, batch.tryAddEvents method always return false and the reference in
                concurrentBatch never changes and the while(true) loop just keeps on spinning without sending anything
                or creating new batch.
                Still not sure why it is happening as its not happening for all deployments, might be some concurrency
                issue.
                @leventov
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.druid.java.util.emitter.core.Batch.java</file>
            <file type="M">org.apache.druid.java.util.emitter.core.HttpPostEmitter.java</file>
        </fixedFiles>
    </bug>
    <bug id="1360" opendate="2015-05-13 00:00:00" fixdate="2020-01-10 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>Race condition in autoscaling terminates nodes that were just assigned tasks.
            </summary>
            <description>It is possible for the indexing service auto-scaling to terminate nodes that have just been
                assigned a task, but that have not updated the task status to running yet.
                Autoscaling currently only relies on task status provided by the workers, but does not take into account
                tasks that are known to be running or about to run on a given worker. If you are unlucky and autoscaling
                checks right after the task has been assigned, but right before the worker announces the task status,
                and the worker is not running any other tasks, your tasks will get killed.
                Running task replicas may not help in this situations. If the indexing service was recently updated and
                spawned new middle-managers that aren't running anything yet, it is possible for multiple nodes to be
                killed that were about to run tasks.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.druid.query.dimension.ExpressionDimensionSpec.java</file>
            <file type="M">org.apache.druid.query.groupby.GroupByQueryRunnerGenericTest.java</file>
            <file type="M">org.apache.druid.indexing.overlord.RemoteTaskRunner.java</file>
<file type="M">org.apache.druid.indexing.overlord.autoscaling.ResourceManagementScheduler.java</file>
<file type="M">org.apache.druid.indexing.overlord.autoscaling.ResourceManagementStrategy.java</file>
<file type="M">org.apache.druid.indexing.overlord.autoscaling.SimpleResourceManagementStrategy.java</file>
            <file type="M">org.apache.druid.indexing.overlord.RemoteTaskRunnerTest.java</file>
<file type="M">org.apache.druid.indexing.overlord.autoscaling.SimpleResourceManagementStrategyTest.java</file>
        </fixedFiles>
    </bug>
    <bug id="582" opendate="2014-06-05 00:00:00" fixdate="2018-07-06 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>Race in jar upload during hadoop indexing.
            </summary>
            <description>Hadoop indexing (through io.druid.indexer.JobHelper) does something like:
                if (!fs.exists(jar)) {
                upload jar using fs.create() to workingPath/classpath/jarName.jar
                }

                The same jar path is used for all druid jobs running on the same cluster. If two jobs start at the same
                time, both can find that the file doesn't exist, and the second one can overwrite the first one. This
                will cause the AM to fail because the hadoop AM is pretty particular about the mtime of its jar files.
                Possible solutions: (a) Better locking, or (b) have each job upload its jars to a separate directory.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.druid.indexer.DetermineHashedPartitionsJob.java</file>
            <file type="M">org.apache.druid.indexer.DeterminePartitionsJob.java</file>
            <file type="M">org.apache.druid.indexer.IndexGeneratorJob.java</file>
            <file type="M">org.apache.druid.indexer.JobHelper.java</file>
            <file type="M">org.apache.druid.indexer.updater.HadoopConverterJob.java</file>
            <file type="M">org.apache.druid.indexer.HdfsClasspathSetupTest.java</file>
        </fixedFiles>
    </bug>
    <bug id="1014" opendate="2015-01-06 00:00:00" fixdate="2022-02-24 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>real time node lead to wrong result(empty result) for the request with regexp filter when real time
                node is ingesting kafka msg.
            </summary>
            <description>increase the kafka message send speed gradually, real time query result will be wrong for the
                request with regex filter.
                request is:
                "filter": {
                "type": "regex",
                "dimension": "sub5",
                "pattern": "B"
                }
                ...
                "orderBy": {
                "type": "default",
                "columns": [
                {
                "dimension": "clicks",
                "direction": "DESCENDING"
                }
                ],
                "limit": 3
                }

                result is:
                [ {
                "version" : "v1",
                "timestamp" : "2015-01-06T05:37:00.000Z",
                "event" : {
                "clicks" : 28
                }
                }, {
                "version" : "v1",
                "timestamp" : "2015-01-06T05:37:00.000Z",
                "event" : {
                "clicks" : 3,
                "sub5" : "BB1"
                }
                }
                ...
                ]
                Note, I did not send empty value for the dimension I queried. But result show one null value for the
                sub5 dimension.
                query later, the empty result disappear. It seems thread race condition leads to this error.
                If not solve the problem, druid will not be really real time system.
                both tested on 0.6.121.1 and 0.6.160
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.druid.timeline.VersionedIntervalTimeline.java</file>
            <file type="M">org.apache.druid.query.extraction.DimExtractionFn.java</file>
            <file type="M">org.apache.druid.query.extraction.JavascriptDimExtractionFn.java</file>
            <file type="M">org.apache.druid.query.extraction.MatchingDimExtractionFn.java</file>
            <file type="M">org.apache.druid.query.filter.BitmapIndexSelector.java</file>
            <file type="M">org.apache.druid.query.timeseries.TimeseriesQuery.java</file>
            <file type="M">org.apache.druid.query.timeseries.TimeseriesQueryEngine.java</file>
            <file type="M">org.apache.druid.query.topn.DimExtractionTopNAlgorithm.java</file>
            <file type="M">org.apache.druid.query.topn.TopNQueryQueryToolChest.java</file>
            <file type="M">org.apache.druid.query.topn.TopNQueryRunnerFactory.java</file>
            <file type="M">org.apache.druid.segment.ColumnSelectorBitmapIndexSelector.java</file>
            <file type="M">org.apache.druid.segment.NullDimensionSelector.java</file>
            <file type="M">org.apache.druid.segment.QueryableIndexStorageAdapter.java</file>
            <file type="M">org.apache.druid.segment.filter.SelectorFilter.java</file>
            <file type="M">org.apache.druid.segment.incremental.IncrementalIndex.java</file>
            <file type="M">org.apache.druid.segment.incremental.IncrementalIndexStorageAdapter.java</file>
<file type="M">org.apache.druid.query.extraction.extraction.MatchingDimExtractionFnTest.java</file>
            <file type="M">org.apache.druid.query.groupby.GroupByQueryRunnerTest.java</file>
            <file type="M">org.apache.druid.query.timeseries.TimeseriesQueryRunnerTest.java</file>
            <file type="M">org.apache.druid.query.topn.TopNQueryRunnerTest.java</file>
            <file type="M">org.apache.druid.segment.NullDimensionSelectorTest.java</file>
            <file type="M">org.apache.druid.segment.SchemalessTestFull.java</file>
            <file type="M">org.apache.druid.server.initialization.ServerConfig.java</file>
            <file type="M">org.apache.druid.server.metrics.NoopServiceEmitter.java</file>
            <file type="M">org.apache.druid.guice.RealtimeModule.java</file>
        </fixedFiles>
    </bug>
    <bug id="4226" opendate="2017-04-28 00:00:00" fixdate="2017-10-13 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>druid-lookups-cached-global race condition when realtime node starts.
            </summary>
            <description>When we spawn a new realtime node from tranquility (from storm), it takes a bit to load all the
                lookups (aprox 36 seconds), and during that time, the node already published the segment and queries
                using lookups obviously fail
                This can be seen in the log of the realtime task:
                2017-04-28T08:00:25,880 ERROR [timeseries_stats_[2017-04-28T08:00:00.000Z/2017-04-28T08:00:25.000Z]]
                io.druid.query.ChainedExecutionQueryRunner - Exception with one of the sequences!
                java.lang.NullPointerException: Lookup [advertiser_account_manager_id] not found
                at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:253) ~[guava-16.0.1.jar:?]
                at
                io.druid.query.lookup.RegisteredLookupExtractionFn.ensureDelegate(RegisteredLookupExtractionFn.java:143)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at io.druid.query.lookup.RegisteredLookupExtractionFn.apply(RegisteredLookupExtractionFn.java:115)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at io.druid.segment.filter.InFilter$2$1.apply(InFilter.java:110) ~[druid-processing-0.9.2.jar:0.9.2]
                at io.druid.segment.filter.InFilter$2$1.apply(InFilter.java:106) ~[druid-processing-0.9.2.jar:0.9.2]
                at io.druid.segment.StringDimensionIndexer.makeIndexingValueMatcher(StringDimensionIndexer.java:532)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at
                io.druid.segment.incremental.IncrementalIndexStorageAdapter$CursorAndEntryHolderValueMatcherFactory.makeValueMatcher(IncrementalIndexStorageAdapter.java:647)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at io.druid.segment.filter.InFilter.makeMatcher(InFilter.java:88) ~[druid-processing-0.9.2.jar:0.9.2]
                at io.druid.segment.filter.AndFilter.makeMatcher(AndFilter.java:75) ~[druid-processing-0.9.2.jar:0.9.2]
                at
                io.druid.segment.incremental.IncrementalIndexStorageAdapter.makeFilterMatcher(IncrementalIndexStorageAdapter.java:558)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at
                io.druid.segment.incremental.IncrementalIndexStorageAdapter.access$000(IncrementalIndexStorageAdapter.java:67)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at io.druid.segment.incremental.IncrementalIndexStorageAdapter$1$1.
                #init>
                (IncrementalIndexStorageAdapter.java:243) ~[druid-processing-0.9.2.jar:0.9.2]
                at
                io.druid.segment.incremental.IncrementalIndexStorageAdapter$1.apply(IncrementalIndexStorageAdapter.java:241)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at
                io.druid.segment.incremental.IncrementalIndexStorageAdapter$1.apply(IncrementalIndexStorageAdapter.java:233)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at com.metamx.common.guava.MappingAccumulator.accumulate(MappingAccumulator.java:39)
                ~[java-util-0.27.10.jar:?]
                at com.metamx.common.guava.BaseSequence.accumulate(BaseSequence.java:67) ~[java-util-0.27.10.jar:?]
                at com.metamx.common.guava.MappedSequence.accumulate(MappedSequence.java:40) ~[java-util-0.27.10.jar:?]
                at com.metamx.common.guava.MappedSequence.accumulate(MappedSequence.java:40) ~[java-util-0.27.10.jar:?]
                at com.metamx.common.guava.FilteredSequence.accumulate(FilteredSequence.java:42)
                ~[java-util-0.27.10.jar:?]
                at com.metamx.common.guava.ResourceClosingSequence.accumulate(ResourceClosingSequence.java:38)
                ~[java-util-0.27.10.jar:?]
                at com.metamx.common.guava.Sequences.toList(Sequences.java:113) ~[java-util-0.27.10.jar:?]
                at io.druid.query.ChainedExecutionQueryRunner$1$1$1.call(ChainedExecutionQueryRunner.java:129)
                [druid-processing-0.9.2.jar:0.9.2]
                at io.druid.query.ChainedExecutionQueryRunner$1$1$1.call(ChainedExecutionQueryRunner.java:119)
                [druid-processing-0.9.2.jar:0.9.2]
                at java.util.concurrent.FutureTask.run(Unknown Source) [?:1.8.0_65]
                at
                com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)
                [guava-16.0.1.jar:?]
                at java.util.concurrent.AbstractExecutorService.submit(Unknown Source) [?:1.8.0_65]
                at
                com.google.common.util.concurrent.AbstractListeningExecutorService.submit(AbstractListeningExecutorService.java:58)
                [guava-16.0.1.jar:?]
                at io.druid.query.ChainedExecutionQueryRunner$1$1.apply(ChainedExecutionQueryRunner.java:117)
                [druid-processing-0.9.2.jar:0.9.2]
                at io.druid.query.ChainedExecutionQueryRunner$1$1.apply(ChainedExecutionQueryRunner.java:109)
                [druid-processing-0.9.2.jar:0.9.2]
                at com.google.common.collect.Iterators$8.transform(Iterators.java:794) [guava-16.0.1.jar:?]
                at com.google.common.collect.TransformedIterator.next(TransformedIterator.java:48) [guava-16.0.1.jar:?]
                at com.google.common.collect.Iterators.addAll(Iterators.java:357) [guava-16.0.1.jar:?]
                at com.google.common.collect.Lists.newArrayList(Lists.java:147) [guava-16.0.1.jar:?]
                at com.google.common.collect.Lists.newArrayList(Lists.java:129) [guava-16.0.1.jar:?]
                at io.druid.query.ChainedExecutionQueryRunner$1.make(ChainedExecutionQueryRunner.java:105)
                [druid-processing-0.9.2.jar:0.9.2]
                at com.metamx.common.guava.BaseSequence.accumulate(BaseSequence.java:64) [java-util-0.27.10.jar:?]
                at io.druid.query.MetricsEmittingQueryRunner$1.accumulate(MetricsEmittingQueryRunner.java:104)
                [druid-processing-0.9.2.jar:0.9.2]
                at io.druid.query.MetricsEmittingQueryRunner$1.accumulate(MetricsEmittingQueryRunner.java:104)
                [druid-processing-0.9.2.jar:0.9.2]
                at io.druid.query.CPUTimeMetricQueryRunner$1.accumulate(CPUTimeMetricQueryRunner.java:81)
                [druid-processing-0.9.2.jar:0.9.2]
                at com.metamx.common.guava.Sequences$1.accumulate(Sequences.java:90) [java-util-0.27.10.jar:?]
                at io.druid.query.spec.SpecificSegmentQueryRunner$2$1.call(SpecificSegmentQueryRunner.java:87)
                [druid-processing-0.9.2.jar:0.9.2]
                at io.druid.query.spec.SpecificSegmentQueryRunner.doNamed(SpecificSegmentQueryRunner.java:171)
                [druid-processing-0.9.2.jar:0.9.2]
                at io.druid.query.spec.SpecificSegmentQueryRunner.access$400(SpecificSegmentQueryRunner.java:41)
                [druid-processing-0.9.2.jar:0.9.2]
                at io.druid.query.spec.SpecificSegmentQueryRunner$2.doItNamed(SpecificSegmentQueryRunner.java:162)
                [druid-processing-0.9.2.jar:0.9.2]
                at io.druid.query.spec.SpecificSegmentQueryRunner$2.accumulate(SpecificSegmentQueryRunner.java:80)
                [druid-processing-0.9.2.jar:0.9.2]
                at com.metamx.common.guava.Sequences.toList(Sequences.java:113) [java-util-0.27.10.jar:?]
                at io.druid.query.ChainedExecutionQueryRunner$1$1$1.call(ChainedExecutionQueryRunner.java:129)
                [druid-processing-0.9.2.jar:0.9.2]
                at io.druid.query.ChainedExecutionQueryRunner$1$1$1.call(ChainedExecutionQueryRunner.java:119)
                [druid-processing-0.9.2.jar:0.9.2]
                at java.util.concurrent.FutureTask.run(Unknown Source) [?:1.8.0_65]
                at io.druid.query.PrioritizedListenableFutureTask.run(PrioritizedExecutorService.java:271)
                [druid-processing-0.9.2.jar:0.9.2]
                at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [?:1.8.0_65]
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [?:1.8.0_65]
                at java.lang.Thread.run(Unknown Source) [?:1.8.0_65]

                later on it loads successfully (but it's 36 seconds too late :P)
                2017-04-28T08:00:36,210 INFO [NamespaceExtractionCacheManager-0]
                io.druid.server.lookup.namespace.JDBCExtractionNamespaceCacheFactory - Finished loading 1079 values for
                namespace[namespace-factory-JDBCExtractionNamespace = { connectorConfig = {
                DbConnectorConfig{createTables=false, connectURI='jdbc:mysql:/*********', user='****',
                passwordProvider=io.druid.metadata.DefaultPasswordProvider} }, table =
                adserver_sf_backoffice_user_advertiser, keyColumn = id, valueColumn = acc_manager_id, tsColumn = null,
                pollPeriod = PT5M}-cb7e90b1-cf67-495c-9ce7-0473aeb68c79]

            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.druid.indexing.common.TestUtils.java</file>
            <file type="M">org.apache.druid.query.dimension.DimensionSpec.java</file>
            <file type="M">org.apache.druid.query.extraction.ExtractionFn.java</file>
            <file type="M">org.apache.druid.query.lookup.LookupConfig.java</file>
            <file type="M">org.apache.druid.query.expression.TestExprMacroTable.java</file>
            <file type="M">org.apache.druid.query.lookup.LookupConfigTest.java</file>
            <file type="M">org.apache.druid.query.dimension.LookupDimensionSpec.java</file>
            <file type="M">org.apache.druid.query.expression.LookupExprMacro.java</file>
            <file type="M">org.apache.druid.query.lookup.LookupModule.java</file>
            <file type="M">org.apache.druid.query.lookup.LookupReferencesManager.java</file>
            <file type="M">org.apache.druid.query.lookup.RegisteredLookupExtractionFn.java</file>
            <file type="M">org.apache.druid.server.http.LookupCoordinatorResource.java</file>
            <file type="M">org.apache.druid.query.dimension.LookupDimensionSpecTest.java</file>
            <file type="M">org.apache.druid.query.expression.ExprMacroTest.java</file>
            <file type="M">org.apache.druid.query.expression.TestExpressionMacroTable.java</file>
            <file type="M">org.apache.druid.query.lookup.LookupReferencesManagerTest.java</file>
            <file type="M">org.apache.druid.query.lookup.RegisteredLookupExtractionFnTest.java</file>
            <file type="M">org.apache.druid.server.http.LookupCoordinatorResourceTest.java</file>
            <file type="M">org.apache.druid.sql.calcite.util.CalciteTests.java</file>
        </fixedFiles>
    </bug>
    <bug id="812" opendate="2014-10-27 00:00:00" fixdate="2014-11-15 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>RealtimePlumber: Race between server view callback and merge/push thread.
            </summary>
            <description>If you have redundant realtime tasks, a historical node can load a segment pushed by task A
                while task B is still performing final merging of the segment.
                In this case, task B's server view callback will see the loaded segment and call abandonSegment. This
                will unannounce the segment, delete the local files, and release locks in the indexing service (if you
                are running inside the indexing service). This will cause havoc in the merge/push thread that is still
                trying to merge the segment.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.druid.segment.realtime.plumber.RealtimePlumber.java</file>
        </fixedFiles>
    </bug>
    <bug id="8622" opendate="2019-10-02 00:00:00" fixdate="2019-10-18 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>Proposal: concept of supervisor type task slots for resolving parallel task deadlocks.
            </summary>
            <description>Motivation
                ParallelIndexSupervisorTask is a supervisor style task that delegates work to one or more spawned
                subtasks. Since both, supervisor task and subtasks, are using same task slot pool, there is possibility
                of a deadlock.
                For example, say a druid cluster has 4 task slots and 4 ParallelIndexSupervisorTask tasks are submitted
                simultaneously and started running on available 4 task slots. Subtasks spawned by the supervisor tasks
                would never be able to run and supervisor tasks would keep on waiting.
                It is also discussed in #8061 (comment) .
                Proposed changes
                Add a method boolean Task.isSupervisor() to Task interface which returns true if the task is a
                supervisor task that spawns subtasks to delegate work. ParallelIndexSupervisorTask would return true
                while all other current task impls would return false.
                Add a druid.worker.supervisorCapacity configuration on middleManagers, which designates available slots
                to run supervisor tasks. This config is similar to druid.worker.capacity which designates available
                non-supervisor task slots.
                [Http]RemoteTaskRunner code would be updated to recognize that supervisor tasks consume slot from
                supervisorCapacity and not capacity .
                Rationale
                A potential alternative is to use #7066 to send all supervisor tasks to a dedicated set of
                middleManagers which only get supervisor tasks.
                Operational impact
                User of ParallelIndexSupervisorTask would need to set property druid.worker.supervisorCapacity on
                middleManagers.
                Test plan (optional)
                will run it on a staging cluster.
                Future work (optional)
                for reliability: supervisor tasks could be treated further specially be imposing a "always restartable"
                restriction on them and also not failing them if middleManager running them crashed .
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.druid.java.util.emitter.EmittingLogger.java</file>
            <file type="M">org.apache.druid.indexing.overlord.TaskRunnerWorkItem.java</file>
<file type="M">org.apache.druid.indexing.overlord.config.HttpRemoteTaskRunnerConfig.java</file>
<file type="M">org.apache.druid.indexing.overlord.hrtr.HttpRemoteTaskRunner.java</file>
<file type="M">org.apache.druid.indexing.overlord.hrtr.HttpRemoteTaskRunnerResource.java</file>
            <file type="M">org.apache.druid.indexing.overlord.hrtr.WorkerHolder.java</file>
<file type="M">org.apache.druid.indexing.overlord.hrtr.HttpRemoteTaskRunnerTest.java</file>
        </fixedFiles>
    </bug>
    <bug id="6020" opendate="2018-07-18 00:00:00" fixdate="2018-09-27 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>Race in taskMaster when the overlord becomes the leader.
            </summary>
            <description>TaskMaster has the interfaces to return the variables (taskRunner, taskQueue, etc) which are
                initialized only when the overlord becomes the leader. The code of the interfaces is like this:
                public Optional
                #TaskRunner>
                getTaskRunner()
                {
                if (overlordLeaderSelector.isLeader()) {
                return Optional.of(taskRunner);
                } else {
                return Optional.absent();
                }
                }
                However, taskRunner is initialized in DruidLeaderSelector.Listener.becomeLeader() which is called after
                the overlord becomes the leader, and thus Optional.of() throws an NPE. The full stack trace is as
                follows:
                java.lang.NullPointerException
                at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:213) ~[guava-16.0.1.jar:?]
                at com.google.common.base.Optional.of(Optional.java:85) ~[guava-16.0.1.jar:?]
                at io.druid.indexing.overlord.TaskMaster.getTaskRunner(TaskMaster.java:214)
                ~[druid-indexing-service-0.12.1-iap8.jar:0.12.1-iap8]
                at io.druid.indexing.overlord.http.OverlordResource.getWorkers(OverlordResource.java:810)
                ~[druid-indexing-service-0.12.1-iap8.jar:0.12.1-iap8]
                at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_163]
                at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_163]
                at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_163]
                at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_163]
                at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
                ~[jersey-server-1.19.3.jar:1.19.3]
                at
                com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)
                ~[jersey-server-1.19.3.jar:1.19.3]
                at
                com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
                ~[jersey-server-1.19.3.jar:1.19.3]
                at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)
                ~[jersey-server-1.19.3.jar:1.19.3]
                at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
                ~[jersey-server-1.19.3.jar:1.19.3]
                at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
                ~[jersey-server-1.19.3.jar:1.19.3]
                at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
                ~[jersey-server-1.19.3.jar:1.19.3]
                at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
                ~[jersey-server-1.19.3.jar:1.19.3]
                at
                com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)
                ~[jersey-server-1.19.3.jar:1.19.3]
                at
                com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)
                [jersey-server-1.19.3.jar:1.19.3]
                at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)
                [jersey-server-1.19.3.jar:1.19.3]
                at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)
                [jersey-server-1.19.3.jar:1.19.3]
                at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)
                [jersey-servlet-1.19.3.jar:1.19.3]
                at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)
                [jersey-servlet-1.19.3.jar:1.19.3]
                at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)
                [jersey-servlet-1.19.3.jar:1.19.3]
                at javax.servlet.http.HttpServlet.service(HttpServlet.java:790) [javax.servlet-api-3.1.0.jar:3.1.0]
                at com.google.inject.servlet.ServletDefinition.doServiceImpl(ServletDefinition.java:286)
                [guice-servlet-4.1.0.jar:?]
                at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:276)
                [guice-servlet-4.1.0.jar:?]
                at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:181)
                [guice-servlet-4.1.0.jar:?]
                at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91)
                [guice-servlet-4.1.0.jar:?]
                at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:85)
                [guice-servlet-4.1.0.jar:?]
                at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:120)
                [guice-servlet-4.1.0.jar:?]
                at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:135) [guice-servlet-4.1.0.jar:?]
                at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
                [jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]
                at io.druid.server.http.RedirectFilter.doFilter(RedirectFilter.java:72)
                [druid-server-0.12.1-iap8.jar:0.12.1-iap8]
                at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
                [jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]
                at
                io.druid.server.security.PreResponseAuthorizationCheckFilter.doFilter(PreResponseAuthorizationCheckFilter.java:84)
                [druid-server-0.12.1-iap8.jar:0.12.1-iap8]
                at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
                [jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]
                at io.druid.server.security.AllowOptionsResourceFilter.doFilter(AllowOptionsResourceFilter.java:76)
                [druid-server-0.12.1-iap8.jar:0.12.1-iap8]
                at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
                [jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]
                at io.imply.security.ImplyAuthenticator$ImplyAuthenticationFilter.doFilter(ImplyAuthenticator.java:149)
                [imply-druid-security-0.12.0.3.jar:?]
                at io.druid.server.security.AuthenticationWrappingFilter.doFilter(AuthenticationWrappingFilter.java:60)
                [druid-server-0.12.1-iap8.jar:0.12.1-iap8]
                at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
                [jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]
                at
                io.druid.security.basic.authentication.BasicHTTPAuthenticator$BasicHTTPAuthenticationFilter.doFilter(BasicHTTPAuthenticator.java:162)
                [druid-basic-security-0.12.1-iap8.jar:0.12.1-iap8]
                at io.druid.server.security.AuthenticationWrappingFilter.doFilter(AuthenticationWrappingFilter.java:60)
                [druid-server-0.12.1-iap8.jar:0.12.1-iap8]
                at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
                [jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]
                at io.druid.server.security.SecuritySanityCheckFilter.doFilter(SecuritySanityCheckFilter.java:86)
                [druid-server-0.12.1-iap8.jar:0.12.1-iap8]
                at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
                [jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]
                at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)
                [jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]
                at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:224)
                [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]
                at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)
                [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]
                at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)
                [jetty-servlet-9.3.19.v20170502.jar:9.3.19.v20170502]
                at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)
                [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]
                at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)
                [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]
                at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
                [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]
                at org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:493)
                [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]
                at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52)
                [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]
                at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
                [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]
                at org.eclipse.jetty.server.Server.handle(Server.java:534)
                [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]
                at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)
                [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]
                at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
                [jetty-server-9.3.19.v20170502.jar:9.3.19.v20170502]
                at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
                [jetty-io-9.3.19.v20170502.jar:9.3.19.v20170502]
                at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)
                [jetty-io-9.3.19.v20170502.jar:9.3.19.v20170502]
                at org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:240)
                [jetty-io-9.3.19.v20170502.jar:9.3.19.v20170502]
                at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
                [jetty-io-9.3.19.v20170502.jar:9.3.19.v20170502]
                at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)
                [jetty-io-9.3.19.v20170502.jar:9.3.19.v20170502]
                at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
                [jetty-io-9.3.19.v20170502.jar:9.3.19.v20170502]
                at
                org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
                [jetty-util-9.3.19.v20170502.jar:9.3.19.v20170502]
                at
                org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
                [jetty-util-9.3.19.v20170502.jar:9.3.19.v20170502]
                at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
                [jetty-util-9.3.19.v20170502.jar:9.3.19.v20170502]
                at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
                [jetty-util-9.3.19.v20170502.jar:9.3.19.v20170502]
                at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
                [jetty-util-9.3.19.v20170502.jar:9.3.19.v20170502]
                at java.lang.Thread.run(Thread.java:748) [?:1.8.0_163]

            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.druid.indexing.overlord.TaskMaster.java</file>
            <file type="M">org.apache.druid.server.coordinator.DruidCoordinator.java</file>
        </fixedFiles>
    </bug>
    <bug id="3063" opendate="2016-06-02 00:00:00" fixdate="2016-06-03 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>Unloadable lookups can tie up all query threads.
            </summary>
            <description>In 0.9.1-rc1, if the coordinator post to /druid/listen/v1 involves a lookup that can't be
                loaded, that will cause the request to block until the lookup load times out. After 10s the coordinator
                will time out this request. Later on it will issue a new one, but if the original request's thread is
                still active (which it totally could be), the new request's thread will just block while trying to
                acquire the lock held by the first request thread. Fast-forward a few minutes, and all jetty threads are
                blocked on lookup-related stuff and no queries can be made.
                Possible solutions:

                Make /druid/listen/v1 requests nonblocking, by having them insert into a queue rather than actually do
                the work. The work would be done by another thread. The request thread would have to fail when the queue
                is full rather than block on full.
                Or if the requests really need to be blocking, limit the number of concurrent /druid/listen/v1 requests
                (by failing any requests if there are more than X in flight). X = 1 seems reasonable to me.

            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
<file type="M">org.apache.druid.query.lookup.NamespaceLookupExtractorFactory.java</file>
<file type="M">org.apache.druid.query.lookup.NamespaceLookupExtractorFactoryTest.java</file>
<file type="M">org.apache.druid.query.lookup.KafkaLookupExtractorFactory.java</file>
<file type="M">org.apache.druid.query.lookup.NamespaceLookupExtractorFactory.java</file>
<file type="M">org.apache.druid.server.lookup.namespace.cache.NamespaceExtractionCacheManager.java</file>
<file type="M">org.apache.druid.server.lookup.namespace.cache.OffHeapNamespaceExtractionCacheManager.java</file>
<file type="M">org.apache.druid.server.lookup.namespace.cache.OffHeapNamespaceExtractionCacheManagerTest.java</file>
            <file type="M">org.apache.druid.query.lookup.LookupReferencesManager.java</file>
            <file type="M">org.apache.druid.query.lookup.LookupModule.java</file>
            <file type="M">org.apache.druid.query.lookup.LookupReferencesManagerTest.java</file>
        </fixedFiles>
    </bug>
    <bug id="6867" opendate="2019-01-16 00:00:00" fixdate="2019-01-21 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>deadlock found in DruidStatement.
            </summary>
            <description>We found deadlock in DruidStatement and DruidConnection, this will cause exhaust connections.
                Found one Java-level deadlock:
                =============================
                "DruidMeta@4e50ae56-ScheduledExecutor":
                waiting to lock monitor 0x00007fa5084802b8 (object 0x00000000d6389830, a java.lang.Object),
                which is held by "qtp1871838170-210"
                "qtp1871838170-210":
                waiting to lock monitor 0x00007fa5084916b8 (object 0x00000000d603e4b8, a java.util.HashMap),
                which is held by "DruidMeta@4e50ae56-ScheduledExecutor"

                Java stack information for the threads listed above:
                ===================================================
                "DruidMeta@4e50ae56-ScheduledExecutor":
                at io.druid.sql.avatica.DruidStatement.close(DruidStatement.java:310)
                - waiting to lock#0x00000000d6389830> (a java.lang.Object)
                at io.druid.sql.avatica.DruidConnection.close(DruidConnection.java:150)
                - locked#0x00000000d603e4b8> (a java.util.HashMap)
                at io.druid.sql.avatica.DruidMeta.closeConnection(DruidMeta.java:120)
                at io.druid.sql.avatica.DruidMeta.lambda$getDruidConnection$0(DruidMeta.java:588)
                at io.druid.sql.avatica.DruidMeta$$Lambda$90/10531789.run(Unknown Source)
                at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
                at java.util.concurrent.FutureTask.run(FutureTask.java:266)
                at
                java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
                at
                java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
                at java.lang.Thread.run(Thread.java:748)
                "qtp1871838170-210":
                at io.druid.sql.avatica.DruidConnection.lambda$createStatement$0(DruidConnection.java:107)
                - waiting to lock#0x00000000d603e4b8> (a java.util.HashMap)
                at io.druid.sql.avatica.DruidConnection$$Lambda$91/1439788572.run(Unknown Source)
                at io.druid.sql.avatica.DruidStatement.close(DruidStatement.java:348)
                - locked#0x00000000d6389830> (a java.lang.Object)
                at io.druid.sql.avatica.DruidStatement.execute(DruidStatement.java:213)
                - locked#0x00000000d6389830> (a java.lang.Object)
                at io.druid.sql.avatica.DruidMeta.prepareAndExecute(DruidMeta.java:187)
                at org.apache.calcite.avatica.remote.LocalService.apply(LocalService.java:206)
                at org.apache.calcite.avatica.remote.Service$PrepareAndExecuteRequest.accept(Service.java:928)
                at org.apache.calcite.avatica.remote.Service$PrepareAndExecuteRequest.accept(Service.java:880)
                at org.apache.calcite.avatica.remote.AbstractHandler.apply(AbstractHandler.java:94)
                at org.apache.calcite.avatica.remote.JsonHandler.apply(JsonHandler.java:52)
                at org.apache.calcite.avatica.server.AvaticaJsonHandler.handle(AvaticaJsonHandler.java:130)
                at io.druid.sql.avatica.DruidAvaticaHandler.handle(DruidAvaticaHandler.java:60)
                at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52)
                at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
                at org.eclipse.jetty.server.Server.handle(Server.java:534)
                at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)
                at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
                at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
                at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)
                at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
                at
                org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
                at
                org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
                at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
                at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
                at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
                at java.lang.Thread.run(Thread.java:748)

                Found one Java-level deadlock:
                =============================
                "qtp1871838170-283":
                waiting to lock monitor 0x00007fa50b53c918 (object 0x00000007712471d8, a java.lang.Object),
                which is held by "qtp1871838170-199"
                "qtp1871838170-199":
                waiting to lock monitor 0x00007fa508d0ae68 (object 0x00000007766ce418, a java.util.HashMap),
                which is held by "qtp1871838170-283"

                Java stack information for the threads listed above:
                ===================================================
                "qtp1871838170-283":
                at io.druid.sql.avatica.DruidStatement.close(DruidStatement.java:310)
                - waiting to lock#0x00000007712471d8> (a java.lang.Object)
                at io.druid.sql.avatica.DruidConnection.close(DruidConnection.java:150)
                - locked#0x00000007766ce418> (a java.util.HashMap)
                at io.druid.sql.avatica.DruidMeta.closeConnection(DruidMeta.java:120)
                at org.apache.calcite.avatica.remote.LocalService.apply(LocalService.java:292)
                at org.apache.calcite.avatica.remote.Service$CloseConnectionRequest.accept(Service.java:1907)
                at org.apache.calcite.avatica.remote.Service$CloseConnectionRequest.accept(Service.java:1890)
                at org.apache.calcite.avatica.remote.AbstractHandler.apply(AbstractHandler.java:94)
                at org.apache.calcite.avatica.remote.JsonHandler.apply(JsonHandler.java:52)
                at org.apache.calcite.avatica.server.AvaticaJsonHandler.handle(AvaticaJsonHandler.java:130)
                at io.druid.sql.avatica.DruidAvaticaHandler.handle(DruidAvaticaHandler.java:60)
                at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52)
                at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
                at org.eclipse.jetty.server.Server.handle(Server.java:534)
                at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)
                at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
                at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
                at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)
                at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
                at
                org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
                at
                org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
                at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
                at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
                at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
                at java.lang.Thread.run(Thread.java:748)
                "qtp1871838170-199":
                at io.druid.sql.avatica.DruidConnection.lambda$createStatement$0(DruidConnection.java:107)
                - waiting to lock#0x00000007766ce418> (a java.util.HashMap)
                at io.druid.sql.avatica.DruidConnection$$Lambda$91/1439788572.run(Unknown Source)
                at io.druid.sql.avatica.DruidStatement.close(DruidStatement.java:348)
                - locked#0x00000007712471d8> (a java.lang.Object)
                at io.druid.sql.avatica.DruidStatement.nextFrame(DruidStatement.java:290)
                - locked#0x00000007712471d8> (a java.lang.Object)
                at io.druid.sql.avatica.DruidMeta.prepareAndExecute(DruidMeta.java:188)
                at org.apache.calcite.avatica.remote.LocalService.apply(LocalService.java:206)
                at org.apache.calcite.avatica.remote.Service$PrepareAndExecuteRequest.accept(Service.java:928)
                at org.apache.calcite.avatica.remote.Service$PrepareAndExecuteRequest.accept(Service.java:880)
                at org.apache.calcite.avatica.remote.AbstractHandler.apply(AbstractHandler.java:94)
                at org.apache.calcite.avatica.remote.JsonHandler.apply(JsonHandler.java:52)
                at org.apache.calcite.avatica.server.AvaticaJsonHandler.handle(AvaticaJsonHandler.java:130)
                at io.druid.sql.avatica.DruidAvaticaHandler.handle(DruidAvaticaHandler.java:60)
                at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52)
                at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
                at org.eclipse.jetty.server.Server.handle(Server.java:534)
                at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)
                at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
                at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
                at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)
                at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
                at
                org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
                at
                org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
                at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
                at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
                at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
                at java.lang.Thread.run(Thread.java:748)

            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.druid.sql.avatica.DruidConnection.java</file>
            <file type="M">org.apache.druid.sql.avatica.DruidStatement.java</file>
        </fixedFiles>
    </bug>
    <bug id="4296" opendate="2017-05-18 00:00:00" fixdate="2017-05-23 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>HLL BufferUnderflowException querying realtime indexing tasks.
            </summary>
            <description>I already posted in the druid-user google group about this:
                https://groups.google.com/forum/#!topic/druid-user/X71sEX1Ia_8 but I've decided to also post here since
                I think it may be an actual issue.
                I believe I'm seeing an issue related to #3560, even though I'm running the 0.9.2 final release
                downloaded from http://static.druid.io/artifacts/releases/druid-0.9.2-bin.tar.gz, which should contain
                the fix (#3578).
                A little context: I'm running indexing realtime tasks on middleManagers that have events pushed to them
                from applications using tranquility. When I run timeseries queries for intervals that are being indexed
                on an indexing node, although the queries may initially return data, after a time, if I repeatedly run
                the queries I'll eventually begin receiving BufferUnderflowException errors from peon nodes. Here is the
                relevant query and stack trace pulled from the peon task logs
                Query
                {
                "queryType": "timeseries",
                "dataSource": "memcached_v1",
                "intervals": "2017-05-18T17:20Z/2017-05-18T18:20Z",
                "granularity": "all",
                "context": {
                "timeout": 120000
                },
                "aggregations": [
                {
                "name": "count",
                "type": "doubleSum",
                "fieldName": "count"
                },
                {
                "name": "requests",
                "type": "hyperUnique",
                "fieldName": "requests_hll"
                },
                {
                "name": "unique_keys",
                "type": "hyperUnique",
                "fieldName": "unique_keys_hll"
                },
                {
                "name": "!T_0",
                "type": "doubleSum",
                "fieldName": "duration"
                }
                ],
                "postAggregations": [
                {
                "type": "arithmetic",
                "fn": "/",
                "fields": [
                {
                "type": "fieldAccess",
                "fieldName": "count"
                },
                {
                "type": "hyperUniqueCardinality",
                "fieldName": "requests"
                }
                ],
                "name": "avg_count_request"
                },
                {
                "type": "arithmetic",
                "fn": "/",
                "fields": [
                {
                "type": "fieldAccess",
                "fieldName": "!T_0"
                },
                {
                "type": "constant",
                "value": 1000000
                }
                ],
                "name": "duration"
                }
                ]
                }

                Stacktrace
                2017-05-18T18:21:43,439 ERROR [processing-1] io.druid.query.ChainedExecutionQueryRunner - Exception with
                one of the sequences!
                java.nio.BufferUnderflowException
                at java.nio.Buffer.nextGetIndex(Buffer.java:506) ~[?:1.8.0_74]
                at java.nio.DirectByteBuffer.getShort(DirectByteBuffer.java:590) ~[?:1.8.0_74]
                at io.druid.query.aggregation.hyperloglog.HyperLogLogCollector.fold(HyperLogLogCollector.java:398)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at
                io.druid.query.aggregation.hyperloglog.HyperUniquesAggregator.aggregate(HyperUniquesAggregator.java:48)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at io.druid.query.timeseries.TimeseriesQueryEngine$1.apply(TimeseriesQueryEngine.java:73)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at io.druid.query.timeseries.TimeseriesQueryEngine$1.apply(TimeseriesQueryEngine.java:57)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at io.druid.query.QueryRunnerHelper$1.apply(QueryRunnerHelper.java:80)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at io.druid.query.QueryRunnerHelper$1.apply(QueryRunnerHelper.java:75)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at com.metamx.common.guava.MappingAccumulator.accumulate(MappingAccumulator.java:39)
                ~[java-util-0.27.10.jar:?]
                at com.metamx.common.guava.FilteringAccumulator.accumulate(FilteringAccumulator.java:40)
                ~[java-util-0.27.10.jar:?]
                at com.metamx.common.guava.MappingAccumulator.accumulate(MappingAccumulator.java:39)
                ~[java-util-0.27.10.jar:?]
                at com.metamx.common.guava.BaseSequence.accumulate(BaseSequence.java:67) ~[java-util-0.27.10.jar:?]
                at com.metamx.common.guava.MappedSequence.accumulate(MappedSequence.java:40) ~[java-util-0.27.10.jar:?]
                at com.metamx.common.guava.ResourceClosingSequence.accumulate(ResourceClosingSequence.java:38)
                ~[java-util-0.27.10.jar:?]
                at com.metamx.common.guava.FilteredSequence.accumulate(FilteredSequence.java:42)
                ~[java-util-0.27.10.jar:?]
                at com.metamx.common.guava.MappedSequence.accumulate(MappedSequence.java:40) ~[java-util-0.27.10.jar:?]
                at com.metamx.common.guava.FilteredSequence.accumulate(FilteredSequence.java:42)
                ~[java-util-0.27.10.jar:?]
                at com.metamx.common.guava.ResourceClosingSequence.accumulate(ResourceClosingSequence.java:38)
                ~[java-util-0.27.10.jar:?]
                at com.metamx.common.guava.Sequences.toList(Sequences.java:113) ~[java-util-0.27.10.jar:?]
                at io.druid.query.ChainedExecutionQueryRunner$1$1$1.call(ChainedExecutionQueryRunner.java:129)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at io.druid.query.ChainedExecutionQueryRunner$1$1$1.call(ChainedExecutionQueryRunner.java:119)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_74]
                at
                com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)
                ~[guava-16.0.1.jar:?]
                at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:134) ~[?:1.8.0_74]
                at
                com.google.common.util.concurrent.AbstractListeningExecutorService.submit(AbstractListeningExecutorService.java:58)
                ~[guava-16.0.1.jar:?]
                at io.druid.query.ChainedExecutionQueryRunner$1$1.apply(ChainedExecutionQueryRunner.java:117)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at io.druid.query.ChainedExecutionQueryRunner$1$1.apply(ChainedExecutionQueryRunner.java:109)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at com.google.common.collect.Iterators$8.transform(Iterators.java:794) ~[guava-16.0.1.jar:?]
                at com.google.common.collect.TransformedIterator.next(TransformedIterator.java:48) ~[guava-16.0.1.jar:?]
                at com.google.common.collect.Iterators.addAll(Iterators.java:357) ~[guava-16.0.1.jar:?]
                at com.google.common.collect.Lists.newArrayList(Lists.java:147) ~[guava-16.0.1.jar:?]
                at com.google.common.collect.Lists.newArrayList(Lists.java:129) ~[guava-16.0.1.jar:?]
                at io.druid.query.ChainedExecutionQueryRunner$1.make(ChainedExecutionQueryRunner.java:105)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at com.metamx.common.guava.BaseSequence.accumulate(BaseSequence.java:64) ~[java-util-0.27.10.jar:?]
                at io.druid.query.MetricsEmittingQueryRunner$1.accumulate(MetricsEmittingQueryRunner.java:104)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at io.druid.query.MetricsEmittingQueryRunner$1.accumulate(MetricsEmittingQueryRunner.java:104)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at io.druid.query.CPUTimeMetricQueryRunner$1.accumulate(CPUTimeMetricQueryRunner.java:81)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at com.metamx.common.guava.Sequences$1.accumulate(Sequences.java:90) ~[java-util-0.27.10.jar:?]
                at io.druid.query.spec.SpecificSegmentQueryRunner$2$1.call(SpecificSegmentQueryRunner.java:87)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at io.druid.query.spec.SpecificSegmentQueryRunner.doNamed(SpecificSegmentQueryRunner.java:171)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at io.druid.query.spec.SpecificSegmentQueryRunner.access$400(SpecificSegmentQueryRunner.java:41)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at io.druid.query.spec.SpecificSegmentQueryRunner$2.doItNamed(SpecificSegmentQueryRunner.java:162)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at io.druid.query.spec.SpecificSegmentQueryRunner$2.accumulate(SpecificSegmentQueryRunner.java:80)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at com.metamx.common.guava.Sequences.toList(Sequences.java:113) ~[java-util-0.27.10.jar:?]
                at io.druid.query.ChainedExecutionQueryRunner$1$1$1.call(ChainedExecutionQueryRunner.java:129)
                [druid-processing-0.9.2.jar:0.9.2]
                at io.druid.query.ChainedExecutionQueryRunner$1$1$1.call(ChainedExecutionQueryRunner.java:119)
                [druid-processing-0.9.2.jar:0.9.2]
                at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_74]
                at io.druid.query.PrioritizedListenableFutureTask.run(PrioritizedExecutorService.java:271)
                [druid-processing-0.9.2.jar:0.9.2]
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_74]
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_74]
                at java.lang.Thread.run(Thread.java:745) [?:1.8.0_74]
                2017-05-18T18:21:43,441 WARN
                [qtp1259065345-100[timeseries_memcached_v1_858753f9-49f0-4fa6-9916-f60f48a42ad9]]
                io.druid.server.QueryResource - Exception occurred on request
                [TimeseriesQuery{dataSource='memcached_v1',
                querySegmentSpec=MultipleSpecificSegmentSpec{descriptors=[SegmentDescriptor{interval=2017-05-18T18:15:00.000Z/2017-05-18T18:20:00.000Z,
                version='2017-05-18T18:15:04.604Z', partitionNumber=15}]}, descending=false, dimFilter=null,
                granularity='AllGranularity', aggregatorSpecs=[DoubleSumAggregatorFactory{fieldName='count',
                name='count'}, HyperUniquesAggregatorFactory{name='requests', fieldName='requests_hll'},
                HyperUniquesAggregatorFactory{name='unique_keys', fieldName='unique_keys_hll'},
                DoubleSumAggregatorFactory{fieldName='duration', name='!T_0'}],
                postAggregatorSpecs=[ArithmeticPostAggregator{name='avg_count_request', fnName='/',
                fields=[FieldAccessPostAggregator{name='null', fieldName='count'},
                io.druid.query.aggregation.hyperloglog.HyperUniqueFinalizingPostAggregator@7d250277], op=DIV},
                ArithmeticPostAggregator{name='duration', fnName='/', fields=[FieldAccessPostAggregator{name='null',
                fieldName='!T_0'}, ConstantPostAggregator{name='null', constantValue=1000000}], op=DIV}],
                context={finalize=false, queryId=858753f9-49f0-4fa6-9916-f60f48a42ad9, timeout=120000}}]
                java.nio.BufferUnderflowException
                at java.nio.Buffer.nextGetIndex(Buffer.java:506) ~[?:1.8.0_74]
                at java.nio.DirectByteBuffer.getShort(DirectByteBuffer.java:590) ~[?:1.8.0_74]
                at io.druid.query.aggregation.hyperloglog.HyperLogLogCollector.fold(HyperLogLogCollector.java:398)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at
                io.druid.query.aggregation.hyperloglog.HyperUniquesAggregator.aggregate(HyperUniquesAggregator.java:48)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at io.druid.query.timeseries.TimeseriesQueryEngine$1.apply(TimeseriesQueryEngine.java:73)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at io.druid.query.timeseries.TimeseriesQueryEngine$1.apply(TimeseriesQueryEngine.java:57)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at io.druid.query.QueryRunnerHelper$1.apply(QueryRunnerHelper.java:80)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at io.druid.query.QueryRunnerHelper$1.apply(QueryRunnerHelper.java:75)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at com.metamx.common.guava.MappingAccumulator.accumulate(MappingAccumulator.java:39)
                ~[java-util-0.27.10.jar:?]
                at com.metamx.common.guava.FilteringAccumulator.accumulate(FilteringAccumulator.java:40)
                ~[java-util-0.27.10.jar:?]
                at com.metamx.common.guava.MappingAccumulator.accumulate(MappingAccumulator.java:39)
                ~[java-util-0.27.10.jar:?]
                at com.metamx.common.guava.BaseSequence.accumulate(BaseSequence.java:67) ~[java-util-0.27.10.jar:?]
                at com.metamx.common.guava.MappedSequence.accumulate(MappedSequence.java:40) ~[java-util-0.27.10.jar:?]
                at com.metamx.common.guava.ResourceClosingSequence.accumulate(ResourceClosingSequence.java:38)
                ~[java-util-0.27.10.jar:?]
                at com.metamx.common.guava.FilteredSequence.accumulate(FilteredSequence.java:42)
                ~[java-util-0.27.10.jar:?]
                at com.metamx.common.guava.MappedSequence.accumulate(MappedSequence.java:40) ~[java-util-0.27.10.jar:?]
                at com.metamx.common.guava.FilteredSequence.accumulate(FilteredSequence.java:42)
                ~[java-util-0.27.10.jar:?]
                at com.metamx.common.guava.ResourceClosingSequence.accumulate(ResourceClosingSequence.java:38)
                ~[java-util-0.27.10.jar:?]
                at com.metamx.common.guava.Sequences.toList(Sequences.java:113) ~[java-util-0.27.10.jar:?]
                at io.druid.query.ChainedExecutionQueryRunner$1$1$1.call(ChainedExecutionQueryRunner.java:129)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at io.druid.query.ChainedExecutionQueryRunner$1$1$1.call(ChainedExecutionQueryRunner.java:119)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_74]
                at
                com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)
                ~[guava-16.0.1.jar:?]
                at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:134) ~[?:1.8.0_74]
                at
                com.google.common.util.concurrent.AbstractListeningExecutorService.submit(AbstractListeningExecutorService.java:58)
                ~[guava-16.0.1.jar:?]
                at io.druid.query.ChainedExecutionQueryRunner$1$1.apply(ChainedExecutionQueryRunner.java:117)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at io.druid.query.ChainedExecutionQueryRunner$1$1.apply(ChainedExecutionQueryRunner.java:109)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at com.google.common.collect.Iterators$8.transform(Iterators.java:794) ~[guava-16.0.1.jar:?]
                at com.google.common.collect.TransformedIterator.next(TransformedIterator.java:48) ~[guava-16.0.1.jar:?]
                at com.google.common.collect.Iterators.addAll(Iterators.java:357) ~[guava-16.0.1.jar:?]
                at com.google.common.collect.Lists.newArrayList(Lists.java:147) ~[guava-16.0.1.jar:?]
                at com.google.common.collect.Lists.newArrayList(Lists.java:129) ~[guava-16.0.1.jar:?]
                at io.druid.query.ChainedExecutionQueryRunner$1.make(ChainedExecutionQueryRunner.java:105)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at com.metamx.common.guava.BaseSequence.accumulate(BaseSequence.java:64) ~[java-util-0.27.10.jar:?]
                at io.druid.query.MetricsEmittingQueryRunner$1.accumulate(MetricsEmittingQueryRunner.java:104)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at io.druid.query.MetricsEmittingQueryRunner$1.accumulate(MetricsEmittingQueryRunner.java:104)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at io.druid.query.CPUTimeMetricQueryRunner$1.accumulate(CPUTimeMetricQueryRunner.java:81)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at com.metamx.common.guava.Sequences$1.accumulate(Sequences.java:90) ~[java-util-0.27.10.jar:?]
                at io.druid.query.spec.SpecificSegmentQueryRunner$2$1.call(SpecificSegmentQueryRunner.java:87)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at io.druid.query.spec.SpecificSegmentQueryRunner.doNamed(SpecificSegmentQueryRunner.java:171)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at io.druid.query.spec.SpecificSegmentQueryRunner.access$400(SpecificSegmentQueryRunner.java:41)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at io.druid.query.spec.SpecificSegmentQueryRunner$2.doItNamed(SpecificSegmentQueryRunner.java:162)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at io.druid.query.spec.SpecificSegmentQueryRunner$2.accumulate(SpecificSegmentQueryRunner.java:80)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at com.metamx.common.guava.Sequences.toList(Sequences.java:113) ~[java-util-0.27.10.jar:?]
                at io.druid.query.ChainedExecutionQueryRunner$1$1$1.call(ChainedExecutionQueryRunner.java:129)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at io.druid.query.ChainedExecutionQueryRunner$1$1$1.call(ChainedExecutionQueryRunner.java:119)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_74]
                at io.druid.query.PrioritizedListenableFutureTask.run(PrioritizedExecutorService.java:271)
                ~[druid-processing-0.9.2.jar:0.9.2]
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[?:1.8.0_74]
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[?:1.8.0_74]
                at java.lang.Thread.run(Thread.java:745) [?:1.8.0_74]

                My middleManger configuration is here:
                druid.worker.capacity=5
                druid.indexer.task.baseTaskDir=/mnt/druid/0/task/
                druid.indexer.task.restoreTasksOnRestart=true
                druid.indexer.fork.property.druid.processing.numThreads=2
                druid.indexer.fork.property.druid.processing.buffer.sizeBytes=536870912
                druid.segmentCache.locations=[{"path":"/mnt/druid/0/segment_cache","maxSize":0}]
                druid.indexer.runner.javaOpts=-server -Xmx3221225472 -XX:+UseG1GC -XX:MaxGCPauseMillis=100

                A fix I've found is that if I set
                druid.indexer.fork.property.druid.processing.numThreads=1
                Then I no longer have any issues, which leads to me to think I am running into the race condition
                described in the linked issue which ends up corrupting the HLL segment.
                Setting the processing threads to 1 is not ideal though since I would like the efficiency of having an
                indexing thread and a query thread per task if possible.
                I also tried upgrading the cluster to 0.10.0 and still ran into this issue as well.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.druid.query.aggregation.cardinality.CardinalityAggregator.java</file>
            <file type="M">org.apache.druid.query.aggregation.hyperloglog.HyperUniquesAggregator.java</file>
        </fixedFiles>
    </bug>
    <bug id="2793" opendate="2016-04-05 00:00:00" fixdate="2020-01-10 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>Duplicate primary key errors cause TaskQueue big lock to be held for way longer than it should.
            </summary>
            <description>#1896 introduced an addition of e4e5f03#diff-e677e1ba7e3cf3b5b97660cfc17749beR144 which causes
                errors on duplicate primary key entries to be retried way more than they should.
                2016-04-05T20:52:57,269 WARN [qtp726762476-61] com.metamx.common.RetryUtils - Failed on try 1, retrying
                in 1,976ms.
                org.skife.jdbi.v2.exceptions.CallbackFailedException:
                org.skife.jdbi.v2.exceptions.UnableToExecuteStatementException:
                com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Duplicate entry
                'index_realtime_REDACTED_2016-04-05T20:00:0' for key 'PRIMARY' [statement:"INSERT INTO REDACTED (id,
                created_date, datasource, payload, active, status_payload) VALUES (:id, :created_date, :datasource,
                :payload, :active, :status_payload)", located:"INSERT INTO REDACTED (id, created_date, datasource,
                payload, active, status_payload) VALUES (:id, :created_date, :datasource, :payload, :active,
                :status_payload)", rewritten:"INSERT INTO REDACTED (id, created_date, datasource, payload, active,
                status_payload) VALUES (?, ?, ?, ?, ?, ?)", arguments:{ positional:{}, named:{payload:[
                ...
                at org.skife.jdbi.v2.DBI.withHandle(DBI.java:284)
                ~[druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]
                at io.druid.metadata.SQLMetadataConnector$2.call(SQLMetadataConnector.java:110)
                ~[druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]
                at com.metamx.common.RetryUtils.retry(RetryUtils.java:38)
                [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]
                at io.druid.metadata.SQLMetadataConnector.retryWithHandle(SQLMetadataConnector.java:115)
                [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]
                at io.druid.metadata.SQLMetadataStorageActionHandler.insert(SQLMetadataStorageActionHandler.java:97)
                [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]
                at io.druid.indexing.overlord.MetadataTaskStorage.insert(MetadataTaskStorage.java:134)
                [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]
                at io.druid.indexing.overlord.TaskQueue.add(TaskQueue.java:321)
                [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]
                at io.druid.indexing.overlord.http.OverlordResource$1.apply(OverlordResource.java:124)
                [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]
                at io.druid.indexing.overlord.http.OverlordResource$1.apply(OverlordResource.java:119)
                [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]
                at io.druid.indexing.overlord.http.OverlordResource.asLeaderWith(OverlordResource.java:518)
                [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]
                at io.druid.indexing.overlord.http.OverlordResource.taskPost(OverlordResource.java:116)
                [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]
                at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_60]
                at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_60]
                at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_60]
                at java.lang.reflect.Method.invoke(Method.java:497) ~[?:1.8.0_60]
                at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
                [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]
                at
                com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)
                [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]
                at
                com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
                [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]
                at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)
                [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]
                at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
                [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]
                at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
                [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]
                at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
                [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]
                at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
                [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]
                at
                com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)
                [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]
                at
                com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)
                [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]
                at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)
                [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]
                at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)
                [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]
                at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)
                [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]
                at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)
                [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]
                at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)
                [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]
                at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
                [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]
                at com.google.inject.servlet.ServletDefinition.doServiceImpl(ServletDefinition.java:278)
                [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]
                at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:268)
                [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]
                at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:180)
                [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]
                at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:93)
                [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]
                at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:85)
                [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]
                at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:120)
                [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]
                at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:132)
                [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]
                at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:129)
                [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]
                at com.google.inject.servlet.GuiceFilter$Context.call(GuiceFilter.java:206)
                [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]
                at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:129)
                [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]
                at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1652)
                [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]
                at io.druid.server.http.RedirectFilter.doFilter(RedirectFilter.java:71)
                [druid-selfcontained-0.9.0-rc3-mmx2.jar:0.9.0-rc3-mmx2]
                ...

                This retry holds onto the big lock in TaskQueue while it exhausts its attempts, causing a big backlog of
                things wanting to use the big lock in TaskQueue
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.druid.metadata.SQLMetadataConnector.java</file>
            <file type="M">org.apache.druid.metadata.SQLMetadataStorageActionHandler.java</file>
            <file type="M">org.apache.druid.metadata.SQLMetadataStorageActionHandlerTest.java</file>
        </fixedFiles>
    </bug>
    <bug id="6287" opendate="2018-09-01 00:00:00" fixdate="2018-09-19 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>Race condition in Kafka Indexing Service.
            </summary>
            <description>verifyAndMergeCheckpoints in KafkaSupervisor has an issue where Futures.addCallback(...) may
                not have been completed before its results are used in taskSequences.sort((o1, o2) ->
                o2.rhs.firstKey().compareTo(o1.rhs.firstKey()))
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
<file type="M">org.apache.druid.indexing.kafka.supervisor.KafkaSupervisor.java</file>
        </fixedFiles>
    </bug>
    <bug id="1715" opendate="2015-09-09 00:00:00" fixdate="2018-07-06 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>Zombie tasks able to acquire locks after failure.
            </summary>
            <description>We had an issue where disconnect somewhere occurred long enough for the overlord to log a task
                as failed, but the task was still running. After 45 mins or so the task submitted a lock acquire request
                and it was granted, but since the task had already been failed, it never was properly cleaned up.
                These are from the overlord
                2015-09-09T07:33:58,102 INFO [RemoteTaskRunner-Scheduled-Cleanup--0]
                io.druid.indexing.overlord.RemoteTaskRunner - Running scheduled cleanup for Worker[REDACTED:8080]
                2015-09-09T07:33:58,106 INFO [RemoteTaskRunner-Scheduled-Cleanup--0]
                io.druid.indexing.overlord.RemoteTaskRunner - Failing task[index_realtime_REDACTED]
                2015-09-09T07:33:58,106 INFO [RemoteTaskRunner-Scheduled-Cleanup--0]
                io.druid.indexing.overlord.TaskQueue - Received FAILED status for task: index_realtime_REDACTED
                2015-09-09T07:33:58,109 INFO [RemoteTaskRunner-Scheduled-Cleanup--0]
                io.druid.indexing.overlord.RemoteTaskRunner - Can't shutdown! No worker running task
                index_realtime_REDACTED
                2015-09-09T07:33:58,110 INFO [RemoteTaskRunner-Scheduled-Cleanup--0]
                io.druid.indexing.overlord.MetadataTaskStorage - Updating task index_realtime_REDACTED to status:
                TaskStatus{id=index_realtime_REDACTED, status=FAILED, duration=-1}
                2015-09-09T07:33:58,132 INFO [RemoteTaskRunner-Scheduled-Cleanup--0]
                io.druid.indexing.overlord.TaskLockbox - Removing task[index_realtime_REDACTED] from
                TaskLock[index_realtime_REDACTED]

                2015-09-09T08:15:15,428 INFO [qtpREDACTED] io.druid.indexing.common.actions.LocalTaskActionClient -
                Performing action for task[index_realtime_REDACTED]:
                LockAcquireAction{interval=2015-09-09T08:00:00.000Z/2015-09-09T09:00:00.000Z}
                2015-09-09T08:15:15,428 INFO [qtpREDACTED] io.druid.indexing.overlord.TaskLockbox - Created new
                TaskLockPosse: TaskLockPosse{taskLock=TaskLock{groupId=index_realtime_REDACTED, dataSource=REDACTED,
                interval=2015-09-09T08:00:00.000Z/2015-09-09T09:00:00.000Z, version=2015-09-09T08:15:15.428Z},
                taskIds=[]}
                2015-09-09T08:15:15,428 INFO [qtpREDACTED] io.druid.indexing.overlord.TaskLockbox - Added
                task[index_realtime_REDACTED] to TaskLock[index_realtime_REDACTED]

                Suffice to say, this is a long-running task that acquires multiple 1-hr locks during the course of its
                execution.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.druid.indexing.overlord.TaskLockbox.java</file>
            <file type="M">org.apache.druid.indexing.overlord.TaskQueue.java</file>
            <file type="M">org.apache.druid.indexing.overlord.TaskLockboxTest.java</file>
            <file type="M">org.apache.druid.indexing.overlord.http.OverlordResourceTest.java</file>
        </fixedFiles>
    </bug>
    <bug id="7400" opendate="2019-04-02 00:00:00" fixdate="2020-04-25 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>Allow users to use a provider for metadata username.
            </summary>
            <description>Description
                Currently, we can use a PasswordProvider to provide credentials for connecting to the metadata store
                without hardcoding them. However, we still need to hardcode the username, and we need to restart Druid
                if we want to change the username.
                Motivation
                We would like to be able to change the user Druid uses to interact with the metadata database without
                needing to restart Druid. To do so, we need to be able to change the
                druid.metadata.storage.connector.user property during operation in the same way we can change
                druid.metadata.storage.connector.password while Druid is running. In our specific case, we would like to
                be able to use a single provider to furnish both the user name and the password, and such an
                implementation would also avoid introducing a race condition a la #6666. If users wanted to use separate
                providers for the username and the password, we could build a PasswordProvider that yoked together two
                other PasswordProviders to provide each field.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.druid.metadata.DynamicConfigProvider.java</file>
            <file type="M">org.apache.druid.metadata.MapStringDynamicConfigProvider.java</file>
            <file type="M">org.apache.druid.metadata.PasswordProvider.java</file>
            <file type="M">org.apache.druid.metadata.MapStringDynamicConfigProviderTest.java</file>
<file type="M">org.apache.druid.indexing.kafka.KafkaRecordSupplier.java</file>
<file type="M">org.apache.druid.indexing.kafka.supervisor.KafkaSupervisorIOConfig.java</file>
<file type="M">org.apache.druid.indexing.kafka.KafkaRecordSupplierTest.java</file>
        </fixedFiles>
    </bug>
    <bug id="3459" opendate="2016-09-14 00:00:00" fixdate="2022-02-25 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>Deadlock in global lookup cache.
            </summary>
            <description>Had an issue with a query node which ended up reporting a deadlock on kill -3 I'm still
                investigating and intend to make sure #3071 avoids this case.
                Found one Java-level deadlock:
                =============================
                "NamespaceExtractionCacheManager-0":
                waiting for ownable synchronizer 0x00000004c42eaba8, (a
                java.util.concurrent.locks.ReentrantLock$NonfairSync),
                which is held by "topN_REDACTED_[2016-09-01T15:00:00.000Z/2016-09-01T16:00:00.000Z]"
                "topN_REDACTED_[2016-09-01T15:00:00.000Z/2016-09-01T16:00:00.000Z]":
                waiting for ownable synchronizer 0x00000004c42eacc8, (a
                java.util.concurrent.locks.ReentrantLock$NonfairSync),
                which is held by "topN_REDACTED_[2016-09-01T15:00:00.000Z/2016-09-01T16:00:00.000Z]"
                "topN_REDACTED_[2016-09-01T15:00:00.000Z/2016-09-01T16:00:00.000Z]":
                waiting for ownable synchronizer 0x00000004c42eaba8, (a
                java.util.concurrent.locks.ReentrantLock$NonfairSync),
                which is held by "topN_REDACTED_[2016-09-01T15:00:00.000Z/2016-09-01T16:00:00.000Z]"

                Java stack information for the threads listed above:
                ===================================================
                "NamespaceExtractionCacheManager-0":
                at sun.misc.Unsafe.park(Native Method)
                - parking to wait for#0x00000004c42eaba8> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
                at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
                at
                java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
                at
                java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:870)
                at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1199)
                at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:209)
                at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:285)
                at
                io.druid.server.lookup.namespace.cache.OffHeapNamespaceExtractionCacheManager.swapAndClearCache(OffHeapNamespaceExtractionCacheManager.java:112)
                at
                io.druid.server.lookup.namespace.cache.NamespaceExtractionCacheManager$2.run(NamespaceExtractionCacheManager.java:173)
                - locked#0x00000004c42c11a8> (a java.util.concurrent.atomic.AtomicBoolean)
                at
                io.druid.server.lookup.namespace.cache.NamespaceExtractionCacheManager$4.run(NamespaceExtractionCacheManager.java:368)
                at
                com.google.common.util.concurrent.MoreExecutors$ScheduledListeningDecorator$NeverSuccessfulListenableFutureTask.run(MoreExecutors.java:582)
                at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
                at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
                at
                java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
                at
                java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
                at java.lang.Thread.run(Thread.java:745)
                "topN_REDACTED_[2016-09-01T15:00:00.000Z/2016-09-01T16:00:00.000Z]":
                at sun.misc.Unsafe.park(Native Method)
                - parking to wait for#0x00000004c42eacc8> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
                at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
                at
                java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
                at
                java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:870)
                at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1199)
                at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:209)
                at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:285)
                at
                io.druid.server.lookup.namespace.cache.OffHeapNamespaceExtractionCacheManager.getCacheMap(OffHeapNamespaceExtractionCacheManager.java:170)
                at io.druid.query.lookup.NamespaceLookupExtractorFactory.get(NamespaceLookupExtractorFactory.java:270)
                at io.druid.query.lookup.NamespaceLookupExtractorFactory.get(NamespaceLookupExtractorFactory.java:50)
                at
                io.druid.query.lookup.RegisteredLookupExtractionFn.ensureDelegate(RegisteredLookupExtractionFn.java:142)
                - locked#0x00000005f4d39248> (a java.lang.Object)
                at io.druid.query.lookup.RegisteredLookupExtractionFn.getCacheKey(RegisteredLookupExtractionFn.java:97)
                at io.druid.query.dimension.ExtractionDimensionSpec.getCacheKey(ExtractionDimensionSpec.java:96)
                at io.druid.query.topn.TopNQueryQueryToolChest$7.computeCacheKey(TopNQueryQueryToolChest.java:309)
                at io.druid.query.topn.TopNQueryQueryToolChest$7.computeCacheKey(TopNQueryQueryToolChest.java:298)
                at io.druid.client.CachingQueryRunner.run(CachingQueryRunner.java:103)
                at io.druid.query.BySegmentQueryRunner.run(BySegmentQueryRunner.java:70)
                at io.druid.query.MetricsEmittingQueryRunner$1.accumulate(MetricsEmittingQueryRunner.java:118)
                at io.druid.query.spec.SpecificSegmentQueryRunner$2$1.call(SpecificSegmentQueryRunner.java:87)
                at io.druid.query.spec.SpecificSegmentQueryRunner.doNamed(SpecificSegmentQueryRunner.java:171)
                at io.druid.query.spec.SpecificSegmentQueryRunner.access$400(SpecificSegmentQueryRunner.java:41)
                at io.druid.query.spec.SpecificSegmentQueryRunner$2.doItNamed(SpecificSegmentQueryRunner.java:162)
                at io.druid.query.spec.SpecificSegmentQueryRunner$2.accumulate(SpecificSegmentQueryRunner.java:80)
                at io.druid.query.CPUTimeMetricQueryRunner$1.accumulate(CPUTimeMetricQueryRunner.java:81)
                at com.metamx.common.guava.Sequences$1.accumulate(Sequences.java:90)
                at com.metamx.common.guava.Sequences.toList(Sequences.java:113)
                at io.druid.query.ChainedExecutionQueryRunner$1$1$1.call(ChainedExecutionQueryRunner.java:129)
                at io.druid.query.ChainedExecutionQueryRunner$1$1$1.call(ChainedExecutionQueryRunner.java:119)
                at java.util.concurrent.FutureTask.run(FutureTask.java:266)
                at io.druid.query.PrioritizedListenableFutureTask.run(PrioritizedExecutorService.java:271)
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
                at java.lang.Thread.run(Thread.java:745)
                "topN_REDACTED_[2016-09-01T15:00:00.000Z/2016-09-01T16:00:00.000Z]":
                at sun.misc.Unsafe.park(Native Method)
                - parking to wait for#0x00000004c42eaba8> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
                at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
                at
                java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
                at
                java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:870)
                at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1199)
                at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:209)
                at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:285)
                at
                io.druid.server.lookup.namespace.cache.OffHeapNamespaceExtractionCacheManager.getCacheMap(OffHeapNamespaceExtractionCacheManager.java:170)
                at io.druid.query.lookup.NamespaceLookupExtractorFactory.get(NamespaceLookupExtractorFactory.java:270)
                at io.druid.query.lookup.NamespaceLookupExtractorFactory.get(NamespaceLookupExtractorFactory.java:50)
                at
                io.druid.query.lookup.RegisteredLookupExtractionFn.ensureDelegate(RegisteredLookupExtractionFn.java:142)
                - locked#0x00000005f4d35518> (a java.lang.Object)
                at io.druid.query.lookup.RegisteredLookupExtractionFn.getCacheKey(RegisteredLookupExtractionFn.java:97)
                at io.druid.query.dimension.ExtractionDimensionSpec.getCacheKey(ExtractionDimensionSpec.java:96)
                at io.druid.query.topn.TopNQueryQueryToolChest$7.computeCacheKey(TopNQueryQueryToolChest.java:309)
                at io.druid.query.topn.TopNQueryQueryToolChest$7.computeCacheKey(TopNQueryQueryToolChest.java:298)
                at io.druid.client.CachingQueryRunner.run(CachingQueryRunner.java:103)
                at io.druid.query.BySegmentQueryRunner.run(BySegmentQueryRunner.java:70)
                at io.druid.query.MetricsEmittingQueryRunner$1.accumulate(MetricsEmittingQueryRunner.java:118)
                at io.druid.query.spec.SpecificSegmentQueryRunner$2$1.call(SpecificSegmentQueryRunner.java:87)
                at io.druid.query.spec.SpecificSegmentQueryRunner.doNamed(SpecificSegmentQueryRunner.java:171)
                at io.druid.query.spec.SpecificSegmentQueryRunner.access$400(SpecificSegmentQueryRunner.java:41)
                at io.druid.query.spec.SpecificSegmentQueryRunner$2.doItNamed(SpecificSegmentQueryRunner.java:162)
                at io.druid.query.spec.SpecificSegmentQueryRunner$2.accumulate(SpecificSegmentQueryRunner.java:80)
                at io.druid.query.CPUTimeMetricQueryRunner$1.accumulate(CPUTimeMetricQueryRunner.java:81)
                at com.metamx.common.guava.Sequences$1.accumulate(Sequences.java:90)
                at com.metamx.common.guava.Sequences.toList(Sequences.java:113)
                at io.druid.query.ChainedExecutionQueryRunner$1$1$1.call(ChainedExecutionQueryRunner.java:129)
                at io.druid.query.ChainedExecutionQueryRunner$1$1$1.call(ChainedExecutionQueryRunner.java:119)
                at java.util.concurrent.FutureTask.run(FutureTask.java:266)
                at io.druid.query.PrioritizedListenableFutureTask.run(PrioritizedExecutorService.java:271)
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
                at java.lang.Thread.run(Thread.java:745)

                Found 1 deadlock.

            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
<file type="M">org.apache.druid.query.lookup.KafkaLookupExtractorFactory.java</file>
<file type="M">org.apache.druid.query.lookup.KafkaLookupExtractorFactoryTest.java</file>
<file type="M">org.apache.druid.query.lookup.NamespaceLookupExtractorFactory.java</file>
<file type="M">org.apache.druid.query.lookup.NamespaceLookupIntrospectHandler.java</file>
<file type="M">org.apache.druid.query.lookup.namespace.ExtractionNamespaceCacheFactory.java</file>
<file type="M">org.apache.druid.server.lookup.namespace.JDBCExtractionNamespaceCacheFactory.java</file>
<file type="M">org.apache.druid.server.lookup.namespace.NamespaceExtractionModule.java</file>
<file type="M">org.apache.druid.server.lookup.namespace.StaticMapExtractionNamespaceCacheFactory.java</file>
<file type="M">org.apache.druid.server.lookup.namespace.URIExtractionNamespaceCacheFactory.java</file>
<file type="M">org.apache.druid.server.lookup.namespace.cache.CacheHandler.java</file>
<file type="M">org.apache.druid.server.lookup.namespace.cache.CacheProxy.java</file>
<file type="M">org.apache.druid.server.lookup.namespace.cache.CacheScheduler.java</file>
<file type="M">org.apache.druid.server.lookup.namespace.cache.NamespaceExtractionCacheManager.java</file>
<file type="M">org.apache.druid.server.lookup.namespace.cache.OffHeapNamespaceExtractionCacheManager.java</file>
<file type="M">org.apache.druid.server.lookup.namespace.cache.OnHeapNamespaceExtractionCacheManager.java</file>
<file type="M">org.apache.druid.server.lookup.namespace.cache.UpdateCounter.java</file>
<file type="M">org.apache.druid.query.lookup.NamespaceLookupExtractorFactoryTest.java</file>
<file type="M">org.apache.druid.server.lookup.namespace.NamespacedExtractorModuleTest.java</file>
<file type="M">org.apache.druid.server.lookup.namespace.StaticMapExtractionNamespaceCacheFactoryTest.java</file>
<file type="M">org.apache.druid.server.lookup.namespace.URIExtractionNamespaceCacheFactoryTest.java</file>
<file type="M">org.apache.druid.server.lookup.namespace.cache.JDBCExtractionNamespaceTest.java</file>
<file type="M">org.apache.druid.server.lookup.namespace.cache.NamespaceExtractionCacheManagerExecutorsTest.java</file>
<file type="M">org.apache.druid.server.lookup.namespace.cache.NamespaceExtractionCacheManagersTest.java</file>
<file type="M">org.apache.druid.server.lookup.namespace.cache.OffHeapNamespaceExtractionCacheManagerTest.java</file>
        </fixedFiles>
    </bug>
    <bug id="10005" opendate="2020-06-08 00:00:00" fixdate="2020-06-17 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>API to verify that published segments are loaded and available for a datasource.
            </summary>
            <description>Description
                This new API will be use to verify that published segments are loaded and available for a datasource.
                The new API will be able to do the following:


                new api takes in datasource. This will returns false if any used segment (of the past 2 weeks) of the
                given datasource are not available to be query (i.e. not loaded onto historical yet). Return true
                otherwise.


                (same) new api takes in datasource and a time interval (start + end): This will returns false if any
                used segment (between the given start and given end time) of the given datasource are not available to
                be query (i.e. not loaded onto historical yet). Return true otherwise.


                Note that the above are both the same API. The time interval is an optional parameter. The time interval
                referred above is the timestamp of the data in the segment (nothing to do with when the segment is
                ingested). This can be the same time interval as the time interval the user want to query data from.
                Basically if the user wants to query from x to y then they can call this new api with the datasource and
                time interval x to y. This will ensure that all segments of the datasource for the timestamp from x to y
                is ready to be query (loaded onto historical).
                Important differences between this API from the existing coordinator loadstatus API:

                Takes datasource (required) to be able to check faster (iterate smaller number of segments)
                Takes interval (optional) to be able to check faster (iterate smaller number of segments)
                Important. Takes boolean forceMetadataRefresh. If this is true, this will force poll the metadata source
                to get latest published segment information.

                API Path:
                /druid/coordinator/v1/datasources/{dataSourceName}/loadstatus
                Request:
                @GET
                @Path("/{dataSourceName}/loadstatus")
                @Produces(MediaType.APPLICATION_JSON)
                @ResourceFilters(DatasourceResourceFilter.class)
                public Response getDatasourceLoadstatus(
                @PathParam("dataSourceName") String dataSourceName,
                @QueryParam("interval") @Nullable final String interval,
                @QueryParam("forceMetadataRefresh") @Nullable final Boolean forceMetadataRefresh
                @QueryParam("simple") @Nullabl final String simple,
                @QueryParam("full") @Nullabl final String full
                )


                Response:
                Default (No simple/full given):
                Returns the percentage of segments actually loaded in the cluster versus segments that should be loaded
                in the cluster for the given datasource over the given interval (or last 2 weeks if not given).
                value in response is percentage (% )
                {
                #GIVEN_DATASOURCE>:95.0
                }

                Simple:
                Returns the number of segments left to load until segments that should be loaded in the cluster are
                available for queries. This does not include replication.
                value in response is number of segments (# )
                {
                #GIVEN_DATASOURCE>:5
                }

                full:
                Returns the number of segments left to load in each tier until segments that should be loaded in the
                cluster are all available. This includes replication.
                value in response is number of segments (# )
                {
                "_default_tier":{
                #GIVEN_DATASOURCE>:1
                }
                }

                interval can be null - default to last 2 weeks
                forceMetadataRefresh can be null - default to true
                Motivation
                This is to address #5721
                The existing loadstatus API on the Coordinator reads segments from SqlSegmentsMetadataManager of the
                Coordinator which caches segments in memory and periodically updates them. Hence, there can be a race
                condition as this API implementation compares segments metadata from the mentioned cache with published
                segments in historical. Particularly, when there is a new ingestion after the initial load of the
                datasource, the cache still only contains the metadata of old segments. The API would compares list of
                old segments with what is published by historical and returns that everything is available when the new
                segments are not actually available yet.
                The workflow will be :

                submit ingestion task
                poll task api until task succeeded
                poll the new api with datasource, interval, and forceMetadataRefresh=true once. If false, go to step 4,
                otherwise the data is available and user can query.
                poll the new api with datasource, interval, and forceMetadataRefresh=false until return true. After
                true, data is available and user can query.

            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.druid.client.CoordinatorServerView.java</file>
            <file type="M">org.apache.druid.metadata.SegmentsMetadataManager.java</file>
            <file type="M">org.apache.druid.metadata.SqlSegmentsMetadataManager.java</file>
            <file type="M">org.apache.druid.server.coordinator.DruidCoordinator.java</file>
            <file type="M">org.apache.druid.server.http.DataSourcesResource.java</file>
            <file type="M">org.apache.druid.metadata.SqlSegmentsMetadataManagerTest.java</file>
            <file type="M">org.apache.druid.server.http.DataSourcesResourceTest.java</file>
        </fixedFiles>
    </bug>
    <bug id="1513" opendate="2015-07-14 00:00:00" fixdate="2020-01-10 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>Preemption for indexing service locks.
            </summary>
            <description>From https://groups.google.com/forum/#!topic/druid-development/kHgHTgqKFlQ,

                There's potential for conflicts between merging and appending new data. Consider a merge task that reads
                a set of segments with version A (let's say A0, A1, A2) and then publishes a new set of segments with
                version B (let's say B0, B1). If a realtime task spins up between those two points and wants to add a
                new segment, it could create A3, which is bad, because it will end up being shadowed by set B and the
                writes will be lost. Or it could create B2, which is also bad because it will not be visible until B0
                and B1 are published and uploaded. Even worse, if a new merge task can start up after the first one
                finishes (but while B2 is still ingesting) and create a set of C-segments, that will cause the writes in
                B2 to be lost forever.
                There are a few different ways to handle this:
                a) Merging wins and ingestion loses. This is the current method and is mediated through exclusive
                interval locks. Ingestion losing would probably mean ingestion blocks.
                b) Ingestion wins and merging loses. This could be done by aborting a merge in progress if new data
                comes in. This has the advantage of not affecting ingestion at all, but will cause liveness issues with
                merging if a trickle of data comes in for the same interval for an excessive amount of time.
                c) Something else?
                For the first version, I think (b) is best since I think most people would prefer liveness issues in
                merging to liveness issues with ingestion.

                To do Ingestion wins and merging loses, we need a way for an ingestion task to pre-empt locks held by a
                merge task. This could possibly be done by having a lock priority such that higher priority locks cancel
                out lower priority locks. The task that has the pre-empted lock would presumably figure that out when it
                tries to insert a segment and fails. It could also be notified about it proactively somehow, but it's
                unclear how that would happen.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.druid.metadata.MetadataStorageActionHandler.java</file>
<file type="M">org.apache.druid.indexing.kafka.KafkaIndexTask.java</file>
<file type="M">org.apache.druid.indexing.kafka.KafkaIndexTaskTest.java</file>
            <file type="M">org.apache.druid.indexing.common.TaskLock.java</file>
            <file type="M">org.apache.druid.indexing.common.actions.RemoteTaskActionClient.java</file>
<file type="M">org.apache.druid.indexing.common.actions.SetLockCriticalStateAction.java</file>
            <file type="M">org.apache.druid.indexing.common.actions.TaskAction.java</file>
            <file type="M">org.apache.druid.indexing.common.actions.TaskActionToolbox.java</file>
            <file type="M">org.apache.druid.indexing.common.actions.TaskLockCriticalState.java</file>
            <file type="M">org.apache.druid.indexing.common.index.YeOldePlumberSchool.java</file>
            <file type="M">org.apache.druid.indexing.common.task.AbstractTask.java</file>
            <file type="M">org.apache.druid.indexing.common.task.ArchiveTask.java</file>
            <file type="M">org.apache.druid.indexing.common.task.ConvertSegmentTask.java</file>
            <file type="M">org.apache.druid.indexing.common.task.HadoopIndexTask.java</file>
            <file type="M">org.apache.druid.indexing.common.task.HadoopTask.java</file>
            <file type="M">org.apache.druid.indexing.common.task.IndexTask.java</file>
            <file type="M">org.apache.druid.indexing.common.task.KillTask.java</file>
            <file type="M">org.apache.druid.indexing.common.task.MergeTaskBase.java</file>
            <file type="M">org.apache.druid.indexing.common.task.MoveTask.java</file>
            <file type="M">org.apache.druid.indexing.common.task.RealtimeIndexTask.java</file>
            <file type="M">org.apache.druid.indexing.common.task.RestoreTask.java</file>
            <file type="M">org.apache.druid.indexing.common.task.Task.java</file>
            <file type="M">org.apache.druid.indexing.overlord.HeapMemoryTaskStorage.java</file>
            <file type="M">org.apache.druid.indexing.overlord.MetadataTaskStorage.java</file>
            <file type="M">org.apache.druid.indexing.overlord.TaskLockbox.java</file>
            <file type="M">org.apache.druid.indexing.overlord.TaskLockboxV1.java</file>
            <file type="M">org.apache.druid.indexing.overlord.TaskLockboxV2.java</file>
            <file type="M">org.apache.druid.indexing.overlord.TaskStorage.java</file>
            <file type="M">org.apache.druid.indexing.overlord.ThreadPoolTaskRunner.java</file>
            <file type="M">org.apache.druid.indexing.common.TaskActionSerdeTest.java</file>
            <file type="M">org.apache.druid.indexing.common.TaskLockSerdeTest.java</file>
<file type="M">org.apache.druid.indexing.common.actions.RemoteTaskActionClientTest.java</file>
            <file type="M">org.apache.druid.indexing.common.actions.SegmentInsertActionTest.java</file>
<file type="M">org.apache.druid.indexing.common.actions.SegmentTransactionalInsertActionTest.java</file>
            <file type="M">org.apache.druid.indexing.common.actions.TaskActionTestKit.java</file>
            <file type="M">org.apache.druid.indexing.common.task.IndexTaskTest.java</file>
            <file type="M">org.apache.druid.indexing.common.task.RealtimeIndexTaskTest.java</file>
            <file type="M">org.apache.druid.indexing.common.task.TaskSerdeTest.java</file>
<file type="M">org.apache.druid.indexing.firehose.IngestSegmentFirehoseFactoryTest.java</file>
            <file type="M">org.apache.druid.indexing.overlord.RealtimeishTask.java</file>
            <file type="M">org.apache.druid.indexing.overlord.TaskLifecycleTest.java</file>
            <file type="M">org.apache.druid.indexing.overlord.TaskLockboxTest.java</file>
            <file type="M">org.apache.druid.indexing.overlord.TestIndexTask.java</file>
            <file type="M">org.apache.druid.metadata.SQLMetadataStorageActionHandler.java</file>
            <file type="M">org.apache.druid.cli.CliPeon.java</file>
        </fixedFiles>
    </bug>
</bugrepository>