<?xml version = "1.0" encoding = "UTF-8" ?>
<bugrepository name="MATH">
	<bug id="60" opendate="2006-05-14 04:20:21" fixdate="2006-06-05 10:16:21" resolution="Fixed">
		<buginformation>
			<summary>[math] Function math.fraction.ProperFractionFormat.parse(String, ParsePosition) return illogical result</summary>
			<description>Hello,
I find illogical returned result from function "Fraction parse(String source, 
ParsePostion pos)" (in class ProperFractionFormat of the Fraction Package) of 
the Commons Math library. Please see the following code segment for more 
details:
"
ProperFractionFormat properFormat = new ProperFractionFormat();
result = null;
String source = "1 -1 / 2";
ParsePosition pos = new ParsePosition(0);
//Test 1 : fail 
public void testParseNegative(){
   String source = "-1 -2 / 3";
   ParsePosition pos = new ParsePosition(0);
   Fraction actual = properFormat.parse(source, pos);
   assertNull(actual);
}
// Test2: success
public void testParseNegative(){
   String source = "-1 -2 / 3";
   ParsePosition pos = new ParsePosition(0);
   Fraction actual = properFormat.parse(source, pos);  // return Fraction 1/3
   assertEquals(1, source.getNumerator());
   assertEquals(3, source.getDenominator());
}
"
Note: Similarly, when I passed in the following inputs: 
  input 2: (source = 1 2 / -3, pos = 0)
  input 3: ( source =  -1 -2 / 3, pos = 0)
Function "Fraction parse(String, ParsePosition)" returned Fraction 1/3 (means 
the result Fraction had numerator = 1 and  denominator = 3)for all 3 inputs 
above.
I think the function does not handle parsing the numberator/ denominator 
properly incase input string provide invalid numerator/denominator. 
Thank you!</description>
			<version>1.1</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.fraction.ProperFractionFormat.java</file>
			<file type="M">org.apache.commons.math.fraction.FractionFormatTest.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">42</link>
		</links>
	</bug>
	<bug id="42" opendate="2006-05-14 05:08:05" fixdate="2006-06-05 10:17:24" resolution="Fixed">
		<buginformation>
			<summary>[math] Function "Fraction math.fraction.FractionFormat.parse(String, ParsePosition) " does not handle parsing the numerater properly</summary>
			<description>Hello,
While testing function "Fraction math.fraction.FractionFormat.parse(String, 
ParsePosition) ", I found it did not handle properly the case the input string 
passed in is incorrect. 
When I passed in a String that represented a Fraction to be parsed, if the 
Fraction embedded in the String had a whole value, while either the numerator 
or denominator had the negative sign, the function just returned 
1/abs(denomitator).
Please see the following code segment for more details:
"   
  NumberFormat nf = null; 
  FractionFormat properFormat = FractionFormat.getProperInstance
(Locale.getDefault());
  FractionFormat improperFormat = FractionFormat.getImproperInstance
(Locale.getDefault());
//Test 1 : fail 
public void testParseNegative(){
   String source = "1 -2 / 3";
   ParsePosition pos = new ParsePosition(0);
   Fraction actual = properFormat.parse(source, pos);
   assertNull(actual);
}
// Test2: success
public void testParseNegative(){
   String source = "1 -2 / 3";
   ParsePosition pos = new ParsePosition(0);
   Fraction actual = properFormat.parse(source, pos);  // return Fraction 1/3
   assertEquals(1, source.getNumerator());
   assertEquals(3, source.getDenominator());
}
"
Note: Similarly, when I passed in the following inputs: 
  input 2: (source = 1 2 / -3, pos = 0)
  input 3: ( source =  -1 -2 / 3, pos = 0)
Function "Fraction parse(String, ParsePosition)" returned Fraction 1/3 (means 
the result Fraction had numerator = 1 and  denominator = 3)for all 3 inputs 
above.
I think the function does not handle parsing the numberator/ denominator 
properly incase input string provides negative numerator or negative 
denominator while there is the whole value.
Thank you!
Nhung</description>
			<version>1.1</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.fraction.ProperFractionFormat.java</file>
			<file type="M">org.apache.commons.math.fraction.FractionFormatTest.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">60</link>
		</links>
	</bug>
	<bug id="151" opendate="2006-06-05 22:22:11" fixdate="2006-07-04 15:19:28" resolution="Fixed">
		<buginformation>
			<summary>MathUtils.round incorrect result</summary>
			<description>MathUtils.round(39.245, 2) results 39.24, however it should be 39.25, with default rounding mode BigDecimal.ROUND_HALF_UP.
I found that internally MathUtils.round multiplies the given number by 10^scale.
 39.245 * 100.0 results 3924.49999...5 , and after this the calculation is not correct any more.</description>
			<version>1.1</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.util.MathUtilsTest.java</file>
			<file type="M">org.apache.commons.math.util.MathUtils.java</file>
		</fixedFiles>
	</bug>
	<bug id="85" opendate="2006-04-28 00:00:55" fixdate="2006-07-06 12:11:34" resolution="Fixed">
		<buginformation>
			<summary>[math]  SimpleRegression getSumSquaredErrors</summary>
			<description>getSumSquaredErrors returns -ve value. See test below:
public void testSimpleRegression() {
		double[] y = 
{  8915.102, 8919.302, 8923.502}
;
		double[] x = 
{ 1.107178495, 1.107264895, 1.107351295}
;
		double[] x2 = 
{ 1.107178495E2, 1.107264895E2, 1.107351295E2}
;
		SimpleRegression reg = new SimpleRegression();
		for (int i = 0; i &amp;lt; x.length; i++) 
{
			reg.addData(x[i],y[i]);
		}
		assertTrue(reg.getSumSquaredErrors() &amp;gt;= 0.0); // OK
		reg.clear();
		for (int i = 0; i &amp;lt; x.length; i++) 
{
			reg.addData(x2[i],y[i]);
		}
		assertTrue(reg.getSumSquaredErrors() &amp;gt;= 0.0); // FAIL
	}</description>
			<version>1.1</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.stat.regression.SimpleRegressionTest.java</file>
			<file type="M">org.apache.commons.math.stat.regression.SimpleRegression.java</file>
		</fixedFiles>
	</bug>
	<bug id="153" opendate="2006-07-31 03:48:24" fixdate="2007-04-05 15:20:45" resolution="Fixed">
		<buginformation>
			<summary>RandomDataImpl nextInt(int, int) nextLong(long, long)</summary>
			<description>RandomDataImpl.nextInt(Integer.MIN_VALUE, Integer.MAX_VALUE) suffers from overflow errors.
change
return lower + (int) (rand.nextDouble() * (upper - lower + 1));
to
return (int) (lower + (long) (rand.nextDouble()*((long) upper - lower + 1)));
additional checks must be made for the nextlong(long, long) method.
At first I thought about using MathUtils.subAndCheck(long, long) but there is only an int version avalible, and the problem is that subAndCheck internaly uses long values to check for overflow just as my previous channge proposes.  The only thing I can think of is using a BigInteger to check for the number of bits required to express the difference.</description>
			<version>1.1</version>
			<fixedVersion>1.2</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.random.RandomDataTest.java</file>
			<file type="M">org.apache.commons.math.random.RandomDataImpl.java</file>
		</fixedFiles>
	</bug>
	<bug id="166" opendate="2007-06-04 09:34:54" fixdate="2007-06-20 22:24:30" resolution="Fixed">
		<buginformation>
			<summary>Special functions not very accurate</summary>
			<description>The Gamma and Beta functions return values in double precision but the default epsilon is set to 10e-9. I think that the default should be set to the highest possible accuracy, as this is what I&amp;amp;apos;d expect to be returned by a double precision routine. Note that the erf function already uses a call to Gamma.regularizedGammaP with an epsilon of 1.0e-15.</description>
			<version>1.1</version>
			<fixedVersion>1.2</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.special.Gamma.java</file>
			<file type="M">org.apache.commons.math.special.GammaTest.java</file>
			<file type="M">org.apache.commons.math.special.Beta.java</file>
			<file type="M">org.apache.commons.math.special.BetaTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="175" opendate="2007-12-04 02:31:53" fixdate="2008-01-09 05:17:45" resolution="Fixed">
		<buginformation>
			<summary>chiSquare(double[] expected, long[] observed) is returning incorrect test statistic</summary>
			<description>ChiSquareTestImpl is returning incorrect chi-squared value. An implicit assumption of public double chiSquare(double[] expected, long[] observed) is that the sum of expected and observed are equal. That is, in the code:
for (int i = 0; i &amp;lt; observed.length; i++) 
{
            dev = ((double) observed[i] - expected[i]);
            sumSq += dev * dev / expected[i];
        }
this calculation is only correct if sum(observed)==sum(expected). When they are not equal then one must rescale the expected value by sum(observed) / sum(expected) so that they are.
Ironically, it is an example in the unit test ChiSquareTestTest that highlights the error:
long[] observed1 = 
{ 500, 623, 72, 70, 31 }
;
        double[] expected1 = 
{ 485, 541, 82, 61, 37 }
;
        assertEquals( "chi-square test statistic", 16.4131070362, testStatistic.chiSquare(expected1, observed1), 1E-10);
        assertEquals("chi-square p-value", 0.002512096, testStatistic.chiSquareTest(expected1, observed1), 1E-9);
16.413 is not correct because the expected values do not make sense, they should be: 521.19403 581.37313  88.11940  65.55224  39.76119 so that the sum of expected equals 1296 which is the sum of observed.
Here is some R code (r-project.org) which proves it:
&amp;gt; o1
[1] 500 623  72  70  31
&amp;gt; e1
[1] 485 541  82  61  37
&amp;gt; chisq.test(o1,p=e1,rescale.p=TRUE)
        Chi-squared test for given probabilities
data:  o1 
X-squared = 9.0233, df = 4, p-value = 0.06052
&amp;gt; chisq.test(o1,p=e1,rescale.p=TRUE)$observed
[1] 500 623  72  70  31
&amp;gt; chisq.test(o1,p=e1,rescale.p=TRUE)$expected
[1] 521.19403 581.37313  88.11940  65.55224  39.76119
</description>
			<version>1.1</version>
			<fixedVersion>1.2</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.stat.inference.ChiSquareTestTest.java</file>
			<file type="M">org.apache.commons.math.stat.inference.TestUtilsTest.java</file>
			<file type="M">org.apache.commons.math.stat.inference.ChiSquareTestImpl.java</file>
		</fixedFiles>
	</bug>
	<bug id="182" opendate="2008-02-01 11:59:39" fixdate="2008-02-01 12:03:18" resolution="Fixed">
		<buginformation>
			<summary>converting some double numbers to Fraction can lead to integer overflows</summary>
			<description>converting 0.75000000001455192 leads to an overflow at the third iteration
 converting 1.0e10 leads to an overflow before the loop (while computiong a0)</description>
			<version>1.2</version>
			<fixedVersion>1.2</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.fraction.Fraction.java</file>
			<file type="M">org.apache.commons.math.fraction.FractionTest.java</file>
			<file type="M">org.apache.commons.math.fraction.FractionConversionException.java</file>
			<file type="M">org.apache.commons.math.MessagesResources_fr.java</file>
		</fixedFiles>
	</bug>
	<bug id="185" opendate="2008-02-02 17:57:05" fixdate="2008-02-02 18:02:08" resolution="Fixed">
		<buginformation>
			<summary>using an empty file to ValueServer in REPLAY_MODE triggers a NULL_POINTER_EXCEPTION</summary>
			<description>when the URL provided to ValueServer.setValuesFileURL() contains no data,
subsequent calls to ValueServer.getNext() in replay mode triggers an exception at the
end of the private method getNextReplay.</description>
			<version>1.2</version>
			<fixedVersion>1.2</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.random.ValueServer.java</file>
			<file type="M">org.apache.commons.math.random.ValueServerTest.java</file>
			<file type="M">org.apache.commons.math.random.EmpiricalDistributionImpl.java</file>
		</fixedFiles>
	</bug>
	<bug id="186" opendate="2008-02-03 22:31:21" fixdate="2008-02-03 22:47:32" resolution="Fixed">
		<buginformation>
			<summary>test results depend on java version</summary>
			<description>Running the tests with maven and changing the java version used by changing JAVA_HOME in the ~/.mavenrc file, I get different results.
With the Eclipse compiler set to 1.3 compatibility and with blackdown jvm (1.4), the tests succed.
With the Sun jvm (1.6), SummaryStatisticsAbstractTest.testEqualsAndHashCode (which is used both by SummaryStatisticsTest and SynchronizedSummaryStatisticsTest) fails.
The error is related to geometric mean computation, which lead to slightly different results depending on the order of added elements. One instance returns 2.213363839400643 and the other returns 2.2133638394006434. Both results are consistent with IEEE754 arithmetic (they differ in the last two bits).
Using Sun 1.6.0_03 jvm, the different values induce a test failure when SummaryStatistics.equals() method is called (it checks for exact equality). If this part of the test is commented out, another failure occurs when the SummaryStatistics.hashcode() method is called.
Changing the equals method would be possible, but would be a change of semantics and would imply choosing some threshold which would never suit everybody needs. Changing the hashcode method simply does not seem realistic to me. So I would like to keep these methods as they are now. So the main conclusion would be that the test is too sensitive to jvm implementation (which are consistent with IEEE754 arithmetic in this case).
I don&amp;amp;apos;t know what to do about this issue.</description>
			<version>1.2</version>
			<fixedVersion>1.2</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.stat.descriptive.SummaryStatisticsAbstractTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="191" opendate="2008-02-09 16:23:08" fixdate="2008-02-09 23:43:24" resolution="Fixed">
		<buginformation>
			<summary>SummaryStatistics computes sum of logs but does not provide access to it</summary>
			<description>A SumOfLogs instances is maintained and incremented by SummaryStatistics, but there is no getSumOfLogs method to report the value of this statistic.</description>
			<version>1.0</version>
			<fixedVersion>1.2</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.stat.descriptive.SummaryStatistics.java</file>
			<file type="M">org.apache.commons.math.stat.descriptive.SummaryStatisticsTest.java</file>
			<file type="M">org.apache.commons.math.stat.descriptive.moment.GeometricMean.java</file>
		</fixedFiles>
	</bug>
	<bug id="184" opendate="2008-02-01 22:17:45" fixdate="2008-02-11 01:05:28" resolution="Fixed">
		<buginformation>
			<summary>cumulativeProbability((double)n, (double)n) returns 0 for integer distributions</summary>
			<description>cumulativeProbability((double)n, (double)n) returns 0 for
discrete/integer distributions
I suppose AbstractIntegerDistribution.cumulativeProbability(double,
double) should be overridden to call its (int, int) version instead of
using default one from AbstractDistribution</description>
			<version>1.1</version>
			<fixedVersion>1.2</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.distribution.AbstractIntegerDistribution.java</file>
			<file type="M">org.apache.commons.math.distribution.IntegerDistributionAbstractTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="198" opendate="2008-03-18 23:06:45" fixdate="2008-03-23 12:23:59" resolution="Fixed">
		<buginformation>
			<summary>java.lang.StringIndexOutOfBoundsException in ComplexFormat.parse(String source, ParsePosition pos)</summary>
			<description>The parse(String source, ParsePosition pos) method in the ComplexFormat class does not check whether the imaginary character is set or not which produces StringIndexOutOfBoundsException in the substring method :
(line 375 of ComplexFormat)
...
        // parse imaginary character
        int n = getImaginaryCharacter().length();
        startIndex = pos.getIndex();
        int endIndex = startIndex + n;
        if (source.substring(startIndex, endIndex).compareTo(
            getImaginaryCharacter()) != 0) {
...
I encoutered this exception typing in a JTextFied with ComplexFormat set to look up an AbstractFormatter.
If only the user types the imaginary part of the complex number first, he gets this exception.
Solution: Before setting to n length of the imaginary character, check if the source contains it. My proposal:
...
        int n = 0;
        if (source.contains(getImaginaryCharacter()))
        n = getImaginaryCharacter().length();
...		 
F.S.</description>
			<version>1.2</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.complex.ComplexFormat.java</file>
			<file type="M">org.apache.commons.math.complex.ComplexFormatAbstractTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="199" opendate="2008-03-20 22:56:33" fixdate="2008-03-23 13:40:12" resolution="Fixed">
		<buginformation>
			<summary>exception in LevenbergMarquardtEstimator</summary>
			<description>I get this exception:
Exception in thread "main" java.lang.ArrayIndexOutOfBoundsException: -1
       at org.apache.commons.math.estimation.LevenbergMarquardtEstimator.qrDecomposition(LevenbergMarquardtEstimator.java:772)
       at org.apache.commons.math.estimation.LevenbergMarquardtEstimator.estimate(LevenbergMarquardtEstimator.java:232)
       at quadraticFitterProblem.QuadraticFitterProblem.&amp;lt;init&amp;gt;(QuadraticFitterProblem.java:27)
       at quadraticFitterProblem.QuadraticFitterProblem.main(QuadraticFitterProblem.java:40)
on the code below.
The exception does not occur all the weights in the quadraticFitter are 0.0;
---------------------------------------------------------------------------------------------
package quadraticFitterProblem;
import org.apache.commons.math.estimation.EstimationException;
import org.apache.commons.math.estimation.LevenbergMarquardtEstimator;
//import org.apache.commons.math.estimation.WeightedMeasurement;
import com.strategicanalytics.dtd.data.smoothers.QuadraticFitter;
public class QuadraticFitterProblem {
       private QuadraticFitter quadraticFitter;
       public QuadraticFitterProblem() {
         // create the uninitialized fitting problem
         quadraticFitter = new QuadraticFitter();
         quadraticFitter.addPoint (0,  -3.182591015485607, 0.0);
         quadraticFitter.addPoint (1,  -2.5581184967730577, 4.4E-323);
         quadraticFitter.addPoint (2,  -2.1488478161387325, 1.0);
         quadraticFitter.addPoint (3,  -1.9122489313410047, 4.4E-323);
         quadraticFitter.addPoint (4,  1.7785661310051026, 0.0);
         try 
{
           // solve the problem, using a Levenberg-Marquardt algorithm with
default settings
           LevenbergMarquardtEstimator estimator = new LevenbergMarquardtEstimator();
           //WeightedMeasurement[] wm = quadraticFitter.getMeasurements();
           estimator.estimate(quadraticFitter);

         }
 catch (EstimationException ee) 
{
               System.err.println(ee.getMessage());
         }
       }
       /**

@param args
        *
        */
       public static void main(String[] args) 
{

                       new QuadraticFitterProblem();
                       System.out.println ("Done.");
       }

}
----------------------------------------------------------------------------------------------
import org.apache.commons.math.estimation.EstimatedParameter;
//import org.apache.commons.math.estimation.EstimationException;
//import org.apache.commons.math.estimation.LevenbergMarquardtEstimator;
import org.apache.commons.math.estimation.SimpleEstimationProblem;
import org.apache.commons.math.estimation.WeightedMeasurement;
public class QuadraticFitter extends SimpleEstimationProblem {
       // y = a x&amp;lt;sup&amp;gt;2&amp;lt;/sup&amp;gt; + b x + c
   private EstimatedParameter a;
   private EstimatedParameter b;
   private EstimatedParameter c;
   /**

constructor
    *
    *Fitter for a quadratic model to a sample of 2D points.
&amp;lt;p&amp;gt;The model is y = a x&amp;lt;sup&amp;gt;2&amp;lt;/sup&amp;gt; + b x + c
its three parameters of the model are a, b and c.&amp;lt;/p&amp;gt;
    */
   public QuadraticFitter() 
{

       // three parameters of the model
       a = new EstimatedParameter("a", 0.0);
       b = new EstimatedParameter("b", 0.0);
       c = new EstimatedParameter("c", 0.0);

       // provide the parameters to the base class which
       // implements the getAllParameters and getUnboundParameters methods
       addParameter(a);
       addParameter(b);
       addParameter(c);
   }

   /**

Add a sample point
    *
@param x abscissa
@param y ordinate
@param w weight
    */
   public void addPoint(double x, double y, double w) 
{
       addMeasurement(new LocalMeasurement(x, y, w));
   }

   /**

Get the value of the quadratic coefficient.
    *
@return the value of a for the quadratic model
y = a x&amp;lt;sup&amp;gt;2&amp;lt;/sup&amp;gt; + b x + c
    */
   public double getA() 
{
       return a.getEstimate();
   }

   /**

Get the value of the linear coefficient.
    *
@return the value of b for the quadratic model
y = a x&amp;lt;sup&amp;gt;2&amp;lt;/sup&amp;gt; + b x + c
    */
   public double getB() 
{
       return b.getEstimate();
   }

   /**

Get the value of the constant coefficient.
    *
@return the value of ac for the quadratic model
y = a x&amp;lt;sup&amp;gt;2&amp;lt;/sup&amp;gt; + b x + c
    */
   public double getC() 
{
       return c.getEstimate();
   }

   /**

Get the theoretical value of the model for some x.
&amp;lt;p&amp;gt;The theoretical value is the value computed using
the current state of the problem parameters.&amp;lt;/p&amp;gt;
    *
Note the use of Hrner&amp;amp;apos;s method (synthetic division) for
evaluating polynomials,
(more efficient)
    *
@param x explanatory variable
@return the theoretical value y = a x&amp;lt;sup&amp;gt;2&amp;lt;/sup&amp;gt; + b x + c
    */
   public double theoreticalValue(double x) 
{
       //System.out.println ("x = " + x + "  a.getEstimate() = " +
a.getEstimate() + "  b.getEstimate() = " + b.getEstimate() + "
c.getEstimate() = " + c.getEstimate());
       return ( (a.getEstimate() * x + b.getEstimate() ) * x +
c.getEstimate());
   }

   /**

Get the partial derivative of the theoretical value
of the model for some x.
&amp;lt;p&amp;gt;The derivative is computed using
the current state of the problem parameters.&amp;lt;/p&amp;gt;
    *
@param x explanatory variable
@param parameter estimated parameter (either a, b, or c)
@return the partial derivative dy/dp
    */
   private double partial(double x, EstimatedParameter parameter) {
       // since we know the only parameters are a, b and c in this
       // class we simply use "==" for efficiency
       if (parameter == a) 
{
           return x * x;
       }
 else if (parameter == b) 
{
           return x;
       }
 else 
{
           return 1.0;
       }

   }
   /** Internal measurements class.

&amp;lt;p&amp;gt;The measurement is the y value for a fixed specified x.&amp;lt;/p&amp;gt;
    */
   private class LocalMeasurement extends WeightedMeasurement {

       static final long serialVersionUID = 1;
       private final double x;
       // constructor
       public LocalMeasurement(double x, double y, double w) 
{
           super(w, y);
           this.x = x;
       }

       public double getTheoreticalValue() 
{
           // the value is provided by the model for the local x
           return theoreticalValue(x);
       }

       public double getPartial(EstimatedParameter parameter) 
{
           // the value is provided by the model for the local x
           return partial(x, parameter);
       }

   }
 }</description>
			<version>1.2</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.estimation.LevenbergMarquardtEstimator.java</file>
			<file type="M">org.apache.commons.math.estimation.LevenbergMarquardtEstimatorTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="200" opendate="2008-03-25 22:22:21" fixdate="2008-03-28 20:11:06" resolution="Fixed">
		<buginformation>
			<summary>AbstractEstimator: getCovariances() and guessParametersErrors() crash when having bound parameters</summary>
			<description>the two methods getCovariances() and guessParametersErrors() from org.apache.commons.math.estimation.AbstractEstimator crash with ArrayOutOfBounds exception when some of the parameters are bound. The reason is that the Jacobian is calculated only for the unbound parameters. in the code you loop through all parameters.
line #166: final int cols = problem.getAllParameters().length;
should be replaced by:  final int cols = problem.getUnboundParameters().length;
(similar changes could be done in guessParametersErrors())
the dissadvantage of the above bug fix is that what is returned to the user is an array with smaller size than the number of all parameters. Alternatively, you can have some logic in the code which writes zeros for the elements of the covariance matrix corresponding to the bound parameters</description>
			<version>1.2</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.estimation.AbstractEstimator.java</file>
			<file type="M">org.apache.commons.math.estimation.GaussNewtonEstimatorTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="201" opendate="2008-04-04 16:31:20" fixdate="2008-04-06 01:24:56" resolution="Fixed">
		<buginformation>
			<summary>T-test p-value precision hampered by machine epsilon</summary>
			<description>The smallest p-value returned by TTestImpl.tTest() is the machine epsilon, which is 2.220446E-16 with IEEE754 64-bit double precision floats.
We found this bug porting some analysis software from R to java, and noticed that the p-values did not match up.  We believe we&amp;amp;apos;ve identified why this is happening in commons-math-1.2, and a possible solution.
Please be gentle, as I am not a statistics expert!
The following method in org.apache.commons.math.stat.inference.TTestImpl currently implements the following method to calculate the p-value for a 2-sided, 2-sample t-test:
protected double tTest(double m1, double m2, double v1, double v2,  double n1, double n2)
and it returns:
        1.0 - distribution.cumulativeProbability(-t, t);
at line 1034 in version 1.2.
double cumulativeProbability(double x0, double x1) is implemented by org.apache.commons.math.distribution.AbstractDisstribution, and returns:
        return cumulativeProbability(x1) - cumulativeProbability(x0);
So in essence, the p-value returned by TTestImpl.tTest() is:
1.0 - (cumulativeProbability(t) - cumulativeProbabily(-t))
For large-ish t-statistics, cumulativeProbabilty(-t) can get quite small, and cumulativeProbabilty(t) can get very close to 1.0.  When cumulativeProbability(-t) is less than the machine epsilon, we get p-values equal to zero because:
1.0 - 1.0 + 0.0 = 0.0
An alternative calculation for the p-value of a 2-sided, 2-sample t-test is:
p = 2.0 * cumulativeProbability(-t)
This calculation does not suffer from the machine epsilon problem, and we are now getting p-values much smaller than the 2.2E-16 limit we were seeing previously.</description>
			<version>1.2</version>
			<fixedVersion>2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.stat.inference.TTestImpl.java</file>
		</fixedFiles>
	</bug>
	<bug id="204" opendate="2008-05-06 20:33:57" fixdate="2008-05-07 13:37:07" resolution="Fixed">
		<buginformation>
			<summary>BrentSolver throws IllegalArgumentException </summary>
			<description>I am getting this exception:
java.lang.IllegalArgumentException: Function values at endpoints do not have different signs.  Endpoints: [-100000.0,1.7976931348623157E308]  Values: [0.0,-101945.04630982173]
at org.apache.commons.math.analysis.BrentSolver.solve(BrentSolver.java:99)
at org.apache.commons.math.analysis.BrentSolver.solve(BrentSolver.java:62)
The exception should not be thrown with values  [0.0,-101945.04630982173] because 0.0 is positive.
According to Brent Worden, the algorithm should stop and return 0 as the root instead of throwing an exception.
The problem comes from this method:
    public double solve(double min, double max) throws MaxIterationsExceededException, 
        FunctionEvaluationException {
        clearResult();
        verifyInterval(min, max);
        double yMin = f.value(min);
        double yMax = f.value(max);
        // Verify bracketing
        if (yMin * yMax &amp;gt;= 0) 
{
            throw new IllegalArgumentException
            ("Function values at endpoints do not have different signs." +
                    "  Endpoints: [" + min + "," + max + "]" + 
                    "  Values: [" + yMin + "," + yMax + "]");       
        }

        // solve using only the first endpoint as initial guess
        return solve(min, yMin, max, yMax, min, yMin);
    }
One way to fix it would be to add this code after the assignment of yMin and yMax:
        if (yMin ==0 || yMax == 0) 
{
        	return 0;
       	}</description>
			<version>1.2</version>
			<fixedVersion>2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.transform.FastCosineTransformer.java</file>
			<file type="M">org.apache.commons.math.analysis.BrentSolver.java</file>
			<file type="M">org.apache.commons.math.analysis.BrentSolverTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="205" opendate="2008-05-14 15:27:39" fixdate="2008-05-15 19:56:46" resolution="Fixed">
		<buginformation>
			<summary>Mathematical error in comment for FastCosineTransformer</summary>
			<description>The formula in the comments for transform() method of FastCosineTransformer has the form:
F_n = (1/2) [f_0 + (-1)^n f_N] + \Sum_
{k=0}
^
{N-1} f_k \cos(\pi nk/N)

This is incorrect and should be

F_n = (1/2) [f_0 + (-1)^n f_N] + \Sum_{k=1}^{N-1}
 f_k \cos(\pi nk/N)</description>
			<version>1.2</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.transform.FastCosineTransformer.java</file>
			<file type="M">org.apache.commons.math.analysis.BrentSolver.java</file>
			<file type="M">org.apache.commons.math.analysis.BrentSolverTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="209" opendate="2008-06-17 03:02:48" fixdate="2008-06-17 19:10:53" resolution="Fixed">
		<buginformation>
			<summary>RealMatrixImpl#operate gets result vector dimensions wrong</summary>
			<description>org.apache.commons.math.linear.RealMatrixImpl#operate tries to create a result vector that always has the same length as the input vector. This can result in runtime exceptions if the matrix is non-square and it always yields incorrect results if the matrix is non-square. The correct behaviour would of course be to create a vector with the same length as the row dimension of the matrix.
Thus line 640 in RealMatrixImpl.java should read
double[] out = new double[nRows];
instead of
double[] out = new double[v.length];</description>
			<version>1.2</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.linear.RealMatrixImplTest.java</file>
			<file type="M">org.apache.commons.math.linear.BigMatrixImpl.java</file>
			<file type="M">org.apache.commons.math.linear.RealMatrixImpl.java</file>
			<file type="M">org.apache.commons.math.linear.BigMatrixImplTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="214" opendate="2008-07-10 12:34:24" fixdate="2008-07-10 12:43:08" resolution="Fixed">
		<buginformation>
			<summary>fixed step Runge-Kutta integrators slightly change step size</summary>
			<description>When a fixed step Runge-Kutta integrator is used, it may slightly change the step size for all integration range.
This is due to a step recomputation feature which ensures the last step ends exactly at the end of the range.
This feature should be removed, since it is unnatural and does not obey users choices.
It should be replaced by the same kind of solution already adopted for discrete events: truncating only the last step.</description>
			<version>1.2</version>
			<fixedVersion>2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.ode.AbstractIntegrator.java</file>
			<file type="M">org.apache.commons.math.ode.nonstiff.GillIntegratorTest.java</file>
			<file type="M">org.apache.commons.math.ode.nonstiff.MidpointIntegratorTest.java</file>
			<file type="M">org.apache.commons.math.ode.nonstiff.ThreeEighthesIntegratorTest.java</file>
			<file type="M">org.apache.commons.math.ode.nonstiff.ClassicalRungeKuttaIntegratorTest.java</file>
			<file type="M">org.apache.commons.math.ode.nonstiff.RungeKuttaIntegrator.java</file>
			<file type="M">org.apache.commons.math.ode.nonstiff.EulerIntegratorTest.java</file>
			<file type="M">org.apache.commons.math.ode.nonstiff.StepInterpolatorAbstractTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="221" opendate="2008-08-29 13:31:56" fixdate="2008-08-29 15:52:38" resolution="Fixed">
		<buginformation>
			<summary>Result of multiplying and equals for complex numbers is wrong</summary>
			<description>Hi.
The bug relates on complex numbers.
The methods "multiply" and "equals" of the class Complex are involved.
mathematic background:  (0,i) * (-1,0i) = (0,-i).
little java program + output that shows the bug:
-----------------------------------------------------------------------

import org.apache.commons.math.complex.*;
public class TestProg {
        public static void main(String[] args) {

                ComplexFormat f = new ComplexFormat();
                Complex c1 = new Complex(0,1);
                Complex c2 = new Complex(-1,0);

                Complex res = c1.multiply(c2);
                Complex comp = new Complex(0,-1);

                System.out.println("res:  "+f.format(res));
                System.out.println("comp: "+f.format(comp));

                System.out.println("res=comp: "+res.equals(comp));
        }
}


-----------------------------------------------------------------------
res:  -0 - 1i
comp: 0 - 1i
res=comp: false
-----------------------------------------------------------------------
I think the "equals" should return "true".
The problem could either be the "multiply" method that gives (-0,-1i) instead of (0,-1i),
or if you think thats right, the equals method has to be modified.
Good Luck
Dieter</description>
			<version>1.2</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.linear.RealMatrixImpl.java</file>
			<file type="M">org.apache.commons.math.geometry.Vector3D.java</file>
			<file type="M">org.apache.commons.math.linear.RealVectorImpl.java</file>
			<file type="M">org.apache.commons.math.complex.Complex.java</file>
			<file type="M">org.apache.commons.math.complex.ComplexTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="226" opendate="2008-09-16 20:08:48" fixdate="2008-09-16 21:05:59" resolution="Fixed">
		<buginformation>
			<summary>CorrelatedRandomVectorGenerator generates invariant samples</summary>
			<description>For the following code sample, the output is:
1.0,3.9432161557722925,16.66859790068678,1.0743673824292688,
1.0,-1.4103098147521094,-2.670854139636077,1.8368602953644368,
1.0,0.230029048125738,12.67864233710285,0.1124537698401884,
Why is the first column of each row fixed at 1.0?
Here is the code that generated this:
import org.apache.commons.math.linear.RealMatrix;
import org.apache.commons.math.linear.RealMatrixImpl;
import org.apache.commons.math.random.CorrelatedRandomVectorGenerator;
import org.apache.commons.math.random.GaussianRandomGenerator;
import org.apache.commons.math.random.JDKRandomGenerator;
public class TestMath {
	public static void sampler(double[] mean, double[][] cov, double[][] s) {
		RealMatrix covRM = new RealMatrixImpl(cov);
		try {
			CorrelatedRandomVectorGenerator sg = new CorrelatedRandomVectorGenerator(
					mean, covRM, 0.00001, new GaussianRandomGenerator(
							new JDKRandomGenerator()));
			for (int i = 0; i &amp;lt; s.length; i++) 
{
				s[i] = sg.nextVector();
			}
		} catch (Exception e) 
{
			e.printStackTrace();
			System.exit(-1);
		}

	}
	static void print(double[][] s) {
		for (int r = 0; r &amp;lt; s.length; r++) 
{
			for (int c = 0; c &amp;lt; s[r].length; c++)
				System.out.print(s[r][c] + ",");
			System.out.println();

		}

	}
	public static void main(String[] args) {
		double[] mean = 
{ 1, 1, 10, 1 }
;
		double[][] cov = { 
{ 1, 3, 2, 6 }
, 
{ 3, 13, 16, 2 }
, 
{ 2, 16, 38, -1 }
,

{ 6, 2, -1, 197 }
 };
		double[][] s = new double[3][4];
		TestMath.sampler(mean, cov, s);
		TestMath.print(s);
	}
}</description>
			<version>1.2</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.random.CorrelatedRandomVectorGenerator.java</file>
			<file type="M">org.apache.commons.math.random.CorrelatedRandomVectorGeneratorTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="227" opendate="2008-09-25 08:23:07" fixdate="2008-09-26 03:11:58" resolution="Fixed">
		<buginformation>
			<summary>denominatorDegreeOfFreedom in FDistribution leads to IllegalArgumentsException in UnivariateRealSolverUtils.bracket</summary>
			<description>We are using the FDistributionImpl from the commons.math project to do
some statistical calculations, namely receiving the upper and lower
boundaries of a confidence interval. Everything is working fine and the
results are matching our reference calculations.
However, the FDistribution behaves strange if a
denominatorDegreeOfFreedom of 2 is used, with an alpha-value of 0.95.
This results in an IllegalArgumentsException, stating:
Invalid endpoint parameters:  lowerBound=0.0 initial=Infinity
upperBound=1.7976931348623157E308
coming from
org.apache.commons.math.analysis.UnivariateRealSolverUtils.bracket
The problem is the &amp;amp;apos;initial&amp;amp;apos; parameter to that function, wich is
POSITIVE_INFINITY and therefore not within the boundaries. I already
pinned down the problem to the FDistributions getInitialDomain()-method,
wich goes like:
        return getDenominatorDegreesOfFreedom() /
                    (getDenominatorDegreesOfFreedom() - 2.0);
Obviously, in case of denominatorDegreesOfFreedom == 2, this must lead
to a division-by-zero, resulting in POSTIVE_INFINITY. The result of this
operation is then directly passed into the
UnivariateRealSolverUtils.bracket() - method as second argument.</description>
			<version>1.2</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.distribution.FDistributionTest.java</file>
			<file type="M">org.apache.commons.math.distribution.FDistributionImpl.java</file>
		</fixedFiles>
	</bug>
	<bug id="238" opendate="2009-01-16 22:24:24" fixdate="2009-01-16 23:07:17" resolution="Fixed">
		<buginformation>
			<summary>MathUtils.gcd(u, v) fails when u and v both contain a high power of 2</summary>
			<description>The test at the beginning of MathUtils.gcd(u, v) for arguments equal to zero fails when u and v contain high enough powers of 2 so that their product overflows to zero.
        assertEquals(3 * (1&amp;lt;&amp;lt;15), MathUtils.gcd(3 * (1&amp;lt;&amp;lt;20), 9 * (1&amp;lt;&amp;lt;15)));
Fix: Replace the test at the start of MathUtils.gcd()
        if (u * v == 0) {
by
        if (u == 0 || v == 0) {</description>
			<version>2.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.util.MathUtilsTest.java</file>
			<file type="M">org.apache.commons.math.util.MathUtils.java</file>
		</fixedFiles>
	</bug>
	<bug id="240" opendate="2009-01-16 22:43:04" fixdate="2009-01-19 19:43:52" resolution="Fixed">
		<buginformation>
			<summary>MathUtils.factorial(n) fails for n &gt;= 17</summary>
			<description>The result of MathUtils.factorial( n ) for n = 17, 18, 19 is wrong, probably because of rounding errors in the double calculations.
Replace the first line of MathUtilsTest.testFactorial() by
        for (int i = 1; i &amp;lt;= 20; i++) {
to check all valid arguments for the long result and see the failure.
I suggest implementing a simple loop to multiply the long result - or even using a precomputed long[] - instead of adding logarithms.</description>
			<version>2.0</version>
			<fixedVersion>2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.util.MathUtilsTest.java</file>
			<file type="M">org.apache.commons.math.util.MathUtils.java</file>
		</fixedFiles>
	</bug>
	<bug id="241" opendate="2009-01-16 23:34:23" fixdate="2009-01-19 23:53:48" resolution="Fixed">
		<buginformation>
			<summary>MathUtils.binomialCoefficient(n,k) fails for large results</summary>
			<description>Probably due to rounding errors, MathUtils.binomialCoefficient(n,k) fails for results near Long.MAX_VALUE.
The existence of failures can be demonstrated by testing the recursive property:

         assertEquals(MathUtils.binomialCoefficient(65,32) + MathUtils.binomialCoefficient(65,33),
                 MathUtils.binomialCoefficient(66,33));


Or by directly using the (externally calculated and hopefully correct) expected value:

         assertEquals(7219428434016265740L, MathUtils.binomialCoefficient(66,33));


I suggest a nonrecursive test implementation along the lines of
MathUtilsTest.java
    /**
     * Exact implementation using BigInteger and the explicit formula
     * (n, k) == ((k-1)*...*n) / (1*...*(n-k))
     */
	public static long binomialCoefficient(int n, int k) {
		if (k == 0 || k == n)
			return 1;
		BigInteger result = BigInteger.ONE;
		for (int i = k + 1; i &amp;lt;= n; i++) {
			result = result.multiply(BigInteger.valueOf(i));
		}
		for (int i = 1; i &amp;lt;= n - k; i++) {
			result = result.divide(BigInteger.valueOf(i));
		}
		if (result.compareTo(BigInteger.valueOf(Long.MAX_VALUE)) &amp;gt; 0) {
			throw new ArithmeticException(
                                "Binomial coefficient overflow: " + n + ", " + k);
		}
		return result.longValue();
	}


Which would allow you to test the expected values directly:

         assertEquals(binomialCoefficient(66,33), MathUtils.binomialCoefficient(66,33));

</description>
			<version>2.0</version>
			<fixedVersion>2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.util.MathUtilsTest.java</file>
			<file type="M">org.apache.commons.math.util.MathUtils.java</file>
		</fixedFiles>
	</bug>
	<bug id="252" opendate="2009-03-29 16:50:35" fixdate="2009-03-29 16:52:34" resolution="Fixed">
		<buginformation>
			<summary>Fraction.comparTo returns 0 for some differente fractions</summary>
			<description>If two different fractions evaluate to the same double due to limited precision,
the compareTo methode returns 0 as if they were identical.


// value is roughly PI - 3.07e-18
Fraction pi1 = new Fraction(1068966896, 340262731);

// value is roughly PI + 1.936e-17
Fraction pi2 = new Fraction( 411557987, 131002976);

System.out.println(pi1.doubleValue() - pi2.doubleValue()); // exactly 0.0 due to limited IEEE754 precision
System.out.println(pi1.compareTo(pi2)); // display 0 instead of a negative value

</description>
			<version>1.2</version>
			<fixedVersion>2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.fraction.Fraction.java</file>
			<file type="M">org.apache.commons.math.fraction.FractionTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="272" opendate="2009-05-30 01:01:37" fixdate="2009-06-02 19:38:46" resolution="Fixed">
		<buginformation>
			<summary>Simplex Solver arrives at incorrect solution</summary>
			<description>I have reduced the problem reported to me down to a minimal test case which I will attach.</description>
			<version>2.0</version>
			<fixedVersion>2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.optimization.linear.SimplexSolverTest.java</file>
			<file type="M">org.apache.commons.math.optimization.linear.SimplexTableau.java</file>
		</fixedFiles>
	</bug>
	<bug id="273" opendate="2009-06-03 05:11:34" fixdate="2009-06-03 09:07:06" resolution="Fixed">
		<buginformation>
			<summary>Basic variable is not found correctly in simplex tableau</summary>
			<description>The last patch to SimplexTableau caused an automated test suite I&amp;amp;apos;m running at work to go down a new code path and uncover what is hopefully the last bug remaining in the Simplex code.
SimplexTableau was assuming an entry in the tableau had to be nonzero to indicate a basic variable, which is incorrect - the entry should have a value equal to 1.</description>
			<version>2.0</version>
			<fixedVersion>2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.optimization.linear.SimplexSolverTest.java</file>
			<file type="M">org.apache.commons.math.optimization.linear.SimplexTableau.java</file>
		</fixedFiles>
	</bug>
	<bug id="274" opendate="2009-06-04 19:24:12" fixdate="2009-06-04 20:11:26" resolution="Fixed">
		<buginformation>
			<summary>testing for symmetric positive definite matrix in CholeskyDecomposition</summary>
			<description>I used this matrix:
        double[][] cv = {

{0.40434286, 0.09376327, 0.30328980, 0.04909388}
,

{0.09376327, 0.10400408, 0.07137959, 0.04762857}
,

{0.30328980, 0.07137959, 0.30458776, 0.04882449},
            {0.04909388, 0.04762857, 0.04882449, 0.07543265}
        };

And it works fine, because it is symmetric positive definite

I tried this matrix:

        double[][] cv = {
            {0.40434286, -0.09376327, 0.30328980, 0.04909388},
            {-0.09376327, 0.10400408, 0.07137959, 0.04762857},
            {0.30328980, 0.07137959, 0.30458776, 0.04882449}
,
            {0.04909388, 0.04762857, 0.04882449, 0.07543265}
        };
And it should throw an exception but it does not.  I tested the matrix in R and R&amp;amp;apos;s cholesky decomposition method returns that the matrix is not symmetric positive definite.
Obviously your code is not catching this appropriately.
By the way (in my opinion) the use of exceptions to check these conditions is not the best design or use for exceptions.  If you are going to force the use to try and catch these exceptions at least provide methods  to test the conditions prior to the possibility of the exception.  
</description>
			<version>2.0</version>
			<fixedVersion>2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.linear.CholeskyDecompositionImplTest.java</file>
			<file type="M">org.apache.commons.math.linear.CholeskyDecompositionImpl.java</file>
		</fixedFiles>
	</bug>
	<bug id="197" opendate="2008-03-17 11:55:26" fixdate="2009-06-21 15:10:50" resolution="Fixed">
		<buginformation>
			<summary>RandomDataImpl.nextPoisson() is extreme slow for large lambdas</summary>
			<description>The RandomDataImpl.nextPoisson() is extreme slow for large lambdas:
E.g. drawing 100 random numbers with lambda = 1000 takes around 10s on my dual core with 2.2GHz.
With lambda smaller than 500 everything is fine. Any ideas?
    RandomDataImpl r = new RandomDataImpl();
    r.reSeed(101);
    int d = 100;
    long poissonLambda = 1000;
    long st = System.currentTimeMillis();
    for (int row = 0; row &amp;lt; d; row++) 
{
      long nxtRnd = r.nextPoisson(poissonLambda);
    }
    System.out.println("delta " + (System.currentTimeMillis() - st));</description>
			<version>1.0</version>
			<fixedVersion>2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.random.RandomDataTest.java</file>
			<file type="M">org.apache.commons.math.random.RandomDataImpl.java</file>
		</fixedFiles>
	</bug>
	<bug id="207" opendate="2008-05-21 21:50:37" fixdate="2009-06-30 00:44:50" resolution="Fixed">
		<buginformation>
			<summary>Implementation of GeneticAlgorithm.nextGeneration() is wrong</summary>
			<description>The implementation of GeneticAlgorithm.nextGeneration() is wrong, since the only way how a Chromosome can get into the new generation is by mutation. 
Enclosed, I am sending a patch for this.</description>
			<version>2.0</version>
			<fixedVersion>2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.genetics.GeneticAlgorithm.java</file>
			<file type="M">org.apache.commons.math.genetics.SelectionPolicy.java</file>
			<file type="M">org.apache.commons.math.genetics.Fitness.java</file>
			<file type="M">org.apache.commons.math.genetics.ChromosomePair.java</file>
			<file type="M">org.apache.commons.math.genetics.Population.java</file>
			<file type="M">org.apache.commons.math.genetics.CrossoverPolicy.java</file>
			<file type="M">org.apache.commons.math.genetics.StoppingCondition.java</file>
			<file type="M">org.apache.commons.math.genetics.MutationPolicy.java</file>
			<file type="M">org.apache.commons.math.genetics.Chromosome.java</file>
		</fixedFiles>
	</bug>
	<bug id="279" opendate="2009-06-20 07:13:00" fixdate="2009-07-05 13:31:29" resolution="Fixed">
		<buginformation>
			<summary>MultipleLinearRegression - test for minimum number of samples</summary>
			<description>It&amp;amp;apos;s currently possible to pass in so few rows (samples) that there isn&amp;amp;apos;t a row for each column (predictor).  Does this look like the right thing to do?


Index: AbstractMultipleLinearRegression.java
===================================================================
--- AbstractMultipleLinearRegression.java       (revision 786758)
+++ AbstractMultipleLinearRegression.java       (working copy)
@@ -91,6 +91,9 @@
                   "dimension mismatch {0} != {1}",
                   (x == null) ? 0 : x.length,
                   (y == null) ? 0 : y.length);
+        } else if (x[0].length &amp;gt; x.length){
+            throw MathRuntimeException.createIllegalArgumentException(
+                    "not enough data (" + x.length + " rows) for this many predictors (" + x[0].length + " predictors)");
         }
     }
 

</description>
			<version>2.0</version>
			<fixedVersion>2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.stat.regression.GLSMultipleLinearRegressionTest.java</file>
			<file type="M">org.apache.commons.math.stat.regression.AbstractMultipleLinearRegression.java</file>
			<file type="M">org.apache.commons.math.MessagesResources_fr.java</file>
		</fixedFiles>
	</bug>
	<bug id="280" opendate="2009-07-06 21:26:57" fixdate="2009-07-07 09:21:22" resolution="Fixed">
		<buginformation>
			<summary>bug in inverseCumulativeProbability() for Normal Distribution</summary>
			<description>

@version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $
 */
public class NormalDistributionImpl extends AbstractContinuousDistribution 


@version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $
 */
public abstract class AbstractContinuousDistribution

This code:
        	DistributionFactory factory = app.getDistributionFactory();
        	NormalDistribution normal = factory.createNormalDistribution(0,1);
        	double result = normal.inverseCumulativeProbability(0.9772498680518209);
gives the exception below. It should return (approx) 2.0000...
normal.inverseCumulativeProbability(0.977249868051820); works fine
These also give errors:
0.9986501019683698 (should return 3.0000...)
0.9999683287581673 (should return 4.0000...)
org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0
	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103)
	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)
</description>
			<version>1.2</version>
			<fixedVersion>2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.analysis.solvers.UnivariateRealSolver.java</file>
			<file type="M">org.apache.commons.math.analysis.solvers.UnivariateRealSolverUtilsTest.java</file>
			<file type="M">org.apache.commons.math.distribution.NormalDistributionTest.java</file>
			<file type="M">org.apache.commons.math.analysis.solvers.UnivariateRealSolverUtils.java</file>
			<file type="M">org.apache.commons.math.distribution.AbstractContinuousDistribution.java</file>
		</fixedFiles>
	</bug>
	<bug id="283" opendate="2009-08-12 14:51:15" fixdate="2009-08-14 19:25:15" resolution="Fixed">
		<buginformation>
			<summary>MultiDirectional optimzation loops forver if started at the correct solution</summary>
			<description>MultiDirectional.iterateSimplex loops forever if the starting point is the correct solution.
see the attached test case (testMultiDirectionalCorrectStart) as an example.</description>
			<version>2.0</version>
			<fixedVersion>2.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.optimization.direct.MultiDirectional.java</file>
			<file type="M">org.apache.commons.math.optimization.direct.MultiDirectionalTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="288" opendate="2009-08-24 22:31:11" fixdate="2009-08-25 18:10:08" resolution="Fixed">
		<buginformation>
			<summary>SimplexSolver not working as expected 2</summary>
			<description>SimplexSolver didn&amp;amp;apos;t find the optimal solution.
Program for Lpsolve:
=====================
/* Objective function */
max: 7 a 3 b;
/* Constraints */
R1: +3 a -5 c &amp;lt;= 0;
R2: +2 a -5 d &amp;lt;= 0;
R3: +2 b -5 c &amp;lt;= 0;
R4: +3 b -5 d &amp;lt;= 0;
R5: +3 a +2 b &amp;lt;= 5;
R6: +2 a +3 b &amp;lt;= 5;
/* Variable bounds */
a &amp;lt;= 1;
b &amp;lt;= 1;
=====================
Results(correct): a = 1, b = 1, value = 10
Program for SimplexSolve:
=====================
LinearObjectiveFunction kritFcia = new LinearObjectiveFunction(new double[]
{7, 3, 0, 0}
, 0);
Collection&amp;lt;LinearConstraint&amp;gt; podmienky = new ArrayList&amp;lt;LinearConstraint&amp;gt;();
podmienky.add(new LinearConstraint(new double[]
{1, 0, 0, 0}
, Relationship.LEQ, 1));
podmienky.add(new LinearConstraint(new double[]
{0, 1, 0, 0}
, Relationship.LEQ, 1));
podmienky.add(new LinearConstraint(new double[]
{3, 0, -5, 0}
, Relationship.LEQ, 0));
podmienky.add(new LinearConstraint(new double[]
{2, 0, 0, -5}
, Relationship.LEQ, 0));
podmienky.add(new LinearConstraint(new double[]
{0, 2, -5, 0}
, Relationship.LEQ, 0));
podmienky.add(new LinearConstraint(new double[]
{0, 3, 0, -5}
, Relationship.LEQ, 0));
podmienky.add(new LinearConstraint(new double[]
{3, 2, 0, 0}
, Relationship.LEQ, 5));
podmienky.add(new LinearConstraint(new double[]
{2, 3, 0, 0}
, Relationship.LEQ, 5));
SimplexSolver solver = new SimplexSolver();
RealPointValuePair result = solver.optimize(kritFcia, podmienky, GoalType.MAXIMIZE, true);
=====================
Results(incorrect): a = 1, b = 0.5, value = 8.5
P.S. I used the latest software from the repository (including MATH-286 fix).</description>
			<version>2.1</version>
			<fixedVersion>2.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.optimization.linear.SimplexSolverTest.java</file>
			<file type="M">org.apache.commons.math.optimization.linear.SimplexSolver.java</file>
		</fixedFiles>
	</bug>
	<bug id="290" opendate="2009-08-25 10:53:33" fixdate="2009-08-27 08:08:17" resolution="Fixed">
		<buginformation>
			<summary>NullPointerException in SimplexTableau.initialize</summary>
			<description>SimplexTableau throws a NullPointerException when no solution can be found instead of a NoFeasibleSolutionException
Here is the code that causes the NullPointerException:
LinearObjectiveFunction f = new LinearObjectiveFunction(new double[] 
{ 1, 5 }
, 0 );
Collection&amp;lt;LinearConstraint&amp;gt; constraints = new ArrayList&amp;lt;LinearConstraint&amp;gt;();
constraints.add(new LinearConstraint(new double[] 
{ 2, 0 }
, Relationship.GEQ, -1.0));
RealPointValuePair solution = new SimplexSolver().optimize(f, constraints, GoalType.MINIMIZE, true);
Note: Tested both with Apache Commons Math 2.0 release and SVN trunk</description>
			<version>2.0</version>
			<fixedVersion>2.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.optimization.linear.SimplexSolverTest.java</file>
			<file type="M">org.apache.commons.math.optimization.linear.SimplexTableau.java</file>
		</fixedFiles>
	</bug>
	<bug id="292" opendate="2009-09-06 22:04:34" fixdate="2009-09-09 10:12:01" resolution="Fixed">
		<buginformation>
			<summary>TestUtils.assertRelativelyEquals() generates misleading error on failure</summary>
			<description>TestUtils.assertRelativelyEquals() generates misleading error on failure.
For example:
TestUtils.assertRelativelyEquals(1.0, 0.10427661385154971, 1.0e-9)
generates the error message:
junit.framework.AssertionFailedError: expected:&amp;lt;0.0&amp;gt; but was:&amp;lt;0.8957233861484503&amp;gt;
which is not very helpful.</description>
			<version>2.0</version>
			<fixedVersion>2.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.TestUtils.java</file>
		</fixedFiles>
	</bug>
	<bug id="286" opendate="2009-08-20 15:04:09" fixdate="2009-09-10 08:22:11" resolution="Fixed">
		<buginformation>
			<summary>SimplexSolver not working as expected?</summary>
			<description>I guess (but I could be wrong) that SimplexSolver does not always return the optimal solution, nor satisfies all the constraints...
Consider this LP:
max: 0.8 x0 + 0.2 x1 + 0.7 x2 + 0.3 x3 + 0.6 x4 + 0.4 x5;
r1: x0 + x2 + x4 = 23.0;
r2: x1 + x3 + x5 = 23.0;
r3: x0 &amp;gt;= 10.0;
r4: x2 &amp;gt;= 8.0;
r5: x4 &amp;gt;= 5.0;
LPSolve returns 25.8, with x0 = 10.0, x1 = 0.0, x2 = 8.0, x3 = 0.0, x4 = 5.0, x5 = 23.0;
The same LP expressed in Apache commons math is:
LinearObjectiveFunction f = new LinearObjectiveFunction(new double[] 
{ 0.8, 0.2, 0.7, 0.3, 0.6, 0.4 }
, 0 );
Collection&amp;lt;LinearConstraint&amp;gt; constraints = new ArrayList&amp;lt;LinearConstraint&amp;gt;();
constraints.add(new LinearConstraint(new double[] 
{ 1, 0, 1, 0, 1, 0 }
, Relationship.EQ, 23.0));
constraints.add(new LinearConstraint(new double[] 
{ 0, 1, 0, 1, 0, 1 }
, Relationship.EQ, 23.0));
constraints.add(new LinearConstraint(new double[] 
{ 1, 0, 0, 0, 0, 0 }
, Relationship.GEQ, 10.0));
constraints.add(new LinearConstraint(new double[] 
{ 0, 0, 1, 0, 0, 0 }
, Relationship.GEQ, 8.0));
constraints.add(new LinearConstraint(new double[] 
{ 0, 0, 0, 0, 1, 0 }
, Relationship.GEQ, 5.0));
RealPointValuePair solution = new SimplexSolver().optimize(f, constraints, GoalType.MAXIMIZE, true);
that returns 22.20, with x0 = 15.0, x1 = 23.0, x2 = 8.0, x3 = 0.0, x4 = 0.0, x5 = 0.0;
Is it possible SimplexSolver is buggy that way? The returned value is 22.20 instead of 25.8, and the last constraint (x4 &amp;gt;= 5.0) is not satisfied...
Am I using the interface wrongly?</description>
			<version>2.0</version>
			<fixedVersion>2.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.optimization.linear.SimplexSolverTest.java</file>
			<file type="M">org.apache.commons.math.optimization.linear.SimplexTableauTest.java</file>
			<file type="M">org.apache.commons.math.optimization.linear.SimplexTableau.java</file>
			<file type="M">org.apache.commons.math.optimization.linear.SimplexSolver.java</file>
		</fixedFiles>
	</bug>
	<bug id="293" opendate="2009-09-09 14:07:53" fixdate="2009-09-10 08:23:11" resolution="Fixed">
		<buginformation>
			<summary>Matrix&amp;apos;s "OutOfBoundException" in SimplexSolver</summary>
			<description>Hi all,
This bug is somehow related to incident MATH-286, but not necessarily...
Let&amp;amp;apos;s say I have an LP and I solve it using SimplexSolver. Then I create a second LP similar to the first one, but with "stronger" constraints. The second LP has the following properties:

the only point in the feasible region for the second LP is the solution returned for the first LP
the solution returned for the first LP is also the (only possible) solution to the second LP

This shows the problem:


LinearObjectiveFunction f = new LinearObjectiveFunction(new double[] { 0.8, 0.2, 0.7, 0.3, 0.4, 0.6}, 0 );
Collection&amp;lt;LinearConstraint&amp;gt; constraints = new ArrayList&amp;lt;LinearConstraint&amp;gt;();
constraints.add(new LinearConstraint(new double[] { 1, 0, 1, 0, 1, 0 }, Relationship.EQ, 30.0));
constraints.add(new LinearConstraint(new double[] { 0, 1, 0, 1, 0, 1 }, Relationship.EQ, 30.0));
constraints.add(new LinearConstraint(new double[] { 0.8, 0.2, 0.0, 0.0, 0.0, 0.0 }, Relationship.GEQ, 10.0));
constraints.add(new LinearConstraint(new double[] { 0.0, 0.0, 0.7, 0.3, 0.0, 0.0 }, Relationship.GEQ, 10.0));
constraints.add(new LinearConstraint(new double[] { 0.0, 0.0, 0.0, 0.0, 0.4, 0.6 }, Relationship.GEQ, 10.0));

RealPointValuePair solution = new SimplexSolver().optimize(f, constraints, GoalType.MAXIMIZE, true);

double valA = 0.8 * solution.getPoint()[0] + 0.2 * solution.getPoint()[1];
double valB = 0.7 * solution.getPoint()[2] + 0.3 * solution.getPoint()[3];
double valC = 0.4 * solution.getPoint()[4] + 0.6 * solution.getPoint()[5];

f = new LinearObjectiveFunction(new double[] { 0.8, 0.2, 0.7, 0.3, 0.4, 0.6}, 0 );
constraints = new ArrayList&amp;lt;LinearConstraint&amp;gt;();
constraints.add(new LinearConstraint(new double[] { 1, 0, 1, 0, 1, 0 }, Relationship.EQ, 30.0));
constraints.add(new LinearConstraint(new double[] { 0, 1, 0, 1, 0, 1 }, Relationship.EQ, 30.0));
constraints.add(new LinearConstraint(new double[] { 0.8, 0.2, 0.0, 0.0, 0.0, 0.0 }, Relationship.GEQ, valA));
constraints.add(new LinearConstraint(new double[] { 0.0, 0.0, 0.7, 0.3, 0.0, 0.0 }, Relationship.GEQ, valB));
constraints.add(new LinearConstraint(new double[] { 0.0, 0.0, 0.0, 0.0, 0.4, 0.6 }, Relationship.GEQ, valC));

solution = new SimplexSolver().optimize(f, constraints, GoalType.MAXIMIZE, true);


Instead of returning the solution, SimplexSolver throws an Exception:

 Exception in thread "main" org.apache.commons.math.linear.MatrixIndexException: no entry at indices (0, 7) in a 6x7 matrix
	at org.apache.commons.math.linear.Array2DRowRealMatrix.getEntry(Array2DRowRealMatrix.java:356)
	at org.apache.commons.math.optimization.linear.SimplexTableau.getEntry(SimplexTableau.java:408)
	at org.apache.commons.math.optimization.linear.SimplexTableau.getBasicRow(SimplexTableau.java:258)
	at org.apache.commons.math.optimization.linear.SimplexTableau.getSolution(SimplexTableau.java:336)
	at org.apache.commons.math.optimization.linear.SimplexSolver.doOptimize(SimplexSolver.java:182)
	at org.apache.commons.math.optimization.linear.AbstractLinearOptimizer.optimize(AbstractLinearOptimizer.java:106)

I was too optimistic with the bug MATH-286 </description>
			<version>2.0</version>
			<fixedVersion>2.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.optimization.linear.SimplexSolverTest.java</file>
			<file type="M">org.apache.commons.math.optimization.linear.SimplexTableau.java</file>
		</fixedFiles>
	</bug>
	<bug id="298" opendate="2009-09-20 14:46:42" fixdate="2009-09-21 01:32:21" resolution="Fixed">
		<buginformation>
			<summary>EmpiriicalDisributionImpl.getUpperBounds does not return upper bounds on data bins</summary>
			<description>Per the javadoc, the getUpperBounds method in the EmpiricalDistribution should return upper bounds for the bins used in computing the empirical distribution and the bin statistics.  What the method actually returns is the upper bounds of the subintervals of [0,1] used in generating data following the empirical distribution.</description>
			<version>1.0</version>
			<fixedVersion>2.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.random.EmpiricalDistributionImpl.java</file>
		</fixedFiles>
	</bug>
	<bug id="294" opendate="2009-09-11 17:34:29" fixdate="2009-10-12 02:08:12" resolution="Fixed">
		<buginformation>
			<summary>RandomDataImpl.nextPoisson fails for means in range 6.0 - 19.99</summary>
			<description>math.random.RandomDataImpl.nextPoisson(double mean) fails frequently (but not always) for values of mean between 6.0 and 19.99 inclusive. For values below 6.0 (where I see there is a branch in the logic) and above 20.0 it seems to be okay (though I&amp;amp;apos;ve only randomly sampled the space and run a million trials for the values I&amp;amp;apos;ve tried)
When it fails, the exception is as follows (this for a mean of 6.0)
org.apache.commons.math.MathRuntimeException$4: must have n &amp;gt;= 0 for n!, got n = -2
	at org.apache.commons.math.MathRuntimeException.createIllegalArgumentException(MathRuntimeException.java:282)
	at org.apache.commons.math.util.MathUtils.factorialLog(MathUtils.java:561)
	at org.apache.commons.math.random.RandomDataImpl.nextPoisson(RandomDataImpl.java:434) 
ie MathUtils.factorialLog is being called with a negative input
To reproduce:
    JDKRandomGenerator random = new JDKRandomGenerator();
    random.setSeed(123456);
    RandomData randomData = new RandomDataImpl(random);
    for (int i=0; i&amp;lt; 1000000; i++)
{
        randomData.nextPoisson(6.0);
    }</description>
			<version>1.2</version>
			<fixedVersion>2.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.random.RandomDataImpl.java</file>
			<file type="M">org.apache.commons.math.random.RandomDataTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="306" opendate="2009-10-24 14:23:13" fixdate="2009-10-27 01:33:42" resolution="Fixed">
		<buginformation>
			<summary>Method &amp;apos;divide&amp;apos; in class &amp;apos;Complex&amp;apos; uses a false formula for a special case resulting in erroneous division by zero.</summary>
			<description>The formula that &amp;amp;apos;divide&amp;amp;apos; wants to implement is
( a + bi )  /  ( c + di )  =  ( ac + bd + ( bc - ad ) i )  /  ( c^2 + d^2 )
as correctly written in the description.
When c == 0.0 this leads to the special case
( a + bi )  /  di  = ( b / d ) - ( a / d ) i
But the corresponding code is:
if (c == 0.0) 
{
    return createComplex(imaginary/d, -real/c);
}

The bug is the last division -real/c, which should obviously be -real/d.</description>
			<version>1.1</version>
			<fixedVersion>2.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.complex.Complex.java</file>
			<file type="M">org.apache.commons.math.complex.ComplexTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="309" opendate="2009-10-27 13:20:53" fixdate="2009-10-31 02:33:15" resolution="Fixed">
		<buginformation>
			<summary>nextExponential parameter check bug - patch supplied</summary>
			<description>Index: src/main/java/org/apache/commons/math/random/RandomDataImpl.java
===================================================================
 src/main/java/org/apache/commons/math/random/RandomDataImpl.java	(revision 830102)
+++ src/main/java/org/apache/commons/math/random/RandomDataImpl.java	(working copy)
@@ -462,7 +462,7 @@

@return the random Exponential value
      */
     public double nextExponential(double mean) {


if (mean &amp;lt; 0.0) {
+        if (mean &amp;lt;= 0.0) 
Unknown macro: {             throw MathRuntimeException.createIllegalArgumentException(                   "mean must be positive ({0})", mean);         } 

</description>
			<version>1.0</version>
			<fixedVersion>2.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.random.RandomDataImpl.java</file>
			<file type="M">org.apache.commons.math.random.RandomDataTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="308" opendate="2009-10-25 22:25:23" fixdate="2009-11-03 22:19:04" resolution="Fixed">
		<buginformation>
			<summary>ArrayIndexOutOfBoundException in EigenDecompositionImpl</summary>
			<description>The following test triggers an ArrayIndexOutOfBoundException:


    public void testMath308() {

        double[] mainTridiagonal = {
            22.330154644539597, 46.65485522478641, 17.393672330044705, 54.46687435351116, 80.17800767709437
        };
        double[] secondaryTridiagonal = {
            13.04450406501361, -5.977590941539671, 2.9040909856707517, 7.1570352792841225
        };

        // the reference values have been computed using routine DSTEMR
        // from the fortran library LAPACK version 3.2.1
        double[] refEigenValues = {
            14.138204224043099, 18.847969733754262, 52.536278520113882, 53.456697699894512, 82.044413207204002
        };
        RealVector[] refEigenVectors = {
            new ArrayRealVector(new double[] {  0.584677060845929, -0.367177264979103, -0.721453187784497,  0.052971054621812, -0.005740715188257 }),
            new ArrayRealVector(new double[] {  0.713933751051495, -0.190582113553930,  0.671410443368332, -0.056056055955050,  0.006541576993581 }),
            new ArrayRealVector(new double[] {  0.222368839324646,  0.514921891363332, -0.021377019336614,  0.801196801016305, -0.207446991247740 }),
            new ArrayRealVector(new double[] {  0.314647769490148,  0.750806415553905, -0.167700312025760, -0.537092972407375,  0.143854968127780 }),
            new ArrayRealVector(new double[] { -0.000462690386766, -0.002118073109055,  0.011530080757413,  0.252322434584915,  0.967572088232592 })
        };

        // the following line triggers the exception
        EigenDecomposition decomposition =
            new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);

        double[] eigenValues = decomposition.getRealEigenvalues();
        for (int i = 0; i &amp;lt; refEigenValues.length; ++i) {
            assertEquals(refEigenValues[i], eigenValues[i], 1.0e-6);
            if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) &amp;lt; 0) {
                assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-6);
            } else {
                assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-6);
            }
        }

    }


Running the previous method as a Junit test triggers the exception when the EigenDecompositionImpl instance is built. The first few lines of the stack trace are:

java.lang.ArrayIndexOutOfBoundsException: -1
	at org.apache.commons.math.linear.EigenDecompositionImpl.computeShiftIncrement(EigenDecompositionImpl.java:1545)
	at org.apache.commons.math.linear.EigenDecompositionImpl.goodStep(EigenDecompositionImpl.java:1072)
	at org.apache.commons.math.linear.EigenDecompositionImpl.processGeneralBlock(EigenDecompositionImpl.java:894)
	at org.apache.commons.math.linear.EigenDecompositionImpl.findEigenvalues(EigenDecompositionImpl.java:658)
	at org.apache.commons.math.linear.EigenDecompositionImpl.decompose(EigenDecompositionImpl.java:246)
	at org.apache.commons.math.linear.EigenDecompositionImpl.&amp;lt;init&amp;gt;(EigenDecompositionImpl.java:205)
	at org.apache.commons.math.linear.EigenDecompositionImplTest.testMath308(EigenDecompositionImplTest.java:136)


I&amp;amp;apos;m currently investigating this bug. It is not a simple index translation error between the original fortran (Lapack) and commons-math implementation.</description>
			<version>2.0</version>
			<fixedVersion>2.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.linear.EigenDecompositionImpl.java</file>
			<file type="M">org.apache.commons.math.linear.EigenDecompositionImplTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="318" opendate="2009-11-06 15:09:36" fixdate="2009-11-06 15:12:47" resolution="Fixed">
		<buginformation>
			<summary>wrong result in eigen decomposition</summary>
			<description>Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0


    public void testMathpbx02() {

        double[] mainTridiagonal = {
        	  7484.860960227216, 18405.28129035345, 13855.225609560746,
        	 10016.708722343366, 559.8117399576674, 6750.190788301587, 
        	    71.21428769782159
        };
        double[] secondaryTridiagonal = {
        	 -4175.088570476366,1975.7955858241994,5193.178422374075, 
        	  1995.286659169179,75.34535882933804,-234.0808002076056
        };

        // the reference values have been computed using routine DSTEMR
        // from the fortran library LAPACK version 3.2.1
        double[] refEigenValues = {
        		20654.744890306974412,16828.208208485466457,
        		6893.155912634994820,6757.083016675340332,
        		5887.799885688558788,64.309089923240379,
        		57.992628792736340
        };
        RealVector[] refEigenVectors = {
        		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),
        		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),
        		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),
        		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),
        		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),
        		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),
        		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})
        };

        // the following line triggers the exception
        EigenDecomposition decomposition =
            new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);

        double[] eigenValues = decomposition.getRealEigenvalues();
        for (int i = 0; i &amp;lt; refEigenValues.length; ++i) {
            assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);
            if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) &amp;lt; 0) {
                assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);
            } else {
                assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);
            }
        }

    }

</description>
			<version>2.0</version>
			<fixedVersion>2.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.linear.EigenDecompositionImpl.java</file>
			<file type="M">org.apache.commons.math.linear.EigenDecompositionImplTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="305" opendate="2009-10-22 06:35:08" fixdate="2009-11-27 21:46:44" resolution="Fixed">
		<buginformation>
			<summary>NPE in  KMeansPlusPlusClusterer unittest</summary>
			<description>When running this unittest, I am facing this NPE:
java.lang.NullPointerException
	at org.apache.commons.math.stat.clustering.KMeansPlusPlusClusterer.assignPointsToClusters(KMeansPlusPlusClusterer.java:91)
This is the unittest:
package org.fao.fisheries.chronicles.calcuation.cluster;
import static org.junit.Assert.assertEquals;
import static org.junit.Assert.assertTrue;
import java.util.Arrays;
import java.util.List;
import java.util.Random;
import org.apache.commons.math.stat.clustering.Cluster;
import org.apache.commons.math.stat.clustering.EuclideanIntegerPoint;
import org.apache.commons.math.stat.clustering.KMeansPlusPlusClusterer;
import org.fao.fisheries.chronicles.input.CsvImportProcess;
import org.fao.fisheries.chronicles.input.Top200Csv;
import org.junit.Test;
public class ClusterAnalysisTest {
	@Test
	public void testPerformClusterAnalysis2() {
		KMeansPlusPlusClusterer&amp;lt;EuclideanIntegerPoint&amp;gt; transformer = new KMeansPlusPlusClusterer&amp;lt;EuclideanIntegerPoint&amp;gt;(
				new Random(1746432956321l));
		EuclideanIntegerPoint[] points = new EuclideanIntegerPoint[] {
				new EuclideanIntegerPoint(new int[] 
{ 1959, 325100 }
),
				new EuclideanIntegerPoint(new int[] 
{ 1960, 373200 }
), };
		List&amp;lt;Cluster&amp;lt;EuclideanIntegerPoint&amp;gt;&amp;gt; clusters = transformer.cluster(Arrays.asList(points), 1, 1);
		assertEquals(1, clusters.size());
	}
}</description>
			<version>2.0</version>
			<fixedVersion>2.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.stat.clustering.KMeansPlusPlusClustererTest.java</file>
			<file type="M">org.apache.commons.math.util.MathUtils.java</file>
		</fixedFiles>
	</bug>
	<bug id="322" opendate="2009-12-06 23:01:35" fixdate="2009-12-06 23:06:04" resolution="Fixed">
		<buginformation>
			<summary>during ODE integration, the last event in a pair of very close event may not be detected</summary>
			<description>When an events follows a previous one very closely, it may be ignored. The occurrence of the bug depends on the side of the bracketing interval that was selected. For example consider a switching function that is increasing around first event around t = 90, reaches its maximum and is decreasing around the second event around t = 135. If an integration step spans from 67.5 and 112.5, the switching function values at start and end of step will  have opposite signs, so the first event will be detected. The solver will find the event really occurs at 90.0 and will therefore truncate the step at 90.0. The next step will start from where the first step ends, i.e. it will start at 90.0. Let&amp;amp;apos;s say this step spans from 90.0 to 153.0. The switching function switches once again in this step.
If the solver for the first event converged to a value slightly before 90.0 (say 89.9999999), then the switch will not be detected because g(89.9999999) and g(153.0) are both negative.
This bug was introduced as of r781157 (2009-06-02) when special handling of events very close to step start was added.</description>
			<version>2.0</version>
			<fixedVersion>2.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.ode.events.EventState.java</file>
		</fixedFiles>
	</bug>
	<bug id="326" opendate="2009-12-29 00:09:20" fixdate="2009-12-29 12:26:59" resolution="Fixed">
		<buginformation>
			<summary>getLInfNorm() uses wrong formula in both ArrayRealVector and OpenMapRealVector (in different ways)</summary>
			<description>the L_infinity norm of a finite dimensional vector is just the max of the absolute value of its entries.
The current implementation in ArrayRealVector has a typo:


    public double getLInfNorm() {
        double max = 0;
        for (double a : data) {
            max += Math.max(max, Math.abs(a));
        }
        return max;
    }


the += should just be an =.
There is sadly a unit test assuring us that this is the correct behavior (effectively a regression-only test, not a test for correctness).
Worse, the implementation in OpenMapRealVector is not even positive semi-definite:

   
    public double getLInfNorm() {
        double max = 0;
        Iterator iter = entries.iterator();
        while (iter.hasNext()) {
            iter.advance();
            max += iter.value();
        }
        return max;
    }


I would suggest that this method be moved up to the AbstractRealVector superclass and implemented using the sparseIterator():


  public double getLInfNorm() {
    double norm = 0;
    Iterator&amp;lt;Entry&amp;gt; it = sparseIterator();
    Entry e;
    while(it.hasNext() &amp;amp;&amp;amp; (e = it.next()) != null) {
      norm = Math.max(norm, Math.abs(e.getValue()));
    }
    return norm;
  }


Unit tests with negative valued vectors would be helpful to check for this kind of thing in the future.</description>
			<version>2.0</version>
			<fixedVersion>2.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.linear.ArrayRealVectorTest.java</file>
			<file type="M">org.apache.commons.math.linear.SparseRealVectorTest.java</file>
			<file type="M">org.apache.commons.math.linear.AbstractRealVector.java</file>
			<file type="M">org.apache.commons.math.linear.OpenMapRealVector.java</file>
			<file type="M">org.apache.commons.math.linear.ArrayRealVector.java</file>
		</fixedFiles>
	</bug>
	<bug id="260" opendate="2009-04-14 13:38:52" fixdate="2009-12-30 20:12:52" resolution="Fixed">
		<buginformation>
			<summary>Inconsistent API in Frequency</summary>
			<description>The overloaded Frequency methods are not consistent in the parameter types that they handle.
addValue() has an Integer version which converts the parameter to a Long, and then calls addValue(Object).
The various getxxx() methods all handle Integer parameters as an Object.
Seems to me that it would be better to treat Integer consistently.
But perhaps there is a good reason for having an addValue(Integer) method but no getxxx(Integer) methods?
If so, then it would be helpful to document this.</description>
			<version>2.0</version>
			<fixedVersion>2.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.stat.Frequency.java</file>
		</fixedFiles>
	</bug>
	<bug id="320" opendate="2009-11-10 15:42:34" fixdate="2009-12-31 17:55:04" resolution="Fixed">
		<buginformation>
			<summary>NaN singular value from SVD</summary>
			<description>The following jython code
Start code
from org.apache.commons.math.linear import *
Alist = [[1.0, 2.0, 3.0],[2.0,3.0,4.0],[3.0,5.0,7.0]]
A = Array2DRowRealMatrix(Alist)
decomp = SingularValueDecompositionImpl(A)
print decomp.getSingularValues()
End code
prints
array(&amp;amp;apos;d&amp;amp;apos;, [11.218599757513008, 0.3781791648535976, nan])
The last singular value should be something very close to 0 since the matrix
is rank deficient.  When i use the result from getSolver() to solve a system, i end 
up with a bunch of NaNs in the solution.  I assumed i would get back a least squares solution.
Does this SVD implementation require that the matrix be full rank?  If so, then i would expect
an exception to be thrown from the constructor or one of the methods.
</description>
			<version>2.0</version>
			<fixedVersion>2.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.linear.SingularValueSolverTest.java</file>
			<file type="M">org.apache.commons.math.linear.SingularValueDecompositionImpl.java</file>
			<file type="M">org.apache.commons.math.linear.SingularValueDecomposition.java</file>
		</fixedFiles>
	</bug>
	<bug id="307" opendate="2009-10-24 14:36:24" fixdate="2010-01-24 10:35:39" resolution="Fixed">
		<buginformation>
			<summary>BigReal/Fieldelement divide without setting a proper scale -&gt; exception: no exact representable decimal result</summary>
			<description>BigReal implements the methode divide of Fieldelement. The problem is that there is no scale defined for the BigDecimal so the class will throw an error when the outcome is not a representable decimal result. 
(Exception: no exact representable decimal result)
The workaround for me was to copy the BigReal and set the scale and roundingMode the same as version 1.2.
Maybe is it possible to set the scale in FieldMatrix and implements it also a divide(BigReal b, int scale, int roundMode) ?? 
</description>
			<version>2.0</version>
			<fixedVersion>2.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.util.BigReal.java</file>
		</fixedFiles>
	</bug>
	<bug id="297" opendate="2009-09-20 14:33:07" fixdate="2010-01-25 17:58:04" resolution="Fixed">
		<buginformation>
			<summary>Eigenvector computation incorrectly returning vectors of NaNs</summary>
			<description>As reported by Axel Kramer on commons-dev, the following test case succeeds, but should fail:


public void testEigenDecomposition() {
    double[][] m = { { 0.0, 1.0, -1.0 }, { 1.0, 1.0, 0.0 }, { -1.0,0.0, 1.0 } };
    RealMatrix rm = new Array2DRowRealMatrix(m);
    assertEquals(rm.toString(),
        "Array2DRowRealMatrix{{0.0,1.0,-1.0},{1.0,1.0,0.0},{-1.0,0.0,1.0}}");
    EigenDecompositionImpl ed = new EigenDecompositionImpl(rm,
        MathUtils.SAFE_MIN);
    RealVector rv0 = ed.getEigenvector(0);
    assertEquals(rv0.toString(), "{(NaN); (NaN); (NaN)}");
  }


ed.getRealEigenvalues() returns the correct eigenvalues (2, 1, -1), but all three eigenvectors contain only NaNs.</description>
			<version>2.0</version>
			<fixedVersion>2.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.linear.EigenDecompositionImpl.java</file>
		</fixedFiles>
	</bug>
	<bug id="296" opendate="2009-09-16 21:14:20" fixdate="2010-01-26 21:54:45" resolution="Fixed">
		<buginformation>
			<summary>LoessInterpolator.smooth() not working correctly</summary>
			<description>I have been comparing LoessInterpolator.smooth output with the loessFit output from R (R-project.org, probably the most widely used loess implementation) and have had strangely different numbers. I have created a small set to test the difference and something seems to be wrong with the smooth method but I do no know what and I do not understand the code.
Example 1


x-input: 
1.5
 3.0
 6
 8
 12
13
 22
 24
28
31


y-input: 
3.1
6.1
3.1
2.1
1.4
5.1
5.1
6.1
7.1
7.2


Output LoessInterpolator.smooth():
NaN
NaN
NaN
NaN
NaN
NaN
NaN
NaN
NaN
NaN


Output from loessFit() from R: 
3.191178027520974
3.0407201231474037
2.7089538903778636
2.7450823274490297
4.388011000549519
4.60078952381848
5.2988217587114805
5.867536388457898
6.7797794777879705
7.444888598397342


Example 2 (same x-values, y-values just floored)


x-input: 
1.5
 3.0
 6
 8
 12
13
 22
 24
28
31


y-input: 
3
6
3
2
1
5
5
6
7
7


Output LoessInterpolator.smooth(): 
3
6
3
2
0.9999999999999005
5.0000000000001705
5
5.999999999999972
7
6.999999999999967


Output from loessFit() from R: 
3.091423927353068
2.9411521572524237
2.60967950675505
2.7421759322272248
4.382996912300442
4.646774316632562
5.225153658563424
5.768301917477015
6.637079139313073
7.270482144410326


As you see the output is practically the replicated y-input.
At this point this funtionality is critical for us but I could not find any other suitable java-implementation. Help. Maybe this strange behaviour gives someone a clue?
</description>
			<version>2.0</version>
			<fixedVersion>2.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.analysis.interpolation.LoessInterpolator.java</file>
			<file type="M">org.apache.commons.math.analysis.interpolation.LoessInterpolatorTest.java</file>
			<file type="M">org.apache.commons.math.MessagesResources_fr.java</file>
		</fixedFiles>
	</bug>
	<bug id="338" opendate="2010-01-28 10:59:13" fixdate="2010-01-28 15:17:46" resolution="Fixed">
		<buginformation>
			<summary>Wrong parameter for first step size guess for Embedded Runge Kutta methods</summary>
			<description>In a space application using DOP853 i detected what seems to be a bad parameter in the call to the method  initializeStep of class AdaptiveStepsizeIntegrator.
Here, DormandPrince853Integrator is a subclass for EmbeddedRungeKuttaIntegrator which perform the call to initializeStep at the beginning of its method integrate(...)
The problem comes from the array "scale" that is used as a parameter in the call off initializeStep(..)
Following the theory described by Hairer in his book "Solving Ordinary Differential Equations 1 : Nonstiff Problems", the scaling should be :
sci = Atol i + |y0i| * Rtoli
Whereas EmbeddedRungeKuttaIntegrator uses :  sci = Atoli
Note that the Gragg-Bulirsch-Stoer integrator uses the good implementation "sci = Atol i + |y0i| * Rtoli  " when he performs the call to the same method initializeStep(..)
In the method initializeStep, the error leads to a wrong step size h used to perform an  Euler step. Most of the time it is unvisible for the user.
But in my space application the Euler step with this wrong step size h (much bigger than it should be)  makes an exception occur (my satellite hits the ground...)
To fix the bug, one should use the same algorithm as in the rescale method in GraggBulirschStoerIntegrator
For exemple :
 final double[] scale= new double[y0.length];;
          if (vecAbsoluteTolerance == null) {
              for (int i = 0; i &amp;lt; scale.length; ++i) 
{
                final double yi = Math.max(Math.abs(y0[i]), Math.abs(y0[i]));
                scale[i] = scalAbsoluteTolerance + scalRelativeTolerance * yi;
              }
            } else {
              for (int i = 0; i &amp;lt; scale.length; ++i) 
{
                final double yi = Math.max(Math.abs(y0[i]), Math.abs(y0[i]));
                scale[i] = vecAbsoluteTolerance[i] + vecRelativeTolerance[i] * yi;
              }
            }
          hNew = initializeStep(equations, forward, getOrder(), scale,
                           stepStart, y, yDotK[0], yTmp, yDotK[1]);
Sorry for the length of this message, looking forward to hearing from you soon
Vincent Morand
</description>
			<version>2.0</version>
			<fixedVersion>2.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.ode.nonstiff.EmbeddedRungeKuttaIntegrator.java</file>
			<file type="M">org.apache.commons.math.ode.nonstiff.HighamHall54IntegratorTest.java</file>
			<file type="M">org.apache.commons.math.ode.nonstiff.AdamsMoultonIntegratorTest.java</file>
			<file type="M">org.apache.commons.math.ode.nonstiff.HighamHall54StepInterpolatorTest.java</file>
			<file type="M">org.apache.commons.math.ode.nonstiff.DormandPrince853IntegratorTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="341" opendate="2010-02-05 20:32:59" fixdate="2010-02-09 20:08:51" resolution="Fixed">
		<buginformation>
			<summary>Test for firsst Derivative in PolynomialFunction ERROR</summary>
			<description>I have written the attached test using our data for generating a curve function
However the first derivative test fails see: testfirstDerivativeComparisonFullPower
Either my test is in error or there is a bug in PolynomialFunction class.
Roger Ball
Creoss Business Solutions </description>
			<version>2.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.analysis.polynomials.PolynomialFunctionTest.java</file>
			<file type="M">org.apache.commons.math.analysis.polynomials.PolynomialFunctionLagrangeForm.java</file>
		</fixedFiles>
	</bug>
	<bug id="343" opendate="2010-02-23 20:21:00" fixdate="2010-02-23 21:02:31" resolution="Fixed">
		<buginformation>
			<summary>Brent solver doesn&amp;apos;t throw IllegalArgumentException when initial guess has the wrong sign</summary>
			<description>Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn&amp;amp;apos;t even checked.</description>
			<version>2.0</version>
			<fixedVersion>2.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.analysis.solvers.BrentSolver.java</file>
			<file type="M">org.apache.commons.math.analysis.solvers.BrentSolverTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="344" opendate="2010-02-23 20:23:21" fixdate="2010-02-23 21:10:00" resolution="Fixed">
		<buginformation>
			<summary>Brent solver returns the wrong value if either bracket endpoint is root</summary>
			<description>The solve(final UnivariateRealFunction f, final double min, final double max, final double initial) function returns yMin or yMax if min or max are deemed to be roots, respectively, instead of min or max.</description>
			<version>2.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.analysis.solvers.BrentSolverTest.java</file>
			<file type="M">org.apache.commons.math.analysis.solvers.BrentSolver.java</file>
		</fixedFiles>
	</bug>
	<bug id="347" opendate="2010-02-25 14:33:30" fixdate="2010-03-01 19:39:28" resolution="Fixed">
		<buginformation>
			<summary>Brent solver shouldn&amp;apos;t need strict ordering of min, max and initial</summary>
			<description>The "solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" function calls verifySequence() which enforces a strict ordering of min, max and initial parameters. I can&amp;amp;apos;t see why that is necessary - the rest of solve() seems to be able to handle "initial == min" and "initial == min" cases just fine. In fact, the JavaDoc suggests setting initial to min when not known but that is not possible at the moment.</description>
			<version>2.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.analysis.solvers.BrentSolver.java</file>
		</fixedFiles>
	</bug>
	<bug id="282" opendate="2009-08-07 18:24:07" fixdate="2010-03-08 23:00:26" resolution="Fixed">
		<buginformation>
			<summary>ChiSquaredDistributionImpl.cumulativeProbability &gt; 1</summary>
			<description>Calling 
new ChiSquaredDistributionImpl(1.0).cumulativeProbability(66.41528551683048)
returns 1.000000000000004, which is bogus (should never be &amp;gt; 1)</description>
			<version>1.0</version>
			<fixedVersion>2.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.random.RandomDataTest.java</file>
			<file type="M">org.apache.commons.math.distribution.PoissonDistributionTest.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">301</link>
		</links>
	</bug>
	<bug id="358" opendate="2010-03-24 17:25:37" fixdate="2010-03-24 22:13:19" resolution="Fixed">
		<buginformation>
			<summary>ODE integrator goes past specified end of integration range</summary>
			<description>End of integration range in ODE solving is handled as an event.
In some cases, numerical accuracy in events detection leads to error in events location.
The following test case shows the end event is not handled properly and an integration that should cover a 60s range in fact covers a 160s range, more than twice the specified range.


  public void testMissedEvent() throws IntegratorException, DerivativeException {
          final double t0 = 1878250320.0000029;
          final double t =  1878250379.9999986;
          FirstOrderDifferentialEquations ode = new FirstOrderDifferentialEquations() {
            
            public int getDimension() {
                return 1;
            }
            
            public void computeDerivatives(double t, double[] y, double[] yDot)
                throws DerivativeException {
                yDot[0] = y[0] * 1.0e-6;
            }
        };

        DormandPrince853Integrator integrator = new DormandPrince853Integrator(0.0, 100.0,
                                                                               1.0e-10, 1.0e-10);

        double[] y = { 1.0 };
        integrator.setInitialStepSize(60.0);
        double finalT = integrator.integrate(ode, t0, y, t, y);
        Assert.assertEquals(t, finalT, 1.0e-6);
    }


</description>
			<version>2.0</version>
			<fixedVersion>2.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.ode.nonstiff.ClassicalRungeKuttaIntegratorTest.java</file>
			<file type="M">org.apache.commons.math.ode.nonstiff.DormandPrince853IntegratorTest.java</file>
			<file type="M">org.apache.commons.math.ode.nonstiff.EmbeddedRungeKuttaIntegrator.java</file>
			<file type="M">org.apache.commons.math.ode.nonstiff.AdamsMoultonIntegrator.java</file>
			<file type="M">org.apache.commons.math.ode.nonstiff.AdamsBashforthIntegrator.java</file>
			<file type="M">org.apache.commons.math.ode.nonstiff.RungeKuttaIntegrator.java</file>
		</fixedFiles>
	</bug>
	<bug id="365" opendate="2010-04-20 14:21:20" fixdate="2010-04-21 14:35:53" resolution="Fixed">
		<buginformation>
			<summary>Issue with "SmoothingBicubicSplineInterpolator"</summary>
			<description>I figured out that the name of this class is misleading as the implementation doesn&amp;amp;apos;t perform the intended smoothing.
In order to solve this issue, I propose to:

deprecate the "SmoothingBicubicSplineInterpolator" class
create a "BicubicSplineInterpolator" class (similar to the above class but with the useless code removed)
remove the "SmoothingBicubicSplineInterpolatorTest" class
add a "BicubicSplineInterpolatorTest" with essentially the same contents as the above one

Then I would also add a new "SmoothingPolynomialBicubicSplineInterpolator" where I used the "PolynomialFitter" class to smooth the input data along both dimensions before the interpolating function is computed.
Does someone object to these changes?</description>
			<version>2.1</version>
			<fixedVersion>2.2</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.optimization.fitting.PolynomialFitter.java</file>
			<file type="M">org.apache.commons.math.analysis.BivariateRealFunction.java</file>
			<file type="M">org.apache.commons.math.analysis.interpolation.SmoothingBicubicSplineInterpolatorTest.java</file>
			<file type="M">org.apache.commons.math.analysis.interpolation.SmoothingBicubicSplineInterpolator.java</file>
			<file type="M">org.apache.commons.math.analysis.interpolation.BivariateRealGridInterpolator.java</file>
		</fixedFiles>
	</bug>
	<bug id="369" opendate="2010-05-03 15:48:27" fixdate="2010-05-03 18:43:59" resolution="Fixed">
		<buginformation>
			<summary>BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial) throws NullPointerException</summary>
			<description>Method 
    BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial)  
invokes 
    BisectionSolver.solve(double min, double max) 
which throws NullPointerException, as member variable
    UnivariateRealSolverImpl.f 
is null.
Instead the method:
    BisectionSolver.solve(final UnivariateRealFunction f, double min, double max)
should be called.
Steps to reproduce:
invoke:
     new BisectionSolver().solve(someUnivariateFunctionImpl, 0.0, 1.0, 0.5);
NullPointerException will be thrown.
</description>
			<version>2.1</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.analysis.solvers.BisectionSolver.java</file>
			<file type="M">org.apache.commons.math.analysis.solvers.BisectionSolverTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="368" opendate="2010-04-29 03:41:10" fixdate="2010-05-09 23:07:24" resolution="Fixed">
		<buginformation>
			<summary>OpenMapRealVector.getSparcity should be getSparsity</summary>
			<description>The term for describing the ratio of nonzero elements to zero elements in a matrix/vector is sparsity, not sparcity.  Suggest renaming getSparcity() to getSparsity()</description>
			<version>2.1</version>
			<fixedVersion>2.2</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.linear.OpenMapRealVector.java</file>
		</fixedFiles>
	</bug>
	<bug id="367" opendate="2010-04-22 18:31:06" fixdate="2010-05-10 01:17:14" resolution="Fixed">
		<buginformation>
			<summary>AbstractRealVector.sparseIterator fails when vector has exactly one non-zero entry</summary>
			<description>The following program:
===
import java.util.Iterator;
import org.apache.commons.math.linear.*;
public class SparseIteratorTester
{
    public static void main(String[] args) {
        double vdata[] = 
{ 0.0, 1.0, 0.0 }
;
        RealVector v = new ArrayRealVector(vdata);
        Iterator&amp;lt;RealVector.Entry&amp;gt; iter = v.sparseIterator();
        while(iter.hasNext()) 
{
            RealVector.Entry entry = iter.next();
            System.out.printf("%d: %f\n", entry.getIndex(), entry.getValue());
        }
 
    }       
} 
===
generates this output:
1: 1.000000
Exception in thread "main" java.lang.ArrayIndexOutOfBoundsException: -1
	at org.apache.commons.math.linear.ArrayRealVector.getEntry(ArrayRealVector.java:995)
	at org.apache.commons.math.linear.AbstractRealVector$EntryImpl.getValue(AbstractRealVector.java:850)
	at test.SparseIteratorTester.main(SparseIteratorTester.java:13)
===
This patch fixes it, and simplifies AbstractRealVector.SparseEntryIterator  (sorry, i don&amp;amp;apos;t see any form entry for attaching a file)
===
Index: src/main/java/org/apache/commons/math/linear/AbstractRealVector.java
===================================================================
 src/main/java/org/apache/commons/math/linear/AbstractRealVector.java	(revision 936985)
+++ src/main/java/org/apache/commons/math/linear/AbstractRealVector.java	(working copy)
@@ -18,6 +18,7 @@
 package org.apache.commons.math.linear;
 import java.util.Iterator;
+import java.util.NoSuchElementException;
 import org.apache.commons.math.FunctionEvaluationException;
 import org.apache.commons.math.MathRuntimeException;
@@ -875,40 +876,25 @@
         /** Dimension of the vector. */
         private final int dim;

/** Temporary entry (reused on each call to 
{@link #next()}
. */
private EntryImpl tmp = new EntryImpl();
-
/** Current entry. */
+        /** Last entry returned by #next(). */
         private EntryImpl current;


/** Next entry. */
+        /** Next entry for #next() to return. */
         private EntryImpl next;

         /** Simple constructor. */
         protected SparseEntryIterator() {
             dim = getDimension();
             current = new EntryImpl();

if (current.getValue() == 0) 
{
-                advance(current);
-            }
if(current.getIndex() &amp;gt;= 0)
{
-                // There is at least one non-zero entry
-                next = new EntryImpl();
-                next.setIndex(current.getIndex());
+            next = new EntryImpl();
+            if(next.getValue() == 0)
                 advance(next);
-            }
 else 
{
-                // The vector consists of only zero entries, so deny having a next
-                current = null;
-            }
         }


/** Advance an entry up to the next non null one.
+        /** Advance an entry up to the next nonzero value.


@param e entry to advance
          */
         protected void advance(EntryImpl e) {


if (e == null) 
{
-                return;
-            }
             do 
{
                 e.setIndex(e.getIndex() + 1);
             }
 while (e.getIndex() &amp;lt; dim &amp;amp;&amp;amp; e.getValue() == 0);
@@ -919,22 +905,17 @@

         /** 
{@inheritDoc} */
         public boolean hasNext() {
-            return current != null;
+            return next.getIndex() &amp;gt;= 0;
         }

         /** {@inheritDoc}
 */
         public Entry next() {

tmp.setIndex(current.getIndex());
if (next != null) {
current.setIndex(next.getIndex());
advance(next);
if (next.getIndex() &amp;lt; 0) 
{
-                    next = null;
-                }
} else 
{
-                current = null;
-            }
return tmp;
+            int index = next.getIndex();
+            if(index &amp;lt; 0)
+                throw new NoSuchElementException();
+            current.setIndex(index);
+            advance(next);
+            return current;
         }

         /** 
{@inheritDoc}
 */</description>
			<version>2.1</version>
			<fixedVersion>2.2</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.linear.AbstractRealVectorTest.java</file>
			<file type="M">org.apache.commons.math.linear.AbstractRealVector.java</file>
		</fixedFiles>
	</bug>
	<bug id="371" opendate="2010-05-13 19:48:46" fixdate="2010-05-16 23:49:08" resolution="Fixed">
		<buginformation>
			<summary>PearsonsCorrelation.getCorrelationPValues() precision limited by machine epsilon</summary>
			<description>Similar to the issue described in MATH-201, using PearsonsCorrelation.getCorrelationPValues() with many treatments results in p-values that are continuous down to 2.2e-16 but that drop to 0 after that.
In MATH-201, the problem was described as such:
&amp;gt; So in essence, the p-value returned by TTestImpl.tTest() is:
&amp;gt; 
&amp;gt; 1.0 - (cumulativeProbability(t) - cumulativeProbabily(-t))
&amp;gt; 
&amp;gt; For large-ish t-statistics, cumulativeProbabilty(-t) can get quite small, and cumulativeProbabilty(t) can get very close to 1.0. When 
&amp;gt; cumulativeProbability(-t) is less than the machine epsilon, we get p-values equal to zero because:
&amp;gt; 
&amp;gt; 1.0 - 1.0 + 0.0 = 0.0
The solution in MATH-201 was to modify the p-value calculation to this:
&amp;gt; p = 2.0 * cumulativeProbability(-t)
Here, the problem is similar.  From PearsonsCorrelation.getCorrelationPValues():
  p = 2 * (1 - tDistribution.cumulativeProbability(t));
Directly calculating the p-value using identical code as PearsonsCorrelation.getCorrelationPValues(), but with the following change seems to solve the problem:
  p = 2 * (tDistribution.cumulativeProbability(-t));
</description>
			<version>2.0</version>
			<fixedVersion>2.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.stat.correlation.PearsonsCorrelation.java</file>
			<file type="M">org.apache.commons.math.stat.correlation.PearsonsCorrelationTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="362" opendate="2010-04-06 11:38:46" fixdate="2010-05-29 18:16:50" resolution="Fixed">
		<buginformation>
			<summary>LevenbergMarquardtOptimizer ignores the VectorialConvergenceChecker parameter passed to it</summary>
			<description>LevenbergMarquardtOptimizer ignores the VectorialConvergenceChecker parameter passed to it. This makes it hard to specify custom stopping criteria for the optimizer.</description>
			<version>2.0</version>
			<fixedVersion>2.2</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.optimization.general.LevenbergMarquardtOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.general.LevenbergMarquardtOptimizerTest.java</file>
			<file type="M">org.apache.commons.math.optimization.general.MinpackTest.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">404</link>
		</links>
	</bug>
	<bug id="352" opendate="2010-03-10 03:58:01" fixdate="2010-06-06 14:04:42" resolution="Fixed">
		<buginformation>
			<summary>Jacobian rank determination in LevenbergMarquardtOptimizer is not numerically robust</summary>
			<description>LevenbergMarquardtOptimizer is designed to handle singular jacobians,  i.e. situations when some of the fitted parameters depend on each other. The check for that condition is in LevenbergMarquardtOptimizer.qrDecomposition uses precise comparison to 0.
    if (ak2 == 0 ) 
{
                rank = k;
                return;
        }

A correct check would be comparison with a small epsilon. Hard coded 2.2204e-16 is used elsewhere in the same file for similar purpose.

final double QR_RANK_EPS = Math.ulp(1d); //2.220446049250313E-16
....
    if (ak2  &amp;lt; QR_RANK_EPS) {
                rank = k;
                return;
        }

Current exact equality check is not tolerant of the real world poorly conditioned situations. For example I am trying to fit a cylinder into sample 3d points. Although theoretically cylinder has only 5 independent variables, derivatives for optimizing function (signed distance) for such minimal parametrization are complicated and it  it much easier to work with a 7 variable parametrization (3 for axis direction, 3 for axis origin and 1 for radius). This naturally results in rank-deficient jacobian, but because of the numeric errors the actual ak2 values for the dependent rows ( I am seeing values of 1e-18 and less), rank handling code does not kick in.
Keeping these tiny values around then leads to huge corrections for the corresponding very slowly changing parameters, and consequently to numeric errors and instabilities. I have noticed the problem because tiny shift in the initial guess (on the order of 1e-12 in the axis component and origins) resulted in significantly different finally converged answers (origins and radii differing by as much as 0.02) which I tracked to loss of precision due to numeric error with root cause described above.
Providing a cutoff as suggested fixes the issue. After the fix, small perturbations in the initial guess had practically no effect to the converged result - as expected from a robust algorithm.</description>
			<version>2.0</version>
			<fixedVersion>2.2</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.optimization.general.LevenbergMarquardtOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.general.LevenbergMarquardtOptimizerTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="382" opendate="2010-07-05 12:57:19" fixdate="2010-07-05 14:11:28" resolution="Fixed">
		<buginformation>
			<summary>Wrong variable in precondition check</summary>
			<description>In the class MicrosphereInterpolator (package analysis.interpolation), the method


  public void setMicropshereElements(int elements)


is checking the precondition with the instance variable instead of the passed argument.</description>
			<version>2.1</version>
			<fixedVersion>2.2</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.analysis.interpolation.SplineInterpolatorTest.java</file>
			<file type="M">org.apache.commons.math.analysis.interpolation.MicrosphereInterpolator.java</file>
			<file type="M">org.apache.commons.math.exception.NotStrictlyPositiveException.java</file>
			<file type="M">org.apache.commons.math.analysis.interpolation.LinearInterpolatorTest.java</file>
			<file type="M">org.apache.commons.math.util.LocalizedFormats.java</file>
			<file type="M">org.apache.commons.math.analysis.interpolation.BicubicSplineInterpolatingFunction.java</file>
			<file type="M">org.apache.commons.math.exception.NotStrictlyPositiveExceptionTest.java</file>
			<file type="M">org.apache.commons.math.analysis.interpolation.BicubicSplineInterpolator.java</file>
			<file type="M">org.apache.commons.math.analysis.interpolation.LinearInterpolator.java</file>
			<file type="M">org.apache.commons.math.exception.NotPositiveExceptionTest.java</file>
			<file type="M">org.apache.commons.math.exception.NotPositiveException.java</file>
			<file type="M">org.apache.commons.math.analysis.interpolation.SplineInterpolator.java</file>
			<file type="M">org.apache.commons.math.analysis.interpolation.MicrosphereInterpolatingFunction.java</file>
		</fixedFiles>
	</bug>
	<bug id="377" opendate="2010-06-17 09:06:03" fixdate="2010-07-25 19:49:09" resolution="Fixed">
		<buginformation>
			<summary>weight versus sigma in AbstractLeastSquares</summary>
			<description>In AbstractLeastSquares, residualsWeights contains the WEIGHTS assigned to each observation.  In the method getRMS(), these weights are multiplicative as they should. unlike in getChiSquare() where it appears at the denominator!   If the weight is really the weight of the observation, it should multiply the square of the residual even in the computation of the chi2.
 Once corrected, getRMS() can even reduce
 public double getRMS() 
{return Math.sqrt(getChiSquare()/rows);}</description>
			<version>2.1</version>
			<fixedVersion>2.2</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.optimization.general.LevenbergMarquardtOptimizerTest.java</file>
			<file type="M">org.apache.commons.math.optimization.general.AbstractLeastSquaresOptimizer.java</file>
			<file type="M">org.apache.commons.math.linear.SingularValueDecompositionImpl.java</file>
			<file type="M">org.apache.commons.math.linear.EigenDecompositionImpl.java</file>
		</fixedFiles>
	</bug>
	<bug id="395" opendate="2010-07-25 21:26:33" fixdate="2010-07-28 12:11:09" resolution="Fixed">
		<buginformation>
			<summary>Bugs in "BrentOptimizer"</summary>
			<description>I apologize for having provided a buggy implementation of Brent&amp;amp;apos;s optimization algorithm (class "BrentOptimizer" in package "optimization.univariate").
The unit tests didn&amp;amp;apos;t show that there was something wrong, although (from the "changes.xml" file) I discovered that, at the time, Luc had noticed something weird in the implementation&amp;amp;apos;s behaviour.
Comparing with an implementation in Python, I could figure out the fixes. I&amp;amp;apos;ll modify "BrentOptimizer" and add a test. I also propose to change the name of the unit test class from "BrentMinimizerTest" to "BrentOptimizerTest".</description>
			<version>2.1</version>
			<fixedVersion>2.2</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.optimization.univariate.AbstractUnivariateRealOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.univariate.BrentOptimizerTest.java</file>
			<file type="M">org.apache.commons.math.optimization.MultiStartUnivariateRealOptimizerTest.java</file>
			<file type="M">org.apache.commons.math.ConvergingAlgorithmImpl.java</file>
			<file type="M">org.apache.commons.math.optimization.univariate.BrentOptimizer.java</file>
		</fixedFiles>
	</bug>
	<bug id="398" opendate="2010-07-28 17:31:36" fixdate="2010-07-28 18:49:31" resolution="Fixed">
		<buginformation>
			<summary>MathUtilsTest uses Math.nextUp from Java 6</summary>
			<description>The class  org.apache.commons.math.util.MathUtilsTest uses the method Math.nextUp(double) which is only available in Java 6, not in Java 5.  This makes Commons Math dependent on Java 6 rather than Java 5 which is in conflict with item 5 at the top of Commons Math Overview.</description>
			<version>2.2</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.util.MathUtilsTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="405" opendate="2010-08-11 13:24:39" fixdate="2010-08-11 13:46:55" resolution="Fixed">
		<buginformation>
			<summary>Inconsistent result from Levenberg-Marquardt</summary>
			<description>Levenberg-Marquardt (its method doOptimize) returns a VectorialPointValuePair.  However, the class holds the optimum point, the vector of the objective function, the cost and residuals.  The value returns by doOptimize does not always corresponds to the point which leads to the residuals and cost</description>
			<version>2.1</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.optimization.general.AbstractLeastSquaresOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.general.MinpackTest.java</file>
			<file type="M">org.apache.commons.math.optimization.general.LevenbergMarquardtOptimizer.java</file>
		</fixedFiles>
	</bug>
	<bug id="406" opendate="2010-08-14 21:57:56" fixdate="2010-08-14 22:02:03" resolution="Fixed">
		<buginformation>
			<summary>Wrong weight handling in Levenberg-Marquardt</summary>
			<description>A comparison with a Fortran version of Levenberg-Marquardt reveals that when observations have different weights, the 2.1 version reaches a value of the function which does not necessary correspond to the minimum</description>
			<version>2.1</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.optimization.general.AbstractLeastSquaresOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.general.LevenbergMarquardtOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.general.GaussNewtonOptimizerTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="392" opendate="2010-07-21 18:43:10" fixdate="2010-08-22 13:16:29" resolution="Fixed">
		<buginformation>
			<summary>calculateYVariance in OLS/GLSMultipleLinearRegression uses residuals not Y vars</summary>
			<description>Implementation of OLS/GLSMultipleLinearRegression is:
@Override
173        protected double calculateYVariance() 
{
174            RealVector residuals = calculateResiduals();
175            return residuals.dotProduct(residuals) /
176                   (X.getRowDimension() - X.getColumnDimension());
177        }

This gives variance of residuals not variance of the dependent (Y) variable as the documentation suggests.</description>
			<version>2.1</version>
			<fixedVersion>2.2</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.stat.regression.AbstractMultipleLinearRegression.java</file>
			<file type="M">org.apache.commons.math.stat.regression.GLSMultipleLinearRegression.java</file>
			<file type="M">org.apache.commons.math.stat.regression.OLSMultipleLinearRegressionTest.java</file>
			<file type="M">org.apache.commons.math.stat.regression.GLSMultipleLinearRegressionTest.java</file>
			<file type="M">org.apache.commons.math.stat.regression.OLSMultipleLinearRegression.java</file>
		</fixedFiles>
	</bug>
	<bug id="410" opendate="2010-08-26 08:57:09" fixdate="2010-08-26 09:01:33" resolution="Fixed">
		<buginformation>
			<summary>Wrong variable in "FunctionEvaluationException"</summary>
			<description>Some constructors use both argument and arguments as argument names and, in the body, argument is sometimes used in places where it should have been arguments.</description>
			<version>2.1</version>
			<fixedVersion>2.2</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.FunctionEvaluationException.java</file>
		</fixedFiles>
	</bug>
	<bug id="397" opendate="2010-07-27 15:18:06" fixdate="2010-08-30 13:52:18" resolution="Duplicate">
		<buginformation>
			<summary>Inconsistencies between "optimization.univariate" and "optimization.general"</summary>
			<description>I think that we could make the usage (from the developer&amp;amp;apos;s point-of-view) of "optimization.univariate" more similar to what is done in "optimization.general". At first this looked like a small change but then I discovered that "AbstractUnivariateRealOptimizer" competes with "ConvergingAlgorithmImpl" for some functionality, and that everything could be more coherent by enforcing the use of accessors and avoiding "protected" fields.
Moreover the logic inside  "AbstractUnivariateRealOptimizer" seems convoluted and one change leading to another...
Currently only "BrentOptimizer" inherits from "AbstractUnivariateRealOptimizer", so I hope that it&amp;amp;apos;s OK to revise that class.
In "ConvergingAlgorithmImpl", I propose to add a method:


protected void incrementIterationsCounter()
    throws ConvergenceException {
    if (++iterationCount &amp;gt; maximalIterationCount) {
        throw new ConvergenceException(new MaxIterationsExceededException(maximalIterationCount));
    }
}


This is still not the best since in "BaseAbstractScalarOptimizer", we have


protected void incrementIterationsCounter()
    throws OptimizationException {
    if (++iterations &amp;gt; maxIterations) {
        throw new OptimizationException(new MaxIterationsExceededException(maxIterations));
    }
}


(thus: two codes for the same problem, throwing different exceptions).
Then it seems that there is also a functionality overlap between "ConvergingAlgorithm" and "ConvergenceChecker"...</description>
			<version>2.1</version>
			<fixedVersion>3.0</fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.optimization.direct.NelderMead.java</file>
			<file type="M">org.apache.commons.math.optimization.MultiStartDifferentiableMultivariateRealOptimizerTest.java</file>
			<file type="M">org.apache.commons.math.optimization.SimpleVectorialValueChecker.java</file>
			<file type="M">org.apache.commons.math.optimization.SimpleScalarValueChecker.java</file>
			<file type="M">org.apache.commons.math.optimization.MultiStartMultivariateRealOptimizerTest.java</file>
			<file type="M">org.apache.commons.math.optimization.MultiStartDifferentiableMultivariateVectorialOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.general.LevenbergMarquardtOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.DifferentiableMultivariateRealOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.general.PowellOptimizerTest.java</file>
			<file type="M">org.apache.commons.math.ConvergingAlgorithm.java</file>
			<file type="M">org.apache.commons.math.optimization.DifferentiableMultivariateVectorialOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.direct.MultiDirectional.java</file>
			<file type="M">org.apache.commons.math.optimization.direct.DirectSearchOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.univariate.BrentOptimizerTest.java</file>
			<file type="M">org.apache.commons.math.optimization.MultiStartMultivariateRealOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.general.MinpackTest.java</file>
			<file type="M">org.apache.commons.math.ConvergingAlgorithmImpl.java</file>
			<file type="M">org.apache.commons.math.optimization.general.LevenbergMarquardtOptimizerTest.java</file>
			<file type="D">org.apache.commons.math.optimization.MultiStartUnivariateRealOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.univariate.BrentOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.MultivariateRealOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.direct.NelderMeadTest.java</file>
			<file type="M">org.apache.commons.math.optimization.general.GaussNewtonOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.univariate.BracketFinderTest.java</file>
			<file type="M">org.apache.commons.math.optimization.general.GaussNewtonOptimizerTest.java</file>
			<file type="M">org.apache.commons.math.optimization.direct.MultiDirectionalTest.java</file>
			<file type="M">org.apache.commons.math.optimization.general.NonLinearConjugateGradientOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.general.AbstractLeastSquaresOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.MultiStartDifferentiableMultivariateVectorialOptimizerTest.java</file>
			<file type="M">org.apache.commons.math.optimization.BaseMultivariateRealOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.univariate.AbstractUnivariateRealOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.fitting.CurveFitter.java</file>
			<file type="M">org.apache.commons.math.optimization.general.NonLinearConjugateGradientOptimizerTest.java</file>
			<file type="M">org.apache.commons.math.optimization.fitting.PolynomialFitter.java</file>
			<file type="M">org.apache.commons.math.optimization.univariate.BracketFinder.java</file>
			<file type="D">org.apache.commons.math.optimization.UnivariateRealOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.general.AbstractScalarDifferentiableOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.SimpleRealPointChecker.java</file>
			<file type="M">org.apache.commons.math.optimization.general.BaseAbstractScalarOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.RealConvergenceChecker.java</file>
			<file type="M">org.apache.commons.math.optimization.general.AbstractScalarOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.fitting.PolynomialFitterTest.java</file>
			<file type="M">org.apache.commons.math.optimization.SimpleVectorialPointChecker.java</file>
			<file type="M">org.apache.commons.math.optimization.general.PowellOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.MultiStartDifferentiableMultivariateRealOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.MultiStartUnivariateRealOptimizerTest.java</file>
		</fixedFiles>
		<links>
			<link type="Incorporates" description="is part of">413</link>
		</links>
	</bug>
	<bug id="404" opendate="2010-08-09 11:44:12" fixdate="2010-08-30 13:53:12" resolution="Later">
		<buginformation>
			<summary>Confusing interface for "LevenbergMarquardtOptimizer"</summary>
			<description>LevenbergMarquardtOptimizer inherits from AbstractLeastSquaresOptimizer which in turn implements DifferentiableMultivariateVectorialOptimizer. That interface mandates methods for setting and getting a VectorialConvergenceChecker.
In v2.1, however, that checker is never used! The convergence check is performed using parameters specific to the Levenberg-Marquardt algorithm. Such circumvention of the superclass interface is confusing and leads to totally unexpected behaviour (such as changing the values of the thresholds of the VectorialConvergenceChecker being ineffective).
In the development version, the default constructor of LevenbergMarquardtOptimizer sets the the VectorialConvergenceChecker field to "null" and when such is the case, the behaviour is as in v2.1. Although it is documented, this is still confusing since it is impossible to use LevenbergMarquardtOptimizer through its DifferentiableMultivariateVectorialOptimizer interface: When using the VectorialConvergenceChecker, one does not know what parameters to use in order to reproduce the results obtained with the LM-specific convergence check (i.e. how to reproduce the result from v2.1).
Unless I&amp;amp;apos;m missing something, I think that there should be an LM-specific implementation of VectorialConvergenceChecker that, when given the usual relative and absolute thresholds, can perform a check that will give the same result as the currently specific check (when the "checker" field is "null").</description>
			<version>2.1</version>
			<fixedVersion>3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.optimization.direct.NelderMead.java</file>
			<file type="M">org.apache.commons.math.optimization.MultiStartDifferentiableMultivariateRealOptimizerTest.java</file>
			<file type="M">org.apache.commons.math.optimization.SimpleVectorialValueChecker.java</file>
			<file type="M">org.apache.commons.math.optimization.SimpleScalarValueChecker.java</file>
			<file type="M">org.apache.commons.math.optimization.MultiStartMultivariateRealOptimizerTest.java</file>
			<file type="M">org.apache.commons.math.optimization.MultiStartDifferentiableMultivariateVectorialOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.general.LevenbergMarquardtOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.DifferentiableMultivariateRealOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.general.PowellOptimizerTest.java</file>
			<file type="M">org.apache.commons.math.ConvergingAlgorithm.java</file>
			<file type="M">org.apache.commons.math.optimization.DifferentiableMultivariateVectorialOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.direct.MultiDirectional.java</file>
			<file type="M">org.apache.commons.math.optimization.direct.DirectSearchOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.univariate.BrentOptimizerTest.java</file>
			<file type="M">org.apache.commons.math.optimization.MultiStartMultivariateRealOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.general.MinpackTest.java</file>
			<file type="M">org.apache.commons.math.ConvergingAlgorithmImpl.java</file>
			<file type="M">org.apache.commons.math.optimization.general.LevenbergMarquardtOptimizerTest.java</file>
			<file type="D">org.apache.commons.math.optimization.MultiStartUnivariateRealOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.univariate.BrentOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.MultivariateRealOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.direct.NelderMeadTest.java</file>
			<file type="M">org.apache.commons.math.optimization.general.GaussNewtonOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.univariate.BracketFinderTest.java</file>
			<file type="M">org.apache.commons.math.optimization.general.GaussNewtonOptimizerTest.java</file>
			<file type="M">org.apache.commons.math.optimization.direct.MultiDirectionalTest.java</file>
			<file type="M">org.apache.commons.math.optimization.general.NonLinearConjugateGradientOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.general.AbstractLeastSquaresOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.MultiStartDifferentiableMultivariateVectorialOptimizerTest.java</file>
			<file type="M">org.apache.commons.math.optimization.BaseMultivariateRealOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.univariate.AbstractUnivariateRealOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.fitting.CurveFitter.java</file>
			<file type="M">org.apache.commons.math.optimization.general.NonLinearConjugateGradientOptimizerTest.java</file>
			<file type="M">org.apache.commons.math.optimization.fitting.PolynomialFitter.java</file>
			<file type="M">org.apache.commons.math.optimization.univariate.BracketFinder.java</file>
			<file type="D">org.apache.commons.math.optimization.UnivariateRealOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.general.AbstractScalarDifferentiableOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.SimpleRealPointChecker.java</file>
			<file type="M">org.apache.commons.math.optimization.general.BaseAbstractScalarOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.RealConvergenceChecker.java</file>
			<file type="M">org.apache.commons.math.optimization.general.AbstractScalarOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.fitting.PolynomialFitterTest.java</file>
			<file type="M">org.apache.commons.math.optimization.SimpleVectorialPointChecker.java</file>
			<file type="M">org.apache.commons.math.optimization.general.PowellOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.MultiStartDifferentiableMultivariateRealOptimizer.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">362</link>
			<link type="Incorporates" description="is part of">413</link>
		</links>
	</bug>
	<bug id="373" opendate="2010-06-07 14:54:00" fixdate="2010-09-02 04:52:33" resolution="Fixed">
		<buginformation>
			<summary>StatUtils.sum returns NaN for zero-length arrays</summary>
			<description>StatUtils.sum returns NaN for zero-length arrays, which is:
1. inconsistent with the mathematical notion of sum: in maths, sum_
{i=0}^{N-1} a_i will be 0 for N=0. In particular, the identity

sum_{i=0}
^
{k-1}
 a_i + sum_
{i=k}
^
{N-1} = sum_{i=0}^{N-1}

is broken for k = 0, since NaN + x = NaN, not x.
2. introduces hard to debug erros (returning a NaN is one of the worst forms of reporting an exceptional condition, as NaNs propagate silently and require manual tracing during the debugging)
3. enforces "special case" handling when the user expects that the summed array can have a zero length.
The correct behaviour is, in my opinion, to return 0.0, not NaN in the above case.</description>
			<version>2.1</version>
			<fixedVersion>3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.stat.StatUtilsTest.java</file>
			<file type="M">org.apache.commons.math.stat.descriptive.summary.SumLogTest.java</file>
			<file type="M">org.apache.commons.math.stat.descriptive.AbstractUnivariateStatistic.java</file>
			<file type="M">org.apache.commons.math.stat.descriptive.summary.Sum.java</file>
			<file type="M">org.apache.commons.math.stat.descriptive.summary.SumTest.java</file>
			<file type="M">org.apache.commons.math.stat.descriptive.summary.SumSqTest.java</file>
			<file type="M">org.apache.commons.math.stat.descriptive.summary.SumOfLogs.java</file>
			<file type="M">org.apache.commons.math.stat.descriptive.StorelessUnivariateStatisticAbstractTest.java</file>
			<file type="M">org.apache.commons.math.stat.descriptive.AbstractUnivariateStatisticTest.java</file>
			<file type="M">org.apache.commons.math.stat.descriptive.summary.Product.java</file>
			<file type="M">org.apache.commons.math.stat.descriptive.summary.ProductTest.java</file>
			<file type="M">org.apache.commons.math.stat.descriptive.summary.SumOfSquares.java</file>
		</fixedFiles>
	</bug>
	<bug id="415" opendate="2010-09-11 17:05:30" fixdate="2010-09-11 17:21:33" resolution="Fixed">
		<buginformation>
			<summary>MathRuntimeException#createInternalError() method loosing the exception "cause"</summary>
			<description>The MathRuntimeException#createInternalError(Throwable cause) method doesn&amp;amp;apos;t store the exception "cause".
Is this intentionally?
</description>
			<version>3.0</version>
			<fixedVersion>2.2</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.MathRuntimeException.java</file>
		</fixedFiles>
	</bug>
	<bug id="409" opendate="2010-08-24 09:55:32" fixdate="2010-09-13 02:02:43" resolution="Fixed">
		<buginformation>
			<summary>Multiple Regression API should allow specification of whether or not to estimate intercept term</summary>
			<description>The OLS and GLS regression APIs should support estimating models including intercepts using design matrices including only variable data.</description>
			<version>2.0</version>
			<fixedVersion>2.2</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.stat.regression.OLSMultipleLinearRegression.java</file>
			<file type="M">org.apache.commons.math.stat.regression.AbstractMultipleLinearRegression.java</file>
			<file type="M">org.apache.commons.math.stat.regression.OLSMultipleLinearRegressionTest.java</file>
			<file type="M">org.apache.commons.math.stat.regression.MultipleLinearRegressionAbstractTest.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">411</link>
		</links>
	</bug>
	<bug id="411" opendate="2010-08-28 22:14:32" fixdate="2010-09-13 02:04:01" resolution="Fixed">
		<buginformation>
			<summary>Multiple Regression newSampleData methods inconsistently create / omit intercepts</summary>
			<description>The newSampleData(double[], nrows, ncols) method used in the unit tests adds a unitary column to the design matrix, resulting in an intercept term being estimated among the regression parameters.  The other newSampleData methods do not do this, forcing users to add the column of "1"s to estimate models with intercept.  Behavior should be consistent and users should not have to add the column.</description>
			<version>2.0</version>
			<fixedVersion>2.2</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.stat.regression.MultipleLinearRegressionAbstractTest.java</file>
			<file type="M">org.apache.commons.math.stat.regression.OLSMultipleLinearRegressionTest.java</file>
			<file type="M">org.apache.commons.math.stat.regression.OLSMultipleLinearRegression.java</file>
			<file type="M">org.apache.commons.math.stat.regression.GLSMultipleLinearRegressionTest.java</file>
			<file type="M">org.apache.commons.math.exception.util.LocalizedFormats.java</file>
			<file type="M">org.apache.commons.math.stat.regression.AbstractMultipleLinearRegression.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="is blocked by">409</link>
		</links>
	</bug>
	<bug id="421" opendate="2010-09-29 18:24:56" fixdate="2010-09-29 19:51:49" resolution="Fixed">
		<buginformation>
			<summary>restarting an ODE solver that has been stopped by an event doesn&amp;apos;t work</summary>
			<description>If an ODE solver is setup with an EventHandler that return STOP when the even is triggered, the integrators stops (which is exactly the expected behavior).
If however the user want to restart the solver from the final state reached at the event with the same configuration (expecting the event to be triggered again at a later time), then the integrator may fail to start. It can get stuck at the previous event.
The occurrence of the bug depends on the residual sign of the g function which is not exactly 0, it depends on the convergence of the first event.
As this use case is fairly general, event occurring less than epsilon after the solver start in the first step should be ignored, where epsilon is the convergence threshold of the event. The sign of the g function should be evaluated after this initial ignore zone, not exactly at beginning (if there are no event at the very beginning g(t0) and g(t0+epsilon) have the same sign, so this does not hurt ; if there is an event at the very beginning, g(t0) and g(t0+epsilon) have opposite signs and we want to start with the second one. Of course, the sign of epsilon depend on the integration direction (forward or backward).</description>
			<version>2.1</version>
			<fixedVersion>2.2</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.ode.events.EventStateTest.java</file>
			<file type="M">org.apache.commons.math.ode.events.EventState.java</file>
			<file type="M">org.apache.commons.math.ode.events.CombinedEventsManager.java</file>
		</fixedFiles>
	</bug>
	<bug id="391" opendate="2010-07-21 08:57:46" fixdate="2010-10-03 16:43:11" resolution="Fixed">
		<buginformation>
			<summary>Inconsistent behaviour of constructors in ArrayRealVector class</summary>
			<description>ArrayRealVector(double[] d) allows to construct a zero-length vector, but ArrayRealVector(double[] d, boolean copyArray) doesn&amp;amp;apos;t. Both should allow this as zero-length vectors are mathematically well-defined objects and they are useful boundary cases in many algorithms.
This breaks some arithmetic operators (addition) on zero-length real vectors which worked in 2.0 but don&amp;amp;apos;t work in 2.1</description>
			<version>2.1</version>
			<fixedVersion>2.2</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.linear.ArrayFieldVector.java</file>
			<file type="M">org.apache.commons.math.linear.ArrayFieldVectorTest.java</file>
			<file type="M">org.apache.commons.math.linear.ArrayRealVectorTest.java</file>
			<file type="M">org.apache.commons.math.linear.ArrayRealVector.java</file>
		</fixedFiles>
	</bug>
	<bug id="429" opendate="2010-10-22 08:01:54" fixdate="2010-10-23 19:35:26" resolution="Fixed">
		<buginformation>
			<summary>KMeansPlusPlusClusterer breaks by division by zero</summary>
			<description>For a certain space, KMeansPlusPlusClusterer  breaks. This is a blocker because this space occurs in our domain. </description>
			<version>2.1</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.exception.util.LocalizedFormats.java</file>
			<file type="M">org.apache.commons.math.stat.clustering.KMeansPlusPlusClusterer.java</file>
			<file type="M">org.apache.commons.math.exception.ConvergenceException.java</file>
			<file type="M">org.apache.commons.math.exception.MathIllegalStateException.java</file>
			<file type="M">org.apache.commons.math.stat.clustering.KMeansPlusPlusClustererTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="408" opendate="2010-08-23 03:11:23" fixdate="2010-12-12 21:49:44" resolution="Fixed">
		<buginformation>
			<summary>GLSMultipleLinearRegression has no nontrivial validation tests</summary>
			<description>There are no non-trivial tests verifying the computations for GLSMultipleLinearRegression.  Tests verifying computations against analytically determined models, R or some other reference package / datasets should be added to ensure that the statistics reported by this class are valid.</description>
			<version>2.0</version>
			<fixedVersion>2.2</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.stat.regression.GLSMultipleLinearRegressionTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="467" opendate="2011-01-06 14:49:39" fixdate="2011-01-06 19:43:47" resolution="Fixed">
		<buginformation>
			<summary>HarmonicCoefficientsGuesser.sortObservations() potentlal NPE warning</summary>
			<description>HarmonicCoefficientsGuesser.sortObservations()
generates an NPE warning from Eclipse which thinks that mI can be null in the while condition.
The code looks like:


WeightedObservedPoint mI = observations[i];
while ((i &amp;gt;= 0) &amp;amp;&amp;amp; (curr.getX() &amp;lt; mI.getX())) {
    observations[i + 1] = mI;
    if (i-- != 0) {
        mI = observations[i];
    } else {
        mI = null;
    }
}
// mI is not used further


It looks to me as though the "mI = null" statement is either redundant or wrong - why would one want to replace one of the observations with null during a sort?</description>
			<version>2.2</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.optimization.fitting.HarmonicCoefficientsGuesser.java</file>
		</fixedFiles>
	</bug>
	<bug id="466" opendate="2011-01-06 14:33:28" fixdate="2011-01-07 17:01:48" resolution="Fixed">
		<buginformation>
			<summary>BaseMultiStartMultivariateRealOptimizer.optimize() can generate NPE if starts &lt; 1</summary>
			<description>The Javadoc for BaseMultiStartMultivariateRealOptimizer says that starts can be &amp;lt;= 1; however if it is set to 0, then the optimize() method will try to throw a null exception.
Perhaps starts should be constrained to be at least 1?</description>
			<version>3.0</version>
			<fixedVersion>3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.optimization.univariate.MultiStartUnivariateRealOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.BaseMultiStartMultivariateVectorialOptimizer.java</file>
			<file type="M">org.apache.commons.math.optimization.BaseMultiStartMultivariateRealOptimizer.java</file>
		</fixedFiles>
	</bug>
	<bug id="482" opendate="2011-01-17 19:52:10" fixdate="2011-01-17 20:01:32" resolution="Fixed">
		<buginformation>
			<summary>FastMath.max(50.0f, -50.0f) =&gt; -50.0f; should be +50.0f</summary>
			<description>FastMath.max(50.0f, -50.0f) =&amp;gt; -50.0f; should be +50.0f.
This is because the wrong variable is returned.
The bug was not detected by the test case "testMinMaxFloat()" because that has a bug too - it tests doubles, not floats.</description>
			<version>2.2</version>
			<fixedVersion>2.2, 3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.util.FastMathTest.java</file>
			<file type="M">org.apache.commons.math.util.FastMath.java</file>
		</fixedFiles>
	</bug>
	<bug id="479" opendate="2011-01-14 22:46:08" fixdate="2011-01-19 19:06:25" resolution="Fixed">
		<buginformation>
			<summary>FastMath.signum(-0.0) does not agree with Math.signum(-0.0) ; no tests for signum</summary>
			<description>There are no unit tests for FastMath.signum(double) as yet.
Here is one that should work, but fails:


@Test
public void testSignum() {
    Assert.assertTrue(Double.valueOf(FastMath.signum(+0.0)).equals(Double.valueOf(Math.signum(+0.0)))); // OK
    Assert.assertTrue(Double.valueOf(FastMath.signum(-0.0)).equals(Double.valueOf(Math.signum(-0.0)))); // FAILS
}


</description>
			<version>2.2</version>
			<fixedVersion>2.2, 3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.util.FastMath.java</file>
		</fixedFiles>
	</bug>
	<bug id="480" opendate="2011-01-14 23:51:30" fixdate="2011-01-19 19:18:19" resolution="Fixed">
		<buginformation>
			<summary>"ulp" in "FastMath"</summary>
			<description>When the argument is infinite, method "ulp" in "FastMath" produces "NaN" (whereas "Math" gives "Infinity").</description>
			<version>2.2</version>
			<fixedVersion>2.2, 3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.util.FastMath.java</file>
		</fixedFiles>
	</bug>
	<bug id="483" opendate="2011-01-18 00:24:51" fixdate="2011-01-19 19:51:07" resolution="Fixed">
		<buginformation>
			<summary>FastMath does not handle all special cases correctly</summary>
			<description>FastMath has some issues with special cases such as +0.0 and -0.0.
Here are the double cases so far found:
abs(-0.0) expected:&amp;lt;0.0&amp;gt; but was:&amp;lt;-0.0&amp;gt;
signum(-0.0) expected:&amp;lt;-0.0&amp;gt; but was:&amp;lt;0.0&amp;gt;
asin(-0.0) expected:&amp;lt;-0.0&amp;gt; but was:&amp;lt;0.0&amp;gt;
atan(-0.0) expected:&amp;lt;-0.0&amp;gt; but was:&amp;lt;0.0&amp;gt;
log10(-0.0) expected:&amp;lt;-Infinity&amp;gt; but was:&amp;lt;NaN&amp;gt;
toDegrees(-0.0) expected:&amp;lt;-0.0&amp;gt; but was:&amp;lt;0.0&amp;gt;
toRadians(-0.0) expected:&amp;lt;-0.0&amp;gt; but was:&amp;lt;0.0&amp;gt;
ulp(-Infinity) expected:&amp;lt;Infinity&amp;gt; but was:&amp;lt;NaN&amp;gt;
And float cases:
abs(-0.0) expected:&amp;lt;0.0&amp;gt; but was:&amp;lt;-0.0&amp;gt;</description>
			<version>2.2</version>
			<fixedVersion>2.2, 3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.util.FastMath.java</file>
			<file type="M">org.apache.commons.math.util.FastMathTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="486" opendate="2011-01-19 04:46:26" fixdate="2011-01-19 20:28:06" resolution="Fixed">
		<buginformation>
			<summary>FastMath toRadian and toDegree don&amp;apos;t handle large double numbers well</summary>
			<description>FastMath toRadian and toDegree don&amp;amp;apos;t handle very large double numbers well.
For example, toDegrees(Double.MAX_VALUE) =&amp;gt; NaN, but it should be INFINITY
and toRadian(Double.MAX_VALUE) =&amp;gt; NaN instead of the proper value
This is because of the lines:


double temp = x * 1073741824.0; // == 0x40 00 00 00
double xa = x + temp - temp; // =&amp;gt; NaN for x large enough


This seems to be an attempt to split x into a large and a small part, but fails when x &amp;gt;= MAX_VALUE / 1073741824.0
Not sure how to fix this</description>
			<version>2.2</version>
			<fixedVersion>2.2, 3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.util.FastMath.java</file>
		</fixedFiles>
	</bug>
	<bug id="489" opendate="2011-01-19 17:11:13" fixdate="2011-01-21 03:33:05" resolution="Fixed">
		<buginformation>
			<summary>FastMath acos fails when input abs value is less than about 5.7851920321187236E-300 - returns NaN</summary>
			<description>FastMath acos fails when input absolute value is less than about 5.7851920321187236E-300
It returns NaN instead of an expected value close to PI/2.0
This appears to be due to the following code:


// Compute ratio r = y/x
double r = y/x;
temp = r * 1073741824.0;


r and temp can become infinite or Nan.</description>
			<version>2.2</version>
			<fixedVersion>2.2, 3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.util.FastMath.java</file>
		</fixedFiles>
	</bug>
	<bug id="494" opendate="2011-01-21 21:56:18" fixdate="2011-01-22 20:21:22" resolution="Fixed">
		<buginformation>
			<summary>FastMath atan2 does not agree with StrictMath for special cases</summary>
			<description>FastMath atan2 does not agree with StrictMath for special cases.
There are two sign problems:
atan2(double -0.0, double -Infinity) expected -3.141592653589793 actual 3.141592653589793 entries [1, 4]
atan2(double -0.0, double Infinity) expected -0.0 actual 0.0 entries [1, 5]
A lot of NaNs where there should be a valid return:
atan2(double -1.7976931348623157E308, double -1.7976931348623157E308) expected -2.356194490192345 actual NaN entries [6, 6]
atan2(double -1.7976931348623157E308, double 1.7976931348623157E308) expected -0.7853981633974483 actual NaN entries [6, 7]
atan2(double -1.7976931348623157E308, double -1.1102230246251565E-16) expected -1.5707963267948968 actual NaN entries [6, 8]
atan2(double -1.7976931348623157E308, double 1.1102230246251565E-16) expected -1.5707963267948966 actual NaN entries [6, 9]
atan2(double -1.7976931348623157E308, double -2.2250738585072014E-308) expected -1.5707963267948968 actual NaN entries [6, 10]
atan2(double -1.7976931348623157E308, double 2.2250738585072014E-308) expected -1.5707963267948966 actual NaN entries [6, 11]
atan2(double -1.7976931348623157E308, double -4.9E-324) expected -1.5707963267948968 actual NaN entries [6, 12]
atan2(double -1.7976931348623157E308, double 4.9E-324) expected -1.5707963267948966 actual NaN entries [6, 13]
atan2(double 1.7976931348623157E308, double -1.7976931348623157E308) expected 2.356194490192345 actual NaN entries [7, 6]
atan2(double 1.7976931348623157E308, double 1.7976931348623157E308) expected 0.7853981633974483 actual NaN entries [7, 7]
atan2(double 1.7976931348623157E308, double -1.1102230246251565E-16) expected 1.5707963267948968 actual NaN entries [7, 8]
atan2(double 1.7976931348623157E308, double 1.1102230246251565E-16) expected 1.5707963267948966 actual NaN entries [7, 9]
atan2(double 1.7976931348623157E308, double -2.2250738585072014E-308) expected 1.5707963267948968 actual NaN entries [7, 10]
atan2(double 1.7976931348623157E308, double 2.2250738585072014E-308) expected 1.5707963267948966 actual NaN entries [7, 11]
atan2(double 1.7976931348623157E308, double -4.9E-324) expected 1.5707963267948968 actual NaN entries [7, 12]
atan2(double 1.7976931348623157E308, double 4.9E-324) expected 1.5707963267948966 actual NaN entries [7, 13]
atan2(double -1.1102230246251565E-16, double -1.7976931348623157E308) expected -3.141592653589793 actual NaN entries [8, 6]
atan2(double -1.1102230246251565E-16, double 1.7976931348623157E308) expected -0.0 actual NaN entries [8, 7]
atan2(double -1.1102230246251565E-16, double -4.9E-324) expected -1.5707963267948968 actual NaN entries [8, 12]
atan2(double -1.1102230246251565E-16, double 4.9E-324) expected -1.5707963267948966 actual NaN entries [8, 13]
atan2(double 1.1102230246251565E-16, double -1.7976931348623157E308) expected 3.141592653589793 actual NaN entries [9, 6]
atan2(double 1.1102230246251565E-16, double 1.7976931348623157E308) expected 0.0 actual NaN entries [9, 7]
atan2(double 1.1102230246251565E-16, double -4.9E-324) expected 1.5707963267948968 actual NaN entries [9, 12]
atan2(double 1.1102230246251565E-16, double 4.9E-324) expected 1.5707963267948966 actual NaN entries [9, 13]
atan2(double -2.2250738585072014E-308, double -1.7976931348623157E308) expected -3.141592653589793 actual NaN entries [10, 6]
atan2(double -2.2250738585072014E-308, double 1.7976931348623157E308) expected -0.0 actual NaN entries [10, 7]
atan2(double 2.2250738585072014E-308, double -1.7976931348623157E308) expected 3.141592653589793 actual NaN entries [11, 6]
atan2(double 2.2250738585072014E-308, double 1.7976931348623157E308) expected 0.0 actual NaN entries [11, 7]
atan2(double -4.9E-324, double -1.7976931348623157E308) expected -3.141592653589793 actual NaN entries [12, 6]
atan2(double -4.9E-324, double 1.7976931348623157E308) expected -0.0 actual NaN entries [12, 7]
atan2(double 4.9E-324, double -1.7976931348623157E308) expected 3.141592653589793 actual NaN entries [13, 6]
atan2(double 4.9E-324, double 1.7976931348623157E308) expected 0.0 actual NaN entries [13, 7]
There are also some spurious errors, which are due to a bug in the test case - expecting the values to be exactly the same as StrictMath
atan2(double 2.2250738585072014E-308, double -4.9E-324) expected 1.570796326794897 actual 1.5707963267948968 entries [11, 12]
atan2(double -2.2250738585072014E-308, double -4.9E-324) expected -1.570796326794897 actual -1.5707963267948968 entries [10, 12]
atan2(double 1.1102230246251565E-16, double -2.2250738585072014E-308) expected 1.5707963267948968 actual 1.5707963267948966 entries [9, 10]
atan2(double -1.1102230246251565E-16, double -2.2250738585072014E-308) expected -1.5707963267948968 actual -1.5707963267948966 entries [8, 10]</description>
			<version>2.2</version>
			<fixedVersion>2.2, 3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.util.FastMath.java</file>
			<file type="M">org.apache.commons.math.util.FastMathTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="493" opendate="2011-01-21 21:12:04" fixdate="2011-01-24 12:49:32" resolution="Fixed">
		<buginformation>
			<summary>FastMath min and max fail with (Infinity,-Infinity) and (0,0, -0.0)</summary>
			<description>FastMath min and max fail with (Infinity,-Infinity) and (0,0, -0.0):
min(float 0.0, float -0.0) expected -0.0 actual 0.0
min(float Infinity, float -Infinity) expected -Infinity actual NaN
max(float 0.0, float -0.0) expected 0.0 actual -0.0
max(float Infinity, float -Infinity) expected Infinity actual NaN
Similarly for the double versions.
The Infinity failures are because the code uses Float.isNaN(a + b) which gives NaN when +/1- Infinity are added together.</description>
			<version>2.2</version>
			<fixedVersion>2.2, 3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.util.FastMath.java</file>
		</fixedFiles>
	</bug>
	<bug id="505" opendate="2011-02-01 00:28:56" fixdate="2011-02-01 18:58:30" resolution="Fixed">
		<buginformation>
			<summary>TestUtils is thread-hostile</summary>
			<description>TestUtils has several mutable static fields which are not synchronised, or volatile.
If one of the fields is updated by thread A, there is no guarantee that thread B will see the full update - it may see a partially updated object.
Furthermore, at least some of the static fields reference a mutable object, which can be changed whilst another thread is using it.
As far as I can tell, this class must only ever be used by a single thread otherwise the results will be unpredictable.</description>
			<version>1.2</version>
			<fixedVersion>3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="D">org.apache.commons.math.stat.inference.ChiSquareFactoryTest.java</file>
			<file type="D">org.apache.commons.math.stat.inference.TTestFactoryTest.java</file>
			<file type="M">org.apache.commons.math.stat.inference.TestUtils.java</file>
		</fixedFiles>
	</bug>
	<bug id="484" opendate="2011-01-18 20:49:51" fixdate="2011-02-14 14:20:29" resolution="Fixed">
		<buginformation>
			<summary>events detection in ODE solvers is too complex and not robust</summary>
			<description>All ODE solvers support multiple events detection since a long time. Events are specified by users by implementing the EventHandler interface. Events occur when the g(t, y) function evaluates to 0. When an event occurs, the solver step is shortened to make sure the event is located at the end of the step, and the event is triggered by calling the eventOccurred method in the user defined implementation class. Depending on the return value of this method, integration can continue, it can be stopped, or the state vector can be reset.
Some ODE solvers are adaptive step size solvers. They can modify step size to match an integration error setting, increasing step size when error is low (thus reducing computing costs) or reducing step size when error is high (thus fulfilling accuracy requirements).
The step adaptations due to events on one side and due to adaptive step size solvers are quite intricate by now, due to numerous fixes (MATH-161, MATH-213, MATH-322, MATH-358, MATH-421 and also during standard maintenance - see for example r781157). The code is very difficult to maintain. It seems each bug fix introduces new bugs (r781157/MATH-322) or tighten the link between adaptive step size and event detection (MATH-388/r927202).
A new bug discovered recently on an external library using a slightly modified version of this code could not be retroffitted into commons-math, despite the same problem is present. At the beginning of EventState.evaluateStep, the initial step may be exactly 0 thus preventing root solving, but preventing this size to drop to 0 would reopen MATH-388. I could not fix both bugs at the same time.
So it is now time to untangle events detection and adaptive step size, simplify code, and remove some inefficiency (event root solving is always done twice, once before step truncation and another time after truncation, of course with slightly different results, events shortened steps induce high computation load until the integrator recovers its optimal pace again, steps are rejected even when the event does not requires it ...).</description>
			<version>2.1</version>
			<fixedVersion>2.2</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.ode.AbstractIntegrator.java</file>
			<file type="M">org.apache.commons.math.ode.nonstiff.ThreeEighthesIntegratorTest.java</file>
			<file type="M">org.apache.commons.math.ode.nonstiff.GillStepInterpolatorTest.java</file>
			<file type="M">org.apache.commons.math.ode.nonstiff.EulerIntegratorTest.java</file>
			<file type="M">org.apache.commons.math.ode.sampling.DummyStepInterpolatorTest.java</file>
			<file type="M">org.apache.commons.math.ode.nonstiff.GraggBulirschStoerStepInterpolator.java</file>
			<file type="M">org.apache.commons.math.ode.nonstiff.DormandPrince54StepInterpolatorTest.java</file>
			<file type="M">org.apache.commons.math.ode.nonstiff.HighamHall54IntegratorTest.java</file>
			<file type="M">org.apache.commons.math.ode.nonstiff.GraggBulirschStoerIntegratorTest.java</file>
			<file type="M">org.apache.commons.math.ode.sampling.AbstractStepInterpolator.java</file>
			<file type="M">org.apache.commons.math.ode.events.EventState.java</file>
			<file type="M">org.apache.commons.math.ode.nonstiff.HighamHall54StepInterpolatorTest.java</file>
			<file type="M">org.apache.commons.math.ode.sampling.NordsieckStepInterpolatorTest.java</file>
			<file type="M">org.apache.commons.math.ode.nonstiff.GraggBulirschStoerIntegrator.java</file>
			<file type="M">org.apache.commons.math.ode.nonstiff.AdamsMoultonIntegrator.java</file>
			<file type="M">org.apache.commons.math.ode.TestProblemHandler.java</file>
			<file type="M">org.apache.commons.math.ode.nonstiff.MidpointStepInterpolatorTest.java</file>
			<file type="M">org.apache.commons.math.ode.nonstiff.GraggBulirschStoerStepInterpolatorTest.java</file>
			<file type="M">org.apache.commons.math.ode.nonstiff.DormandPrince54IntegratorTest.java</file>
			<file type="M">org.apache.commons.math.ode.nonstiff.RungeKuttaIntegrator.java</file>
			<file type="M">org.apache.commons.math.ode.nonstiff.DormandPrince853StepInterpolatorTest.java</file>
			<file type="M">org.apache.commons.math.ode.ODEIntegrator.java</file>
			<file type="M">org.apache.commons.math.ode.nonstiff.ClassicalRungeKuttaStepInterpolatorTest.java</file>
			<file type="M">org.apache.commons.math.ode.nonstiff.DormandPrince853StepInterpolator.java</file>
			<file type="M">org.apache.commons.math.ode.nonstiff.DormandPrince853IntegratorTest.java</file>
			<file type="M">org.apache.commons.math.ode.nonstiff.AdamsBashforthIntegratorTest.java</file>
			<file type="M">org.apache.commons.math.ode.nonstiff.AdamsBashforthIntegrator.java</file>
			<file type="D">org.apache.commons.math.ode.events.CombinedEventsManager.java</file>
			<file type="M">org.apache.commons.math.ode.TestProblemAbstract.java</file>
			<file type="M">org.apache.commons.math.ode.nonstiff.EmbeddedRungeKuttaIntegrator.java</file>
			<file type="M">org.apache.commons.math.ode.nonstiff.MidpointIntegratorTest.java</file>
			<file type="M">org.apache.commons.math.ode.nonstiff.ClassicalRungeKuttaIntegratorTest.java</file>
			<file type="M">org.apache.commons.math.ode.TestProblem4.java</file>
			<file type="M">org.apache.commons.math.ode.nonstiff.ThreeEighthesStepInterpolatorTest.java</file>
			<file type="M">org.apache.commons.math.ode.nonstiff.GillIntegratorTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="529" opendate="2011-02-22 15:37:10" fixdate="2011-02-22 19:19:51" resolution="Fixed">
		<buginformation>
			<summary>[Math] Clirr report and &amp;apos;org.apache.commons.math.stat.regression.AbstractMultipleLinearRegression&amp;apos;</summary>
			<description>Clirr report states that method &amp;amp;apos;protected double calculateErrorVariance()&amp;amp;apos; has been added, but in the javadoc no "@since 2.2" tag found...</description>
			<version>2.2</version>
			<fixedVersion>2.2</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.stat.regression.AbstractMultipleLinearRegression.java</file>
		</fixedFiles>
	</bug>
	<bug id="530" opendate="2011-02-22 16:58:56" fixdate="2011-02-22 19:30:50" resolution="Fixed">
		<buginformation>
			<summary>[Math] Clirr report and &amp;apos;org.apache.commons.math.stat.regression.GLSMultipleLinearRegression</summary>
			<description>Clirr report mentions that method &amp;amp;apos;protected double calculateErrorVariance()&amp;amp;apos; has been added... However this addition isn&amp;amp;apos;t reflected to the javadoc with the "@since 2.2" clause.</description>
			<version>2.2</version>
			<fixedVersion>2.2</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.stat.regression.GLSMultipleLinearRegression.java</file>
		</fixedFiles>
	</bug>
	<bug id="531" opendate="2011-02-22 17:19:53" fixdate="2011-02-22 19:36:15" resolution="Fixed">
		<buginformation>
			<summary>[Math] Clirr report and &amp;apos;org.apache.commons.math.stat.regression.OLSMultipleLinearRegression&amp;apos;</summary>
			<description>All the methods that are marked as added in the above class in the clirr report haven&amp;amp;apos;t "@since 2.2" tag on their javadocs respectively... </description>
			<version>2.2</version>
			<fixedVersion>2.2</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.stat.regression.OLSMultipleLinearRegression.java</file>
		</fixedFiles>
	</bug>
	<bug id="532" opendate="2011-02-22 17:57:22" fixdate="2011-02-22 19:50:19" resolution="Fixed">
		<buginformation>
			<summary>[Math] Clirr report and &amp;apos;org.apache.commons.math.util.MathUtils&amp;apos;</summary>
			<description>Clirr report mentions that method &amp;amp;apos;public void checkOrder(double[], org.apache.commons.math.util.MathUtils$OrderDirection, boolean)&amp;amp;apos; has been added, however this doesn&amp;amp;apos;t comply with the javadoc of the very same function (because not "@since 2.2" tag was found). The same thing happens with the methods listed bellow:
&amp;amp;apos;public void checkOrder(double[])&amp;amp;apos;
&amp;amp;apos;public boolean equals(float, float, float)&amp;amp;apos;
&amp;amp;apos;public boolean equals(float, float, int)&amp;amp;apos;
&amp;amp;apos;public boolean equalsIncludingNaN(float, float)&amp;amp;apos;
&amp;amp;apos;public boolean equalsIncludingNaN(float, float, float)&amp;amp;apos;
&amp;amp;apos;public boolean equalsIncludingNaN(float, float, int)&amp;amp;apos;
&amp;amp;apos;public boolean equalsIncludingNaN(float[], float[])&amp;amp;apos;
&amp;amp;apos;public boolean equalsIncludingNaN(double, double)&amp;amp;apos;
&amp;amp;apos;public boolean equalsIncludingNaN(double, double, double)&amp;amp;apos;
&amp;amp;apos;public boolean equalsIncludingNaN(double, double, int)&amp;amp;apos;
&amp;amp;apos;public boolean equalsIncludingNaN(double[], double[])&amp;amp;apos;
&amp;amp;apos;public double safeNorm(double[])&amp;amp;apos;
In addition to this some functions have been deprecated but neither is it mentioned in the javadoc the version as of which they have been deprecated nor the clirr report refers to these methods. These are:
Deprecated Methods:
&amp;amp;apos;public boolean equals(float, float)&amp;amp;apos;
&amp;amp;apos;public boolean equals(float[], float[])&amp;amp;apos;</description>
			<version>2.2</version>
			<fixedVersion>2.2</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.util.MathUtils.java</file>
		</fixedFiles>
	</bug>
	<bug id="533" opendate="2011-02-22 18:08:25" fixdate="2011-02-22 19:53:39" resolution="Fixed">
		<buginformation>
			<summary>[Math] Clirr report and &amp;apos;org.apache.commons.math.util.ResizableDoubleArray&amp;apos;</summary>
			<description>Clirr reports mention that method &amp;amp;apos;public void addElements(double[])&amp;amp;apos; has been added, however there is no "@since 2.2" indication in the javadoc of this function...</description>
			<version>2.2</version>
			<fixedVersion>2.2</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.util.ResizableDoubleArray.java</file>
		</fixedFiles>
	</bug>
	<bug id="519" opendate="2011-02-19 04:37:44" fixdate="2011-02-22 23:51:58" resolution="Fixed">
		<buginformation>
			<summary>GaussianFitter Unexpectedly Throws NotStrictlyPositiveException</summary>
			<description>Running the following:
    	double[] observations = 

{ 
    			1.1143831578403364E-29, 
    			 4.95281403484594E-28, 
    			 1.1171347211930288E-26, 
    			 1.7044813962636277E-25, 
    			 1.9784716574832164E-24, 
    			 1.8630236407866774E-23, 
    			 1.4820532905097742E-22, 
    			 1.0241963854632831E-21, 
    			 6.275077366673128E-21, 
    			 3.461808994532493E-20, 
    			 1.7407124684715706E-19, 
    			 8.056687953553974E-19, 
    			 3.460193945992071E-18, 
    			 1.3883326374011525E-17, 
    			 5.233894983671116E-17, 
    			 1.8630791465263745E-16, 
    			 6.288759227922111E-16, 
    			 2.0204433920597856E-15, 
    			 6.198768938576155E-15, 
    			 1.821419346860626E-14, 
    			 5.139176445538471E-14, 
    			 1.3956427429045787E-13, 
    			 3.655705706448139E-13, 
    			 9.253753324779779E-13, 
    			 2.267636001476696E-12, 
    			 5.3880460095836855E-12, 
    			 1.2431632654852931E-11 
    	}
;
    	GaussianFitter g = 
    		new GaussianFitter(new LevenbergMarquardtOptimizer());
    	for (int index = 0; index &amp;lt; 27; index++)
    	{
    		g.addObservedPoint(index, observations[index]);
    	}
       	g.fit();
Results in:
org.apache.commons.math.exception.NotStrictlyPositiveException: -1.277 is smaller than, or equal to, the minimum (0)
	at org.apache.commons.math.analysis.function.Gaussian$Parametric.validateParameters(Gaussian.java:184)
	at org.apache.commons.math.analysis.function.Gaussian$Parametric.value(Gaussian.java:129)
I&amp;amp;apos;m guessing the initial guess for sigma is off.  </description>
			<version>3.0</version>
			<fixedVersion>3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.optimization.fitting.GaussianFitterTest.java</file>
			<file type="M">org.apache.commons.math.optimization.fitting.GaussianFitter.java</file>
		</fixedFiles>
	</bug>
	<bug id="546" opendate="2011-03-12 04:52:54" fixdate="2011-03-16 12:58:17" resolution="Fixed">
		<buginformation>
			<summary>Truncation issue in KMeansPlusPlusClusterer</summary>
			<description>The for loop inside KMeansPlusPlusClusterer.chooseInitialClusters defines a variable
  int sum = 0;
This variable should have type double, rather than int.  Using an int causes the method to truncate the distances between points to (square roots of) integers.  It&amp;amp;apos;s especially bad when the distances between points are typically less than 1.
As an aside, in version 2.2, this bug manifested itself by making the clusterer return empty clusters.  I wonder if the EmptyClusterStrategy would still be necessary if this bug were fixed.</description>
			<version>3.0</version>
			<fixedVersion>3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.stat.clustering.KMeansPlusPlusClusterer.java</file>
			<file type="M">org.apache.commons.math.stat.clustering.KMeansPlusPlusClustererTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="552" opendate="2011-03-31 22:36:56" fixdate="2011-04-01 10:12:29" resolution="Fixed">
		<buginformation>
			<summary>MultidimensionalCounter.getCounts(int) returns wrong array of indices</summary>
			<description>MultidimensionalCounter counter = new MultidimensionalCounter(2, 4);
for (Integer i : counter) 
{
    int[] x = counter.getCounts(i);
    System.out.println(i + " " + Arrays.toString(x));
}

Output is:
0 [0, 0]
1 [0, 1]
2 [0, 2]
3 [0, 2]   &amp;lt;=== should be [0, 3]
4 [1, 0]
5 [1, 1]
6 [1, 2]
7 [1, 2]   &amp;lt;=== should be [1, 3]</description>
			<version>2.2</version>
			<fixedVersion>3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.util.MultidimensionalCounter.java</file>
			<file type="M">org.apache.commons.math.util.MultidimensionalCounterTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="555" opendate="2011-04-04 04:13:04" fixdate="2011-04-04 04:53:13" resolution="Fixed">
		<buginformation>
			<summary>MathUtils round method should propagate rather than wrap Runitme exceptions</summary>
			<description>MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.</description>
			<version>2.0</version>
			<fixedVersion>3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.util.MathUtilsTest.java</file>
			<file type="M">org.apache.commons.math.util.MathUtils.java</file>
		</fixedFiles>
	</bug>
	<bug id="572" opendate="2011-05-10 08:07:37" fixdate="2011-05-10 18:09:45" resolution="Fixed">
		<buginformation>
			<summary>Constructor parameter not used</summary>
			<description>the constructor public ArrayFieldVector(Field&amp;lt;T&amp;gt; field, T[] v1, T[] v2)
sets this
"this.field = data[0].getField();"
in the fast line...
"this.field = field;"
would be right - field was explicitly provided.</description>
			<version>3.0</version>
			<fixedVersion>3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.linear.ArrayFieldVector.java</file>
		</fixedFiles>
	</bug>
	<bug id="573" opendate="2011-05-10 08:23:22" fixdate="2011-06-05 15:32:33" resolution="Fixed">
		<buginformation>
			<summary>in ArrayFielVector i.e. subtract calls wrong constructor</summary>
			<description>I.E. subtract calls
"return new ArrayFieldVector&amp;lt;T&amp;gt;(out)" this constructor clones the array...
"return new ArrayFieldVector&amp;lt;T&amp;gt;(field, out, false)" would be better (preserving field as well)</description>
			<version>3.0</version>
			<fixedVersion>3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.linear.MatrixUtilsTest.java</file>
			<file type="M">org.apache.commons.math.linear.ArrayFieldVector.java</file>
			<file type="M">org.apache.commons.math.linear.MatrixUtils.java</file>
			<file type="M">org.apache.commons.math.linear.Array2DRowFieldMatrix.java</file>
			<file type="M">org.apache.commons.math.linear.FieldLUDecompositionImplTest.java</file>
			<file type="M">org.apache.commons.math.linear.ArrayFieldVectorTest.java</file>
			<file type="M">org.apache.commons.math.linear.BlockFieldMatrix.java</file>
			<file type="M">org.apache.commons.math.linear.AbstractFieldMatrix.java</file>
			<file type="M">org.apache.commons.math.linear.FieldLUDecompositionImpl.java</file>
			<file type="M">org.apache.commons.math.distribution.KolmogorovSmirnovDistributionImpl.java</file>
			<file type="M">org.apache.commons.math.linear.SparseFieldMatrixTest.java</file>
			<file type="M">org.apache.commons.math.linear.AbstractRealMatrix.java</file>
		</fixedFiles>
	</bug>
	<bug id="582" opendate="2011-05-24 11:33:34" fixdate="2011-06-11 22:30:17" resolution="Fixed">
		<buginformation>
			<summary>Percentile does not work as described in API</summary>
			<description>example call:
StatUtils.percentile(new double[]
{0d, 1d}
, 25)   returns 0.0
The API says that there is a position being computed:  p*(n+1)/100 -&amp;gt; we have p=25 and n=2
I would expect position 0.75 as result. Next step according to the API is: interpolation between both values at floor(0.25) and at ceil(0.25). Those values are 0d and 1d ... so lower + d * (upper - lower) should give 0d + 0.25*(1d - 0d) = 0.25
But the above call returns 0 as result. This does not make sense to me.
another example where I think the result is not correct:
StatUtils.percentile(new double[]
{0d, 1d, 1d, 1d}
, 25)   returns 0.25
we have pos = 25*5/100 = 1.25  ... so d = 0.25
values at position floor(1.25) and ceil(1.25) are 1d and 1d. How comes that the result is not between 1d?
</description>
			<version>2.2</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.stat.descriptive.rank.PercentileTest.java</file>
			<file type="M">org.apache.commons.math.stat.descriptive.rank.Percentile.java</file>
		</fixedFiles>
	</bug>
	<bug id="540" opendate="2011-03-06 00:43:45" fixdate="2011-06-12 05:58:50" resolution="Fixed">
		<buginformation>
			<summary>AbstractIntegerDistribution.inverseCumulativeProbability(...) Bug</summary>
			<description>The AbstractIntegerDistribution.inverseCumulativeProbability(...) function attempts to decrement the lower bound of discrete distributions to values that go below the lower bound.</description>
			<version>2.1</version>
			<fixedVersion>3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.distribution.AbstractIntegerDistribution.java</file>
		</fixedFiles>
	</bug>
	<bug id="589" opendate="2011-06-13 23:00:56" fixdate="2011-06-15 16:43:21" resolution="Fixed">
		<buginformation>
			<summary>JavaDoc error in OneWayAnovaImpl</summary>
			<description>JavaDoc por OneWayAnovaImpl say:
Implements one-way ANOVA statistics defined in the OneWayAnovaImpl interface. 
And the Javadoc must say:
Implements one-way ANOVA statistics defined in the OneWayAnova interface. </description>
			<version>2.2</version>
			<fixedVersion>3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.stat.inference.OneWayAnovaImpl.java</file>
		</fixedFiles>
	</bug>
	<bug id="619" opendate="2011-07-14 05:33:27" fixdate="2011-07-14 06:15:50" resolution="Fixed">
		<buginformation>
			<summary>ADJUSTED R SQUARED INCORRECT IN REGRESSION RESULTS</summary>
			<description>I forgot to cast to double when dividing two integers:
            this.globalFitInfo[ADJRSQ_IDX] = 1.0 - 
                    (1.0 - this.globalFitInfo[RSQ_IDX]) *
                    (  nobs / ( (nobs - rank)));
Should be
            this.globalFitInfo[ADJRSQ_IDX] = 1.0 - 
                    (1.0 - this.globalFitInfo[RSQ_IDX]) *
                    ( (double) nobs / ( (double) (nobs - rank)));
Patch attached.</description>
			<version>3.0</version>
			<fixedVersion>3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.stat.regression.RegressionResults.java</file>
		</fixedFiles>
	</bug>
	<bug id="465" opendate="2011-01-05 17:34:41" fixdate="2011-07-20 12:20:51" resolution="Fixed">
		<buginformation>
			<summary>Incorrect matrix rank via SVD</summary>
			<description>The getRank() function of SingularValueDecompositionImpl does not work properly. This problem is probably related to the numerical stability problems mentioned in MATH-327 and MATH-320.
Example call with the standard matrix from R (rank 2):
TestSVDRank.java

import org.apache.commons.math.linear.Array2DRowRealMatrix;
import org.apache.commons.math.linear.RealMatrix;
import org.apache.commons.math.linear.SingularValueDecomposition;
import org.apache.commons.math.linear.SingularValueDecompositionImpl;

public class TestSVDRank {
	public static void main(String[] args) {
		double[][] d = { { 1, 1, 1 }, { 0, 0, 0 }, { 1, 2, 3 } };
		RealMatrix m = new Array2DRowRealMatrix(d);
		SingularValueDecomposition svd = new SingularValueDecompositionImpl(m);
		int r = svd.getRank();
		System.out.println("Rank: "+r);
	}
}


The rank is computed as 3. This problem also occurs for larger matrices. I discovered the problem when trying to replace the corresponding JAMA method.</description>
			<version>2.1</version>
			<fixedVersion>3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.special.Erf.java</file>
			<file type="M">org.apache.commons.math.special.ErfTest.java</file>
		</fixedFiles>
		<links>
			<link type="Incorporates" description="is part of">611</link>
		</links>
	</bug>
	<bug id="640" opendate="2011-08-02 19:06:35" fixdate="2011-08-03 04:17:43" resolution="Fixed">
		<buginformation>
			<summary>AbstractRandomGenerator nextInt() and nextLong() default implementations generate only positive values</summary>
			<description>The javadoc for these methods (and what is specified in the RandomGenerator interface) says that all int / long values should be in the range of these methods.  The default implementations provided in this class do not generate negative values.</description>
			<version>1.1</version>
			<fixedVersion>3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.random.MersenneTwisterTest.java</file>
			<file type="M">org.apache.commons.math.random.Well44497bTest.java</file>
			<file type="M">org.apache.commons.math.random.Well512aTest.java</file>
			<file type="M">org.apache.commons.math.random.Well44497aTest.java</file>
			<file type="M">org.apache.commons.math.random.Well19937aTest.java</file>
			<file type="M">org.apache.commons.math.random.RandomDataTest.java</file>
			<file type="M">org.apache.commons.math.random.Well1024aTest.java</file>
			<file type="M">org.apache.commons.math.random.AbstractRandomGeneratorTest.java</file>
			<file type="M">org.apache.commons.math.random.Well19937cTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="506" opendate="2011-02-01 18:38:01" fixdate="2011-08-20 21:14:57" resolution="Fixed">
		<buginformation>
			<summary>The static field ChiSquareTestImpl.distribution serves no purpose</summary>
			<description>The static field ChiSquareTestImpl.distribution serves no purpose.
There is a setter for it, but in every case where the field is used, it is first overwritten with a new value.
The field and the setter should be removed, and the methods that create a new instance should create a local variable instead.
For Math 2.1, the field can be removed and the setter deprecated.</description>
			<version>1.2</version>
			<fixedVersion>3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.stat.inference.ChiSquareTestImpl.java</file>
		</fixedFiles>
	</bug>
	<bug id="648" opendate="2011-08-20 21:27:31" fixdate="2011-08-20 21:46:40" resolution="Fixed">
		<buginformation>
			<summary>SimpleRegression has extraneous constructor</summary>
			<description>The SimpleRegression(int) constructor added in version 2.2 has no effect on any statistics computed by the class.  The private TDistributionImpl data member that this constructor initializes is no longer meaningful, as a new distribution instance is created each time it is needed.  The T distribution implementation used in computations is no longer meaningfully pluggable, so the instance field should be removed, along with this constructor.</description>
			<version>2.2</version>
			<fixedVersion>3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.stat.regression.SimpleRegression.java</file>
		</fixedFiles>
	</bug>
	<bug id="645" opendate="2011-08-13 16:18:48" fixdate="2011-09-02 20:54:19" resolution="Fixed">
		<buginformation>
			<summary>MathRuntimeException with simple ebeMultiply on OpenMapRealVector</summary>
			<description>The following piece of code


import org.apache.commons.math.linear.OpenMapRealVector;
import org.apache.commons.math.linear.RealVector;

public class DemoBugOpenMapRealVector {
    public static void main(String[] args) {
        final RealVector u = new OpenMapRealVector(3, 1E-6);
        u.setEntry(0, 1.);
        u.setEntry(1, 0.);
        u.setEntry(2, 2.);
        final RealVector v = new OpenMapRealVector(3, 1E-6);
        v.setEntry(0, 0.);
        v.setEntry(1, 3.);
        v.setEntry(2, 0.);
        System.out.println(u);
        System.out.println(v);
        System.out.println(u.ebeMultiply(v));
    }
}


raises an exception

org.apache.commons.math.linear.OpenMapRealVector@7170a9b6
Exception in thread "main" org.apache.commons.math.MathRuntimeException$6: map has been modified while iterating
	at org.apache.commons.math.MathRuntimeException.createConcurrentModificationException(MathRuntimeException.java:373)
	at org.apache.commons.math.util.OpenIntToDoubleHashMap$Iterator.advance(OpenIntToDoubleHashMap.java:564)
	at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:372)
	at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:1)
	at DemoBugOpenMapRealVector.main(DemoBugOpenMapRealVector.java:17)

</description>
			<version>3.0</version>
			<fixedVersion>3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.linear.SparseRealVectorTest.java</file>
			<file type="M">org.apache.commons.math.linear.OpenMapRealVector.java</file>
		</fixedFiles>
	</bug>
	<bug id="669" opendate="2011-09-15 14:14:04" fixdate="2011-09-15 14:28:55" resolution="Fixed">
		<buginformation>
			<summary>UnivariateRealIntegrator throws ConvergenceException</summary>
			<description>ConvergenceException is a checked exception, which goes against the developer&amp;amp;apos;s guide. It occurs in the throws clause of some methods in package o.a.c.m.analysis.integration. It seems that these occurences are remnants from previous versions, where exceptions were probably checked. This exception is actually never thrown : it is safe to remove it from the throws clause.</description>
			<version>3.0</version>
			<fixedVersion>3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.analysis.integration.UnivariateRealIntegrator.java</file>
			<file type="M">org.apache.commons.math.analysis.integration.UnivariateRealIntegratorImpl.java</file>
			<file type="M">org.apache.commons.math.analysis.integration.TrapezoidIntegrator.java</file>
			<file type="M">org.apache.commons.math.analysis.integration.RombergIntegrator.java</file>
			<file type="M">org.apache.commons.math.analysis.integration.LegendreGaussIntegrator.java</file>
			<file type="M">org.apache.commons.math.analysis.integration.SimpsonIntegrator.java</file>
		</fixedFiles>
	</bug>
	<bug id="601" opendate="2011-06-23 19:31:50" fixdate="2011-09-18 21:07:04" resolution="Fixed">
		<buginformation>
			<summary>SingularValueDecompositionImpl psuedoinverse is not consistent with Rank calculation</summary>
			<description>In the SingularValueDecompositionImpl&amp;amp;apos;s internal private class Solver, a pseudo inverse matrix is calculated:
In lines 2600-264 we have:
                if (singularValues[i] &amp;gt; 0) 
{
                 a = 1 / singularValues[i];
                }
 else 
{
                 a = 0;
                }

This is not consistent with the manner in which rank is determined (lines 225 to 233). That is to say a matrix could potentially be rank deficient, yet the psuedoinverse would still include the redundant columns... 
Also, there is the problem of very small singular values which could result in overflow.  </description>
			<version>2.2</version>
			<fixedVersion>3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.linear.SingularValueDecompositionImpl.java</file>
		</fixedFiles>
	</bug>
	<bug id="380" opendate="2010-06-24 16:47:54" fixdate="2011-10-01 13:54:20" resolution="Fixed">
		<buginformation>
			<summary>Need to (re)initialize dYdY0 for multiple integrate with FirstOrderIntegratorWithJacobians</summary>
			<description>There is a lack in the method integrate of FirstOrderIntegratorWithJacobians. The jacobian DYDY0 can&amp;amp;apos;t be initialized by the user, unlike DFDP with DF0DP.
So, for several successive integrations, the matrix is reinitialized to identity and that is not what we might want.</description>
			<version>2.1</version>
			<fixedVersion>3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="D">org.apache.commons.math.ode.jacobians.FirstOrderIntegratorWithJacobiansTest.java</file>
			<file type="D">org.apache.commons.math.ode.jacobians.StepHandlerWithJacobians.java</file>
			<file type="D">org.apache.commons.math.ode.jacobians.ODEWithJacobians.java</file>
			<file type="D">org.apache.commons.math.ode.jacobians.StepInterpolatorWithJacobians.java</file>
			<file type="D">org.apache.commons.math.ode.jacobians.ParameterizedODE.java</file>
			<file type="D">org.apache.commons.math.ode.jacobians.EventHandlerWithJacobians.java</file>
			<file type="D">org.apache.commons.math.ode.jacobians.FirstOrderIntegratorWithJacobians.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="is blocked by">381</link>
		</links>
	</bug>
	<bug id="691" opendate="2011-10-16 17:18:34" fixdate="2011-11-27 05:20:46" resolution="Fixed">
		<buginformation>
			<summary>Statistics.setVarianceImpl makes getStandardDeviation produce NaN</summary>
			<description>Invoking SummaryStatistics.setVarianceImpl(new Variance(true/false) makes getStandardDeviation produce NaN. The code to reproduce it:


int[] scores = {1, 2, 3, 4};
SummaryStatistics stats = new SummaryStatistics();
stats.setVarianceImpl(new Variance(false)); //use "population variance"
for(int i : scores) {
  stats.addValue(i);
}
double sd = stats.getStandardDeviation();
System.out.println(sd);


A workaround suggested by Mikkel is:


  double sd = FastMath.sqrt(stats.getSecondMoment() / stats.getN());

</description>
			<version>2.2</version>
			<fixedVersion>3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.stat.descriptive.SummaryStatistics.java</file>
			<file type="M">org.apache.commons.math.stat.descriptive.SummaryStatisticsTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="704" opendate="2011-11-08 23:08:28" fixdate="2011-11-30 06:25:55" resolution="Fixed">
		<buginformation>
			<summary>One of Variance.evaluate() methods does not work correctly</summary>
			<description>The method org.apache.commons.math.stat.descriptive.moment.Variance.evaluate(double[] values, double[] weights, double mean, int begin, int length) does not work properly. Looks loke it ignores the length parameter and grabs the whole dataset.
Similar method in Mean class seems to work.
I did not check other methods taking the part of the array; they may have the same problem.
Workaround: I had to shrink my arrays and use the method without the length.</description>
			<version>2.2</version>
			<fixedVersion>3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.stat.descriptive.UnivariateStatisticAbstractTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="699" opendate="2011-11-01 08:35:34" fixdate="2011-12-05 06:50:36" resolution="Fixed">
		<buginformation>
			<summary>inverseCumulativeDistribution fails with cumulative distribution having a plateau</summary>
			<description>This bug report follows MATH-692. The attached unit test fails. As required by the definition in MATH-692, the lower-bound of the interval on which the cdf is constant should be returned. This is not so at the moment.</description>
			<version>2.2</version>
			<fixedVersion>3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.distribution.ChiSquaredDistribution.java</file>
			<file type="M">org.apache.commons.math.distribution.NormalDistribution.java</file>
			<file type="M">org.apache.commons.math.distribution.TDistribution.java</file>
			<file type="M">org.apache.commons.math.distribution.BetaDistribution.java</file>
			<file type="M">org.apache.commons.math.distribution.FDistribution.java</file>
			<file type="M">org.apache.commons.math.distribution.GammaDistribution.java</file>
			<file type="M">org.apache.commons.math.distribution.AbstractRealDistribution.java</file>
			<file type="M">org.apache.commons.math.distribution.BinomialDistribution.java</file>
			<file type="M">org.apache.commons.math.distribution.PoissonDistribution.java</file>
			<file type="M">org.apache.commons.math.distribution.KolmogorovSmirnovDistribution.java</file>
			<file type="M">org.apache.commons.math.distribution.ZipfDistribution.java</file>
			<file type="M">org.apache.commons.math.distribution.PascalDistribution.java</file>
			<file type="M">org.apache.commons.math.distribution.HypergeometricDistribution.java</file>
			<file type="M">org.apache.commons.math.distribution.AbstractRealDistributionTest.java</file>
			<file type="M">org.apache.commons.math.distribution.IntegerDistribution.java</file>
			<file type="M">org.apache.commons.math.distribution.CauchyDistribution.java</file>
			<file type="M">org.apache.commons.math.distribution.WeibullDistribution.java</file>
			<file type="M">org.apache.commons.math.distribution.AbstractIntegerDistribution.java</file>
			<file type="M">org.apache.commons.math.distribution.RealDistribution.java</file>
			<file type="M">org.apache.commons.math.distribution.ExponentialDistribution.java</file>
		</fixedFiles>
	</bug>
	<bug id="723" opendate="2011-12-11 21:03:37" fixdate="2011-12-11 21:59:41" resolution="Fixed">
		<buginformation>
			<summary>BitStreamGenerators (MersenneTwister, Well generators) do not clear normal deviate cache on setSeed</summary>
			<description>The BitStream generators generate normal deviates (for nextGaussian) in pairs, caching the last value generated. When reseeded, the cache should be cleared; otherwise seeding two generators with the same value is not guaranteed to generate the same sequence.</description>
			<version>2.0</version>
			<fixedVersion>3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.random.ISAACTest.java</file>
			<file type="M">org.apache.commons.math.random.ISAACRandom.java</file>
		</fixedFiles>
	</bug>
	<bug id="722" opendate="2011-12-09 21:34:40" fixdate="2012-01-27 07:01:40" resolution="Fixed">
		<buginformation>
			<summary>[math] Complex Tanh for "big" numbers</summary>
			<description>Hi,
In Complex.java the tanh is computed with the following formula:
tanh(a + bi) = sinh(2a)/(cosh(2a)+cos(2b)) + [sin(2b)/(cosh(2a)+cos(2b))]i
The problem that I&amp;amp;apos;m finding is that as soon as "a" is a "big" number,
both sinh(2a) and cosh(2a) are infinity and then the method tanh returns in
the real part NaN (infinity/infinity) when it should return 1.0.
Wouldn&amp;amp;apos;t it be appropiate to add something as in the FastMath library??:
if (real&amp;gt;20.0)
{
      return createComplex(1.0, 0.0);
}
if (real&amp;lt;-20.0)
{
      return createComplex(-1.0, 0.0);
}


Best regards,
JBB</description>
			<version>2.2</version>
			<fixedVersion>3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.complex.Complex.java</file>
			<file type="M">org.apache.commons.math.complex.ComplexTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="692" opendate="2011-10-18 18:01:57" fixdate="2012-02-02 06:45:59" resolution="Fixed">
		<buginformation>
			<summary>Cumulative probability and inverse cumulative probability inconsistencies</summary>
			<description>There are some inconsistencies in the documentation and implementation of functions regarding cumulative probabilities and inverse cumulative probabilities. More precisely, &amp;amp;apos;&amp;lt;&amp;amp;apos; and &amp;amp;apos;&amp;lt;=&amp;amp;apos; are not used in a consistent way.
Besides I would move the function inverseCumulativeProbability(double) to the interface Distribution. A true inverse of the distribution function does neither exist for Distribution nor for ContinuosDistribution. Thus we need to define the inverse in terms of quantiles anyway, and this can already be done for Distribution.
On the whole I would declare the (inverse) cumulative probability functions in the basic distribution interfaces as follows:
Distribution:

cumulativeProbability(double x): returns P(X &amp;lt;= x)
cumulativeProbability(double x0, double x1): returns P(x0 &amp;lt; X &amp;lt;= x1) [see also 1)]
inverseCumulativeProbability(double p):
  returns the quantile function inf
{x in R | P(X&amp;lt;=x) &amp;gt;= p}
 [see also 2), 3), and 4)]

1) An aternative definition could be P(x0 &amp;lt;= X &amp;lt;= x1). But this requires to put the function probability(double x) or another cumulative probability function into the interface Distribution in order be able to calculate P(x0 &amp;lt;= X &amp;lt;= x1) in AbstractDistribution.
2) This definition is stricter than the definition in ContinuousDistribution, because the definition there does not specify what to do if there are multiple x satisfying P(X&amp;lt;=x) = p.
3) A modification could be defined for p=0: Returning sup
{x in R | P(X&amp;lt;=x) = 0}
 would yield the infimum of the distribution&amp;amp;apos;s support instead of a mandatory -infinity.
4) This affects issue MATH-540. I&amp;amp;apos;d prefere the definition from above for the following reasons:

This definition simplifies inverse transform sampling (as mentioned in the other issue).
It is the standard textbook definition for the quantile function.
For integer distributions it has the advantage that the result doesn&amp;amp;apos;t change when switching to "x in Z", i.e. the result is independent of considering the intergers as sole set or as part of the reals.

ContinuousDistribution:
nothing to be added regarding (inverse) cumulative probability functions
IntegerDistribution:

cumulativeProbability(int x): returns P(X &amp;lt;= x)
cumulativeProbability(int x0, int x1): returns P(x0 &amp;lt; X &amp;lt;= x1) [see also 1) above]

</description>
			<version>1.0</version>
			<fixedVersion>3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.distribution.HypergeometricDistribution.java</file>
			<file type="M">org.apache.commons.math.random.ISAACTest.java</file>
			<file type="M">org.apache.commons.math.distribution.PoissonDistribution.java</file>
			<file type="M">org.apache.commons.math.distribution.PascalDistributionTest.java</file>
			<file type="M">org.apache.commons.math.distribution.IntegerDistributionAbstractTest.java</file>
			<file type="M">org.apache.commons.math.distribution.ZipfDistribution.java</file>
			<file type="M">org.apache.commons.math.distribution.PascalDistribution.java</file>
			<file type="M">org.apache.commons.math.random.RandomDataTest.java</file>
			<file type="M">org.apache.commons.math.distribution.IntegerDistribution.java</file>
			<file type="M">org.apache.commons.math.random.RandomGeneratorAbstractTest.java</file>
			<file type="M">org.apache.commons.math.random.Well1024aTest.java</file>
			<file type="M">org.apache.commons.math.random.RandomDataImpl.java</file>
			<file type="M">org.apache.commons.math.distribution.PoissonDistributionTest.java</file>
			<file type="M">org.apache.commons.math.distribution.BinomialDistribution.java</file>
			<file type="M">org.apache.commons.math.distribution.HypergeometricDistributionTest.java</file>
			<file type="M">org.apache.commons.math.distribution.AbstractIntegerDistribution.java</file>
			<file type="M">org.apache.commons.math.distribution.ZipfDistributionTest.java</file>
			<file type="M">org.apache.commons.math.distribution.BinomialDistributionTest.java</file>
			<file type="M">org.apache.commons.math.distribution.AbtractIntegerDistributionTest.java</file>
			<file type="M">org.apache.commons.math.distribution.TDistributionTest.java</file>
			<file type="M">org.apache.commons.math.distribution.ContinuousDistribution.java</file>
			<file type="M">org.apache.commons.math.distribution.BetaDistributionImpl.java</file>
			<file type="M">org.apache.commons.math.distribution.AbstractContinuousDistribution.java</file>
			<file type="M">org.apache.commons.math.distribution.GammaDistributionImpl.java</file>
			<file type="M">org.apache.commons.math.distribution.TDistributionImpl.java</file>
			<file type="M">org.apache.commons.math.distribution.ContinuousDistributionAbstractTest.java</file>
			<file type="M">org.apache.commons.math.distribution.ChiSquaredDistributionImpl.java</file>
			<file type="M">org.apache.commons.math.distribution.AbstractDistribution.java</file>
			<file type="M">org.apache.commons.math.distribution.CauchyDistributionImpl.java</file>
			<file type="M">org.apache.commons.math.distribution.NormalDistributionImpl.java</file>
			<file type="M">org.apache.commons.math.distribution.ExponentialDistributionImpl.java</file>
			<file type="M">org.apache.commons.math.distribution.FDistributionImpl.java</file>
			<file type="M">org.apache.commons.math.distribution.WeibullDistributionImpl.java</file>
			<file type="M">org.apache.commons.math.distribution.Distribution.java</file>
		</fixedFiles>
	</bug>
	<bug id="575" opendate="2011-05-14 16:40:34" fixdate="2012-02-02 11:12:52" resolution="Fixed">
		<buginformation>
			<summary>Exceptions in genetics package or not consistent with the rest of [math]</summary>
			<description>InvalidRepresentationException is checked and non-localized.  This exception should be placed in the [math] hierarchy.  The AbstractListChromosome constructor also throws a non-localised IAE, which should be replaced by an appropriate [math] exception.</description>
			<version>2.0</version>
			<fixedVersion>3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.genetics.RandomKey.java</file>
			<file type="M">org.apache.commons.math.genetics.GeneticAlgorithm.java</file>
			<file type="M">org.apache.commons.math.genetics.OnePointCrossover.java</file>
			<file type="M">org.apache.commons.math.exception.util.LocalizedFormats.java</file>
			<file type="M">org.apache.commons.math.genetics.ElitisticListPopulation.java</file>
			<file type="M">org.apache.commons.math.genetics.FixedGenerationCount.java</file>
			<file type="M">org.apache.commons.math.genetics.BinaryChromosome.java</file>
			<file type="M">org.apache.commons.math.genetics.RandomKeyMutation.java</file>
			<file type="M">org.apache.commons.math.genetics.PermutationChromosome.java</file>
			<file type="M">org.apache.commons.math.genetics.ListPopulation.java</file>
			<file type="M">org.apache.commons.math.genetics.BinaryMutation.java</file>
			<file type="M">org.apache.commons.math.genetics.TournamentSelection.java</file>
			<file type="M">org.apache.commons.math.genetics.InvalidRepresentationException.java</file>
			<file type="M">org.apache.commons.math.genetics.Chromosome.java</file>
			<file type="M">org.apache.commons.math.genetics.AbstractListChromosome.java</file>
			<file type="M">org.apache.commons.math.genetics.RandomKeyTest.java</file>
			<file type="M">org.apache.commons.math.genetics.BinaryChromosomeTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="588" opendate="2011-06-12 18:19:07" fixdate="2012-02-05 19:54:50" resolution="Fixed">
		<buginformation>
			<summary>Weighted Mean evaluation may not have optimal numerics</summary>
			<description>I recently got this in a test run


testWeightedConsistency(org.apache.commons.math.stat.descriptive.moment.MeanTest)  Time elapsed: 0 sec  &amp;lt;&amp;lt;&amp;lt; FAILURE!
java.lang.AssertionError: expected:&amp;lt;0.002282165958997601&amp;gt; but was:&amp;lt;0.002282165958997157&amp;gt;
	at org.junit.Assert.fail(Assert.java:91)
	at org.junit.Assert.failNotEquals(Assert.java:645)
	at org.junit.Assert.assertEquals(Assert.java:441)
	at org.apache.commons.math.TestUtils.assertRelativelyEquals(TestUtils.java:178)
	at org.apache.commons.math.TestUtils.assertRelativelyEquals(TestUtils.java:153)
	at org.apache.commons.math.stat.descriptive.UnivariateStatisticAbstractTest.testWeightedConsistency(UnivariateStatisticAbstractTest.java:170)


The correction formula used to compute the unweighted mean may not be appropriate or optimal in the presence of weights:


// Compute initial estimate using definitional formula
double sumw = sum.evaluate(weights,begin,length);
double xbarw = sum.evaluate(values, weights, begin, length) / sumw;

// Compute correction factor in second pass
double correction = 0;
for (int i = begin; i &amp;lt; begin + length; i++) {
  correction += weights[i] * (values[i] - xbarw);
}
return xbarw + (correction/sumw);

</description>
			<version>2.1</version>
			<fixedVersion>3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.stat.descriptive.UnivariateStatisticAbstractTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="728" opendate="2011-12-20 02:05:20" fixdate="2012-02-11 23:15:55" resolution="Fixed">
		<buginformation>
			<summary>Errors in BOBYQAOptimizer when numberOfInterpolationPoints is greater than 2*dim+1</summary>
			<description>I&amp;amp;apos;ve been having trouble getting BOBYQA to minimize a function (actually a non-linear least squares fit) so as one change I increased the number of interpolation points.  It seems that anything larger than 2*dim+1 causes an error (typically at
line 1662
                   interpolationPoints.setEntry(nfm, ipt, interpolationPoints.getEntry(ipt, ipt));
I&amp;amp;apos;m guessing there is an off by one error in the translation from FORTRAN.  Changing the BOBYQAOptimizerTest as follows (increasing number of interpolation points by one) will cause failures.
Bruce
Index: src/test/java/org/apache/commons/math/optimization/direct/BOBYQAOptimizerTest.java
===================================================================
 src/test/java/org/apache/commons/math/optimization/direct/BOBYQAOptimizerTest.java	(revision 1221065)
+++ src/test/java/org/apache/commons/math/optimization/direct/BOBYQAOptimizerTest.java	(working copy)
@@ -258,7 +258,7 @@
 //        RealPointValuePair result = optim.optimize(100000, func, goal, startPoint);
         final double[] lB = boundaries == null ? null : boundaries[0];
         final double[] uB = boundaries == null ? null : boundaries[1];

BOBYQAOptimizer optim = new BOBYQAOptimizer(2 * dim + 1);
+        BOBYQAOptimizer optim = new BOBYQAOptimizer(2 * dim + 2);
         RealPointValuePair result = optim.optimize(maxEvaluations, func, goal, startPoint, lB, uB);
 //        System.out.println(func.getClass().getName() + " = " 
 //              + optim.getEvaluations() + " f(");

</description>
			<version>3.0</version>
			<fixedVersion>3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.optimization.direct.BOBYQAOptimizerTest.java</file>
			<file type="M">org.apache.commons.math.optimization.direct.BOBYQAOptimizer.java</file>
		</fixedFiles>
	</bug>
	<bug id="744" opendate="2012-02-12 16:11:17" fixdate="2012-02-14 13:31:25" resolution="Fixed">
		<buginformation>
			<summary>BigFraction.doubleValue() returns Double.NaN for large numerators or denominators</summary>
			<description>The current implementation of doubleValue() divides numerator.doubleValue() / denominator.doubleValue().  BigInteger.doubleValue() fails for any number greater than Double.MAX_VALUE.  So if the user has 308-digit numerator or denominator, the resulting quotient fails, even in cases where the result would be well inside Double&amp;amp;apos;s range.
I have a patch to fix it, if I can figure out how to attach it here I will.</description>
			<version>2.2</version>
			<fixedVersion>3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math.fraction.BigFraction.java</file>
			<file type="M">org.apache.commons.math.fraction.BigFractionTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="770" opendate="2012-03-23 07:24:21" fixdate="2012-03-23 08:14:24" resolution="Fixed">
		<buginformation>
			<summary>SymmLQ not tested in SymmLQTest</summary>
			<description>In SymmLQTest, two test actually create instances of ConjugateGradient instead of SymmLQ. These tests are

testUnpreconditionedNormOfResidual()
testPreconditionedNormOfResidual().

</description>
			<version>3.0</version>
			<fixedVersion>3.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.linear.SymmLQTest.java</file>
			<file type="M">org.apache.commons.math3.linear.JacobiPreconditioner.java</file>
		</fixedFiles>
	</bug>
	<bug id="776" opendate="2012-04-02 17:15:33" fixdate="2012-04-02 18:47:29" resolution="Fixed">
		<buginformation>
			<summary>Need range checks for elitismRate in ElitisticListPopulation constructors.</summary>
			<description>There is a range check for setting the elitismRate via ElitisticListPopulation&amp;amp;apos;s setElitismRate method, but not via the constructors.</description>
			<version>3.0</version>
			<fixedVersion>3.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.genetics.ElitisticListPopulation.java</file>
			<file type="M">org.apache.commons.math3.genetics.ElitisticListPopulationTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="775" opendate="2012-04-02 17:09:14" fixdate="2012-04-12 18:19:25" resolution="Fixed">
		<buginformation>
			<summary>In the ListPopulation constructor, the check for a negative populationLimit should occur first.</summary>
			<description>In the ListPopulation constructor, the check to see whether the populationLimit is positive should occur before the check to see if the number of chromosomes is greater than the populationLimit.</description>
			<version>3.0</version>
			<fixedVersion>3.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.genetics.ListPopulation.java</file>
			<file type="M">org.apache.commons.math3.genetics.ElitisticListPopulation.java</file>
			<file type="M">org.apache.commons.math3.genetics.Population.java</file>
			<file type="M">org.apache.commons.math3.genetics.ListPopulationTest.java</file>
			<file type="M">org.apache.commons.math3.genetics.TournamentSelection.java</file>
		</fixedFiles>
	</bug>
	<bug id="779" opendate="2012-04-11 15:28:35" fixdate="2012-04-12 18:34:06" resolution="Fixed">
		<buginformation>
			<summary>ListPopulation Iterator allows you to remove chromosomes from the population.</summary>
			<description>Calling the iterator method of ListPopulation returns an iterator of the protected modifiable list. Before returning the iterator we should wrap it in an unmodifiable list.</description>
			<version>3.0</version>
			<fixedVersion>3.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.genetics.ListPopulation.java</file>
			<file type="M">org.apache.commons.math3.genetics.ListPopulationTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="782" opendate="2012-04-23 12:36:24" fixdate="2012-04-27 23:31:21" resolution="Fixed">
		<buginformation>
			<summary>BrentOptimizer: User-defined check block is badly placed</summary>
			<description>The CM implementation of Brent&amp;amp;apos;s original algorithm was supposed to allow for a user-defined stopping criterion (in addition to Brent&amp;amp;apos;s default one).
However, it turns out that this additional block of code is not at the right location, implying an unwanted early exit.</description>
			<version>3.0</version>
			<fixedVersion>3.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.optimization.univariate.BrentOptimizer.java</file>
			<file type="M">org.apache.commons.math3.optimization.univariate.BrentOptimizerTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="781" opendate="2012-04-21 07:11:57" fixdate="2012-05-02 18:29:29" resolution="Fixed">
		<buginformation>
			<summary>SimplexSolver gives bad results</summary>
			<description>Methode SimplexSolver.optimeze(...) gives bad results with commons-math3-3.0
in a simple test problem. It works well in commons-math-2.2. </description>
			<version>3.0</version>
			<fixedVersion>3.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.optimization.linear.SimplexTableau.java</file>
			<file type="M">org.apache.commons.math3.optimization.linear.SimplexSolverTest.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">813</link>
		</links>
	</bug>
	<bug id="718" opendate="2011-12-04 00:40:44" fixdate="2012-05-21 19:56:36" resolution="Fixed">
		<buginformation>
			<summary>inverseCumulativeProbability of BinomialDistribution returns wrong value for large trials.</summary>
			<description>The inverseCumulativeProbability method of the BinomialDistributionImpl class returns wrong value for large trials.  Following code will be reproduce the problem.
System.out.println(new BinomialDistributionImpl(1000000, 0.5).inverseCumulativeProbability(0.5));
This returns 499525, though it should be 499999.
I&amp;amp;apos;m not sure how it should be fixed, but the cause is that the cumulativeProbability method returns Infinity, not NaN.  As the result the checkedCumulativeProbability method doesn&amp;amp;apos;t work as expected.</description>
			<version>2.2</version>
			<fixedVersion>3.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.distribution.FDistributionTest.java</file>
			<file type="M">org.apache.commons.math3.distribution.BinomialDistributionTest.java</file>
			<file type="M">org.apache.commons.math3.util.ContinuedFraction.java</file>
			<file type="M">org.apache.commons.math3.distribution.PascalDistribution.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">785</link>
		</links>
	</bug>
	<bug id="644" opendate="2011-08-13 06:26:59" fixdate="2012-05-31 23:45:24" resolution="Fixed">
		<buginformation>
			<summary>for the class of hyper-geometric distribution, for some number the method "upperCumulativeProbability" return a probability greater than 1 which is impossible.  </summary>
			<description>In windows 7, I used common.Math library. I used class "HypergeometricDistributionImpl" and method "upperCumulativeProbability" of zero for distribution and the return value is larget than 1. the following code is working error. 
HypergeometricDistributionImpl u = new HypergeometricDistributionImpl(14761461, 1035 ,1841 );
System.out.println(u.upperCumulativeProbability(0))
Thanks</description>
			<version>2.2</version>
			<fixedVersion>3.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.distribution.HypergeometricDistributionTest.java</file>
			<file type="M">org.apache.commons.math3.distribution.HypergeometricDistribution.java</file>
		</fixedFiles>
	</bug>
	<bug id="802" opendate="2012-06-09 13:05:52" fixdate="2012-06-09 13:14:38" resolution="Fixed">
		<buginformation>
			<summary>RealVector.subtract(RealVector) returns wrong answer.</summary>
			<description>The following piece of code


import org.apache.commons.math3.linear.ArrayRealVector;
import org.apache.commons.math3.linear.OpenMapRealVector;
import org.apache.commons.math3.linear.RealVectorFormat;

public class DemoMath {

    public static void main(String[] args) {
        final double[] data1 = {
            0d, 1d, 0d, 0d, 2d
        };
        final double[] data2 = {
            3d, 0d, 4d, 0d, 5d
        };
        final RealVectorFormat format = new RealVectorFormat();
        System.out.println(format.format(new ArrayRealVector(data1)
            .subtract(new ArrayRealVector(data2))));
        System.out.println(format.format(new OpenMapRealVector(data1)
            .subtract(new ArrayRealVector(data2))));
    }
}


prints

{-3; 1; -4; 0; -3}
{3; 1; 4; 0; -3}


the second line being wrong. In fact, when subtracting mixed types, OpenMapRealVector delegates to the default implementation in RealVector which is buggy.</description>
			<version>3.0</version>
			<fixedVersion>3.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.linear.RealVector.java</file>
		</fixedFiles>
	</bug>
	<bug id="790" opendate="2012-05-19 17:01:30" fixdate="2012-06-12 14:28:08" resolution="Fixed">
		<buginformation>
			<summary>Mann-Whitney U Test Suffers From Integer Overflow With Large Data Sets</summary>
			<description>When performing a Mann-Whitney U Test on large data sets (the attached test uses two 1500 element sets), intermediate integer values used in calculateAsymptoticPValue can overflow, leading to invalid results, such as p-values of NaN, or incorrect calculations.
Attached is a patch, including a test, and a fix, which modifies the affected code to use doubles</description>
			<version>3.0</version>
			<fixedVersion>3.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.stat.inference.MannWhitneyUTest.java</file>
			<file type="M">org.apache.commons.math3.stat.inference.MannWhitneyUTestTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="798" opendate="2012-05-31 20:22:01" fixdate="2012-07-04 18:01:53" resolution="Fixed">
		<buginformation>
			<summary>PolynomialFitter.fit() stalls</summary>
			<description>Hi, in certain cases I ran into the problem that the PolynomialFitter.fit() method stalls, meaning that it does not return, nor throw an Exception (even if it runs for 90 min). Is there a way to tell the PolynomialFitter to iterate only N-times to ensure that my program does not stall?</description>
			<version>3.0</version>
			<fixedVersion>3.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.optimization.fitting.PolynomialFitterTest.java</file>
			<file type="M">org.apache.commons.math3.optimization.fitting.PolynomialFitter.java</file>
			<file type="M">org.apache.commons.math3.optimization.fitting.CurveFitterTest.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">799</link>
			<link type="Reference" description="is related to">800</link>
		</links>
	</bug>
	<bug id="813" opendate="2012-07-10 14:58:21" fixdate="2012-07-10 20:05:31" resolution="Duplicate">
		<buginformation>
			<summary>SimplexSolver bug?</summary>
			<description>I am trying to use the SimplexSolver in commons-math3-3.0 and am getting unpredictable results. I am pasting the problem code below. Basically swapping the sequence of the last two constraints results in two different results (of which one is pure sub-optimal). Am I not using the solver correctly?
------------------------------
import java.util.ArrayList;
import java.util.Collection;
import org.apache.commons.math3.optimization.*;
import org.apache.commons.math3.optimization.linear.*;
public class Commons_Solver {
  public static void main(String[] args) {
 // describe the optimization problem
    LinearObjectiveFunction f = new LinearObjectiveFunction(new double[] 
{ 1, 1, 1, 1, 1, 1, 0, 0 }
, 0);
    Collection &amp;lt;LinearConstraint&amp;gt;constraints = new ArrayList&amp;lt;LinearConstraint&amp;gt;();
    //variables upper bounds
    constraints.add(new LinearConstraint(new double[] 
{ 1, 0, 0, 0, 0, 0, 0, 0 }, Relationship.LEQ, 38));
    constraints.add(new LinearConstraint(new double[] { 0, 1, 0, 0, 0, 0, 0, 0 }, Relationship.LEQ, 34));
    constraints.add(new LinearConstraint(new double[] { 0, 0, 1, 0, 0, 0, 0, 0 }, Relationship.LEQ, 1));
    constraints.add(new LinearConstraint(new double[] { 0, 0, 0, 1, 0, 0, 0, 0 }, Relationship.LEQ, 6));
    constraints.add(new LinearConstraint(new double[] { 0, 0, 0, 0, 1, 0, 0, 0 }, Relationship.LEQ, 17));
    constraints.add(new LinearConstraint(new double[] { 0, 0, 0, 0, 0, 1, 0, 0 }, Relationship.LEQ, 11));
    constraints.add(new LinearConstraint(new double[] { 0, 0, 0, 0, 0, 0, 1, 0 }, Relationship.LEQ, 101));
    constraints.add(new LinearConstraint(new double[] { 0, 0, 0, 0, 0, 0, 0, 1 }, Relationship.LEQ, 1e10));

    //variables lower bounds
    constraints.add(new LinearConstraint(new double[] { 1, 0, 0, 0, 0, 0, 0, 0 }
, Relationship.GEQ, 0));
    constraints.add(new LinearConstraint(new double[] 
{ 0, 1, 0, 0, 0, 0, 0, 0 }
, Relationship.GEQ, 0));
    constraints.add(new LinearConstraint(new double[] 
{ 0, 0, 1, 0, 0, 0, 0, 0 }
, Relationship.GEQ, 0));
    constraints.add(new LinearConstraint(new double[] 
{ 0, 0, 0, 1, 0, 0, 0, 0 }
, Relationship.GEQ, 0));
    constraints.add(new LinearConstraint(new double[] 
{ 0, 0, 0, 0, 1, 0, 0, 0 }
, Relationship.GEQ, 0));
    constraints.add(new LinearConstraint(new double[] 
{ 0, 0, 0, 0, 0, 1, 0, 0 }
, Relationship.GEQ, 0));
    constraints.add(new LinearConstraint(new double[] 
{ 0, 0, 0, 0, 0, 0, 1, 0 }
, Relationship.GEQ, 0));
    constraints.add(new LinearConstraint(new double[] 
{ 0, 0, 0, 0, 0, 0, 0, 1 }
, Relationship.GEQ, 0));
    constraints.add(new LinearConstraint(new double[] 
{ -1,-1, -1, -1, -1, -1, 1, 0 }
, Relationship.EQ, 0));
    constraints.add(new LinearConstraint(new double[] 
{ -1, -1, -1, -1, -1, -1,0 , 1 }
, Relationship.EQ, 0));
    constraints.add(new LinearConstraint(new double[] 
{ 1, 0, 0, 0, 0, 0, 0, -0.2841121495327103  }
, Relationship.GEQ, 0));
    constraints.add(new LinearConstraint(new double[] 
{ 0, 1, 0, 0, 0, 0, 0, -0.25420560747663556  }
, Relationship.GEQ, 0));
    constraints.add(new LinearConstraint(new double[] 
{ 0, 0, 0, 1, 0, 0, 0, -0.04485981308411215 }
, Relationship.GEQ, 0));
    /*---------------
    Swapping the sequence of the below two constraints produces two different results 
    ------------------*/
    constraints.add(new LinearConstraint(new double[] 
{ 0, 0, 0, 0, 1, 0, 0, -0.12710280373831778  }
, Relationship.GEQ, 0));
    constraints.add(new LinearConstraint(new double[] 
{ 0, 0, 0, 0, 0, 1, 0, -0.08224299065420561  }
, Relationship.GEQ, 0));
    /------------------/
    PointValuePair solution = new SimplexSolver().optimize(f, constraints, GoalType.MAXIMIZE, false);
    // get the solution
    for (int i = 0 ; i &amp;lt; solution.getPoint().length; i++)      
      System.out.println("x[" + i + "] = " +  solution.getPoint()[i]);
    System.out.println("value = " + solution.getValue());
  }
}
----------------------------------</description>
			<version>3.0</version>
			<fixedVersion>3.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.optimization.linear.SimplexTableau.java</file>
			<file type="M">org.apache.commons.math3.optimization.linear.SimplexSolverTest.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">781</link>
		</links>
	</bug>
	<bug id="812" opendate="2012-07-07 12:01:37" fixdate="2012-07-12 13:38:05" resolution="Fixed">
		<buginformation>
			<summary>In RealVector, dotProduct and outerProduct return wrong results due to misuse of sparse iterators</summary>
			<description>In class RealVector, the default implementation of RealMatrix outerProduct(RealVector) uses sparse iterators on the entries of the two vectors. The rationale behind this is that 0d * x == 0d is true for all double x. This assumption is in fact false, since 0d * NaN == NaN.
Proposed fix is to loop through all entries of both vectors. This can have a significant impact on the CPU cost, but robustness should probably be preferred over speed in default implementations.
Same issue occurs with double dotProduct(RealVector), which uses sparse iterators for this only.
Another option would be to through an exception if isNaN() is true, in which case caching could be used for both isNaN() and isInfinite().</description>
			<version>3.0</version>
			<fixedVersion>3.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.linear.RealVector.java</file>
			<file type="M">org.apache.commons.math3.linear.OpenMapRealVector.java</file>
			<file type="M">org.apache.commons.math3.linear.ArrayRealVector.java</file>
		</fixedFiles>
	</bug>
	<bug id="578" opendate="2011-05-16 14:34:47" fixdate="2012-07-22 15:01:30" resolution="Fixed">
		<buginformation>
			<summary>Decrease DescriptiveStatistics performance from 2.0 to 2.2</summary>
			<description>Switching between commons-math 2.0 to 2.2 we note how the
DescriptiveStatistics.addValue(double) has decrease the performance.
I tested with 2 million values.
DescriptiveStatistics ds = new DescriptiveStatistics();
for(int i = 0; i&amp;lt;1000*1000*2; i++) 
{ //2 million values
    ds.addValue(v);
}

ds.getPercentile(50);
Seems that depending by the values inserted in the DescriptiveStatistics it takes different time:

with a single value (0)
	
2.0 -&amp;gt; take ~500 ms
2.2 -&amp;gt; take more than 10 minutes


with 50% fixed value (0) and 50% Math.random()
	
2.0 -&amp;gt; take ~500 ms
2.2 -&amp;gt; take ~250000 ms -&amp;gt; ~250 second


with 100% Math.random()
	
2.0 -&amp;gt; take ~500 ms
2.2 -&amp;gt; take ~70 ms



</description>
			<version>2.2</version>
			<fixedVersion>3.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.stat.descriptive.rank.Percentile.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">805</link>
		</links>
	</bug>
	<bug id="805" opendate="2012-06-12 15:14:23" fixdate="2012-07-23 20:01:57" resolution="Duplicate">
		<buginformation>
			<summary>Percentile calculation is very slow when input data are constants</summary>
			<description>I use the Percentile class to calculate quantile on a big array (10^6 entries). When I have to test the performance of my code, I notice that the calculation of quantile is at least 100x slower when my data are constants (10^6 of the same nomber). Maybe the Percentile calculation can be improved for this special case.</description>
			<version>3.0</version>
			<fixedVersion>3.1</fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.stat.descriptive.rank.Percentile.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">578</link>
		</links>
	</bug>
	<bug id="835" opendate="2012-07-31 13:09:24" fixdate="2012-07-31 15:01:01" resolution="Fixed">
		<buginformation>
			<summary>Fraction percentageValue rare overflow</summary>
			<description>The percentageValue() method of the Fraction class works by first multiplying the Fraction by 100, then converting the Fraction to a double. This causes overflows when the numerator is greater than Integer.MAX_VALUE/100, even when the value of the fraction is far below this value.
The patch changes the method to first convert to a double value, and then multiply this value by 100 - the result should be the same, but with less overflows. An addition to the test for the method that covers this bug is also included.</description>
			<version>3.0</version>
			<fixedVersion>3.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.fraction.FractionTest.java</file>
			<file type="M">org.apache.commons.math3.fraction.Fraction.java</file>
		</fixedFiles>
	</bug>
	<bug id="836" opendate="2012-07-31 17:04:25" fixdate="2012-08-04 16:27:26" resolution="Fixed">
		<buginformation>
			<summary>Fraction(double, int) constructor strange behaviour</summary>
			<description>The Fraction constructor Fraction(double, int) takes a double value and a int maximal denominator, and approximates a fraction. When the double value is a large, negative number with many digits in the fractional part, and the maximal denominator is a big, positive integer (in the 100&amp;amp;apos;000s), two distinct bugs can manifest:
1: the constructor returns a positive Fraction. Calling Fraction(-33655.1677817278, 371880) returns the fraction 410517235/243036, which both has the wrong sign, and is far away from the absolute value of the given value
2: the constructor does not manage to reduce the Fraction properly. Calling Fraction(-43979.60679604749, 366081) returns the fraction -1651878166/256677, which should have* been reduced to -24654898/3831.
I have, as of yet, not found a solution. The constructor looks like this:
public Fraction(double value, int maxDenominator)
        throws FractionConversionException
    {
       this(value, 0, maxDenominator, 100);
    }

Increasing the 100 value (max iterations) does not fix the problem for all cases. Changing the 0-value (the epsilon, maximum allowed error) to something small does not work either, as this breaks the tests in FractionTest. 
The problem is not neccissarily that the algorithm is unable to approximate a fraction correctly. A solution where a FractionConversionException had been thrown in each of these examples would probably be the best solution if an improvement on the approximation algorithm turns out to be hard to find.
This bug has been found when trying to explore the idea of axiom-based testing (http://bldl.ii.uib.no/testing.html). Attached is a java test class FractionTestByAxiom (junit, goes into org.apache.commons.math3.fraction) which shows these bugs through a simplified approach to this kind of testing, and a text file describing some of the value/maxDenominator combinations which causes one of these failures.

It is never specified in the documentation that the Fraction class guarantees that completely reduced rational numbers are constructed, but a comment inside the equals method claims that "since fractions are always in lowest terms, numerators and can be compared directly for equality", so it seems like this is the intention.

</description>
			<version>3.0</version>
			<fixedVersion>3.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.fraction.FractionTest.java</file>
			<file type="M">org.apache.commons.math3.fraction.Fraction.java</file>
		</fixedFiles>
	</bug>
	<bug id="840" opendate="2012-08-05 02:35:39" fixdate="2012-08-05 02:38:14" resolution="Fixed">
		<buginformation>
			<summary>Failures in "FastMathTestPerformance" when testRuns &gt;= 10,000,002</summary>
			<description>Tests for methods "asin" and "acos" fail because they use


i / 10000000.0


as the argument to those methods, where "i" goes from 0 to the value of "testRuns" minus one (if "testRuns" is defined).
A solution is to replace the above with


i / (double) RUNS

</description>
			<version>3.0</version>
			<fixedVersion>3.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.util.FastMathTestPerformance.java</file>
		</fixedFiles>
	</bug>
	<bug id="828" opendate="2012-07-19 15:07:29" fixdate="2012-08-05 16:33:40" resolution="Fixed">
		<buginformation>
			<summary>Not expected UnboundedSolutionException</summary>
			<description>SimplexSolver throws UnboundedSolutionException when trying to solve minimization linear programming problem. The number of exception thrown depends on the number of variables.
In order to see that behavior of SimplexSolver first try to run JUnit test setting a final variable ENTITIES_COUNT = 2 and that will give almost good result and then set it to 15 and you&amp;amp;apos;ll get a massive of unbounded exceptions.
First iteration is runned with predefined set of input data with which the Solver gives back an appropriate result.
The problem itself is well tested by it&amp;amp;apos;s authors (mathematicians who I believe know what they developed) using Matlab 10 with no unbounded solutions on the same rules of creatnig random variables values.
What is strange to me is the dependence of the number of UnboundedSolutionException exceptions on the number of variables in the problem.
The problem is formulated as
min(1*t + 0*L) (for every r-th subject)
s.t.
-q(r) + QL &amp;gt;= 0
x(r)t - XL &amp;gt;= 0
L &amp;gt;= 0
where 
r = 1..R, 
L = 
{l(1), l(2), ..., l(R)}
 (vector of R rows and 1 column),
Q - coefficients matrix MxR
X - coefficients matrix NxR </description>
			<version>3.0</version>
			<fixedVersion>3.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.optimization.linear.SimplexSolver.java</file>
			<file type="M">org.apache.commons.math3.optimization.linear.SimplexSolverTest.java</file>
			<file type="M">org.apache.commons.math3.optimization.linear.SimplexTableau.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">842</link>
		</links>
	</bug>
	<bug id="844" opendate="2012-08-12 21:03:33" fixdate="2012-08-16 20:45:55" resolution="Fixed">
		<buginformation>
			<summary>"HarmonicFitter.ParameterGuesser" sometimes fails to return sensible values</summary>
			<description>The inner class "ParameterGuesser" in "HarmonicFitter" (package "o.a.c.m.optimization.fitting") fails to compute a usable guess for the "amplitude" parameter.</description>
			<version>3.0</version>
			<fixedVersion>3.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.optimization.fitting.HarmonicFitterTest.java</file>
			<file type="M">org.apache.commons.math3.optimization.fitting.HarmonicFitter.java</file>
			<file type="M">org.apache.commons.math3.exception.util.LocalizedFormats.java</file>
		</fixedFiles>
	</bug>
	<bug id="843" opendate="2012-08-07 19:13:28" fixdate="2012-08-20 07:56:20" resolution="Fixed">
		<buginformation>
			<summary>Precision.EPSILON: wrong documentation</summary>
			<description>The documentation of the Field EPSILON in class org.apache.commons.math3.util.Precision states, that EPSILON is the smallest positive number such that 1 - EPSILON is not numerically equal to 1, and its value is defined as 1.1102230246251565E-16.
However, this is NOT the smallest positive number with this property.
Consider the following program:


public class Eps {
  public static void main(String[] args) {
    double e = Double.longBitsToDouble(0x3c90000000000001L);
	double e1 = 1-e;
	System.out.println(e);
	System.out.println(1-e);
	System.out.println(1-e != 1);
  }
}


The output is:


% java Eps
5.551115123125784E-17
0.9999999999999999
true


This proves, that there are smaller positive numbers with the property that 1-eps != 1.
I propose not to change the constant value, but to update the documentation. The value Precision.EPSILON is 
an upper bound on the relative error which occurs when a real number is
rounded to its nearest Double floating-point number. I propose to update 
the api docs in this sense.</description>
			<version>3.0</version>
			<fixedVersion>3.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.util.Precision.java</file>
			<file type="M">org.apache.commons.math3.util.PrecisionTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="855" opendate="2012-09-02 23:52:50" fixdate="2012-09-05 14:23:33" resolution="Fixed">
		<buginformation>
			<summary>"BrentOptimizer" not always reporting the best point</summary>
			<description>BrentOptimizer (package "o.a.c.m.optimization.univariate") does not check that the point it is going to return is indeed the best one it has encountered. Indeed, the last evaluated point might be slightly worse than the one before last.</description>
			<version>3.0</version>
			<fixedVersion>3.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.optimization.univariate.BrentOptimizer.java</file>
			<file type="M">org.apache.commons.math3.optimization.univariate.BrentOptimizerTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="789" opendate="2012-05-09 10:47:59" fixdate="2012-09-13 15:19:07" resolution="Fixed">
		<buginformation>
			<summary>Correlated random vector generator fails (silently) when faced with zero rows in covariance matrix</summary>
			<description>The following three matrices (which are basically permutations of each other) produce different results when sampling a multi-variate Gaussian with the help of CorrelatedRandomVectorGenerator (sample covariances calculated in R, based on 10,000 samples):
Array2DRowRealMatrix{
{0.0,0.0,0.0,0.0,0.0},
{0.0,0.013445532,0.01039469,0.009881156,0.010499559},
{0.0,0.01039469,0.023006616,0.008196856,0.010732709},
{0.0,0.009881156,0.008196856,0.019023866,0.009210099},
{0.0,0.010499559,0.010732709,0.009210099,0.019107243}}

&amp;gt; cov(data1)
   V1 V2 V3 V4 V5
V1 0 0.000000000 0.00000000 0.000000000 0.000000000
V2 0 0.013383931 0.01034401 0.009913271 0.010506733
V3 0 0.010344006 0.02309479 0.008374730 0.010759306
V4 0 0.009913271 0.00837473 0.019005488 0.009187287
V5 0 0.010506733 0.01075931 0.009187287 0.019021483

Array2DRowRealMatrix{
{0.013445532,0.01039469,0.0,0.009881156,0.010499559},
{0.01039469,0.023006616,0.0,0.008196856,0.010732709},{0.0,0.0,0.0,0.0,0.0}
,
{0.009881156,0.008196856,0.0,0.019023866,0.009210099}
,
{0.010499559,0.010732709,0.0,0.009210099,0.019107243}}
&amp;gt; cov(data2)
            V1 V2 V3 V4 V5
V1 0.006922905 0.010507692 0 0.005817399 0.010330529
V2 0.010507692 0.023428918 0 0.008273152 0.010735568
V3 0.000000000 0.000000000 0 0.000000000 0.000000000
V4 0.005817399 0.008273152 0 0.004929843 0.009048759
V5 0.010330529 0.010735568 0 0.009048759 0.018683544 
Array2DRowRealMatrix{
{0.013445532,0.01039469,0.009881156,0.010499559}
,
{0.01039469,0.023006616,0.008196856,0.010732709}
,
{0.009881156,0.008196856,0.019023866,0.009210099}
,
{0.010499559,0.010732709,0.009210099,0.019107243}}
&amp;gt; cov(data3)
            V1          V2          V3          V4
V1 0.013445047 0.010478862 0.009955904 0.010529542
V2 0.010478862 0.022910522 0.008610113 0.011046353
V3 0.009955904 0.008610113 0.019250975 0.009464442
V4 0.010529542 0.011046353 0.009464442 0.019260317
I&amp;amp;apos;ve traced this back to the RectangularCholeskyDecomposition, which does not seem to handle the second matrix very well (decompositions in the same order as the matrices above):
CorrelatedRandomVectorGenerator.getRootMatrix() = 
Array2DRowRealMatrix{{0.0,0.0,0.0,0.0,0.0},
{0.0759577418122063,0.0876125188474239,0.0,0.0,0.0}
,
{0.07764443622513505,0.05132821221460752,0.11976381821791235,0.0,0.0}
,
{0.06662930527909404,0.05501661744114585,0.0016662506519307997,0.10749324207653632,0.0}
,{0.13822895138139477,0.0,0.0,0.0,0.0}}
CorrelatedRandomVectorGenerator.getRank() = 5
CorrelatedRandomVectorGenerator.getRootMatrix() = 
Array2DRowRealMatrix{{0.0759577418122063,0.034512751379448724,0.0},
{0.07764443622513505,0.13029949164628746,0.0}
,
{0.0,0.0,0.0}
,
{0.06662930527909404,0.023203936694855674,0.0}
,{0.13822895138139477,0.0,0.0}}
CorrelatedRandomVectorGenerator.getRank() = 3
CorrelatedRandomVectorGenerator.getRootMatrix() = 
Array2DRowRealMatrix{{0.0759577418122063,0.034512751379448724,0.033913748226348225,0.07303890149947785},
{0.07764443622513505,0.13029949164628746,0.0,0.0}
,
{0.06662930527909404,0.023203936694855674,0.11851573313229945,0.0}
,{0.13822895138139477,0.0,0.0,0.0}}
CorrelatedRandomVectorGenerator.getRank() = 4
Clearly, the rank of each of these matrices should be 4. The first matrix does not lead to incorrect results, but the second one does. Unfortunately, I don&amp;amp;apos;t know enough about the Cholesky decomposition to find the flaw in the implementation, and I could not find documentation for the "rectangular" variant (also not at the links provided in the javadoc).</description>
			<version>3.0</version>
			<fixedVersion>3.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.random.CorrelatedRandomVectorGeneratorTest.java</file>
			<file type="M">org.apache.commons.math3.linear.RectangularCholeskyDecompositionTest.java</file>
			<file type="M">org.apache.commons.math3.linear.RectangularCholeskyDecomposition.java</file>
		</fixedFiles>
	</bug>
	<bug id="865" opendate="2012-09-19 21:10:42" fixdate="2012-09-22 10:04:55" resolution="Fixed">
		<buginformation>
			<summary>Wide bounds to CMAESOptimizer result in NaN parameters passed to fitness function</summary>
			<description>If you give large values as lower/upper bounds (for example -Double.MAX_VALUE as a lower bound), the optimizer can call the fitness function with parameters set to NaN.  My guess is this is due to FitnessFunction.encode/decode generating NaN when normalizing/denormalizing parameters.  For example, if the difference between the lower and upper bound is greater than Double.MAX_VALUE, encode could divide infinity by infinity.</description>
			<version>3.0</version>
			<fixedVersion>3.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.optimization.direct.CMAESOptimizerTest.java</file>
			<file type="M">org.apache.commons.math3.optimization.direct.CMAESOptimizer.java</file>
		</fixedFiles>
	</bug>
	<bug id="864" opendate="2012-09-19 19:32:01" fixdate="2012-09-22 10:05:11" resolution="Fixed">
		<buginformation>
			<summary>CMAESOptimizer does not enforce bounds</summary>
			<description>The CMAESOptimizer can exceed the bounds passed to optimize.  Looking at the generationLoop in doOptimize(), it does a bounds check by calling isFeasible() but if checkFeasableCount is zero (the default) then isFeasible() is never even called.  Also, even with non-zero checkFeasableCount it may give up before finding an in-bounds offspring and go forward with an out-of-bounds offspring.  This is against svn revision 1387637.  I can provide an example program where the optimizer ends up with a fit outside the prescribed bounds if that would help.</description>
			<version>3.0</version>
			<fixedVersion>3.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.optimization.direct.CMAESOptimizerTest.java</file>
			<file type="M">org.apache.commons.math3.optimization.direct.CMAESOptimizer.java</file>
		</fixedFiles>
	</bug>
	<bug id="848" opendate="2012-08-16 20:09:07" fixdate="2012-09-23 19:35:20" resolution="Fixed">
		<buginformation>
			<summary>EigenDecomposition fails for certain matrices</summary>
			<description>The Schurtransformation of the following matrix fails, which is a preliminary step for the Eigendecomposition:
RealMatrix m = MatrixUtils.DEFAULT_FORMAT.parse("{{0.184944928,-0.0646971046,0.0774755812,-0.0969651755,-0.0692648806,0.3282344352,-0.0177423074,0.206313634},{-0.0742700134,-0.028906303,-0.001726946,-0.0375550146,-0.0487737922,-0.2616837868,-0.0821201295,-0.2530000167},
{0.2549910127,0.0995733692,-0.0009718388,0.0149282808,0.1791878897,-0.0823182816,0.0582629256,0.3219545182}
,{-0.0694747557,-0.1880649148,-0.2740630911,0.0720096468,-0.1800836914,-0.3518996425,0.2486747833,0.6257938167},
{0.0536360918,-0.1339297778,0.2241579764,-0.0195327484,-0.0054103808,0.0347564518,0.5120802482,-0.0329902864}
,{-0.5933332356,-0.2488721082,0.2357173629,0.0177285473,0.0856630593,-0.35671263,-0.1600668126,-0.1010899621},{-0.0514349819,-0.0854319435,0.1125050061,0.006345356,-0.2250000688,-0.220934309,0.1964623477,-0.1512329924},{0.0197395947,-0.1997170581,-0.1425959019,-0.274947791,-0.0969467073,0.060368852,-0.2826905192,0.1794315473}}");</description>
			<version>3.1</version>
			<fixedVersion>3.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.linear.SchurTransformer.java</file>
			<file type="M">org.apache.commons.math3.linear.EigenDecompositionTest.java</file>
			<file type="M">org.apache.commons.math3.linear.SchurTransformerTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="881" opendate="2012-10-18 19:57:32" fixdate="2012-10-19 11:09:26" resolution="Fixed">
		<buginformation>
			<summary>Eliminate meaningless properties in multivariate distribution classes</summary>
			<description>The MultivariateRealDistribution interface includes the following properties which make no sense for multivariate distributions:
getSupportLowerBound, getSupporUpperBound, isSupportLowerBoundInclusive, isSupportUpperBoundInclusive
In addition, the following property makes sense, but is unlikely to be useful:
isSuportConnected
All of these properties should be deprecated in 3.1 and dropped in 4.0</description>
			<version>3.0</version>
			<fixedVersion>3.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.distribution.MultivariateRealDistribution.java</file>
			<file type="M">org.apache.commons.math3.distribution.AbstractMultivariateRealDistribution.java</file>
		</fixedFiles>
	</bug>
	<bug id="778" opendate="2012-04-10 06:06:11" fixdate="2012-10-21 16:23:33" resolution="Fixed">
		<buginformation>
			<summary>Dfp Dfp.multiply(int x) does not comply with the general contract FieldElement.multiply(int n)</summary>
			<description>In class org.apache.commons.math3.Dfp,  the method multiply(int n) is limited to 0 &amp;lt;= n &amp;lt;= 9999. This is not consistent with the general contract of FieldElement.multiply(int n), where there should be no limitation on the values of n.</description>
			<version>3.0</version>
			<fixedVersion>3.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.dfp.DfpTest.java</file>
			<file type="M">org.apache.commons.math3.dfp.Dfp.java</file>
		</fixedFiles>
	</bug>
	<bug id="880" opendate="2012-10-12 17:31:37" fixdate="2012-10-22 19:28:24" resolution="Fixed">
		<buginformation>
			<summary>Polygon difference produces erronious results in some cases</summary>
			<description>The 2D polygon difference method is returning incorrect
results.  Below is a test case of subtracting two polygons (Sorry,
this is the simplest case that I could find that duplicates the
problem).  
There are three problems with the result. The first is that the first
point of the first set of vertices is null (and the first point of the
second set is also null).  The second is that, even if the first null
points are ignored,  the returned polygon is not the correct result.
The first and last points are way off, and the remaining points do not
match the original polygon boundaries.  Additionally, there are two
holes that are returned in the results.  This subtraction case should
not have holes.
"Complex Polygon Difference Test"

public void testComplexDifference() {
        Vector2D[][] vertices1 = new Vector2D[][] {
            new Vector2D[] {
                    new Vector2D( 90.08714908223715,  38.370299337260235),
                    new Vector2D( 90.08709517675004,  38.3702895991413),
                    new Vector2D( 90.08401538704919,  38.368849330127944),
                    new Vector2D( 90.08258210430711,  38.367634558585564),
                    new Vector2D( 90.08251455106665,  38.36763409247078),
                    new Vector2D( 90.08106599752608,  38.36761621664249),
                    new Vector2D( 90.08249585300035,  38.36753627557965),
                    new Vector2D( 90.09075743352184,  38.35914647644972),
                    new Vector2D( 90.09099945896571,  38.35896264724079),
                    new Vector2D( 90.09269383800086,  38.34595756121246),
                    new Vector2D( 90.09638631543191,  38.3457988093121),
                    new Vector2D( 90.09666417351019,  38.34523360999418),
                    new Vector2D( 90.1297082145872,  38.337670454923625),
                    new Vector2D( 90.12971687748956,  38.337669827794684),
                    new Vector2D( 90.1240820219179,  38.34328502001131),
                    new Vector2D( 90.13084259656404,  38.34017811765017),
                    new Vector2D( 90.13378567942857,  38.33860579180606),
                    new Vector2D( 90.13519557833206,  38.33621054663689),
                    new Vector2D( 90.13545616732307,  38.33614965452864),
                    new Vector2D( 90.13553111202748,  38.33613962818305),
                    new Vector2D( 90.1356903436448,  38.33610227127048),
                    new Vector2D( 90.13576283227428,  38.33609255422783),
                    new Vector2D( 90.13595870833188,  38.33604606376991),
                    new Vector2D( 90.1361556630693,  38.3360024198866),
                    new Vector2D( 90.13622408795709,  38.335987048115726),
                    new Vector2D( 90.13696189099994,  38.33581914328681),
                    new Vector2D( 90.13746655304897,  38.33616706665265),
                    new Vector2D( 90.13845973716064,  38.33650776167099),
                    new Vector2D( 90.13950901827667,  38.3368469456463),
                    new Vector2D( 90.14393814424852,  38.337591835857495),
                    new Vector2D( 90.14483839716831,  38.337076122362475),
                    new Vector2D( 90.14565474433601,  38.33769000964429),
                    new Vector2D( 90.14569421179482,  38.3377117256905),
                    new Vector2D( 90.14577067124333,  38.33770883625908),
                    new Vector2D( 90.14600350631684,  38.337714326520995),
                    new Vector2D( 90.14600355139731,  38.33771435193319),
                    new Vector2D( 90.14600369112401,  38.33771443882085),
                    new Vector2D( 90.14600382486884,  38.33771453466096),
                    new Vector2D( 90.14600395205912,  38.33771463904344),
                    new Vector2D( 90.14600407214999,  38.337714751520764),
                    new Vector2D( 90.14600418462749,  38.337714871611695),
                    new Vector2D( 90.14600422249327,  38.337714915811034),
                    new Vector2D( 90.14867838361471,  38.34113888210675),
                    new Vector2D( 90.14923750157374,  38.341582537502575),
                    new Vector2D( 90.14877083250991,  38.34160685841391),
                    new Vector2D( 90.14816667319519,  38.34244232585684),
                    new Vector2D( 90.14797696744586,  38.34248455284745),
                    new Vector2D( 90.14484318014337,  38.34385573215269),
                    new Vector2D( 90.14477919958296,  38.3453797747614),
                    new Vector2D( 90.14202393306448,  38.34464324839456),
                    new Vector2D( 90.14198920640195,  38.344651155237216),
                    new Vector2D( 90.14155207025175,  38.34486424263724),
                    new Vector2D( 90.1415196143314,  38.344871730519),
                    new Vector2D( 90.14128611910814,  38.34500196593859),
                    new Vector2D( 90.14047850603913,  38.34600084496253),
                    new Vector2D( 90.14045907000337,  38.34601860032171),
                    new Vector2D( 90.14039496493928,  38.346223030432384),
                    new Vector2D( 90.14037626063737,  38.346240203360026),
                    new Vector2D( 90.14030005823724,  38.34646920000705),
                    new Vector2D( 90.13799164754806,  38.34903093011013),
                    new Vector2D( 90.11045289492762,  38.36801537312368),
                    new Vector2D( 90.10871471476526,  38.36878044144294),
                    new Vector2D( 90.10424901707671,  38.374300101757),
                    new Vector2D( 90.10263482039932,  38.37310041316073),
                    new Vector2D( 90.09834601753448,  38.373615053823414),
                    new Vector2D( 90.0979455456843,  38.373578376172475),
                    new Vector2D( 90.09086514328669,  38.37527884194668),
                    new Vector2D( 90.09084931407364,  38.37590801712463),
                    new Vector2D( 90.09081227075944,  38.37526295920463),
                    new Vector2D( 90.09081378927135,  38.375193883266434)
            }
        };
        PolygonsSet set1 = buildSet(vertices1);

        Vector2D[][] vertices2 = new Vector2D[][] {
            new Vector2D[] {
                    new Vector2D( 90.13067558880044,  38.36977255037573),
                    new Vector2D( 90.12907570488,  38.36817308242706),
                    new Vector2D( 90.1342774136516,  38.356886880294724),
                    new Vector2D( 90.13090330629757,  38.34664392676211),
                    new Vector2D( 90.13078571364593,  38.344904617518466),
                    new Vector2D( 90.1315602208914,  38.3447185040846),
                    new Vector2D( 90.1316336226821,  38.34470643148342),
                    new Vector2D( 90.134020944832,  38.340936644972885),
                    new Vector2D( 90.13912536387306,  38.335497255122334),
                    new Vector2D( 90.1396178806582,  38.334878075552126),
                    new Vector2D( 90.14083049696671,  38.33316530644106),
                    new Vector2D( 90.14145252901329,  38.33152722916191),
                    new Vector2D( 90.1404779335565,  38.32863516047786),
                    new Vector2D( 90.14282712131586,  38.327504432532066),
                    new Vector2D( 90.14616669875488,  38.3237354115015),
                    new Vector2D( 90.14860976050608,  38.315714862457924),
                    new Vector2D( 90.14999277782437,  38.3164932507504),
                    new Vector2D( 90.15005207194997,  38.316534677663356),
                    new Vector2D( 90.15508513859612,  38.31878731691609),
                    new Vector2D( 90.15919938519221,  38.31852743183782),
                    new Vector2D( 90.16093758658837,  38.31880662005153),
                    new Vector2D( 90.16099420184912,  38.318825953291594),
                    new Vector2D( 90.1665411125756,  38.31859497874757),
                    new Vector2D( 90.16999653861313,  38.32505772048029),
                    new Vector2D( 90.17475243391698,  38.32594398441148),
                    new Vector2D( 90.17940844844992,  38.327427213761325),
                    new Vector2D( 90.20951909541378,  38.330616833491774),
                    new Vector2D( 90.2155400467941,  38.331746223670336),
                    new Vector2D( 90.21559881391778,  38.33175551425302),
                    new Vector2D( 90.21916646426041,  38.332584299620805),
                    new Vector2D( 90.23863749852285,  38.34778978875795),
                    new Vector2D( 90.25459855175802,  38.357790570608984),
                    new Vector2D( 90.25964298227257,  38.356918010203174),
                    new Vector2D( 90.26024593994703,  38.361692743151366),
                    new Vector2D( 90.26146187570015,  38.36311080550837),
                    new Vector2D( 90.26614159359622,  38.36510808579902),
                    new Vector2D( 90.26621342936448,  38.36507942500333),
                    new Vector2D( 90.26652190211962,  38.36494042196722),
                    new Vector2D( 90.26621240678867,  38.365113172030874),
                    new Vector2D( 90.26614057102057,  38.365141832826794),
                    new Vector2D( 90.26380080055299,  38.3660381760273),
                    new Vector2D( 90.26315345241,  38.36670658276421),
                    new Vector2D( 90.26251574942881,  38.367490323488084),
                    new Vector2D( 90.26247873448426,  38.36755266444749),
                    new Vector2D( 90.26234628016698,  38.36787989125406),
                    new Vector2D( 90.26214559424784,  38.36945909356126),
                    new Vector2D( 90.25861728442555,  38.37200753430875),
                    new Vector2D( 90.23905557537864,  38.375405314295904),
                    new Vector2D( 90.22517251874075,  38.38984691662256),
                    new Vector2D( 90.22549955153215,  38.3911564273979),
                    new Vector2D( 90.22434386063355,  38.391476432092134),
                    new Vector2D( 90.22147729457276,  38.39134652252034),
                    new Vector2D( 90.22142070120117,  38.391349167741964),
                    new Vector2D( 90.20665060751588,  38.39475580900313),
                    new Vector2D( 90.20042268367109,  38.39842558622888),
                    new Vector2D( 90.17423771242085,  38.402727751805344),
                    new Vector2D( 90.16756796257476,  38.40913898597597),
                    new Vector2D( 90.16728283954308,  38.411255399912875),
                    new Vector2D( 90.16703538220418,  38.41136059866693),
                    new Vector2D( 90.16725865657685,  38.41013618805954),
                    new Vector2D( 90.16746107640665,  38.40902614307544),
                    new Vector2D( 90.16122795307462,  38.39773101873203)
            }
        };
        PolygonsSet set2 = buildSet(vertices2);
        PolygonsSet set  = (PolygonsSet) new
RegionFactory&amp;lt;Euclidean2D&amp;gt;().difference(set1.copySelf(),

              set2.copySelf());

        Vector2D[][] verticies = set.getVertices();
        Assert.assertTrue(verticies[0][0] != null);
        Assert.assertEquals(1, verticies.length);
    }

</description>
			<version>3.0</version>
			<fixedVersion>3.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.geometry.euclidean.twod.PolygonsSetTest.java</file>
			<file type="M">org.apache.commons.math3.geometry.euclidean.twod.PolygonsSet.java</file>
		</fixedFiles>
	</bug>
	<bug id="900" opendate="2012-11-16 16:16:15" fixdate="2012-11-17 00:22:11" resolution="Fixed">
		<buginformation>
			<summary>Font problem in LocalizedFormatsTest.java header</summary>
			<description>Non-standard charecters in the comment header of the file prevents compiling of the class with error
[javac] /Users/kberlin/Dropbox/Projects/math/src/test/java/org/apache/commons/math3/exception/util/LocalizedFormatsTest.java:21: error: unmappable character for encoding ASCII
[javac]  * Copyright 2010 CS Communication &amp;amp; Syst??mes</description>
			<version>3.1</version>
			<fixedVersion>3.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.analysis.interpolation.HermiteInterpolator.java</file>
			<file type="M">org.apache.commons.math3.exception.util.LocalizedFormatsTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="899" opendate="2012-11-16 08:08:22" fixdate="2012-11-18 21:40:15" resolution="Fixed">
		<buginformation>
			<summary>A random crash of MersenneTwister random generator</summary>
			<description>There is a very small probability that MersenneTwister generator gives a following error: 
java.lang.ArrayIndexOutOfBoundsException: 624
in MersenneTwister.java line 253
The error is completely random and its probability is about 1e-8.
UPD: The problem most probably arises only in multy-thread mode.</description>
			<version>3.0</version>
			<fixedVersion>3.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.random.SynchronizedRandomGenerator.java</file>
		</fixedFiles>
	</bug>
	<bug id="905" opendate="2012-11-20 20:54:39" fixdate="2012-11-26 13:25:55" resolution="Fixed">
		<buginformation>
			<summary>FastMath.[cosh, sinh] do not support the same range of values as the Math counterparts</summary>
			<description>As reported by Jeff Hain:
cosh(double) and sinh(double):
Math.cosh(709.783) = 8.991046692770538E307
FastMath.cosh(709.783) = Infinity
Math.sinh(709.783) = 8.991046692770538E307
FastMath.sinh(709.783) = Infinity
===&amp;gt; This is due to using exp( x )/2 for values of |x|
above 20: the result sometimes should not overflow,
but exp( x ) does, so we end up with some infinity.
===&amp;gt; for values of |x| &amp;gt;= StrictMath.log(Double.MAX_VALUE),
exp will overflow, so you need to use that instead:
for x positive:
double t = exp(x*0.5);
return (0.5*t)*t;
for x negative:
double t = exp(-x*0.5);
return (-0.5*t)*t;</description>
			<version>3.0</version>
			<fixedVersion>3.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.util.FastMath.java</file>
			<file type="M">org.apache.commons.math3.util.FastMathTest.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">580</link>
		</links>
	</bug>
	<bug id="904" opendate="2012-11-20 20:51:23" fixdate="2012-11-26 23:03:35" resolution="Fixed">
		<buginformation>
			<summary>FastMath.pow deviates from Math.pow for negative, finite base values with an exponent 2^52 &lt; y &lt; 2^53 </summary>
			<description>As reported by Jeff Hain:
pow(double,double):
Math.pow(-1.0,5.000000000000001E15) = -1.0
FastMath.pow(-1.0,5.000000000000001E15) = 1.0
===&amp;gt; This is due to considering that power is an even
integer if it is &amp;gt;= 2^52, while you need to test
that it is &amp;gt;= 2^53 for it.
===&amp;gt; replace
"if (y &amp;gt;= TWO_POWER_52 || y &amp;lt;= -TWO_POWER_52)"
with
"if (y &amp;gt;= 2*TWO_POWER_52 || y &amp;lt;= -2*TWO_POWER_52)"
and that solves it.</description>
			<version>3.0</version>
			<fixedVersion>3.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.util.FastMath.java</file>
			<file type="M">org.apache.commons.math3.util.FastMathTest.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">580</link>
		</links>
	</bug>
	<bug id="927" opendate="2013-01-04 11:36:25" fixdate="2013-01-13 18:02:19" resolution="Fixed">
		<buginformation>
			<summary>GammaDistribution cloning broken</summary>
			<description>Serializing a GammaDistribution and deserializing it, does not result in a cloned distribution that produces the same samples.
Cause: GammaDistribution inherits from AbstractRealDistribution, which implements Serializable. AbstractRealDistribution has random, in which we have a Well19937c instance, which inherits from AbstractWell. AbstractWell implements Serializable. AbstractWell inherits from BitsStreamGenerator, which is not Serializable, but does have a private field &amp;amp;apos;nextGaussian&amp;amp;apos;.
Solution: Make BitStreamGenerator implement Serializable as well.
This probably affects other distributions as well.</description>
			<version>3.1</version>
			<fixedVersion>3.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.random.BitsStreamGenerator.java</file>
			<file type="M">org.apache.commons.math3.distribution.RealDistributionAbstractTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="929" opendate="2013-01-15 11:45:28" fixdate="2013-01-15 12:20:21" resolution="Fixed">
		<buginformation>
			<summary>MultivariateNormalDistribution.density(double[]) returns wrong value when the dimension is odd</summary>
			<description>To reproduce:


Assert.assertEquals(0.398942280401433, new MultivariateNormalDistribution(new double[]{0}, new double[][]{{1}}).density(new double[]{0}), 1e-15);

</description>
			<version>3.1.1</version>
			<fixedVersion>3.2</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.distribution.MultivariateNormalDistributionTest.java</file>
			<file type="M">org.apache.commons.math3.distribution.MultivariateNormalDistribution.java</file>
		</fixedFiles>
	</bug>
	<bug id="930" opendate="2013-01-17 09:17:50" fixdate="2013-01-20 10:13:07" resolution="Fixed">
		<buginformation>
			<summary>SimplexSolver finds suboptimal solution or throws NoFeasibleSolutionException</summary>
			<description>When I run this code sometimes I get NoFeasibleSolutionException, and sometimes the result is 0.37522987682323883. 
Octave gives result 0.70679 and a point = {1.59032, 1.00000, 0.70679, 0.40399, 1.04004, 0.67396, 0.37868, 0.22823, 0.98909, 0.68793, 0.17021,
0.09192, 0.67501, 0.44573, 0.07829, 0.00000, 0.81316, 0.63520, 0.55634
0.40399, 0.48504, 0.45944, 0.22823, 0.22823, 0.34873, 0.32313, 0.09192,
0.09192, 0.25681, 0.23122, 0.00000, 0.00000, 1.59032
double[][] coefficients = new double[97][];
        double[] value = new double[97];
        Relationship[] relationship = new Relationship[97];
        int i = 0;
        double[] m0  = 
{1, -1, -1, 1, -1, 1, 1, -1, -1, 1, 1, -1, 1, -1, -1, 1, -1, 1, 1, -1, 1, -1, -1, 1, 1, -1, -1, 1, -1, 1, 1, -1, 0}
;
        coefficients[i] = m0;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m1  = 
{1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1};
        coefficients[i] = m1;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m2  = {1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1}
;
        coefficients[i] = m2;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m3  = 
{0, 1, 0, -1, 0, -1, 0, 1, 0, -1, 0, 1, 0, 1, 0, -1, 0, -1, 0, 1, 0, 1, 0, -1, 0, 1, 0, -1, 0, -1, 0, 1, 0}
;
        coefficients[i] = m3;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m4  = 
{0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.628803}
;
        coefficients[i] = m4;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m5  = 
{0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.676993}
;
        coefficients[i] = m5;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m6  = 
{0, 0, 1, -1, 0, 0, -1, 1, 0, 0, -1, 1, 0, 0, 1, -1, 0, 0, -1, 1, 0, 0, 1, -1, 0, 0, 1, -1, 0, 0, -1, 1, 0}
;
        coefficients[i] = m6;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m7  = 
{0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.136677}
;
        coefficients[i] = m7;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m8  = 
{0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.444434}
;
        coefficients[i] = m8;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m9  = 
{0, 0, 0, 1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 1, 0, 0, 0, -1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, -1, 0}
;
        coefficients[i] = m9;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m10  = 
{0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.254028}
;
        coefficients[i] = m10;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m11  = 
{0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.302218}
;
        coefficients[i] = m11;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m12  = 
{0, 0, 0, 0, 1, -1, -1, 1, 0, 0, 0, 0, -1, 1, 1, -1, 0, 0, 0, 0, -1, 1, 1, -1, 0, 0, 0, 0, 1, -1, -1, 1, 0}
;
        coefficients[i] = m12;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m13  = 
{0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.653981}
;
        coefficients[i] = m13;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m14  = 
{0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.690437}
;
        coefficients[i] = m14;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m15  = 
{0, 0, 0, 0, 0, 1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 1, 0, 0, 0, 0, 0, -1, 0, 1, 0, 0, 0, 0, 0, 1, 0, -1, 0}
;
        coefficients[i] = m15;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m16  = 
{0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.423786}
;
        coefficients[i] = m16;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m17  = 
{0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.486717}
;
        coefficients[i] = m17;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m18  = 
{0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, -1, 1, 0, 0, 0, 0, 0, 0, -1, 1, 0, 0, 0, 0, 0, 0, 1, -1, 0}
;
        coefficients[i] = m18;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m19  = 
{0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.049232}
;
        coefficients[i] = m19;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m20  = 
{0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.304747}
;
        coefficients[i] = m20;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m21  = 
{0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 1, 0}
;
        coefficients[i] = m21;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m22  = 
{0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.129826}
;
        coefficients[i] = m22;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m23  = 
{0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.205625}
;
        coefficients[i] = m23;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m24  = 
{0, 0, 0, 0, 0, 0, 0, 0, 1, -1, -1, 1, -1, 1, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 1, 1, -1, 1, -1, -1, 1, 0}
;
        coefficients[i] = m24;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m25  = 
{0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.621944}
;
        coefficients[i] = m25;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m26  = 
{0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.764385}
;
        coefficients[i] = m26;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m27  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, -1, 0, -1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 1, 0, 1, 0, -1, 0}
;
        coefficients[i] = m27;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m28  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.432572}
;
        coefficients[i] = m28;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m29  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.480762}
;
        coefficients[i] = m29;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m30  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, -1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 1, 0, 0, 1, -1, 0}
;
        coefficients[i] = m30;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m31  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.055983}
;
        coefficients[i] = m31;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m32  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.11378}
;
        coefficients[i] = m32;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m33  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 1, 0}
;
        coefficients[i] = m33;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m34  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.009607}
;
        coefficients[i] = m34;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m35  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.057797}
;
        coefficients[i] = m35;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m36  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, -1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 1, 1, -1, 0}
;
        coefficients[i] = m36;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m37  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.407308}
;
        coefficients[i] = m37;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m38  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.452749}
;
        coefficients[i] = m38;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m39  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 1, 0}
;
        coefficients[i] = m39;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m40  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.269677}
;
        coefficients[i] = m40;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m41  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.321806}
;
        coefficients[i] = m41;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m42  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 1, 0}
;
        coefficients[i] = m42;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m43  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.049232}
;
        coefficients[i] = m43;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m44  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.06902}
;
        coefficients[i] = m44;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m45  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0}
;
        coefficients[i] = m45;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m46  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0}
;
        coefficients[i] = m46;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m47  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.028754}
;
        coefficients[i] = m47;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m48  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, -1, 1, -1, 1, 1, -1, -1, 1, 1, -1, 1, -1, -1, 1, 0}
;
        coefficients[i] = m48;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m49  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.484254}
;
        coefficients[i] = m49;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m50  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.524607}
;
        coefficients[i] = m50;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m51  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, -1, 0, -1, 0, 1, 0, -1, 0, 1, 0, 1, 0, -1, 0}
;
        coefficients[i] = m51;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m52  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.385492}
;
        coefficients[i] = m52;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m53  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.430134}
;
        coefficients[i] = m53;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m54  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, -1, 1, 0, 0, -1, 1, 0, 0, 1, -1, 0}
;
        coefficients[i] = m54;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m55  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.34983}
;
        coefficients[i] = m55;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m56  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.375781}
;
        coefficients[i] = m56;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m57  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 1, 0}
;
        coefficients[i] = m57;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m58  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.254028}
;
        coefficients[i] = m58;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m59  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.281308}
;
        coefficients[i] = m59;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m60  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, -1, 1, 0, 0, 0, 0, -1, 1, 1, -1, 0}
;
        coefficients[i] = m60;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m61  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.304995}
;
        coefficients[i] = m61;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m62  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.345347}
;
        coefficients[i] = m62;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m63  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 1, 0}
;
        coefficients[i] = m63;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m64  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.288899}
;
        coefficients[i] = m64;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m65  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.332212}
;
        coefficients[i] = m65;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m66  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, -1, 1, 0}
;
        coefficients[i] = m66;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m67  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.14351}
;
        coefficients[i] = m67;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m68  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.17057}
;
        coefficients[i] = m68;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m69  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, -1, 0}
;
        coefficients[i] = m69;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m70  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, -0.129826}
;
        coefficients[i] = m70;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m71  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, -0.157435}
;
        coefficients[i] = m71;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m72  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, -1, 1, -1, 1, 1, -1, 0}
;
        coefficients[i] = m72;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m73  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, -0}
;
        coefficients[i] = m73;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m74  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, -1}
;
        coefficients[i] = m74;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m75  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, -1, 0, -1, 0, 1, 0}
;
        coefficients[i] = m75;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m76  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, -0.141071}
;
        coefficients[i] = m76;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m77  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, -0.232574}
;
        coefficients[i] = m77;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m78  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, -1, 1, 0}
;
        coefficients[i] = m78;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m79  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, -0}
;
        coefficients[i] = m79;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m80  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, -1}
;
        coefficients[i] = m80;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m81  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, -1, 0}
;
        coefficients[i] = m81;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m82  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, -0.009607}
;
        coefficients[i] = m82;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m83  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, -0.057797}
;
        coefficients[i] = m83;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m84  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, -1, 1, 0}
;
        coefficients[i] = m84;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m85  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, -0}
;
        coefficients[i] = m85;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m86  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, -1}
;
        coefficients[i] = m86;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m87  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, -1, 0}
;
        coefficients[i] = m87;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m88  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, -0.091644}
;
        coefficients[i] = m88;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m89  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, -0.203531}
;
        coefficients[i] = m89;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m90  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0}
;
        coefficients[i] = m90;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m91  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, -0}
;
        coefficients[i] = m91;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m92  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, -1}
;
        coefficients[i] = m92;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m93  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0};
        coefficients[i] = m93;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m94  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0}
;
        coefficients[i] = m94;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m95  = 
{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -0.028754}
;
        coefficients[i] = m95;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m96  = 
{0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0}
;
        coefficients[i] = m96;
        relationship[i] = Relationship.EQ;
        value[i] = 1.0;
ArrayList&amp;lt;LinearConstraint&amp;gt; constraints = new ArrayList&amp;lt;&amp;gt;(97);
                for (int j = 0; j &amp;lt; 97; j++) 
{
                    constraints.add(new LinearConstraint(coefficients[j], relationship[j], value[j]));
                }
                double[] fooc = new double[33];
                fooc[3] = 1;
                LinearObjectiveFunction foo = new LinearObjectiveFunction(fooc, 0);
                SimplexSolver solver = new SimplexSolver();
                LinearConstraintSet se = new LinearConstraintSet(constraints);
                PointValuePair res = solver.optimize(MaxIter.unlimited(), foo, se, new NonNegativeConstraint(true));</description>
			<version>3.1.1</version>
			<fixedVersion>3.2</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.optim.linear.SimplexTableau.java</file>
			<file type="M">org.apache.commons.math3.optim.linear.SimplexSolver.java</file>
			<file type="M">org.apache.commons.math3.optim.linear.SimplexSolverTest.java</file>
		</fixedFiles>
		<links>
			<link type="Container" description="contains">819</link>
		</links>
	</bug>
	<bug id="914" opendate="2012-12-13 13:28:18" fixdate="2013-03-09 17:38:35" resolution="Fixed">
		<buginformation>
			<summary>Inconsistent multi-start randomization (optimizers)</summary>
			<description>In class "o.a.c.m.optim.BaseMultiStartMultivariateOptimizer", the "starting points" generator is passed at construction. But random initial guesses must fulfill the bound constraint and be somehow related to the user-supplied initial guess; and those are passed to the "optimize" method and thus can change from one call to the other, leading to inconsistent (and probably useless) multi-starts.</description>
			<version>3.0</version>
			<fixedVersion>3.2</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.optim.BaseMultiStartMultivariateOptimizer.java</file>
		</fixedFiles>
	</bug>
	<bug id="949" opendate="2013-03-15 18:11:56" fixdate="2013-03-20 11:38:17" resolution="Fixed">
		<buginformation>
			<summary>LevenbergMarquardtOptimizer reports 0 iterations</summary>
			<description>The method LevenbergMarquardtOptimizer.getIterations() does not report the correct number of iterations; It always returns 0. A quick look at the code shows that only SimplexOptimizer calls BaseOptimizer.incrementEvaluationsCount()
I&amp;amp;apos;ve put a test case below. Notice how the evaluations count is correctly incremented, but the iterations count is not.

    @Test
    public void testGetIterations() {
        // setup
        LevenbergMarquardtOptimizer otim = new LevenbergMarquardtOptimizer();

        // action
        otim.optimize(new MaxEval(100), new Target(new double[] { 1 }),
                new Weight(new double[] { 1 }), new InitialGuess(
                        new double[] { 3 }), new ModelFunction(
                        new MultivariateVectorFunction() {
                            @Override
                            public double[] value(double[] point)
                                    throws IllegalArgumentException {
                                return new double[] { FastMath.pow(point[0], 4) };
                            }
                        }), new ModelFunctionJacobian(
                        new MultivariateMatrixFunction() {
                            @Override
                            public double[][] value(double[] point)
                                    throws IllegalArgumentException {
                                return new double[][] { { 0.25 * FastMath.pow(
                                        point[0], 3) } };
                            }
                        }));

        // verify
        assertThat(otim.getEvaluations(), greaterThan(1));
        assertThat(otim.getIterations(), greaterThan(1));
    }


</description>
			<version>3.2</version>
			<fixedVersion>3.2</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.optim.BaseOptimizer.java</file>
			<file type="M">org.apache.commons.math3.optim.nonlinear.vector.jacobian.LevenbergMarquardtOptimizer.java</file>
			<file type="M">org.apache.commons.math3.optim.nonlinear.vector.jacobian.AbstractLeastSquaresOptimizerAbstractTest.java</file>
			<file type="M">org.apache.commons.math3.optim.nonlinear.scalar.noderiv.SimplexOptimizer.java</file>
			<file type="M">org.apache.commons.math3.optim.nonlinear.scalar.noderiv.SimplexOptimizerMultiDirectionalTest.java</file>
			<file type="M">org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.java</file>
			<file type="M">org.apache.commons.math3.optim.nonlinear.scalar.noderiv.PowellOptimizer.java</file>
			<file type="M">org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizer.java</file>
			<file type="M">org.apache.commons.math3.optim.nonlinear.scalar.gradient.NonLinearConjugateGradientOptimizer.java</file>
			<file type="M">org.apache.commons.math3.optim.nonlinear.scalar.noderiv.SimplexOptimizerNelderMeadTest.java</file>
			<file type="M">org.apache.commons.math3.optim.nonlinear.scalar.noderiv.PowellOptimizerTest.java</file>
			<file type="M">org.apache.commons.math3.optim.nonlinear.scalar.gradient.NonLinearConjugateGradientOptimizerTest.java</file>
			<file type="M">org.apache.commons.math3.optim.nonlinear.vector.jacobian.GaussNewtonOptimizer.java</file>
		</fixedFiles>
	</bug>
	<bug id="891" opendate="2012-11-08 18:31:36" fixdate="2013-03-27 19:44:54" resolution="Fixed">
		<buginformation>
			<summary>SpearmansCorrelation fails when using NaturalRanking together with NaNStrategy.REMOVED</summary>
			<description>As reported by Martin Rosellen on the users mailinglist:
Using a NaturalRanking with a REMOVED NaNStrategy can result in an exception when NaN are contained in the input arrays.
The current implementation just removes the NaN values where they occur, without taken care to remove the corresponding values in the other array.</description>
			<version>3.0</version>
			<fixedVersion>3.2</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.stat.correlation.SpearmansCorrelation.java</file>
			<file type="M">org.apache.commons.math3.stat.correlation.SpearmansRankCorrelationTest.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">958</link>
		</links>
	</bug>
	<bug id="962" opendate="2013-04-03 15:44:15" fixdate="2013-04-14 16:19:56" resolution="Fixed">
		<buginformation>
			<summary>Vector3DFormat.parse does not ignore whitespace</summary>
			<description>Vector3DFormat notes it ingores whitespace in the javadoc but in the below example it does not:
	Vector3DFormat vf = new Vector3DFormat("(", ")", ",");
	System.out.println(vf.parse("(1, 2, 3)")); //prints 
{1; 2; 3}
	System.out.println(vf.parse("(1,2,3)"));   //prints null</description>
			<version>3.1.1</version>
			<fixedVersion>3.3</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.geometry.euclidean.twod.Vector2DFormat.java</file>
			<file type="M">org.apache.commons.math3.geometry.VectorFormat.java</file>
			<file type="M">org.apache.commons.math3.geometry.euclidean.oned.Vector1DFormat.java</file>
			<file type="M">org.apache.commons.math3.geometry.euclidean.threed.Vector3DFormat.java</file>
		</fixedFiles>
	</bug>
	<bug id="993" opendate="2013-06-18 11:40:41" fixdate="2013-06-28 10:23:51" resolution="Fixed">
		<buginformation>
			<summary>GaussNewtonOptimizer convergence on singularity</summary>
			<description>I am (ab-)using the GaussNewtonOptimizer as a MultivariateFunctionSolver (as I could not find one in commons.math). Recently I stumbled upon an interesting behavior in one of my test cases: If a function is defined in a way that yields a minimum (a root in my case) at a singular point, the solver crashes. This is because of the following lines in doOptimize():
catch (SingularMatrixException e) 
{
                throw new ConvergenceException(LocalizedFormats.UNABLE_TO_SOLVE_SINGULAR_PROBLEM);
            }

I would propose to add a convergence check into the catch-phrase, so the solver returns the solution in that special case.</description>
			<version>3.2</version>
			<fixedVersion>3.3</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.optim.nonlinear.vector.jacobian.GaussNewtonOptimizer.java</file>
		</fixedFiles>
	</bug>
	<bug id="1005" opendate="2013-07-12 09:39:41" fixdate="2013-07-12 11:32:18" resolution="Fixed">
		<buginformation>
			<summary>ArrayIndexOutOfBoundsException in MathArrays.linearCombination</summary>
			<description>When MathArrays.linearCombination is passed arguments with length 1, it throws an ArrayOutOfBoundsException. This is caused by this line:
double prodHighNext = prodHigh[1];
linearCombination should check the length of the arguments and fall back to simple multiplication if length == 1.</description>
			<version>3.2</version>
			<fixedVersion>3.3</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.util.MathArraysTest.java</file>
			<file type="M">org.apache.commons.math3.util.MathArrays.java</file>
		</fixedFiles>
	</bug>
	<bug id="1022" opendate="2013-08-13 09:57:24" fixdate="2013-08-13 16:48:22" resolution="Fixed">
		<buginformation>
			<summary>Confused by the API docs for org.apache.commons.math3.analysis.function</summary>
			<description>Something is wrong or unclear...
We read:
http://commons.apache.org/proper/commons-math/apidocs/org/apache/commons/math3/analysis/function/Logistic.Parametric.html
   "Parametric function where the input array contains the parameters
    of the logit function, ordered as follows: "
 --&amp;gt; But the "logit" function is not the "logistic" function.
http://commons.apache.org/proper/commons-math/apidocs/org/apache/commons/math3/analysis/function/Sigmoid.Parametric.html
   "Parametric function where the input array contains the parameters of
   the logit function, ordered as follows: "
 --&amp;gt; But the "logit" function is not the "sigmoid" function, and what is
     the difference between the Logistic Function snd the Sigmoid function?
http://commons.apache.org/proper/commons-math/apidocs/org/apache/commons/math3/analysis/function/Logit.Parametric.html
   "Parametric function where the input array contains the parameters of
    the logit function, ordered as follows: "
 --&amp;gt; That sounds correct.
References:
http://en.wikipedia.org/wiki/Logistic_function
http://en.wikipedia.org/wiki/Logit
http://en.wikipedia.org/wiki/Sigmoid_function
</description>
			<version>3.2</version>
			<fixedVersion>3.3</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.analysis.function.Sigmoid.java</file>
			<file type="M">org.apache.commons.math3.analysis.function.Logistic.java</file>
		</fixedFiles>
	</bug>
	<bug id="996" opendate="2013-06-24 17:21:18" fixdate="2013-08-31 19:43:25" resolution="Fixed">
		<buginformation>
			<summary>Fraction specified with maxDenominator and a value very close to a simple fraction should not throw an overflow exception</summary>
			<description>An overflow exception is thrown when a Fraction is initialized with a maxDenominator from a double that is very close to a simple
fraction.  For example:
double d = 0.5000000001;
Fraction f = new Fraction(d, 10);
Patch with unit test on way.</description>
			<version>3.1.1</version>
			<fixedVersion>3.3</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.fraction.FractionTest.java</file>
			<file type="M">org.apache.commons.math3.fraction.Fraction.java</file>
			<file type="M">org.apache.commons.math3.fraction.BigFraction.java</file>
			<file type="M">org.apache.commons.math3.fraction.BigFractionTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="1037" opendate="2013-09-25 14:09:14" fixdate="2013-10-11 20:47:41" resolution="Fixed">
		<buginformation>
			<summary>Distribution tests are mostly meaningless due to high tolerance</summary>
			<description>The tolerance used for value comparison in IntegerDistributionAbstractTest is 1E-4. However, most values being compared are much smaller, so they are considered equal even if they otherwise differ by orders of magnitude. For example, a typo in GeometricDistributionTest puts 29 in the test points instead of 19, while the test probability value is correctly given for 19. The test passes, disregarding the fact that 2.437439e-05 (test value for 19) and 1.473826e-07 (actual value for 29) differ almost hundredfold.</description>
			<version>3.2</version>
			<fixedVersion>3.3</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.distribution.GeometricDistributionTest.java</file>
			<file type="M">org.apache.commons.math3.distribution.HypergeometricDistributionTest.java</file>
			<file type="M">org.apache.commons.math3.distribution.IntegerDistributionAbstractTest.java</file>
			<file type="M">org.apache.commons.math3.distribution.BinomialDistributionTest.java</file>
			<file type="M">org.apache.commons.math3.distribution.ZipfDistributionTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="1033" opendate="2013-09-05 16:15:43" fixdate="2013-10-11 21:39:29" resolution="Fixed">
		<buginformation>
			<summary>Kalman filter does not work if covarance matrix is not of dimension 1</summary>
			<description>In org.apache.commons.math3.filter.KalmanFilter,
The check below doesn&amp;amp;apos;t look right, it reques measNoise&amp;amp;apos;s column dimension to be 1 at all time.
// row dimension of R must be equal to row dimension of H
        if (measNoise.getRowDimension() != measurementMatrix.getRowDimension() ||
            measNoise.getColumnDimension() != 1) 
{
            throw new MatrixDimensionMismatchException(measNoise.getRowDimension(),
                                                       measNoise.getColumnDimension(),
                                                       measurementMatrix.getRowDimension(), 1);
        }</description>
			<version>3.2</version>
			<fixedVersion>3.3</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.filter.KalmanFilter.java</file>
		</fixedFiles>
	</bug>
	<bug id="1045" opendate="2013-10-22 13:10:57" fixdate="2013-10-29 15:55:15" resolution="Fixed">
		<buginformation>
			<summary>EigenDecomposition.Solver should consider tiny values 0 for purposes of determining singularity</summary>
			<description>EigenDecomposition.Solver tests for singularity by comparing eigenvalues to 0 for exact equality. Elsewhere in the class and in the code, of course, very small values are considered 0. This causes the solver to consider some singular matrices as non-singular.
The patch here includes a test as well showing the behavior  the matrix is clearly singular but isn&amp;amp;apos;t considered as such since one eigenvalue are ~1e-14 rather than exactly 0.
(What I am not sure of is whether we should really be evaluating the norm of the imaginary eigenvalues rather than real/imag components separately. But the javadoc says the solver only supports real eigenvalues anyhow, so it&amp;amp;apos;s kind of moot since imag=0 for all eigenvalues.)</description>
			<version>3.2</version>
			<fixedVersion>3.3</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.linear.EigenDecomposition.java</file>
			<file type="M">org.apache.commons.math3.linear.EigenSolverTest.java</file>
			<file type="M">org.apache.commons.math3.linear.EigenDecompositionTest.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">1049</link>
		</links>
	</bug>
	<bug id="1051" opendate="2013-10-31 20:01:22" fixdate="2013-10-31 20:06:32" resolution="Fixed">
		<buginformation>
			<summary>EigenDecomposition may not converge for certain matrices</summary>
			<description>Jama-1.0.3 contains a bugfix for certain matrices where the original code goes into an infinite loop.
The commons-math translations would throw a MaxCountExceededException, so fails to compute the eigen decomposition.
Port the fix from jama to CM.</description>
			<version>3.2</version>
			<fixedVersion>3.3</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.linear.SchurTransformer.java</file>
			<file type="M">org.apache.commons.math3.linear.EigenDecompositionTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="1058" opendate="2013-11-03 21:47:44" fixdate="2013-11-05 13:52:10" resolution="Fixed">
		<buginformation>
			<summary>Beta, LogNormalDistribution, WeibullDistribution give slightly wrong answer for extremely small args due to log/exp inaccuracy</summary>
			<description>Background for those who aren&amp;amp;apos;t familiar: math libs like Math and FastMath have two mysterious methods, log1p and expm1. log1p = log(1+x) and expm1 = exp-1 mathetmatically, but can return a correct answer even when x was small, where floating-point error due to the addition/subtraction introduces a relatively large error.
There are three instances in the code that can employ these specialized methods and gain a measurable improvement in accuracy. See patch and tests for an example  try the tests without the code change to see the error.</description>
			<version>3.2</version>
			<fixedVersion>3.3</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.distribution.WeibullDistributionTest.java</file>
			<file type="M">org.apache.commons.math3.distribution.WeibullDistribution.java</file>
			<file type="M">org.apache.commons.math3.distribution.LogNormalDistributionTest.java</file>
			<file type="M">org.apache.commons.math3.special.BetaTest.java</file>
			<file type="M">org.apache.commons.math3.special.Beta.java</file>
			<file type="M">org.apache.commons.math3.distribution.LogNormalDistribution.java</file>
		</fixedFiles>
	</bug>
	<bug id="1056" opendate="2013-11-03 14:19:18" fixdate="2013-11-08 23:31:45" resolution="Fixed">
		<buginformation>
			<summary>Small error in PoissonDistribution.nextPoisson() algorithm</summary>
			<description>Here&amp;amp;apos;s a tiny bug I noticed via static inspection, since it flagged the integer division. PoissonDistribution.java:325 says:


final double a1 = FastMath.sqrt(FastMath.PI * twolpd) * FastMath.exp(1 / 8 * lambda);


The "1 / 8 * lambda" is evidently incorrect, since this will always evaluate to 0. I rechecked the original algorithm (http://luc.devroye.org/devroye-poisson.pdf) and it should instead be:


final double a1 = FastMath.sqrt(FastMath.PI * twolpd) * FastMath.exp(1 / (8 * lambda));


(lambda is a double so there is no int division issue.) This matches a later expression.
I&amp;amp;apos;m not sure how to evaluate the effect of the bug. Better to be correct of course; it may never have made much practical difference.</description>
			<version>3.2</version>
			<fixedVersion>3.3</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.distribution.PoissonDistribution.java</file>
		</fixedFiles>
	</bug>
	<bug id="1067" opendate="2013-11-28 10:47:22" fixdate="2013-11-28 11:42:52" resolution="Fixed">
		<buginformation>
			<summary>Stack overflow in Beta.regularizedBeta</summary>
			<description>In org.apache.commons.math3.special.Beta.regularizedBeta(double,double,double,double,int), the case
 } else if (x &amp;gt; (a + 1.0) / (a + b + 2.0)) 
{
      ret = 1.0 - regularizedBeta(1.0 - x, b, a, epsilon, maxIterations);
}
 
is prone to infinite recursion: If x is approximately the tested value, then 1-x is approximately the tested value in the recursion. Thus, due to loss of precision after the subtraction, this condition can be true for the recursive call as well.
Example:
double x= Double.longBitsToDouble(4597303555101269224L);
double a= Double.longBitsToDouble(4634227472812299606L);
double b = Double.longBitsToDouble(4642050131540049920L);
System.out.println(x &amp;gt; (a + 1.0) / (a + b + 2.0));
System.out.println(1-x&amp;gt;(b + 1.0) / (b + a + 2.0));
System.out.println(1-(1-x)&amp;gt;(a + 1.0) / (a + b + 2.0));
Possible solution: change the condition to
x &amp;gt; (a + 1.0) / (a + b + 2.0) &amp;amp;&amp;amp; 1-x&amp;lt;=(b + 1.0) / (b + a + 2.0)</description>
			<version>3.2</version>
			<fixedVersion>3.3</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.special.Beta.java</file>
			<file type="M">org.apache.commons.math3.special.BetaTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="1059" opendate="2013-11-05 14:30:37" fixdate="2013-12-03 23:03:58" resolution="Fixed">
		<buginformation>
			<summary>Use FastMath instead of Math within CM</summary>
			<description>Some code in CM still uses Math.xxx instead of the counterparts in FastMath. This could lead to subtle differences with different jvms as could be seen in MATH-1057.
All calls to Math shall be replaced by calls to FastMath.</description>
			<version>3.2</version>
			<fixedVersion>3.3</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.util.PrecisionTest.java</file>
			<file type="M">org.apache.commons.math3.analysis.solvers.RegulaFalsiSolverTest.java</file>
			<file type="M">org.apache.commons.math3.distribution.fitting.MultivariateNormalMixtureExpectationMaximization.java</file>
			<file type="M">org.apache.commons.math3.stat.regression.AbstractMultipleLinearRegression.java</file>
			<file type="M">org.apache.commons.math3.stat.inference.TestUtilsTest.java</file>
			<file type="M">org.apache.commons.math3.linear.SymmLQTest.java</file>
			<file type="M">org.apache.commons.math3.stat.StatUtilsTest.java</file>
			<file type="M">org.apache.commons.math3.special.ErfTest.java</file>
			<file type="M">org.apache.commons.math3.distribution.AbstractIntegerDistribution.java</file>
			<file type="M">org.apache.commons.math3.distribution.KolmogorovSmirnovDistribution.java</file>
			<file type="M">org.apache.commons.math3.random.RandomDataGeneratorTest.java</file>
			<file type="M">org.apache.commons.math3.analysis.function.SqrtTest.java</file>
			<file type="M">org.apache.commons.math3.stat.inference.GTest.java</file>
			<file type="M">org.apache.commons.math3.fitting.leastsquares.LevenbergMarquardtOptimizerTest.java</file>
			<file type="M">org.apache.commons.math3.complex.QuaternionTest.java</file>
			<file type="M">org.apache.commons.math3.util.MathArrays.java</file>
			<file type="M">org.apache.commons.math3.linear.ConjugateGradientTest.java</file>
			<file type="M">org.apache.commons.math3.analysis.integration.gauss.GaussianQuadratureAbstractTest.java</file>
			<file type="M">org.apache.commons.math3.util.ArithmeticUtils.java</file>
			<file type="M">org.apache.commons.math3.random.AbstractWell.java</file>
			<file type="M">org.apache.commons.math3.optim.nonlinear.vector.jacobian.LevenbergMarquardtOptimizerTest.java</file>
			<file type="M">org.apache.commons.math3.stat.regression.MillerUpdatingRegression.java</file>
			<file type="M">org.apache.commons.math3.optim.univariate.BracketFinder.java</file>
			<file type="M">org.apache.commons.math3.optimization.general.LevenbergMarquardtOptimizerTest.java</file>
			<file type="M">org.apache.commons.math3.linear.UnmodifiableRealVectorAbstractTest.java</file>
			<file type="M">org.apache.commons.math3.random.EmpiricalDistributionTest.java</file>
			<file type="M">org.apache.commons.math3.fraction.BigFraction.java</file>
			<file type="M">org.apache.commons.math3.optimization.univariate.BracketFinder.java</file>
			<file type="M">org.apache.commons.math3.stat.inference.GTestTest.java</file>
			<file type="M">org.apache.commons.math3.analysis.function.Logistic.java</file>
			<file type="M">org.apache.commons.math3.fraction.ProperFractionFormat.java</file>
			<file type="M">org.apache.commons.math3.util.MathUtilsTest.java</file>
			<file type="M">org.apache.commons.math3.stat.regression.SimpleRegressionTest.java</file>
			<file type="M">org.apache.commons.math3.random.ISAACRandom.java</file>
			<file type="M">org.apache.commons.math3.optimization.direct.CMAESOptimizerTest.java</file>
			<file type="M">org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizerTest.java</file>
			<file type="M">org.apache.commons.math3.filter.KalmanFilterTest.java</file>
			<file type="M">org.apache.commons.math3.util.MathArraysTest.java</file>
			<file type="M">org.apache.commons.math3.stat.regression.MillerUpdatingRegressionTest.java</file>
			<file type="M">org.apache.commons.math3.optim.nonlinear.scalar.noderiv.BOBYQAOptimizer.java</file>
			<file type="M">org.apache.commons.math3.optimization.direct.BOBYQAOptimizer.java</file>
			<file type="M">org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizer.java</file>
			<file type="M">org.apache.commons.math3.optimization.direct.CMAESOptimizer.java</file>
			<file type="M">org.apache.commons.math3.complex.Complex.java</file>
			<file type="M">org.apache.commons.math3.analysis.FunctionUtilsTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="1068" opendate="2013-12-01 11:48:25" fixdate="2013-12-07 13:08:49" resolution="Fixed">
		<buginformation>
			<summary>KendallsCorrelation suffers from integer overflow for large arrays.</summary>
			<description>For large array size (say, over 5,000), numPairs &amp;gt; 10 million.
in line 258, (numPairs - tiedXPairs) * (numPairs - tiedYPairs) possibly &amp;gt; 100 billion, which will cause an integer overflow, resulting in a negative number, which will result in the end result in a NaN since the square-root of that number is calculated.
This can easily be solved by changing line 163 to
final long numPairs = ((long)n) * (n - 1) / 2; // to avoid overflow</description>
			<version>3.3</version>
			<fixedVersion>3.3</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.stat.correlation.KendallsCorrelation.java</file>
			<file type="M">org.apache.commons.math3.stat.correlation.KendallsCorrelationTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="1070" opendate="2013-12-03 12:31:37" fixdate="2014-01-09 13:18:50" resolution="Fixed">
		<buginformation>
			<summary>Incorrect rounding of float</summary>
			<description>package org.apache.commons.math3.util 
example of usage of round functions of Precision class:
Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01
Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01
Precision.round((float) 0.0, 2) = 0.0
Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0
Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.
I think, same problem will be found at usage of other round modes.</description>
			<version>3.2</version>
			<fixedVersion>3.3</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.util.PrecisionTest.java</file>
			<file type="M">org.apache.commons.math3.util.Precision.java</file>
		</fixedFiles>
	</bug>
	<bug id="1088" opendate="2014-01-16 15:21:33" fixdate="2014-01-16 15:27:01" resolution="Fixed">
		<buginformation>
			<summary>MultidimensionalCounter does not throw "NoSuchElementException"</summary>
			<description>The iterator should throw when "next()" is called even though "hasNext()" would return false.</description>
			<version>3.2</version>
			<fixedVersion>3.3</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.util.MultidimensionalCounterTest.java</file>
			<file type="M">org.apache.commons.math3.util.MultidimensionalCounter.java</file>
		</fixedFiles>
	</bug>
	<bug id="985" opendate="2013-05-29 06:01:28" fixdate="2014-01-26 15:58:41" resolution="Fixed">
		<buginformation>
			<summary>BicubicSpline interpolation returns unexpected values (BicubicSplineInterpolator/BicubicSplineInterpolationFunction)</summary>
			<description>I&amp;amp;apos;ve been testing out the Tricubic spline functions, and have been getting some strange values.  I dug down a bit, and it seems like they start at the Bicubic level.  SplineInterpolator/PolynomialSplineFunction seem to be returning correct values.  I set up a block of data that increases linearly, so you&amp;amp;apos;d expect the interpolated values to follow the same trend, except the values tend to overshoot to half the distance between knot points, and then undershoot for the remaining half.  Probably the easiest thing would be to show some tests.  First - 1D which works fine:
SplineTest.java

import org.apache.commons.math3.analysis.interpolation.SplineInterpolator;
import org.apache.commons.math3.analysis.polynomials.PolynomialSplineFunction;

public class SplineTest {

	double[] x = new double[]{.52,.54,.56,.58,.6};
	double[] y = new double[]{76,77,78,79,80};
	
	public static void main(String[] args){
		SplineTest st = new SplineTest();
		st.go();
	}
	
	public void go(){
		SplineInterpolator si = new SplineInterpolator();
		PolynomialSplineFunction sf = si.interpolate(x, y);
		
		System.out.println(sf.value(0.52));
		System.out.println(sf.value(0.5225));
		System.out.println(sf.value(0.525));
		System.out.println(sf.value(0.5275));
		System.out.println(sf.value(0.53));
		System.out.println(sf.value(0.5325));
		System.out.println(sf.value(0.535));
		System.out.println(sf.value(0.5375));
		System.out.println(sf.value(0.54));
		System.out.println(sf.value(0.5425));
		System.out.println(sf.value(0.545));
		System.out.println(sf.value(0.5475));
		System.out.println(sf.value(0.55));
		System.out.println(sf.value(0.5525));
		System.out.println(sf.value(0.555));
		System.out.println(sf.value(0.5575));
		System.out.println(sf.value(0.56));
	}
}


and next, 2D which doesn&amp;amp;apos;t:
BicubicSplineTest.java

import org.apache.commons.math3.analysis.interpolation.BicubicSplineInterpolatingFunction;
import org.apache.commons.math3.analysis.interpolation.BicubicSplineInterpolator;

public class BicubicSplineTest {

	double[] x = new double[]{0,1,2};
	double[] y = new double[]{.52,.54,.56,.58,.6};
	double[] v1 = new double[]{76,77,78,79,80};
	double[][] v2 = new double[][]{v1,v1,v1};
	
	public static void main(String[] args){
		BicubicSplineTest bt = new BicubicSplineTest();
		bt.go();
	}
	
	public void go(){
		BicubicSplineInterpolator bi = new BicubicSplineInterpolator();
		BicubicSplineInterpolatingFunction bf = bi.interpolate(x, y, v2);
		
		System.out.println(bf.value(1, 0.52));
		System.out.println(bf.value(1, 0.5225));
		System.out.println(bf.value(1, 0.525));
		System.out.println(bf.value(1, 0.5275));
		System.out.println(bf.value(1, 0.53));
		System.out.println(bf.value(1, 0.5325));
		System.out.println(bf.value(1, 0.535));
		System.out.println(bf.value(1, 0.5375));
		System.out.println(bf.value(1, 0.54));
		System.out.println(bf.value(1, 0.5425));
		System.out.println(bf.value(1, 0.545));
		System.out.println(bf.value(1, 0.5475));
		System.out.println(bf.value(1, 0.55));
		System.out.println(bf.value(1, 0.5525));
		System.out.println(bf.value(1, 0.555));
		System.out.println(bf.value(1, 0.5575));
		System.out.println(bf.value(1, 0.56));
	}
}


The data points increase from 76 to 80 in a linear way.  Incrementing by 1/8 the distance to the next point, the 1D spline returns:
76.0
76.125
76.25
76.375
76.5
76.625
76.75
76.875
77.0
77.125
77.25
77.375
77.5
77.625
77.75
77.875
78.0
The 2D spline returns:
76.0
80.14453124999996
80.84375000000003
79.24609375000007
76.50000000000003
73.75390625000007
72.15625000000001
72.85546874999996
76.99999999999997
81.14453124999993
81.84374999999997
80.24609375000006
77.50000000000003
74.75390625000009
73.15625000000004
73.85546874999997
78.0
Even though it&amp;amp;apos;s still effectively a 1D problem.  I&amp;amp;apos;m not sure exactly what&amp;amp;apos;s causing it - maybe something when multiplying the coefficients, but thought I should flag it.</description>
			<version>3.1.1</version>
			<fixedVersion>3.3</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.analysis.interpolation.TricubicSplineInterpolatorTest.java</file>
			<file type="M">org.apache.commons.math3.analysis.interpolation.BicubicSplineInterpolatorTest.java</file>
			<file type="M">org.apache.commons.math3.analysis.interpolation.BicubicSplineInterpolatingFunctionTest.java</file>
			<file type="M">org.apache.commons.math3.analysis.interpolation.BicubicSplineInterpolatingFunction.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">1138</link>
		</links>
	</bug>
	<bug id="1096" opendate="2014-01-29 20:28:31" fixdate="2014-02-02 20:54:58" resolution="Fixed">
		<buginformation>
			<summary>implementation of smallest enclosing ball algorithm sometime fails</summary>
			<description>The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.
In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.
Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are

WelzlEncloser2DTest.testReducingBall
WelzlEncloser2DTest.testLargeSamples
WelzlEncloser3DTest.testInfiniteLoop
WelzlEncloser3DTest.testLargeSamples

</description>
			<version>3.3</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.geometry.enclosing.WelzlEncloser.java</file>
			<file type="M">org.apache.commons.math3.geometry.enclosing.Encloser.java</file>
			<file type="M">org.apache.commons.math3.geometry.enclosing.WelzlEncloser2DTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="1065" opendate="2013-11-22 11:21:58" fixdate="2014-02-09 11:21:54" resolution="Fixed">
		<buginformation>
			<summary>EnumeratedRealDistribution.inverseCumulativeProbability returns values not in the samples set</summary>
			<description>The method EnumeratedRealDistribution.inverseCumulativeProbability() sometimes returns values that are not in the initial samples domain...
I will attach a test to exploit this bug.</description>
			<version>3.2</version>
			<fixedVersion>3.3</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.distribution.EnumeratedRealDistribution.java</file>
			<file type="M">org.apache.commons.math3.distribution.EnumeratedRealDistributionTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="803" opendate="2012-06-09 18:51:33" fixdate="2014-02-20 15:58:58" resolution="Fixed">
		<buginformation>
			<summary>Bugs in RealVector.ebeMultiply(RealVector) and ebeDivide(RealVector)</summary>
			<description>OpenMapRealVector.ebeMultiply(RealVector) and OpenMapRealVector.ebeDivide(RealVector) return wrong values when one entry of the specified RealVector is nan or infinity. The bug is easy to understand. Here is the current implementation of ebeMultiply


    public OpenMapRealVector ebeMultiply(RealVector v) {
        checkVectorDimensions(v.getDimension());
        OpenMapRealVector res = new OpenMapRealVector(this);
        Iterator iter = entries.iterator();
        while (iter.hasNext()) {
            iter.advance();
            res.setEntry(iter.key(), iter.value() * v.getEntry(iter.key()));
        }
        return res;
    }


The assumption is that for any double x, x * 0d == 0d holds, which is not true. The bug is easy enough to identify, but more complex to solve. The only solution I can come up with is to loop through all entries of v (instead of those entries which correspond to non-zero entries of this). I&amp;amp;apos;m afraid about performance losses.
</description>
			<version>3.0</version>
			<fixedVersion>3.3</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.linear.RealVector.java</file>
			<file type="M">org.apache.commons.math3.linear.UnmodifiableRealVectorAbstractTest.java</file>
			<file type="M">org.apache.commons.math3.linear.RealVectorAbstractTest.java</file>
			<file type="M">org.apache.commons.math3.linear.OpenMapRealVector.java</file>
			<file type="M">org.apache.commons.math3.linear.SparseRealVectorTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="821" opendate="2012-07-12 14:01:58" fixdate="2014-02-20 16:17:17" resolution="Fixed">
		<buginformation>
			<summary>SparseRealVectorTest.testMap and testMapToSelf fail because zero entries lose their sign</summary>
			<description>Mapping Inverse to an OpenMapRealVector can lead to wrong answers, because 1.0 / 0.0 should return +/-Infinity depending on the sign of the zero entry. Since the sign is lost in OpenMapRealVector, the answer must be wrong if the entry is truly -0.0.
This is a difficult bug, because it potentially affects any function passed to OpenMapRealVector.map() or mapToSelf(). I would recommend we relax the requirements in the unit tests of this class, and make people aware of the issue in the class documentation.</description>
			<version>3.0</version>
			<fixedVersion>3.3</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.linear.SparseRealVectorTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="1111" opendate="2014-03-19 17:38:15" fixdate="2014-03-19 18:34:36" resolution="Fixed">
		<buginformation>
			<summary>Spelling mistake in org.apache.commons.math3.fitting </summary>
			<description>in the paragraph containing : 
"should pass through sample points, and were the objective function is the" 
at http://commons.apache.org/proper/commons-math/javadocs/api-3.2/org/apache/commons/math3/fitting/package-summary.html
"were" should be "where"</description>
			<version>3.2</version>
			<fixedVersion>3.3</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.fitting.package-info.java</file>
		</fixedFiles>
	</bug>
	<bug id="1112" opendate="2014-03-22 14:20:30" fixdate="2014-03-23 18:24:11" resolution="Duplicate">
		<buginformation>
			<summary>Implementation of Percentile function that does not need to store values</summary>
			<description>A new implementation of Percentile calculation based on P Square algorithm( http://www.cse.wustl.edu/~jain/papers/psqr.htm) is being proposed here. This new implementation has key advantage that it doesn&amp;amp;apos;t need to store inputs and needs a constant space to compute the percentile as the input is consumed.
This advantage is much required when used in computing the percentiles at big-data scale or for in-stream analytics.</description>
			<version>3.2</version>
			<fixedVersion></fixedVersion>
			<type>New Feature</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.stat.descriptive.rank.PSquarePercentileTest.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">418</link>
		</links>
	</bug>
	<bug id="1110" opendate="2014-03-14 22:27:01" fixdate="2014-05-01 11:54:31" resolution="Fixed">
		<buginformation>
			<summary>OLSMultipleLinearRegression needs a way to specify non-zero singularity threshold when instantiating QRDecomposition</summary>
			<description>OLSMultipleLinearRegression uses QRDecomposition to perform a least-squares solution. QRDecomposition has the capability to use a non-zero threshold for detecting when the design matrix is singular (see https://issues.apache.org/jira/browse/MATH-665, https://issues.apache.org/jira/browse/MATH-1024, https://issues.apache.org/jira/browse/MATH-1100, https://issues.apache.org/jira/browse/MATH-1101) but OLSMultipleLinearRegression does not use this capability and therefore always uses the default singularity test threshold of 0. This can lead to bad solutions (see in particular https://issues.apache.org/jira/browse/MATH-1101?focusedCommentId=13909750&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13909750) when a SingularMatrixException should instead be thrown. 
When I encountered this situation, I noticed it because the solution values were extremely large (in the range 1e09 - 1e12). Normal values in the domain I am working with are on the order of 1e-3. To find out why the values are so large, I traced through the source and found that an rDiag value was on the order of 1e-15, and that this passed the threshold test. I then noticed that two columns of the design matrix are linearly dependent (one column is all 1&amp;amp;apos;s because I want an intercept value in the solution, and another is also all 1&amp;amp;apos;s because that&amp;amp;apos;s how the data worked out). Thus the matrix is definitely singular. 
If I could specify a non-zero threshold, this situation would result in  a SingularMatrixException, but without that, the bad solution values would be blindly propagated. That is a problem because this solution is intended for controlling a physical system, and damage could result from a bad solution. 
Unfortunately, I see no way to change the threshold value from outside  I would have to in effect re-implement OLSMultipleLinearRegression to do this as a user of the package. </description>
			<version>3.2</version>
			<fixedVersion>3.3</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.stat.regression.OLSMultipleLinearRegressionTest.java</file>
			<file type="M">org.apache.commons.math3.stat.regression.OLSMultipleLinearRegression.java</file>
		</fixedFiles>
	</bug>
	<bug id="418" opendate="2010-09-18 15:51:52" fixdate="2014-06-21 18:21:55" resolution="Fixed">
		<buginformation>
			<summary>add a storeless version of Percentile</summary>
			<description>The Percentile class can handle only in-memory data.
It would be interesting to use an on-line algorithm to estimate quantiles as a storeless statistic.
An example of such an algorithm is the exponentially weighted stochastic approximation  described in a 2000 paper by Fei Chen ,  Diane Lambert  and Jos C. Pinheiro "Incremental Quantile Estimation for Massive Tracking" which can be retrieved from CiteSeerX at http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.105.1580.</description>
			<version>2.1</version>
			<fixedVersion>3.4</fixedVersion>
			<type>New Feature</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.stat.descriptive.rank.PSquarePercentileTest.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">1112</link>
		</links>
	</bug>
	<bug id="1135" opendate="2014-07-10 18:00:34" fixdate="2014-07-10 21:40:09" resolution="Fixed">
		<buginformation>
			<summary>Bug in MonotoneChain: a collinear point landing on the existing boundary should be dropped (patch)</summary>
			<description>The is a bug on the code in MonotoneChain.java that attempts to handle the case of a point on the line formed by the previous last points and the last point of the chain being constructed. When `includeCollinearPoints` is false, the point should be dropped entirely. In common-math 3,3, the point is added, which in some cases can cause a `ConvergenceException` to be thrown.
In the patch below, the data points are from a case that showed up in testing before we went to production.


Index: src/main/java/org/apache/commons/math3/geometry/euclidean/twod/hull/MonotoneChain.java
===================================================================
--- src/main/java/org/apache/commons/math3/geometry/euclidean/twod/hull/MonotoneChain.java	(revision 1609491)
+++ src/main/java/org/apache/commons/math3/geometry/euclidean/twod/hull/MonotoneChain.java	(working copy)
@@ -160,8 +160,8 @@
                 } else {
                     if (distanceToCurrent &amp;gt; distanceToLast) {
                         hull.remove(size - 1);
+                        hull.add(point);
                     }
-                    hull.add(point);
                 }
                 return;
             } else if (offset &amp;gt; 0) {
Index: src/test/java/org/apache/commons/math3/geometry/euclidean/twod/hull/ConvexHullGenerator2DAbstractTest.java
===================================================================
--- src/test/java/org/apache/commons/math3/geometry/euclidean/twod/hull/ConvexHullGenerator2DAbstractTest.java	(revision 1609491)
+++ src/test/java/org/apache/commons/math3/geometry/euclidean/twod/hull/ConvexHullGenerator2DAbstractTest.java	(working copy)
@@ -204,6 +204,24 @@
     }
 
     @Test
+    public void testCollinnearPointOnExistingBoundary() {
+        final Collection&amp;lt;Vector2D&amp;gt; points = new ArrayList&amp;lt;Vector2D&amp;gt;();
+        points.add(new Vector2D(7.3152, 34.7472));
+        points.add(new Vector2D(6.400799999999997, 34.747199999999985));
+        points.add(new Vector2D(5.486399999999997, 34.7472));
+        points.add(new Vector2D(4.876799999999999, 34.7472));
+        points.add(new Vector2D(4.876799999999999, 34.1376));
+        points.add(new Vector2D(4.876799999999999, 30.48));
+        points.add(new Vector2D(6.0959999999999965, 30.48));
+        points.add(new Vector2D(6.0959999999999965, 34.1376));
+        points.add(new Vector2D(7.315199999999996, 34.1376));
+        points.add(new Vector2D(7.3152, 30.48));
+
+        final ConvexHull2D hull = generator.generate(points);
+        checkConvexHull(points, hull);
+    }
+
+    @Test
     public void testIssue1123() {
 
         List&amp;lt;Vector2D&amp;gt; points = new ArrayList&amp;lt;Vector2D&amp;gt;();

</description>
			<version>3.3</version>
			<fixedVersion>3.4</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.geometry.euclidean.twod.hull.ConvexHullGenerator2DAbstractTest.java</file>
			<file type="M">org.apache.commons.math3.geometry.euclidean.twod.hull.MonotoneChain.java</file>
		</fixedFiles>
	</bug>
	<bug id="1149" opendate="2014-08-22 11:35:51" fixdate="2014-09-13 15:37:02" resolution="Fixed">
		<buginformation>
			<summary>unsafe initialization in DummyStepInterpolator</summary>
			<description>DummyStepInterpolator.java

  public DummyStepInterpolator(final DummyStepInterpolator interpolator) {
    super(interpolator);
    currentDerivative = interpolator.currentDerivative.clone();
  }
   @Override
  protected StepInterpolator doCopy() {
    return new DummyStepInterpolator(this);
  }


A constructor in DummyStepInterpolator dereferences a field of the parameter, but a NullPointerException can occur during a call to doCopy().
Test.java

public void test1() throws Throwable {
  DummyStepInterpolator var0 = new DummyStepInterpolator();
  var0.copy();
}


Here in Test.java, a NPE occurs because copy() calls doCopy() which calls  DummyStepInterpolator(final DummyStepInterpolator) that passes var0 as an argument.
I think this constructor should have a null check for  interpolator.currentDerivative like NordsieckStepInterpolator does.
NordsieckStepInterpolator.java

    public NordsieckStepInterpolator(final NordsieckStepInterpolator interpolator) {
        super(interpolator);
        scalingH      = interpolator.scalingH;
        referenceTime = interpolator.referenceTime;
        if (interpolator.scaled != null) {
            scaled = interpolator.scaled.clone();
        }
        if (interpolator.nordsieck != null) {
            nordsieck = new Array2DRowRealMatrix(interpolator.nordsieck.getDataRef(), true);
        }
        if (interpolator.stateVariation != null) {
            stateVariation = interpolator.stateVariation.clone();
        }
    }

    @Override
    protected StepInterpolator doCopy() {
        return new NordsieckStepInterpolator(this);
    }

</description>
			<version>3.3</version>
			<fixedVersion>3.4</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.ode.sampling.DummyStepInterpolator.java</file>
		</fixedFiles>
	</bug>
	<bug id="1145" opendate="2014-08-11 18:33:05" fixdate="2014-09-13 15:47:12" resolution="Fixed">
		<buginformation>
			<summary>Integer overflows MannWhitneyUTest#mannWhitneyU</summary>
			<description>In the calculation of MannWhitneyUTest#mannWhitneyU there are two instances where the lengths of the input arrays are multiplied together. Because Array#length is an integer this means that the effective maximum size of your dataset until reaching overflow is Math.sqrt(Integer.MAX_VALUE).
The following is a link to a diff, with a test the exposes the issue, and a fix (casting lengths up into doubles before multiplying).
https://gist.github.com/aconbere/4fef56e5182e510aceb3</description>
			<version>3.3</version>
			<fixedVersion>3.4</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.stat.inference.MannWhitneyUTestTest.java</file>
			<file type="M">org.apache.commons.math3.stat.inference.MannWhitneyUTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="1131" opendate="2014-06-25 08:12:48" fixdate="2014-09-19 16:23:35" resolution="Fixed">
		<buginformation>
			<summary>Kolmogorov-Smirnov Tests takes &amp;apos;forever&amp;apos; on 10,000 item dataset</summary>
			<description>I have code simplified to the following:
    KolmogorovSmirnovTest kst = new KolmogorovSmirnovTest();
    NormalDistribution nd = new NormalDistribution(mean,stddev);
    kst.kolmogorovSmirnovTest(nd,dataset)
I find that for my dataset of 10,000 items, the call to kolmogorovSmirnovTest takes &amp;amp;apos;forever&amp;amp;apos;. It has not returned after nearly 15minutes and in one my my tests has gone over 150MB in  memory usage. </description>
			<version>3.3</version>
			<fixedVersion>3.4</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.stat.inference.KolmogorovSmirnovTest.java</file>
			<file type="M">org.apache.commons.math3.stat.inference.KolmogorovSmirnovTestTest.java</file>
			<file type="M">org.apache.commons.math3.util.MathUtils.java</file>
		</fixedFiles>
	</bug>
	<bug id="1148" opendate="2014-08-20 16:17:24" fixdate="2014-09-29 16:28:42" resolution="Fixed">
		<buginformation>
			<summary>MonotoneChain handling of collinear points drops low points in a near-column</summary>
			<description>This code


val points = List(
  new Vector2D(
    16.078200000000184,
    -36.52519999989808
  ),
  new Vector2D(
    19.164300000000186,
    -36.52519999989808
  ),
  new Vector2D(
    19.1643,
    -25.28136477910407
  ),
  new Vector2D(
    19.1643,
    -17.678400000004157
  )
)
new hull.MonotoneChain().generate(points.asJava)


results in the exception:


org.apache.commons.math3.exception.ConvergenceException: illegal state: convergence failed
	at org.apache.commons.math3.geometry.euclidean.twod.hull.AbstractConvexHullGenerator2D.generate(AbstractConvexHullGenerator2D.java:106)
	at org.apache.commons.math3.geometry.euclidean.twod.hull.MonotoneChain.generate(MonotoneChain.java:50)
	at .&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:13)
	at .&amp;lt;clinit&amp;gt;(&amp;lt;console&amp;gt;)
	at .&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:11)
	at .&amp;lt;clinit&amp;gt;(&amp;lt;console&amp;gt;)
	at $print(&amp;lt;console&amp;gt;)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:704)
	at scala.tools.nsc.interpreter.IMain$Request$$anonfun$14.apply(IMain.scala:920)
	at scala.tools.nsc.interpreter.Line$$anonfun$1.apply$mcV$sp(Line.scala:43)
	at scala.tools.nsc.io.package$$anon$2.run(package.scala:25)
	at java.lang.Thread.run(Thread.java:662)


This will be tricky to fix. Not only is the point (19.164300000000186, -36.52519999989808) is being dropped incorrectly, but any point dropped in one hull risks creating a kink when combined with the other hull.
</description>
			<version>3.3</version>
			<fixedVersion>3.4</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.geometry.euclidean.twod.hull.ConvexHullGenerator2DAbstractTest.java</file>
			<file type="M">org.apache.commons.math3.geometry.euclidean.twod.hull.MonotoneChainTest.java</file>
			<file type="M">org.apache.commons.math3.geometry.euclidean.twod.hull.MonotoneChain.java</file>
			<file type="M">org.apache.commons.math3.geometry.euclidean.twod.hull.ConvexHull2D.java</file>
		</fixedFiles>
	</bug>
	<bug id="1152" opendate="2014-09-12 07:46:38" fixdate="2014-09-30 19:17:20" resolution="Fixed">
		<buginformation>
			<summary>Suboptimal implementation of EnumeratedDistribution.sample()</summary>
			<description>org.apache.commons.math3.distribution.EnumeratedDistribution.sample() performs a linear search to find the appropriate element in the probability space (called singletons here) given a random double value. For large probability spaces, this is not effective. Instead, we should cache the cumulative probabilities and do a binary search.
Rough implementation:
EnumeratedDistribution.java

void computeCumulative() {
  cumulative = new double[size]; 
  double sum = 0;
  for (int i = 1; i &amp;lt; weights.length - 1; i++) {
      cumulative[i] = cumulative[i-1] + weights[i-1];
   }
  cumulative[size - 1] = 1;
}


and then 
EnumeratedDistribution.java

int sampleIndex() {
 double randomValue = random.nextDouble();
 int result = Arrays.binarySearch(cumulative, randomValue);
 if (result &amp;gt;= 0) return result;
 int insertionPoint = -result-1;
 return insertionPoint;
}


</description>
			<version>3.3</version>
			<fixedVersion>3.4</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.distribution.EnumeratedDistribution.java</file>
		</fixedFiles>
	</bug>
	<bug id="1129" opendate="2014-06-17 09:22:55" fixdate="2014-10-05 18:44:26" resolution="Fixed">
		<buginformation>
			<summary>Percentile Computation errs</summary>
			<description>In the following test, the 75th percentile is smaller than the 25th percentile, leaving me with a negative interquartile range.
Bar.java

@Test public void negativePercentiles(){

        double[] data = new double[]{
                -0.012086732064244697, 
                -0.24975668704012527, 
                0.5706168483164684, 
                -0.322111769955327, 
                0.24166759508327315, 
                Double.NaN, 
                0.16698443218942854, 
                -0.10427763937565114, 
                -0.15595963093172435, 
                -0.028075857595882995, 
                -0.24137994506058857, 
                0.47543170476574426, 
                -0.07495595384947631, 
                0.37445697625436497, 
                -0.09944199541668033
        };
        DescriptiveStatistics descriptiveStatistics = new DescriptiveStatistics(data);

        double threeQuarters = descriptiveStatistics.getPercentile(75);
        double oneQuarter = descriptiveStatistics.getPercentile(25);

        double IQR = threeQuarters - oneQuarter;
        
        System.out.println(String.format("25th percentile %s 75th percentile %s", oneQuarter, threeQuarters ));
        
        assert IQR &amp;gt;= 0;
        
    }

</description>
			<version>3.2</version>
			<fixedVersion>3.4</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.stat.descriptive.DescriptiveStatisticsTest.java</file>
			<file type="M">org.apache.commons.math3.stat.descriptive.rank.Percentile.java</file>
		</fixedFiles>
	</bug>
	<bug id="1138" opendate="2014-07-11 18:51:30" fixdate="2014-10-17 08:54:56" resolution="Fixed">
		<buginformation>
			<summary>BicubicSplineInterpolator is returning incorrect interpolated values</summary>
			<description>I have encountered a use case with the BicubicSplineInterpolator where the interpolated values that are being returned seem incorrect.  Furthermore, the values do not match those generated by MatLab using the interp2 &amp;amp;apos;cubic&amp;amp;apos; method.
Here is a snippet of code that uses the interpolator:
        double[] xValues = new double[] 
{36, 36.001, 36.002}
;
        double[] yValues = new double[] {-108.00, -107.999, -107.998};
        double[][] fValues = new double[][] {{1915, 1906, 1931},

{1877, 1889, 1894}
,
                                        {1878, 1873, 1888}};
        BicubicSplineInterpolator interpolator = new BicubicSplineInterpolator();
        BicubicSplineInterpolatingFunction interpolatorFunction = interpolator.interpolate(xValues, yValues, fValues);
        double[][] results = new double[9][9];
        double x = 36;
        int arrayIndexX = 0, arrayIndexY = 0;
        while(x &amp;lt;= 36.002) {
            double y = -108;
            arrayIndexY = 0;
            while (y &amp;lt;= -107.998) 
{
                results[arrayIndexX][arrayIndexY] = interpolatorFunction.value(x,  y);
                System.out.println(results[arrayIndexX][arrayIndexY]);
                y = y + 0.00025;
                arrayIndexY++;
            }

            x = x + 0.00025;
            arrayIndexX++;
        }
Attached is a grid showing x and y values and the corresponding interpolated value from both commons math and MatLab.
The values produced by commons math are far off from those created by MatLab.</description>
			<version>3.3</version>
			<fixedVersion>3.4</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.analysis.interpolation.BicubicSplineInterpolator.java</file>
			<file type="M">org.apache.commons.math3.analysis.interpolation.BicubicSplineInterpolatingFunction.java</file>
			<file type="M">org.apache.commons.math3.analysis.interpolation.TricubicSplineInterpolatorTest.java</file>
			<file type="M">org.apache.commons.math3.analysis.interpolation.BicubicSplineInterpolatorTest.java</file>
			<file type="M">org.apache.commons.math3.analysis.interpolation.TricubicSplineInterpolator.java</file>
			<file type="M">org.apache.commons.math3.analysis.interpolation.BicubicSplineInterpolatingFunctionTest.java</file>
			<file type="M">org.apache.commons.math3.analysis.interpolation.SmoothingPolynomialBicubicSplineInterpolator.java</file>
			<file type="M">org.apache.commons.math3.analysis.interpolation.SmoothingPolynomialBicubicSplineInterpolatorTest.java</file>
			<file type="M">org.apache.commons.math3.analysis.interpolation.SplineInterpolatorTest.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">1166</link>
			<link type="Reference" description="is related to">985</link>
		</links>
	</bug>
	<bug id="1165" opendate="2014-11-06 11:57:49" fixdate="2014-11-07 22:10:00" resolution="Fixed">
		<buginformation>
			<summary>Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer</summary>
			<description>The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:
If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.
So the if condition:
if (membershipMatrix[i][j] &amp;gt; maxMembership) 
{
                    maxMembership = membershipMatrix[i][j];
                    newCluster = j;
}
will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line:
clusters.get(newCluster)
                    .addPoint(point);
Adding the following condition can solve the problem:
double d;
if (sum == 0)
d = 1;
else
d = 1.0/sum;</description>
			<version>3.3</version>
			<fixedVersion>3.4</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.ml.clustering.FuzzyKMeansClustererTest.java</file>
			<file type="M">org.apache.commons.math3.ml.clustering.FuzzyKMeansClusterer.java</file>
		</fixedFiles>
	</bug>
	<bug id="1167" opendate="2014-11-12 11:06:15" fixdate="2014-11-12 11:22:27" resolution="Fixed">
		<buginformation>
			<summary>OLSMultipleLinearRegression STILL needs a way to specify non-zero singularity threshold when instantiating QRDecomposition</summary>
			<description>A fix was made for this issue in MATH-1110 for the newSampleData method but not for the newXSampleData method.
It&amp;amp;apos;s a simple change to propagate the threshold to QRDecomposition:
237c237
&amp;lt;         qr = new QRDecomposition(getX());

&amp;gt;         qr = new QRDecomposition(getX(), threshold);</description>
			<version>3.3</version>
			<fixedVersion>3.4</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.stat.regression.OLSMultipleLinearRegression.java</file>
		</fixedFiles>
	</bug>
	<bug id="1174" opendate="2014-12-02 21:04:09" fixdate="2014-12-02 21:22:26" resolution="Fixed">
		<buginformation>
			<summary>Some thin rectangles are not handled properly as PolygonsSet</summary>
			<description>If the width of a rectangle is smaller than the close point tolerances, some weird effects appear when vertices are extracted. Typically the size will be set to infinity and barycenter will be forced at origin.</description>
			<version>3.3</version>
			<fixedVersion>3.4</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.geometry.euclidean.twod.PolygonsSetTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="1154" opendate="2014-10-05 19:34:56" fixdate="2014-12-15 20:22:08" resolution="Fixed">
		<buginformation>
			<summary>Statistical tests in stat.inference package are very slow due to implicit RandomGenerator initialization</summary>
			<description>Some statistical tests defined in the stat.inference package (e.g. BinomialTest or ChiSquareTest) are unnecessarily very slow (up to a factor 20 slower than necessary). The reason is the implicit slow initialization of a default (Well19937c) random generator instance each time a test is performed. The affected tests create some distribution instance in order to use some methods defined therein. However, they do not use any method for random generation. Nevertheless a random number generator instance is automatically created when creating a distribution instance, which is the reason for the serious slowdown. The problem is related to MATH-1124.
There are following solutions:
1) Fix the affected statistical tests by passing a light-weight RandomGenerator implementation (or even null) to the constructor of the distribution.
2) Or use for all distributions a RandomGenerator implementation that uses lazy initialization to generate the Well19937c instance as late as possible. This would also solve MATH-1124.
I will attach a patch proposal together with a performance test, that will demonstrate the speed up after a fix.</description>
			<version>3.3</version>
			<fixedVersion>3.4</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.distribution.UniformIntegerDistribution.java</file>
			<file type="M">org.apache.commons.math3.distribution.MixtureMultivariateRealDistribution.java</file>
			<file type="M">org.apache.commons.math3.distribution.NormalDistribution.java</file>
			<file type="M">org.apache.commons.math3.distribution.EnumeratedDistribution.java</file>
			<file type="M">org.apache.commons.math3.distribution.LevyDistribution.java</file>
			<file type="M">org.apache.commons.math3.distribution.WeibullDistribution.java</file>
			<file type="M">org.apache.commons.math3.distribution.GammaDistribution.java</file>
			<file type="M">org.apache.commons.math3.distribution.HypergeometricDistribution.java</file>
			<file type="M">org.apache.commons.math3.distribution.TDistribution.java</file>
			<file type="M">org.apache.commons.math3.distribution.BetaDistribution.java</file>
			<file type="M">org.apache.commons.math3.distribution.PascalDistribution.java</file>
			<file type="M">org.apache.commons.math3.distribution.LaplaceDistribution.java</file>
			<file type="M">org.apache.commons.math3.distribution.UniformRealDistribution.java</file>
			<file type="M">org.apache.commons.math3.distribution.GumbelDistribution.java</file>
			<file type="M">org.apache.commons.math3.distribution.ChiSquaredDistribution.java</file>
			<file type="M">org.apache.commons.math3.distribution.EnumeratedRealDistribution.java</file>
			<file type="M">org.apache.commons.math3.distribution.GeometricDistribution.java</file>
			<file type="M">org.apache.commons.math3.distribution.FDistribution.java</file>
			<file type="M">org.apache.commons.math3.distribution.ParetoDistribution.java</file>
			<file type="M">org.apache.commons.math3.distribution.ExponentialDistribution.java</file>
			<file type="M">org.apache.commons.math3.distribution.BinomialDistribution.java</file>
			<file type="M">org.apache.commons.math3.distribution.LogNormalDistribution.java</file>
			<file type="M">org.apache.commons.math3.distribution.TriangularDistribution.java</file>
			<file type="M">org.apache.commons.math3.distribution.ZipfDistribution.java</file>
			<file type="M">org.apache.commons.math3.distribution.MultivariateNormalDistribution.java</file>
			<file type="M">org.apache.commons.math3.distribution.MixtureMultivariateNormalDistribution.java</file>
			<file type="M">org.apache.commons.math3.distribution.CauchyDistribution.java</file>
			<file type="M">org.apache.commons.math3.distribution.EnumeratedIntegerDistribution.java</file>
			<file type="M">org.apache.commons.math3.distribution.LogisticDistribution.java</file>
			<file type="M">org.apache.commons.math3.distribution.PoissonDistribution.java</file>
			<file type="M">org.apache.commons.math3.distribution.NakagamiDistribution.java</file>
			<file type="M">org.apache.commons.math3.stat.inference.BinomialTest.java</file>
			<file type="M">org.apache.commons.math3.stat.inference.OneWayAnova.java</file>
			<file type="M">org.apache.commons.math3.stat.inference.WilcoxonSignedRankTest.java</file>
			<file type="M">org.apache.commons.math3.stat.inference.GTest.java</file>
			<file type="M">org.apache.commons.math3.stat.inference.ChiSquareTest.java</file>
			<file type="M">org.apache.commons.math3.stat.inference.TTest.java</file>
			<file type="M">org.apache.commons.math3.stat.inference.MannWhitneyUTest.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">1124</link>
		</links>
	</bug>
	<bug id="1146" opendate="2014-08-15 18:03:51" fixdate="2014-12-16 23:53:56" resolution="Fixed">
		<buginformation>
			<summary>class Mean returns incorrect result after processing an Infinity value</summary>
			<description>1. Create a Mean object.
2. call increment() with Double.POSITIVE_INFINITY.
3. Call getResult(). Result is INFINITY as expected.
4. call increment() with 0.
5. Call getResult(). Result is NaN; not INFINITY as expected.
This is apparently due to the "optimization" for calculating mean described in the javadoc. Rather than accumulating a sum, it maintains a running mean value using the formula "m = m + (new value - m) / (number of observations)", which unlike the "definition way", fails after an infinity.
I was using Mean within a SummaryStatistics. Other statistics also seem to be affected; for example, the standard deviation also incorrectly gives NaN rather than Infinity. I don&amp;amp;apos;t know if that&amp;amp;apos;s due to the error in Mean or if the other stats classes have similar bugs.</description>
			<version>3.3</version>
			<fixedVersion>3.4</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.stat.descriptive.moment.FirstMoment.java</file>
			<file type="M">org.apache.commons.math3.stat.descriptive.moment.SecondMoment.java</file>
			<file type="M">org.apache.commons.math3.stat.descriptive.moment.FourthMoment.java</file>
			<file type="M">org.apache.commons.math3.stat.descriptive.moment.ThirdMoment.java</file>
			<file type="M">org.apache.commons.math3.stat.descriptive.moment.Variance.java</file>
			<file type="M">org.apache.commons.math3.stat.descriptive.moment.Kurtosis.java</file>
			<file type="M">org.apache.commons.math3.stat.descriptive.moment.Mean.java</file>
			<file type="M">org.apache.commons.math3.stat.descriptive.moment.FirstMomentTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="1192" opendate="2015-01-14 19:23:48" fixdate="2015-02-16 22:51:39" resolution="Duplicate">
		<buginformation>
			<summary>"Descriptive statistics" has a non-final protected field</summary>
			<description>Field "windowSize" should be private (there is setter method).
</description>
			<version>3.4</version>
			<fixedVersion>4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math4.stat.descriptive.DescriptiveStatistics.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">760</link>
		</links>
	</bug>
	<bug id="760" opendate="2012-03-03 01:20:44" fixdate="2015-02-16 23:03:49" resolution="Fixed">
		<buginformation>
			<summary>DescriptiveStatistics.windowSize has a getter and setter, but is protected, so subclasses can bypass the validation check in the setter</summary>
			<description>DescriptiveStatistics.windowSize has a setter which does validation and maintains the list if necessary.
However the field is protected, so classes can ignore the setter.
As it happens, this is exactly what the subclass ListUnivariateImpl.setWindowSize does.
The field should be made private.</description>
			<version>3.4</version>
			<fixedVersion>4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math4.stat.descriptive.DescriptiveStatistics.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">1192</link>
			<link type="Reference" description="relates to">759</link>
		</links>
	</bug>
	<bug id="825" opendate="2012-07-16 09:16:32" fixdate="2015-02-16 23:04:05" resolution="Fixed">
		<buginformation>
			<summary>public method "laguerre" should be private</summary>
			<description>In class "LaguerreSolver" (package "o.a.c.m.analysis.solvers"), the method "laguerre" is public. However, it doesn&amp;amp;apos;t make any sense to call it from outside the class (because its argument list does not contain the function whose roots must be computed).
The method should be made private.</description>
			<version>3.0</version>
			<fixedVersion>4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math4.analysis.solvers.LaguerreSolver.java</file>
			<file type="M">org.apache.commons.math3.analysis.solvers.LaguerreSolver.java</file>
		</fixedFiles>
	</bug>
	<bug id="1204" opendate="2015-02-19 20:14:56" fixdate="2015-02-19 20:22:32" resolution="Fixed">
		<buginformation>
			<summary>bracket function gives up too early </summary>
			<description>In UnivariateSolverUtils.bracket(...) the search ends prematurely if a = lowerBound, which ignores some roots in the interval. </description>
			<version>3.4.1</version>
			<fixedVersion>4.0, 3.5</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math4.analysis.solvers.UnivariateSolverUtils.java</file>
			<file type="M">org.apache.commons.math4.analysis.solvers.UnivariateSolverUtilsTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="859" opendate="2012-09-08 18:15:16" fixdate="2015-02-24 22:09:34" resolution="Fixed">
		<buginformation>
			<summary>Fix and then deprecate isSupportXxxInclusive in RealDistribution interface</summary>
			<description>The conclusion from [1] was never implemented. We should deprecate these
properties from the RealDistribution interface, but since removal
will have to wait until 4.0, we should agree on a precise
definition and fix the code to match it in the mean time.
The definition that I propose is that isSupportXxxInclusive means
that when the density function is applied to the upper or lower
bound of support returned by getSupportXxxBound, a finite (i.e. not
infinite), not NaN value is returned.
[1] http://markmail.org/message/dxuxh7eybl7xejde</description>
			<version>3.0</version>
			<fixedVersion>4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math4.distribution.NormalDistribution.java</file>
			<file type="M">org.apache.commons.math4.distribution.LevyDistribution.java</file>
			<file type="M">org.apache.commons.math4.distribution.LogNormalDistribution.java</file>
			<file type="M">org.apache.commons.math4.distribution.ParetoDistribution.java</file>
			<file type="M">org.apache.commons.math4.distribution.GammaDistribution.java</file>
			<file type="M">org.apache.commons.math4.distribution.TDistribution.java</file>
			<file type="M">org.apache.commons.math4.distribution.ExponentialDistribution.java</file>
			<file type="M">org.apache.commons.math4.distribution.LogisticDistribution.java</file>
			<file type="M">org.apache.commons.math4.distribution.NakagamiDistribution.java</file>
			<file type="M">org.apache.commons.math4.distribution.ConstantRealDistribution.java</file>
			<file type="M">org.apache.commons.math4.distribution.FDistribution.java</file>
			<file type="M">org.apache.commons.math4.distribution.EnumeratedRealDistributionTest.java</file>
			<file type="M">org.apache.commons.math4.distribution.ChiSquaredDistribution.java</file>
			<file type="M">org.apache.commons.math4.distribution.RealDistribution.java</file>
			<file type="M">org.apache.commons.math4.distribution.BetaDistribution.java</file>
			<file type="M">org.apache.commons.math4.distribution.CauchyDistribution.java</file>
			<file type="M">org.apache.commons.math4.distribution.RealDistributionAbstractTest.java</file>
			<file type="M">org.apache.commons.math4.distribution.UniformRealDistribution.java</file>
			<file type="M">org.apache.commons.math4.distribution.WeibullDistribution.java</file>
			<file type="M">org.apache.commons.math4.distribution.TriangularDistribution.java</file>
			<file type="M">org.apache.commons.math4.distribution.EnumeratedRealDistribution.java</file>
			<file type="M">org.apache.commons.math4.distribution.LaplaceDistribution.java</file>
			<file type="M">org.apache.commons.math4.distribution.GumbelDistribution.java</file>
			<file type="M">org.apache.commons.math3.distribution.UniformRealDistribution.java</file>
			<file type="M">org.apache.commons.math3.distribution.FDistribution.java</file>
			<file type="M">org.apache.commons.math3.distribution.RealDistribution.java</file>
			<file type="M">org.apache.commons.math3.distribution.RealDistributionAbstractTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="1134" opendate="2014-06-30 19:35:24" fixdate="2015-02-27 14:13:19" resolution="Fixed">
		<buginformation>
			<summary>unsafe initialization in BicubicSplineInterpolatingFunction</summary>
			<description>The lazy initialization of the internal array of partialDerivatives in BicubicSplineInterpolatingFunction is not thread safe. If multiple threads call any of the partialDerivative functions concurrently one thread may start the initialization and others will see the array is non-null and assume it is fully initialized. If the internal array of partial derivatives was initialized in the constructor this would not be a problem.
i.e. the following check in partialDerivative(which, x, y)
        if (partialDerivatives == null) 
{
            computePartialDerivatives();
        }
will start the initialization. However in computePartialDerivatives()
        partialDerivatives = new BivariateFunction[5][lastI][lastJ];
makes it appear to other threads as the the initialization has completed when it may not have.</description>
			<version>3.3</version>
			<fixedVersion>3.4</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.analysis.interpolation.TricubicSplineInterpolator.java</file>
			<file type="M">org.apache.commons.math3.analysis.interpolation.BicubicSplineInterpolator.java</file>
			<file type="M">org.apache.commons.math3.analysis.interpolation.BicubicSplineInterpolatingFunction.java</file>
		</fixedFiles>
	</bug>
	<bug id="1208" opendate="2015-03-09 00:10:45" fixdate="2015-03-10 00:25:33" resolution="Fixed">
		<buginformation>
			<summary>EmpiricalDistribution cumulativeProbability can return NaN when evaluated within a constant bin</summary>
			<description>If x belongs to a bin with no variance or to which a ConstantRealDistribution kernel has been assigned, cumulativeProbability can return NaN.</description>
			<version>3.4.1</version>
			<fixedVersion>4.0, 3.5</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math4.random.EmpiricalDistributionTest.java</file>
			<file type="M">org.apache.commons.math4.random.EmpiricalDistribution.java</file>
		</fixedFiles>
	</bug>
	<bug id="1209" opendate="2015-03-11 17:09:55" fixdate="2015-03-11 21:15:01" resolution="Fixed">
		<buginformation>
			<summary>Documentation in PoissonDistribution.sample() has dead link</summary>
			<description>The link in the javadoc at http://commons.apache.org/proper/commons-math/apidocs/org/apache/commons/math3/distribution/PoissonDistribution.html for the sample() method points to http://irmi.epfl.ch/cmos/Pmmi/interactive/rng7.htm.  This link is dead.  I found it on the internet archive at https://web.archive.org/web/20090909055517/http://irmi.epfl.ch/cmos/Pmmi/interactive/rng7.htm.  The documentation should be updated or maybe Apache should mirror the page.</description>
			<version>3.4.1</version>
			<fixedVersion>4.0, 3.5</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math4.distribution.PoissonDistribution.java</file>
		</fixedFiles>
	</bug>
	<bug id="1206" opendate="2015-02-26 13:55:22" fixdate="2015-04-09 12:39:52" resolution="Fixed">
		<buginformation>
			<summary>Cost in least-squares fitting</summary>
			<description>In org.apache.commons.math4.fitting.leastsquares.AbstractEvaluation, the value returned by the "getCost" method is not consistent with the definition of "cost" in a least-squares problem: It is the sum of the squares of the residuals, but the method returns the square-root of that quantity.</description>
			<version>3.3</version>
			<fixedVersion>4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math4.fitting.leastsquares.EvaluationRmsCheckerTest.java</file>
			<file type="M">org.apache.commons.math4.fitting.leastsquares.OptimumImpl.java</file>
			<file type="M">org.apache.commons.math4.fitting.leastsquares.LevenbergMarquardtOptimizerTest.java</file>
			<file type="M">org.apache.commons.math4.fitting.leastsquares.LeastSquaresProblem.java</file>
			<file type="M">org.apache.commons.math4.fitting.leastsquares.AbstractEvaluation.java</file>
		</fixedFiles>
	</bug>
	<bug id="1214" opendate="2015-04-10 00:09:57" fixdate="2015-04-10 01:32:22" resolution="Fixed">
		<buginformation>
			<summary>SolutionCallback has incorrect class javadoc</summary>
			<description>The class javadoc for o.a.c.m.optim.linear.SolutionCallback does not describe the class.</description>
			<version>3.3</version>
			<fixedVersion>4.0, 3.5</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math4.optim.linear.SolutionCallback.java</file>
		</fixedFiles>
	</bug>
	<bug id="1205" opendate="2015-02-20 12:44:56" fixdate="2015-04-13 20:17:48" resolution="Fixed">
		<buginformation>
			<summary>AbstractStorelessUnivariateStatistic should not extend AbstractUnivariateStatistic</summary>
			<description>For a storeless statistic it is wrong to extend AbstractUnivariateStatistic as various fields and methods are inherited that do not make sense in case of a storeless statistic.
This means a user can accidentially use a storeless statistic in a wrong way:


        Mean mean = new Mean();
        
        mean.increment(1);
        mean.increment(2);
        
        mean.setData(new double[] { 1, 2, 3});
        
        System.out.println(mean.getResult());
        System.out.println(mean.evaluate());


will output

1.5
2.0

</description>
			<version>3.4.1</version>
			<fixedVersion>4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math4.stat.descriptive.summary.Product.java</file>
			<file type="M">org.apache.commons.math4.stat.descriptive.DescriptiveStatisticsTest.java</file>
			<file type="M">org.apache.commons.math4.stat.descriptive.rank.Median.java</file>
			<file type="M">org.apache.commons.math4.stat.descriptive.moment.KurtosisTest.java</file>
			<file type="M">org.apache.commons.math4.stat.descriptive.MixedListUnivariateImplTest.java</file>
			<file type="M">org.apache.commons.math4.stat.descriptive.AbstractUnivariateStatistic.java</file>
			<file type="M">org.apache.commons.math4.stat.descriptive.moment.StandardDeviation.java</file>
			<file type="M">org.apache.commons.math4.stat.descriptive.summary.ProductTest.java</file>
			<file type="M">org.apache.commons.math4.stat.descriptive.moment.Skewness.java</file>
			<file type="M">org.apache.commons.math4.stat.descriptive.moment.Variance.java</file>
			<file type="M">org.apache.commons.math4.stat.descriptive.moment.StandardDeviationTest.java</file>
			<file type="M">org.apache.commons.math4.stat.descriptive.summary.SumOfLogs.java</file>
			<file type="M">org.apache.commons.math4.stat.descriptive.rank.Min.java</file>
			<file type="M">org.apache.commons.math4.stat.descriptive.moment.VarianceTest.java</file>
			<file type="M">org.apache.commons.math4.stat.descriptive.moment.SecondMoment.java</file>
			<file type="M">org.apache.commons.math4.stat.descriptive.moment.Mean.java</file>
			<file type="M">org.apache.commons.math4.stat.descriptive.summary.SumSqTest.java</file>
			<file type="M">org.apache.commons.math4.stat.descriptive.rank.PSquarePercentileTest.java</file>
			<file type="M">org.apache.commons.math4.stat.descriptive.summary.Sum.java</file>
			<file type="M">org.apache.commons.math4.stat.descriptive.rank.MinTest.java</file>
			<file type="M">org.apache.commons.math4.stat.descriptive.StorelessUnivariateStatisticAbstractTest.java</file>
			<file type="M">org.apache.commons.math4.stat.descriptive.rank.MaxTest.java</file>
			<file type="M">org.apache.commons.math4.stat.descriptive.rank.MedianTest.java</file>
			<file type="M">org.apache.commons.math4.stat.descriptive.summary.SumTest.java</file>
			<file type="M">org.apache.commons.math4.stat.descriptive.StorelessUnivariateStatistic.java</file>
			<file type="D">org.apache.commons.math4.stat.descriptive.AbstractUnivariateStatisticTest.java</file>
			<file type="M">org.apache.commons.math4.stat.descriptive.rank.Percentile.java</file>
			<file type="M">org.apache.commons.math4.stat.descriptive.moment.FourthMoment.java</file>
			<file type="M">org.apache.commons.math4.stat.descriptive.AbstractStorelessUnivariateStatistic.java</file>
			<file type="M">org.apache.commons.math4.stat.descriptive.summary.SumLogTest.java</file>
			<file type="M">org.apache.commons.math4.stat.descriptive.UnivariateStatistic.java</file>
			<file type="M">org.apache.commons.math4.stat.descriptive.moment.Kurtosis.java</file>
			<file type="M">org.apache.commons.math4.stat.descriptive.moment.SkewnessTest.java</file>
			<file type="M">org.apache.commons.math4.stat.descriptive.moment.SemiVariance.java</file>
			<file type="M">org.apache.commons.math4.stat.descriptive.moment.FirstMoment.java</file>
			<file type="M">org.apache.commons.math4.stat.descriptive.moment.ThirdMoment.java</file>
			<file type="M">org.apache.commons.math4.stat.descriptive.summary.SumOfSquares.java</file>
			<file type="M">org.apache.commons.math4.stat.descriptive.rank.PSquarePercentile.java</file>
			<file type="M">org.apache.commons.math4.stat.descriptive.rank.Max.java</file>
			<file type="M">org.apache.commons.math4.stat.descriptive.MultivariateSummaryStatisticsTest.java</file>
			<file type="M">org.apache.commons.math4.stat.descriptive.UnivariateStatisticAbstractTest.java</file>
			<file type="M">org.apache.commons.math4.stat.descriptive.moment.GeometricMean.java</file>
		</fixedFiles>
	</bug>
	<bug id="1203" opendate="2015-02-19 17:46:40" fixdate="2015-04-14 12:18:42" resolution="Fixed">
		<buginformation>
			<summary>getKernel fails for buckets with only multiple instances of the same value in random.EmpiricalDistribution</summary>
			<description>After loading a set of values into an EmpericalDistribution, assume that there&amp;amp;apos;s a case where a single bin ONLY contains multiple instances of the same value.  In this case the standard deviation will equal zero.  This will fail when getKernel attempts to create a NormalDistribution.  The other case where stddev=0 is when there is only a single value in the bin, and this is handled by returning a ConstantRealDistribution rather than a NormalDistrbution.
See: https://issues.apache.org/jira/browse/MATH-984</description>
			<version>3.4</version>
			<fixedVersion>4.0, 3.5</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math4.random.EmpiricalDistributionTest.java</file>
			<file type="M">org.apache.commons.math4.random.EmpiricalDistribution.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">1234</link>
		</links>
	</bug>
	<bug id="1197" opendate="2015-01-20 05:40:41" fixdate="2015-04-26 19:14:15" resolution="Fixed">
		<buginformation>
			<summary>Incorrect KolmogorovSmirnov Statistic for two samples </summary>
			<description>kolmogorovSmirnovTest(double[],double[]) against the samples given below gives 5.699107852308316E-12 instead of 0.9793 (approx.) Traced the issue to kolmogorovSmirnovStatistic(double[],double[]) which gives 0.49507389162561577 instead of 0.064 (verified with ks.test in R and JDistlib)
  double[] x = 
{0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000
                ,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000
                ,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000
                ,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000
                ,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000
                ,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000
                ,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000
                ,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000
                ,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000
                ,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,2.202653,2.202653,2.202653
                ,2.202653,2.202653,2.202653,2.202653,2.202653,2.202653,2.202653,2.202653,2.202653,2.202653,2.202653,2.202653,2.202653
                ,2.202653,2.202653,2.202653,2.202653,2.202653,2.202653,2.202653,2.202653,2.202653,2.202653,2.202653,2.202653,2.202653
                ,2.202653,2.202653,2.202653,2.202653,2.202653,2.202653,3.181199,3.181199,3.181199,3.181199,3.181199,3.181199,3.723539
                ,3.723539,3.723539,3.723539,4.383482,4.383482,4.383482,4.383482,5.320671,5.320671,5.320671,5.717284,6.964001,7.352165
                ,8.710510,8.710510,8.710510,8.710510,8.710510,8.710510,9.539004,9.539004, 10.720619, 17.726077, 17.726077, 17.726077, 17.726077
                ,22.053875 ,23.799144 ,27.355308 ,30.584960 ,30.584960 ,30.584960, 30.584960, 30.751808}
;
         double[] y = 
{0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000
                 ,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000
                 ,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000
                 ,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,2.202653
                 ,2.202653,2.202653,2.202653,2.202653,2.202653,2.202653,2.202653,3.061758,3.723539,5.628420,5.628420,5.628420,5.628420
                 ,5.628420,6.916982,6.916982,6.916982, 10.178538, 10.178538, 10.178538, 10.178538, 10.178538 }
;</description>
			<version>3.4.1</version>
			<fixedVersion>4.0, 3.6</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math4.stat.inference.KolmogorovSmirnovTestTest.java</file>
			<file type="M">org.apache.commons.math4.stat.inference.KolmogorovSmirnovTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="1118" opendate="2014-04-14 06:04:42" fixdate="2015-05-07 14:47:35" resolution="Fixed">
		<buginformation>
			<summary>Complex: semantics of equals != Double equals, mismatch with hashCode</summary>
			<description>Two complex numbers with real/imaginary parts 0.0d but different signs compare as equal numbers. This is according to their mathematical value; the comparison is done via 
                return (real == c.real) &amp;amp;&amp;amp; (imaginary == c.imaginary);
Unfortunately, two Double values do NOT compare as equal in that case, so real.equals(c.real) would return false if the signs differ.
This becomes a problem because for the hashCode, MathUtils.hash is used on the real and imaginary parts, which in turn uses Double.hash.
This violates the contract on equals/hashCode, so Complex numbers cannot be used in a hashtable or similar data structure:
    Complex c1 = new Complex(0.0, 0.0);
    Complex c2 = new Complex(0.0, -0.0);
    // Checks the contract:  equals-hashcode on c1 and c2
    assertTrue("Contract failed: equals-hashcode on c1 and c2", c1.equals(c2) ? c1.hashCode() == c2.hashCode() : true);</description>
			<version>3.2</version>
			<fixedVersion>4.0, 3.6</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.complex.ComplexTest.java</file>
			<file type="M">org.apache.commons.math3.complex.Complex.java</file>
			<file type="M">org.apache.commons.math3.util.MathUtils.java</file>
			<file type="M">org.apache.commons.math3.util.MathUtilsTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="1116" opendate="2014-04-14 02:51:00" fixdate="2015-05-19 11:47:22" resolution="Fixed">
		<buginformation>
			<summary>NullPointerException not advertized in Javadoc</summary>
			<description>The following statement produces a NullPointerException:
new org.apache.commons.math3.optim.nonlinear.vector.jacobian.LevenbergMarquardtOptimizer().getWeight();
The documentation does not seem to indicate that other data must be set before getWeight is used (at least I could not find that information). In this case, weightMatrix is still null because it has not been initialized.
This call should probably throw an IllegalStateException, which makes it clear that this API usage is incorrect.
This test uses LevenbergMarquardtOptimizer but any instantiable subclass of MultivariateVectorOptimizer probably works the same way.</description>
			<version>3.2</version>
			<fixedVersion>4.0, 3.6</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.random.EmpiricalDistribution.java</file>
			<file type="M">org.apache.commons.math3.stat.regression.OLSMultipleLinearRegression.java</file>
			<file type="M">org.apache.commons.math3.random.ValueServer.java</file>
		</fixedFiles>
	</bug>
	<bug id="1224" opendate="2015-05-11 00:43:24" fixdate="2015-05-19 11:47:52" resolution="Fixed">
		<buginformation>
			<summary>NullPointerExceptions not documented in some classes</summary>
			<description>In general, the need to initialize newly constructed objects with more data is now documented, but we have found two cases where a NullPointerException is thrown because of missing data.
The documentation should be updated to reflect this. This is similar to issues report in MATH-1116 but concerns classes that are not going to be deprecated (as far as we can tell).
I have previously posted this as a new comment on issue 1116, but that comment has not elicited any response. As the original issue is one year old, I post this bug as a new issue.
Below is the code that produces the two cases:
org.apache.commons.math3.ode.nonstiff.HighamHall54Integrator var1 = new org.apache.commons.math3.ode.nonstiff.HighamHall54Integrator(0.0d, 0.0d, 0.0d, 0.0d);
double[] var2 = new double[] 
{ 0.0d }
;
var1.computeDerivatives(0.0d, var2, var2); // NPE
new org.apache.commons.math3.stat.correlation.SpearmansCorrelation().getCorrelationMatrix(); // NPE
</description>
			<version>3.3</version>
			<fixedVersion>4.0, 3.6</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math4.stat.correlation.SpearmansCorrelation.java</file>
		</fixedFiles>
	</bug>
	<bug id="1229" opendate="2015-05-30 16:25:57" fixdate="2015-05-30 17:27:41" resolution="Fixed">
		<buginformation>
			<summary>ResizableDoubleArray: Wrong "initialCapacity"</summary>
			<description>In o.a.c.m.util.ResizableDoubleArray, in the constructor


public ResizableDoubleArray(double[] initialArray) {
    this(DEFAULT_INITIAL_CAPACITY,
            DEFAULT_EXPANSION_FACTOR,
            DEFAULT_CONTRACTION_DELTA + DEFAULT_EXPANSION_FACTOR,
            DEFAULT_EXPANSION_MODE,
            initialArray);
}


the initial capacity should be set to the length on the input, and not to the hard-coded default.</description>
			<version>3.5</version>
			<fixedVersion>3.6</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math4.util.ResizableDoubleArray.java</file>
		</fixedFiles>
	</bug>
	<bug id="1230" opendate="2015-06-09 15:07:08" fixdate="2015-06-09 18:49:44" resolution="Fixed">
		<buginformation>
			<summary>SimplexSolver returning wrong answer from optimize</summary>
			<description>SimplexSolver fails for the following linear program:
min 2x1 +15x2 +18x3
Subject to
  -x1 +2x2  -6x3 &amp;lt;=-10
            x2  +2x3 &amp;lt;= 6
   2x1      +10x3 &amp;lt;= 19
    -x1  +x2       &amp;lt;= -2
    x1,x2,x3 &amp;gt;= 0
Solution should be
x1 = 7
x2 = 0
x3 = 1/2
Objective function = 23
Instead, it is returning
x1 = 9.5
x2 = 1/8
x3 = 0
Objective function = 20.875
Constraint number 1 is violated by this answer</description>
			<version>3.5</version>
			<fixedVersion>4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math4.optim.linear.SimplexSolver.java</file>
			<file type="M">org.apache.commons.math4.optim.linear.SimplexTableau.java</file>
			<file type="M">org.apache.commons.math4.optim.linear.SimplexSolverTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="1231" opendate="2015-06-10 11:59:50" fixdate="2015-06-11 22:13:37" resolution="Fixed">
		<buginformation>
			<summary>MicrosphereInterpolator: Unnecessery restriction in constructor</summary>
			<description>In o.a.c.m.analysis.interpolation.MicrosphereInterpolator, the constructor requires that the "exponent" be an integer, whereas the algorithm has no such restriction.</description>
			<version>3.5</version>
			<fixedVersion>4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math4.analysis.interpolation.MicrosphereInterpolatingFunction.java</file>
			<file type="M">org.apache.commons.math4.analysis.interpolation.MicrosphereInterpolator.java</file>
			<file type="M">org.apache.commons.math4.analysis.interpolation.MicrosphereInterpolatorTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="1234" opendate="2015-06-17 13:32:11" fixdate="2015-06-19 08:21:42" resolution="Duplicate">
		<buginformation>
			<summary>Impossible NotStrictlyPositiveException after getStandardDeviation()</summary>
			<description>org.apache.commons.math3.random.EmpiricalDistribution.density() calls 
EmpiricalDistribution.getKernel which calls
bStats.getStandardDeviation()  bStats is SummaryStatistics
and return result caused NotStrictlyPositiveException: standard deviation (0)
in new NormalDistribution(randomData.getRandomGenerator(),
                bStats.getMean(), bStats.getStandardDeviation(),
                NormalDistribution.DEFAULT_INVERSE_ABSOLUTE_ACCURACY);
As I understand, it shouldn&amp;amp;apos;t be so by contract of SummaryStatistics.</description>
			<version>3.2</version>
			<fixedVersion>3.5</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math4.random.EmpiricalDistributionTest.java</file>
			<file type="M">org.apache.commons.math4.random.EmpiricalDistribution.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">1203</link>
		</links>
	</bug>
	<bug id="1241" opendate="2015-06-24 12:49:05" fixdate="2015-06-24 13:43:21" resolution="Fixed">
		<buginformation>
			<summary>Digamma calculation produces SOE on NaN argument</summary>
			<description>Digamma doesn&amp;amp;apos;t work particularly well with NaNs.
How to reproduce: call Gamma.digamma(Double.NaN)
Expected outcome: returns NaN or throws a meaningful exception
Real outcome: crashes with StackOverflowException, as digamma enters infinite recursion.</description>
			<version>3.5</version>
			<fixedVersion>4.0, 3.6</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math4.special.Gamma.java</file>
			<file type="M">org.apache.commons.math4.special.GammaTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="1251" opendate="2015-07-19 21:07:18" fixdate="2015-07-20 13:51:12" resolution="Fixed">
		<buginformation>
			<summary>Wrong "number of calls" in "KohonenUpdateAction"</summary>
			<description>In class KohonenUpdateAction (package o.a.c.m.ml.neuralnet.sofm), the method getNumberOfCalls is off by 1 due to counter being initialized to -1.</description>
			<version>3.5</version>
			<fixedVersion>4.0, 3.6</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math4.ml.neuralnet.sofm.KohonenUpdateAction.java</file>
		</fixedFiles>
	</bug>
	<bug id="1255" opendate="2015-08-13 21:14:51" fixdate="2015-08-13 21:35:26" resolution="Fixed">
		<buginformation>
			<summary>Bug in "o.a.c.m.ml.neuralnet.sofm.KohonenUpdateAction"</summary>
			<description>In method "update", the standard deviation of the "Gaussian" function is set to "1 / n" instead of "n" where n is the current neighbourhood size.</description>
			<version>3.5</version>
			<fixedVersion>4.0, 3.6</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math4.ml.neuralnet.sofm.KohonenUpdateAction.java</file>
		</fixedFiles>
	</bug>
	<bug id="1257" opendate="2015-08-18 22:31:26" fixdate="2015-08-19 21:16:23" resolution="Fixed">
		<buginformation>
			<summary>NormalDistribution.cumulativeProbability() suffers from cancellation</summary>
			<description>I see the following around line 194:

        return 0.5 * (1 + Erf.erf(dev / (standardDeviation * SQRT2)));


When erf() returns a very small value, this cancels in the addition with the "1.0" which leads to poor precision in the results.
I would suggest changing this line to read more like:

return 0.5 * Erf.erfc( -dev / standardDeviation * SQRT2 );


Should you want some test cases for "extreme values" (one might argue that within 10 standard deviations isn&amp;amp;apos;t all that extreme) then you can check the following: http://www.jstatsoft.org/v52/i07/ then look in the v52i07-xls.zip at replication-01-distribution-standard-normal.xls
I think you will also find that evaluation of expressions such as 

NormalDistribution( 0, 1 ).cumulativeProbability( -10.0 );

are pretty far off.</description>
			<version>3.5</version>
			<fixedVersion>4.0, 3.6</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math4.distribution.NormalDistributionTest.java</file>
			<file type="M">org.apache.commons.math4.distribution.NormalDistribution.java</file>
		</fixedFiles>
	</bug>
	<bug id="879" opendate="2012-10-11 12:33:50" fixdate="2015-08-20 20:04:27" resolution="Fixed">
		<buginformation>
			<summary>"CMAESOptimizer" silently changes invalid input</summary>
			<description>The "lambda" input parameter must be strictly positive. But when it&amp;amp;apos;s not the case, an undocumented default is used (cf. line 526).
When a precondition is not satisfied, the code must throw an exception.
Instead of the code unknowingly changing the input, it is rather the documentation that should suggest a good default.
This change would allow to make "lambda" a constant (final) field.</description>
			<version>3.0</version>
			<fixedVersion>4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math3.optimization.direct.CMAESOptimizerTest.java</file>
			<file type="M">org.apache.commons.math3.optimization.direct.CMAESOptimizer.java</file>
		</fixedFiles>
	</bug>
	<bug id="1277" opendate="2015-09-18 21:39:27" fixdate="2015-09-20 08:15:39" resolution="Fixed">
		<buginformation>
			<summary>Incorrect Kendall Tau calc due to data type mistmatch</summary>
			<description>The Kendall Tau calculation returns a number from -1.0 to 1.0
due to a mixing of ints and longs, a mistake occurs on large size columns (arrays) passed to the function. an array size of &amp;gt; 50350 triggers the condition in my case - although it may be data dependent
the ver 3.5 library returns 2.6 as a result (outside of the defined range of Kendall Tau)
with the cast to long below - the result reutns to its expected value
commons.math3.stat.correlation.KendallsCorrelation.correlation
here&amp;amp;apos;s the sample code I used:
I added the cast to long of swaps in the 
			int swaps = 1077126315;
			 final long numPairs = sum(50350 - 1);
			    long tiedXPairs = 0;
		        long tiedXYPairs = 0;
		        long tiedYPairs = 0;
		  final long concordantMinusDiscordant = numPairs - tiedXPairs - tiedYPairs + tiedXYPairs - 2 * (long) swaps;
	        final double nonTiedPairsMultiplied = 1.6e18;
	        double myTest = concordantMinusDiscordant / FastMath.sqrt(nonTiedPairsMultiplied);</description>
			<version>3.5</version>
			<fixedVersion>4.0, 3.6</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math4.stat.correlation.KendallsCorrelation.java</file>
			<file type="M">org.apache.commons.math4.stat.correlation.KendallsCorrelationTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="1283" opendate="2015-10-22 08:40:27" fixdate="2015-10-22 20:34:05" resolution="Fixed">
		<buginformation>
			<summary>Gamma function computation</summary>
			<description>In the gamma method, when handling the case "absX &amp;gt; 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX".
For large negative values of x, the function returns with the wrong sign.
final double gammaAbs = SQRT_TWO_PI / x *
                                     FastMath.pow(y, absX + 0.5) *
                                     FastMath.exp(-y) * lanczos(absX);</description>
			<version>3.5</version>
			<fixedVersion>4.0, 3.6</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math4.special.Gamma.java</file>
			<file type="M">org.apache.commons.math4.special.GammaTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="1269" opendate="2015-09-06 11:49:24" fixdate="2015-11-05 20:30:04" resolution="Fixed">
		<buginformation>
			<summary>FastMath.exp may return NaN for non-NaN arguments</summary>
			<description>I have observed that FastMath.exp(709.8125) returns NaN. However, the exponential function must never return NaN (if the argument is not NaN). The result must always be non-negative or positive infinity.</description>
			<version>4.0</version>
			<fixedVersion>4.0, 3.6</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math4.util.FastMath.java</file>
			<file type="M">org.apache.commons.math4.util.FastMathTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="1294" opendate="2015-11-22 18:38:53" fixdate="2015-11-23 22:20:08" resolution="Fixed">
		<buginformation>
			<summary>Data race PolynomialUtils::buildPolynomial</summary>
			<description>If you run PolynomialUtilsTest methods concurrently there will occur problem with ComparisonFailure due to incorrect building of coefficient list. https://github.com/apache/commons-math/blob/master/src/main/java/org/apache/commons/math4/analysis/polynomials/PolynomialsUtils.java#L368 should be in synchronized block. Explanation: polynomial of given degree can be created by other thread  and when primary thread access synchronized block, there is already created coefficients for this degree, therefore no coefficients should be added to coefficients list.</description>
			<version>4.0</version>
			<fixedVersion>4.0, 3.6</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math4.analysis.polynomials.PolynomialsUtils.java</file>
		</fixedFiles>
	</bug>
	<bug id="1295" opendate="2015-12-01 15:56:26" fixdate="2015-12-02 14:58:12" resolution="Fixed">
		<buginformation>
			<summary>NonLinearConjugateGradientOptimizer and BracketFinder TooManyEvaluationsException</summary>
			<description>I am getting the exception below when using NonLinearConjugateGradientOptimizer.  
 org.apache.commons.math3.exception.TooManyEvaluationsException: illegal state: maximal count (50) exceeded: evaluations
	at org.apache.commons.math3.optim.univariate.BracketFinder.eval(BracketFinder.java:287)
	at org.apache.commons.math3.optim.univariate.BracketFinder.search(BracketFinder.java:181)
	at org.apache.commons.math3.optim.nonlinear.scalar.LineSearch.search(LineSearch.java:127)
	at org.apache.commons.math3.optim.nonlinear.scalar.gradient.NonLinearConjugateGradientOptimizer.doOptimize(NonLinearConjugateGradientOptimizer.java:283)
	at org.apache.commons.math3.optim.nonlinear.scalar.gradient.NonLinearConjugateGradientOptimizer.doOptimize(NonLinearConjugateGradientOptimizer.java:47)
	at org.apache.commons.math3.optim.BaseOptimizer.optimize(BaseOptimizer.java:154)
	at org.apache.commons.math3.optim.BaseMultivariateOptimizer.optimize(BaseMultivariateOptimizer.java:66)
	at org.apache.commons.math3.optim.nonlinear.scalar.MultivariateOptimizer.optimize(MultivariateOptimizer.java:64)
	at org.apache.commons.math3.optim.nonlinear.scalar.GradientMultivariateOptimizer.optimize(GradientMultivariateOptimizer.java:74)
	at org.apache.commons.math3.optim.nonlinear.scalar.gradient.NonLinearConjugateGradientOptimizer.optimize(NonLinearConjugateGradientOptimizer.java:245)
NonLinearConjugateGradientOptimizer calls the no argument constructor of BracketFinder which defaults its max evaluations to 50.  I tried changing the source code of BracketFinder so that the default max evaluations is 200 and since making the change have not encountered the problem.  I was wondering if BracketFinder could have its default max evaluations increased or if NonLinearConjugateGradientOptimizer could set a higher max evaluations when it constructs a BracketFinder.</description>
			<version>3.5</version>
			<fixedVersion>4.0, 3.6</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math4.optim.univariate.BracketFinder.java</file>
		</fixedFiles>
	</bug>
	<bug id="1296" opendate="2015-12-02 09:50:01" fixdate="2015-12-03 03:43:00" resolution="Fixed">
		<buginformation>
			<summary>DescriptiveStatistics return geometric mean as 0 when product of values is zero, expected to return NaN</summary>
			<description>	@Test
	public void test() 
{
		DescriptiveStatistics stats = new DescriptiveStatistics();
		stats.addValue(1);
		stats.addValue(2);
		stats.addValue(4);
		System.out.println(stats.getGeometricMean()); //prints 2

		stats.addValue(0);
		System.out.println(stats.getGeometricMean()); //prints 0, expected NaN as per the documentation
}

The class in consideration is: org.apache.commons.math3.stat.descriptive.DescriptiveStatistics</description>
			<version>3.4.1</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math4.stat.descriptive.moment.Skewness.java</file>
			<file type="M">org.apache.commons.math4.stat.descriptive.DescriptiveStatistics.java</file>
		</fixedFiles>
	</bug>
	<bug id="1310" opendate="2015-12-31 20:03:40" fixdate="2015-12-31 22:22:59" resolution="Fixed">
		<buginformation>
			<summary>Improve accuracy and performance of 2-sample Kolmogorov-Smirnov test</summary>
			<description>As of 3.5, the exactP method used to compute exact  p-values for 2-sample Kolmogorov-Smirnov tests is very slow, as it is based on a naive implementation that enumarates all n-m partitions of the combined sample.  As a result, its use is not recommended for problems where the product of the two sample sizes exceeds 100 and the kolmogorovSmirnovTest method uses it only for samples in this range.  To handle sample size products between 100 and 10000, where the asymptotic KS distribution can be used, this method currently uses Monte Carlo simulation.  Convergence is poor for many problem instances, resulting in inaccurate results.
To eliminate the need for the Monte Carlo simulation and increase the performance of exactP itself, a faster exactP implementation should be added.  This can be implemented by unwinding the recursive functions defined in Chapter 5, table 5.2 in:
Wilcox, Rand. 2012. Introduction to Robust Estimation and Hypothesis Testing, Chapter 5, 3rd Ed. Academic Press.</description>
			<version>3.5</version>
			<fixedVersion>3.6</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math4.stat.inference.KolmogorovSmirnovTestTest.java</file>
			<file type="M">org.apache.commons.math4.stat.inference.KolmogorovSmirnovTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="1300" opendate="2015-12-19 01:57:14" fixdate="2015-12-31 23:21:58" resolution="Fixed">
		<buginformation>
			<summary>BitsStreamGenerator#nextBytes(byte[]) is wrong</summary>
			<description>Sequential calls to the BitsStreamGenerator#nextBytes(byte[]) must generate the same sequence of bytes, no matter by chunks of which size it was divided. This is also how java.util.Random#nextBytes(byte[]) works.
When nextBytes(byte[]) is called with a bytes array of length multiple of 4 it makes one unneeded call to next(int) method. This is wrong and produces an inconsistent behavior of classes like MersenneTwister.
I made a new implementation of the BitsStreamGenerator#nextBytes(byte[]) see attached code.</description>
			<version>3.5</version>
			<fixedVersion>3.6</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math4.random.AbstractRandomGenerator.java</file>
			<file type="M">org.apache.commons.math4.random.RandomGeneratorAbstractTest.java</file>
			<file type="M">org.apache.commons.math4.random.BitsStreamGenerator.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">1304</link>
			<link type="Reference" description="is related to">1305</link>
		</links>
	</bug>
	<bug id="1356" opendate="2016-04-13 12:27:15" fixdate="2016-05-06 21:31:32" resolution="Fixed">
		<buginformation>
			<summary>HypergeometricDistribution probability give NaN result</summary>
			<description>Hi,
Unless I am mistaken the HypergeometricDistribution probability method returns NaN for the following cases :
HypergeometricDistribution hgd = new HypergeometricDistribution(11,11,1);
double probIT = hgd.probability(1);
HypergeometricDistribution hgd = new HypergeometricDistribution(11,11,11);
double probIT = hgd.probability(11);
I think it should return 1.0
Thanks,
Thomas</description>
			<version>3.6.1</version>
			<fixedVersion>4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math4.distribution.SaddlePointExpansion.java</file>
			<file type="M">org.apache.commons.math4.distribution.HypergeometricDistributionTest.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">1384</link>
		</links>
	</bug>
	<bug id="1376" opendate="2016-06-11 12:38:36" fixdate="2016-06-11 19:31:43" resolution="Fixed">
		<buginformation>
			<summary>SimplexOptimizer.doOptimize(): Wrong Iteration Number (0) Passed to Convergence Checker</summary>
			<description>The convergence checker used in method doOptimize() of SimplexOptimizer always receives 0 as iteration counter. This can very easily be fixed. Check this out:
Original (with added comments):


int iteration = 0; // XXXXXXXXX set to zero and never update
        final ConvergenceChecker&amp;lt;PointValuePair&amp;gt; checker = getConvergenceChecker();
        while (true) {
            if (getIterations() &amp;gt; 0) {
                boolean converged = true;
                for (int i = 0; i &amp;lt; simplex.getSize(); i++) {
                    PointValuePair prev = previous[i];
                    converged = converged &amp;amp;&amp;amp; // XXXXXXXXX ouch below
                        checker.converged(iteration, prev, simplex.getPoint(i));
                }
                if (converged) {
                    // We have found an optimum.
                    return simplex.getPoint(0);
                }
            }


should be (with added comments)


int iteration = 0;
        final ConvergenceChecker&amp;lt;PointValuePair&amp;gt; checker = getConvergenceChecker();
        while (true) {
            iteration = getIterations(); // XXXXXXXX CHANGE 1
            if (iteration &amp;gt; 0) {  // XXXXXXXX CHANGE 2
                boolean converged = true;
                for (int i = 0; i &amp;lt; simplex.getSize(); i++) {
                    PointValuePair prev = previous[i];
                    converged = converged &amp;amp;&amp;amp;
                        checker.converged(iteration, prev, simplex.getPoint(i));
                }
                if (converged) {
                    // We have found an optimum.
                    return simplex.getPoint(0);
                }
            }

</description>
			<version>3.6.1</version>
			<fixedVersion>4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math4.optim.nonlinear.scalar.noderiv.SimplexOptimizer.java</file>
		</fixedFiles>
	</bug>
	<bug id="1384" opendate="2016-09-06 02:26:39" fixdate="2016-09-06 06:48:25" resolution="Duplicate">
		<buginformation>
			<summary>HypergeometricDistribution logProbability() returns NaN for edge cases</summary>
			<description>For certain edge cases, HypergeometricDistribution.logProbability() will return NaN.
To compute the hypergeometric log probability, three binomial log probabilities are computed and then combined accordingly. The implementation is essentially the same as in BinomialDistribution.logProbability() and uses the SaddlePointExpansion. However, the Binomial implementation includes an extra check for the edge case of 0 trials which the HyperGeometric lacks.
An example call which fails is:
new HypergeometricDistribution(null, 11, 0, 1).logProbability(0)
which returns NaN instead of 0.0.
Note that
new HypergeometricDistribution(null, 10, 0, 1).logProbability(0)
returns 0 as expected.
Possible fixes:
1. Check for the edge cases and return appropriate values. This would make the code somewhat more complex.
2. Instead of duplicating the implementation use BinomialDistribution.logProbability(). This is much simpler/more readable but will reduce performance as each call to BinomialDistribution.logProbability() makes redundant checks of validity of input parameters etc.
I am happy to submit a PR at the GitHub repo implementing either 1 or 2 with the necessary tests.</description>
			<version>3.0</version>
			<fixedVersion>4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math4.distribution.SaddlePointExpansion.java</file>
			<file type="M">org.apache.commons.math4.distribution.HypergeometricDistributionTest.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">1356</link>
		</links>
	</bug>
	<bug id="1396" opendate="2016-11-17 17:24:50" fixdate="2016-11-23 12:56:26" resolution="Fixed">
		<buginformation>
			<summary>Overflows in "UniformIntegerDistribution"</summary>
			<description>In o.a.c.m.distribution.UniformIntegerDistribution, several methods will compute an invalid result when the lower and upper bounds are such that

upper - lower


overflows.
Affected methods:

probability
cumulativeProbability
getNumericalVariance

Method

getNumericalMean

will return an invalid result when

upper + lower


overflows.
A possible fix is to define instances variables

upperPlusLower = (double) upper + (double) lower;
upperMinusLower = (double) upper - (double) lower;


and use them instead of the respective integer operations in the above methods.</description>
			<version>3.6.1</version>
			<fixedVersion>4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.commons.math4.distribution.UniformIntegerDistributionTest.java</file>
			<file type="M">org.apache.commons.math4.distribution.UniformIntegerDistribution.java</file>
		</fixedFiles>
	</bug>
</bugrepository>