<bugrepository name="redisson">
    <bug id="3484" opendate="2021-03-16 00:00:00" fixdate="2021-03-23 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>There may be concurrency problems in org.redisson.RedissonLock#tryLockInnerAsync
            </summary>
            <description>tryLockInnerAsync(long waitTime, long leaseTime, TimeUnit unit, long threadId,
                RedisStrictCommand
                T>
                command) {

                internalLockLeaseTime = unit.toMillis(leaseTime);


                if two thread get the same lock subject and run the method at same time, one's leaseTime is -1, and
                another's leaseTime is bigger than 30, the "-1" thread first cam in and set internalLockLeaseTime, then
                another thread came in set the internalLockLeaseTime bigger than 30, then if the bigger leaseTime one
                get the real redis lock! It while cause watchDog invaild and the lock be expired ahead of time

                return evalWriteAsync(getName(), LongCodec.INSTANCE, command,
                "if (redis.call('exists', KEYS[1]) == 0) then " +
                "redis.call('hincrby', KEYS[1], ARGV[2], 1); " +
                "redis.call('pexpire', KEYS[1], ARGV[1]); " +
                "return nil; " +
                "end; " +
                "if (redis.call('hexists', KEYS[1], ARGV[2]) == 1) then " +
                "redis.call('hincrby', KEYS[1], ARGV[2], 1); " +
                "redis.call('pexpire', KEYS[1], ARGV[1]); " +
                "return nil; " +
                "end; " +
                "return redis.call('pttl', KEYS[1]);",
                Collections.singletonList(getName()), internalLockLeaseTime, getLockName(threadId));
                }
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.redisson.RedissonFairLock.java</file>
            <file type="M">org.redisson.RedissonLock.java</file>
            <file type="M">org.redisson.RedissonReadLock.java</file>
            <file type="M">org.redisson.RedissonWriteLock.java</file>
        </fixedFiles>
    </bug>
    <bug id="102" opendate="2014-12-04 00:00:00" fixdate="2014-12-15 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>SAdd in RedisAsyncConnection uses wrong AddValues
            </summary>
            <description>line 629 uses addMapValues for sadd rather than addValues

                This causes JsonJacksonCodec to fail on Longs stored with the mapObjectMapper to fail to be deparsed by
                objectMapper;

                Change line 629

                CommandArgs#K, V> args = new CommandArgs#K, V>(codec).addKey(key).addMapValues(members);
                CommandArgs#K, V> args = new CommandArgs#K, V>(codec).addKey(key).addValues(members);
                Test code follows :

                import org.junit.Ignore;

                @ignore
                public class SimpleBean {
                private Long lng;

                public Long getLng() {
                return lng;
                }
                }

                @test
                public void saveLong() {
                SimpleBean cdb = new SimpleBean();
                cdb.setLng(5l);

                String name = "IS_DEBUG_SET";

                {
                Redisson redis = RedissonUtil.getRedissonClient();
                RSet
                Object>
                set = redis.getSet(name);

                set.clear();
                }
                {
                Redisson redis = RedissonUtil.getRedissonClient();
                RSet
                SimpleBean>
                set = redis.getSet(name);
                set.add(cdb);
                }
                {
                Redisson redis = RedissonUtil.getRedissonClient();
                RSet
                SimpleBean>
                set = redis.getSet(name);

                assertFalse("set may not be empty", set.isEmpty());
                for (SimpleBean e : set) {
                assertEquals("key must be a long", (Long.class), e.getLng());
                assertEquals("keys must be identical by value", cdb.getLng(), e.getLng());
                }
                }

                }
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">com.lambdaworks.redis.RedisAsyncConnection.java</file>
            <file type="M">com.lambdaworks.redis.output.ValueSetScanOutput.java</file>
            <file type="M">org.redisson.RedissonSetTest.java</file>
        </fixedFiles>
    </bug>
    <bug id="106" opendate="2014-12-17 00:00:00" fixdate="2015-09-04 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>RedissonList's Iterator Race Conditions.
            </summary>
            <description>RedissonList iterator as it tries to keep "up to date" with data has a race condition in which
                if between the .hasNext() and the .next() call the set is emptied the list will throw
                NoSuchElementException.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.redisson.RedissonList.java</file>
        </fixedFiles>
    </bug>
    <bug id="4064" opendate="2022-01-07 00:00:00" fixdate="2022-01-14 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>Thread safe problem when acquiring locks resulting in a pubsub channel not being unsubscribed
            </summary>
            <description>This seems to be related to: #2575
                Expected behavior
                The pub/sub channels that are created while trying to acquire a lock with RedissonLock should be
                unsubscribed when no longer needed.
                Actual behavior
                In some cases when 2 pub sub channels are created around the same time for (2 different lock keys) one
                of the two throws an error:
                Subscribe timeout: (7500ms). Increase 'subscriptionsPerConnection' and/or
                'subscriptionConnectionPoolSize' parameters.
                After this error the failed lock key will lock fine when not locked. If it is however locked by another
                thread it will fail instantly.
                see the logging from the test case below, notice how thread [pool-1-thread-12] successfully locks key 0.
                When thread [pool-1-thread-19] tries to lock lockkey-0 it instantly throws the subscribe timeout
                exception (notice it does not wait 7500ms).
                If there is no locking going on and the app is still running it kept a channel active for lockkey-0
                127.0.0.1:6379> PUBSUB CHANNELS
                1) "redisson_lock__channel:{lock-0}"

                Some logging from the test case when it fails:
                14:34:42.181 [pool-1-thread-27] INFO com.bprocare.atc.ReddisTest - after -unlock lock-0
                14:34:42.181 [pool-1-thread-27] INFO com.bprocare.atc.ReddisTest - before lock lock-0
                114:34:42.182 [pool-1-thread-27] INFO com.bprocare.atc.ReddisTest - after lock lock-0
                14:34:42.182 [pool-1-thread-13] INFO com.bprocare.atc.ReddisTest - after -unlock lock-4
                14:34:42.182 [pool-1-thread-13] INFO com.bprocare.atc.ReddisTest - before lock lock-3
                14:34:42.183 [pool-1-thread-22] INFO com.bprocare.atc.ReddisTest - after lock lock-4
                14:34:42.192 [pool-1-thread-6] INFO com.bprocare.atc.ReddisTest - after -unlock lock-3
                14:34:42.192 [pool-1-thread-24] INFO com.bprocare.atc.ReddisTest - after -unlock lock-1
                14:34:42.192 [pool-1-thread-24] INFO com.bprocare.atc.ReddisTest - before lock lock-4
                14:34:42.192 [pool-1-thread-6] INFO com.bprocare.atc.ReddisTest - before lock lock-3
                14:34:42.192 [pool-1-thread-12] INFO com.bprocare.atc.ReddisTest - after -unlock lock-2
                14:34:42.192 [pool-1-thread-12] INFO com.bprocare.atc.ReddisTest - before lock lock-0
                14:34:42.192 [pool-1-thread-27] INFO com.bprocare.atc.ReddisTest - after -unlock lock-0
                14:34:42.192 [pool-1-thread-27] INFO com.bprocare.atc.ReddisTest - before lock lock-3
                14:34:42.193 [pool-1-thread-31] INFO com.bprocare.atc.ReddisTest - after lock lock-3
                14:34:42.193 [pool-1-thread-19] INFO com.bprocare.atc.ReddisTest - after lock lock-1
                14:34:42.193 [pool-1-thread-4] INFO com.bprocare.atc.ReddisTest - after lock lock-2
                14:34:42.194 [pool-1-thread-12] INFO com.bprocare.atc.ReddisTest - after lock lock-0
                14:34:42.197 [pool-1-thread-31] INFO com.bprocare.atc.ReddisTest - after -unlock lock-3
                14:34:42.197 [pool-1-thread-31] INFO com.bprocare.atc.ReddisTest - before lock lock-1
                14:34:42.198 [pool-1-thread-13] INFO com.bprocare.atc.ReddisTest - after lock lock-3
                14:34:42.206 [pool-1-thread-22] INFO com.bprocare.atc.ReddisTest - after -unlock lock-4
                14:34:42.206 [pool-1-thread-4] INFO com.bprocare.atc.ReddisTest - after -unlock lock-2
                14:34:42.206 [pool-1-thread-22] INFO com.bprocare.atc.ReddisTest - before lock lock-1
                14:34:42.206 [pool-1-thread-4] INFO com.bprocare.atc.ReddisTest - before lock lock-2
                14:34:42.208 [pool-1-thread-20] INFO com.bprocare.atc.ReddisTest - after lock lock-2
                14:34:42.208 [pool-1-thread-16] INFO com.bprocare.atc.ReddisTest - after lock lock-4
                14:34:42.212 [pool-1-thread-19] INFO com.bprocare.atc.ReddisTest - after -unlock lock-1
                14:34:42.212 [pool-1-thread-19] INFO com.bprocare.atc.ReddisTest - before lock lock-0
                14:34:42.213 [pool-1-thread-19] ERROR com.bprocare.atc.ReddisTest - e
                org.redisson.client.RedisTimeoutException: Subscribe timeout: (7500ms). Increase
                'subscriptionsPerConnection' and/or 'subscriptionConnectionPoolSize' parameters.
                at org.redisson.command.CommandAsyncService.syncSubscription(CommandAsyncService.java:88)
                at org.redisson.RedissonLock.lock(RedissonLock.java:107)
                at org.redisson.RedissonLock.lock(RedissonLock.java:69)
                at com.bprocare.atc.ReddisTest.lambda$main$0(ReddisTest.java:33)
                at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
                at java.util.concurrent.FutureTask.run(FutureTask.java:266)
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
                at java.lang.Thread.run(Thread.java:748)

                Steps to reproduce or test case
                This does not always fail, sometimes it runs a million locks without issues sometimes it throws the
                error and puts a channel in a failed state.
                @Slf4j
                public class ReddisTest {

                public static void main(String[] args) throws InterruptedException {
                Config config = new Config();
                config.useSingleServer()
                .setSubscriptionConnectionPoolSize(2)
                .setSubscriptionConnectionMinimumIdleSize(2)
                .setSubscriptionsPerConnection(2)
                .setAddress("redis://127.0.0.1:6379");

                RedissonClient redisson = Redisson.create(config);
                ExecutorService e = Executors.newFixedThreadPool(32);
                Random random = new Random();
                for (int i = 0; i
                20000; i++) {
                e.submit(() -> {
                try {
                String lockKey = "lock-" + random.nextInt(5);
                RLock lock = redisson.getLock(lockKey);
                log.info("before lock {}", lockKey);
                lock.lock();
                log.info("after lock {}", lockKey);
                Thread.sleep(random.nextInt(20));
                lock.unlock();
                log.info("after -unlock {}", lockKey);
                } catch (Exception exception){
                log.error("e", exception);
                }
                });
                }

                e.shutdown();
                e.awaitTermination(10, TimeUnit.MINUTES);
                }
                }

                Redis version
                5.0.7
                Redisson version
                3.16.3
                Redisson configuration
                see testcase
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.redisson.RedissonLockEntry.java</file>
            <file type="M">org.redisson.pubsub.PublishSubscribe.java</file>
            <file type="M">org.redisson.RedissonLockTest.java</file>
            <file type="M">org.redisson.command.CommandAsyncService.java</file>
            <file type="M">org.redisson.pubsub.PublishSubscribeService.java</file>
        </fixedFiles>
    </bug>
    <bug id="891" opendate="2017-05-25 00:00:00" fixdate="2017-10-20 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>RReadWriteLock is incompatible with reentry r#w op
            </summary>
            <description>While using RReadWriteLock, a bug (or not?) confuse me for a long time.

                I described a wrong ops yesterday, and fix it now. Sorry about this.

                There is a sure logic problem if ops like this:

                write lock A (succeed)
                read lock A (succeed)
                read unlock A (succeed)
                write unlock A (not hold by current thread!!!)
                Assuming every unlock op is after a holding check.

                So if there is no holding check and running in a concurrent environment, op4 may throw
                IllegalMonitorException.

                But it's ok as follows:

                write lock A (succeed)
                write lock A again (succeed)
                write unlock A (succeed)
                write unlock A (succeed)
                I found that "read unlock" will delete the whole lock without checking if there is any other write lock.

                Is it a bug, or it just shouldn't do and need to do in another way?
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.redisson.RedissonReadLock.java</file>
            <file type="M">org.redisson.RedissonReadWriteLockTest.java</file>
        </fixedFiles>
    </bug>
    <bug id="2575" opendate="2020-02-07 00:00:00" fixdate="2022-01-07 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>RedissonLock fails to unsubscribe from channel when a lock is acquired
            </summary>
            <description>Expected behavior
                The pub/sub channels that are created while trying to acquire a lock with RedissonLock should be
                unsubscribed when no longer needed.

                Actual behavior
                Some pub/sub channels are still present (meaning that the client did not unsubscribe) even after some
                locks are no longer used (either they expired or were unlocked). These channels are never cleaned up
                (still present after 24h since created). The only workaround to get rid of these is to restart the
                client.
                This was observed because it caused a connection leak. When calling RedissonLock to obtain new locks
                this returned the error : Subscribe timeout: (7500ms). Increase 'subscriptionsPerConnection' and/or
                'subscriptionConnectionPoolSize' parameters. We have increased the limits in order to have a workaround
                for this bug

                Steps to reproduce or test case

                use RedissonLock.java implementation with method lockInterruptibly(long leaseTime, TimeUnit unit) to
                obtain multiple locks simultaneously.
                use lower limits for subscription-connection-pool-size and subscriptions-per-connection to increase the
                probability to see the bug
                wait until this error shows up Subscribe timeout: (7500ms). Increase 'subscriptionsPerConnection' and/or
                'subscriptionConnectionPoolSize' parameters.
                connect to Redis server and fetch all lock keys and all pub-sub channels.
                if number of pub-sub channels > active locks , then the bug was reproduced
                if number of pub-sub channels does not decrease at all long after locks are being created, this means
                that subscriptions have leaked, active long after these are no longer used in RedissonLock
                implementation
                Redis version
                5.0.5

                Redisson version
                3.11.6

                Redisson configuration

                #redisson:client id="redissonClient">
                #redisson:single-server
                address="${redis.url}"
                idle-connection-timeout="10000"
                ping-timeout="1000"
                connect-timeout="10000"
                timeout="5000"
                connection-pool-size="32"
                connection-minimum-idle-size="12"
                subscription-connection-pool-size="64"
                subscriptions-per-connection="25"
                />
                #redisson:client>
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.redisson.RedissonLockEntry.java</file>
            <file type="M">org.redisson.pubsub.PublishSubscribe.java</file>
            <file type="M">org.redisson.RedissonLockTest.java</file>
            <file type="M">org.redisson.command.CommandAsyncService.java</file>
            <file type="M">org.redisson.pubsub.PublishSubscribeService.java</file>
        </fixedFiles>
    </bug>
    <bug id="1268" opendate="2018-01-30 00:00:00" fixdate="2018-03-28 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>org.redisson.RedissonTopic.removeAllListeners got blocked on CountDownLatch.await.
            </summary>
            <description>Hi there,

                We hit an issue in one of our server using Redission as Redis Client.
                We do have a single thread executor handling subscribe/unsubscribe for multiple channels.
                But last week, we found this got blocked on one server . Below is the thread print

                "pool-12-thread-1" #67 prio=5 os_prio=0 tid=0x00007f9f6c9e1000 nid=0x5091 waiting on condition
                [0x00007f9f6a2f6000]
                java.lang.Thread.State: WAITING (parking)
                at sun.misc.Unsafe.park(Native Method)
                - parking to wait for#0x00000006cc5406a8> (a java.util.concurrent.CountDownLatch$Sync)
                at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
                at
                java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
                at
                java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)
                at
                java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
                at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231)
                at org.redisson.pubsub.AsyncSemaphore.acquireUninterruptibly(AsyncSemaphore.java:49)
                at org.redisson.RedissonTopic.removeAllListeners(RedissonTopic.java:89)

                The org.redisson.RedissonTopic.removeAllListeners is unable to respond request, and blocked in the
                acquireUninterruptibly FOREVER. I tried to dig into logs, there was one exception thrown before this in
                CommandAsyncService.syncSubscription line 125 "Subscribe timeout 9500ms". This is possibly related since
                this was the only "Subscribe timeout" message I saw in last 30 days logs and then this issue happened.
                But I still don't find prove on this. Looks like this is a rare case since we just hit once. I tried to
                reproduce locally but with no luck.

                I think the removeAllListeners should give an option to let caller pass in a timeout. But this is also
                not going to resolve the root cause. I am wondering if other people have observed this issue before.
                Please advise if you have any ideas on this, thanks.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.redisson.connection.MasterSlaveConnectionManager.java</file>
            <file type="M">org.redisson.RedissonPatternTopic.java</file>
            <file type="M">org.redisson.RedissonTopic.java</file>
            <file type="M">org.redisson.pubsub.AsyncSemaphore.java</file>
            <file type="M">org.redisson.reactive.RedissonPatternTopicReactive.java</file>
        </fixedFiles>
    </bug>
    <bug id="39" opendate="2014-07-10 00:00:00" fixdate="2014-07-12 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>Locking on Jersey
            </summary>
            <description>I'm trying to use Lock and Unlock on Jersey Resource.

                User makes a POST request
                System LOCK "A"
                System does some stuff
                System UNLOCK "A"
                User makes another POST request
                System LOCK "A"
                System does some stuff
                System UNLOCK "A"

                The system crashes at point 6 (view attachment).
                If i try to make some LOCK-UNLOCK in a while loop it works, but when i make these LOCK-UNLOCK from
                different Threads it does not works.
                WARN [2014-07-10 10:36:15,734] io.netty.channel.DefaultChannelPipeline: An exceptionCaught() event was
                fired, and it reached at the tail of the pipeline. It usually means the last handler in the pipeline did
                not handle the exception.
                ! java.lang.IllegalStateException: complete already: DefaultPromise@3c519764(incomplete)
                ! at io.netty.util.concurrent.DefaultPromise.setSuccess(DefaultPromise.java:406)
                ~[netty-common-4.0.19.Final.jar:4.0.19.Final]
                ! at org.redisson.RedissonLock$1.subscribed(RedissonLock.java:177) ~[redisson-1.1.3.jar:na]
                ! at com.lambdaworks.redis.pubsub.RedisPubSubConnection.channelRead(RedisPubSubConnection.java:132)
                ~[redisson-1.1.3.jar:na]
                ! at
                io.netty.channel.DefaultChannelHandlerContext.invokeChannelRead(DefaultChannelHandlerContext.java:341)
                [netty-transport-4.0.19.Final.jar:4.0.19.Final]
                ! at
                io.netty.channel.DefaultChannelHandlerContext.fireChannelRead(DefaultChannelHandlerContext.java:327)
                [netty-transport-4.0.19.Final.jar:4.0.19.Final]
                ! at com.lambdaworks.redis.pubsub.PubSubCommandHandler.decode(PubSubCommandHandler.java:46)
                [redisson-1.1.3.jar:na]
                ! at com.lambdaworks.redis.protocol.CommandHandler.channelRead(CommandHandler.java:52)
                [redisson-1.1.3.jar:na]
                ! at
                io.netty.channel.DefaultChannelHandlerContext.invokeChannelRead(DefaultChannelHandlerContext.java:341)
                [netty-transport-4.0.19.Final.jar:4.0.19.Final]
                ! at
                io.netty.channel.DefaultChannelHandlerContext.fireChannelRead(DefaultChannelHandlerContext.java:327)
                [netty-transport-4.0.19.Final.jar:4.0.19.Final]
                ! at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)
                [netty-transport-4.0.19.Final.jar:4.0.19.Final]
                ! at
                io.netty.channel.DefaultChannelHandlerContext.invokeChannelRead(DefaultChannelHandlerContext.java:341)
                [netty-transport-4.0.19.Final.jar:4.0.19.Final]
                ! at
                io.netty.channel.DefaultChannelHandlerContext.fireChannelRead(DefaultChannelHandlerContext.java:327)
                [netty-transport-4.0.19.Final.jar:4.0.19.Final]
                ! at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:785)
                [netty-transport-4.0.19.Final.jar:4.0.19.Final]
                ! at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:126)
                [netty-transport-4.0.19.Final.jar:4.0.19.Final]
                ! at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:507)
                [netty-transport-4.0.19.Final.jar:4.0.19.Final]
                ! at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:464)
                [netty-transport-4.0.19.Final.jar:4.0.19.Final]
                ! at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:378)
                [netty-transport-4.0.19.Final.jar:4.0.19.Final]
                ! at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:350)
                [netty-transport-4.0.19.Final.jar:4.0.19.Final]
                ! at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
                [netty-common-4.0.19.Final.jar:4.0.19.Final]
                ! at java.lang.Thread.run(Thread.java:722) [na:1.7.0_10-ea]
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">com.lambdaworks.redis.pubsub.RedisPubSubConnection.java</file>
            <file type="M">org.redisson.RedissonLock.java</file>
            <file type="M">org.redisson.connection.ConnectionManager.java</file>
            <file type="M">org.redisson.connection.MasterSlaveConnectionManager.java</file>
            <file type="M">org.redisson.connection.PubSubConnectionEntry.java</file>
            <file type="M">org.redisson.RedissonLockTest.java</file>
        </fixedFiles>
    </bug>
    <bug id="2883" opendate="2020-07-02 00:00:00" fixdate="2021-07-14 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>RedissonFairLock deadlock
            </summary>
            <description>version:
                spring-boot : 2.2.2
                redisson-spring-boot-starter: 3.12.1
                redis: 4.0.10

                test case

                @GetMapping("/testRedisson")
                public void testRedisson() {
                String s = UUID.randomUUID().toString();
                ExecutorService executorService = new ThreadPoolExecutor(0, Integer.MAX_VALUE,
                1000L, TimeUnit.SECONDS,
                new SynchronousQueue#Runnable>());
                log.info("startLock ");
                IntStream.rangeClosed(0, 500).boxed().forEach(new Consumer#Integer>() {
                @Override
                public void accept(Integer integer) {
                executorService.submit(new Runnable() {
                @Override
                public void run() {

                RLock test = redissonClient.getFairLock(s);
                test.lock(5, TimeUnit.SECONDS);
                ThreadUtil.sleep(200); // 200ms
                log.info("endLok {}", integer);
                test.unlock();
                }
                });
                }
                });
                executorService.shutdown();

                }
                RedissonFairLock result
                Only more than 100 "endLok" were printed。if use RedissonLock，it can be print all。

                redis has key "redisson_lock_queue:{4e6cbfe2-1dc0-4956-bec3-ba9aaf16f63e}" and
                "redisson_lock_timeout:{4e6cbfe2-1dc0-4956-bec3-ba9aaf16f63e}" Blocking program running

                127.0.0.1:6379> LRANGE redisson_lock_queue:{4e6cbfe2-1dc0-4956-bec3-ba9aaf16f63e} 0 10
                1) "19df2c25-acc0-42c1-b1a3-9f6f2f41e401:1123"
                2) "19df2c25-acc0-42c1-b1a3-9f6f2f41e401:1124"
                3) "19df2c25-acc0-42c1-b1a3-9f6f2f41e401:1125"
                4) "19df2c25-acc0-42c1-b1a3-9f6f2f41e401:1126"
                5) "19df2c25-acc0-42c1-b1a3-9f6f2f41e401:1127"
                6) "19df2c25-acc0-42c1-b1a3-9f6f2f41e401:1128"
                7) "19df2c25-acc0-42c1-b1a3-9f6f2f41e401:1129"
                8) "19df2c25-acc0-42c1-b1a3-9f6f2f41e401:1130"
                9) "19df2c25-acc0-42c1-b1a3-9f6f2f41e401:1131"
                10) "19df2c25-acc0-42c1-b1a3-9f6f2f41e401:1132"
                11) "19df2c25-acc0-42c1-b1a3-9f6f2f41e401:1134"
                127.0.0.1:6379> ZRANGE redisson_lock_timeout:{4e6cbfe2-1dc0-4956-bec3-ba9aaf16f63e} 0 258 WITHSCORES
                1) "19df2c25-acc0-42c1-b1a3-9f6f2f41e401:1186"
                2) "1593669091698"
                3) "19df2c25-acc0-42c1-b1a3-9f6f2f41e401:1188"
                4) "1593669096698"
                5) "19df2c25-acc0-42c1-b1a3-9f6f2f41e401:1191"
                6) "1593669101698"
                7) "19df2c25-acc0-42c1-b1a3-9f6f2f41e401:1187"
                8) "1593669106698"
                9) "19df2c25-acc0-42c1-b1a3-9f6f2f41e401:1189"
                10) "1593669111698"
                11) "19df2c25-acc0-42c1-b1a3-9f6f2f41e401:1190"
                12) "1593669116698"
                13) "19df2c25-acc0-42c1-b1a3-9f6f2f41e401:1192"
                14) "1593669121698"
                15) "19df2c25-acc0-42c1-b1a3-9f6f2f41e401:1193"
                16) "1593669126698"
                17) "19df2c25-acc0-42c1-b1a3-9f6f2f41e401:1194"
                18) "1593669131698"
                19) "19df2c25-acc0-42c1-b1a3-9f6f2f41e401:1195"
                20) "1593669136698"
                21) "19df2c25-acc0-42c1-b1a3-9f6f2f41e401:1196"
                22) "1593669141698"
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.redisson.RedissonFairLock.java</file>
            <file type="M">org.redisson.RedissonLock.java</file>
            <file type="M">org.redisson.RedissonReadLock.java</file>
            <file type="M">org.redisson.RedissonFairLockTest.java</file>
        </fixedFiles>
    </bug>
    <bug id="89" opendate="2014-11-03 00:00:00" fixdate="2014-11-18 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>RedissonLock locks up
            </summary>
            <description>Under heavy use on production, Redisson's locks get all locked up, and the application stalls.
                I'm using Redisson 1.1.5
                I have 1 thread locked trying to release a lock:

                Thread 18640: (state = BLOCKED)

                java.lang.Object.wait(long) @bci=0 (Compiled frame; information may be imprecise)
                java.lang.Object.wait(long, int) @bci=58, line=461 (Compiled frame)
                io.netty.util.concurrent.DefaultPromise.await0(long, boolean) @bci=137, line=361 (Compiled frame)
                io.netty.util.concurrent.DefaultPromise.awaitUninterruptibly(long, java.util.concurrent.TimeUnit)
                @bci=7, line=312 (Compiled frame)
                com.lambdaworks.redis.RedisAsyncConnection.await(io.netty.util.concurrent.Future, long,
                java.util.concurrent.TimeUnit) @bci=4, line=1231 (Compiled frame)
                com.lambdaworks.redis.RedisConnection.await(io.netty.util.concurrent.Future) @bci=13, line=864 (Compiled
                frame)
                com.lambdaworks.redis.RedisConnection.multi() @bci=8, line=408 (Compiled frame)
                org.redisson.RedissonLock.unlock(com.lambdaworks.redis.RedisConnection) @bci=8, line=391 (Compiled
                frame)
                org.redisson.RedissonLock.access$300(org.redisson.RedissonLock, com.lambdaworks.redis.RedisConnection)
                @bci=2, line=45 (Compiled frame)
                org.redisson.RedissonLock$4.execute(com.lambdaworks.redis.RedisConnection) @bci=84, line=374 (Compiled
                frame)
                org.redisson.RedissonLock$4.execute(com.lambdaworks.redis.RedisConnection) @bci=2, line=363 (Compiled
                frame)
                org.redisson.connection.MasterSlaveConnectionManager.write(org.redisson.async.SyncOperation, int)
                @bci=7, line=255 (Compiled frame)
                org.redisson.connection.MasterSlaveConnectionManager.write(org.redisson.async.SyncOperation) @bci=3,
                line=248 (Compiled frame)
                org.redisson.RedissonLock.unlock() @bci=12, line=363 (Compiled frame)


                Also of note, I have about 30 other threads locked awaiting for a lock (a different one from the one
                used by the previous thread).
                I checked the threads with jstack, here is the relevant output:

                Thread 15416: (state = BLOCKED)

                java.lang.Object.wait(long) @bci=0 (Compiled frame; information may be imprecise)
                java.lang.Object.wait() @bci=2, line=503 (Interpreted frame)
                io.netty.util.concurrent.DefaultPromise.awaitUninterruptibly() @bci=31, line=292 (Interpreted frame)
                io.netty.util.concurrent.DefaultPromise.awaitUninterruptibly() @bci=1, line=31 (Interpreted frame)
                org.redisson.RedissonLock.lockInterruptibly(long, java.util.concurrent.TimeUnit) @bci=8, line=233
                (Interpreted frame)
                org.redisson.RedissonLock.lockInterruptibly() @bci=5, line=226 (Interpreted frame)
                org.redisson.RedissonLock.lock() @bci=1, line=206 (Interpreted frame)


                I checked Redis's state:

                $ redis-cli
                127.0.0.1:6379> keys *

                ""betCreation""


                this key is the one corresponding to the thread blocked trying to release a lock. The other threads,
                that are waiting for a separate lock are locked even though there is no-one taking up such lock....
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.redisson.RedissonLock.java</file>
            <file type="M">org.redisson.connection.ClusterPartition.java</file>
        </fixedFiles>
    </bug>
    <bug id="758" opendate="2017-01-29 00:00:00" fixdate="2022-07-08 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>RReadWriteLock is not reentrant
            </summary>
            <description>I was expecting the RReadWriteLock to be reentrant, as the wiki page describes it as (emphasis
                mine):

                Redisson distributed reentrant ReadWriteLock object for Java

                However, in my simple test:
                Config config = new Config();
                config.useSingleServer().setAddress(redisAddress);
                this.client = Redisson.create(config);

                RReadWriteLock readWriteLock = this.client.getReadWriteLock("TEST");
                LOGGER.warn("### Acquiring write lock...");
                readWriteLock.writeLock().lock();
                LOGGER.warn("### Write lock acquired, acquiring read lock...");
                try {
                boolean success = readWriteLock.readLock().tryLock(10, TimeUnit.SECONDS);
                LOGGER.warn("### Lock success: " + success);
                } catch (InterruptedException e) {
                e.printStackTrace();
                }

                I see:
                app_1 | 16:24:28.777 [main] WARN com.craigotis.myapp.core.lock.RedisLockService - ### Acquiring write
                lock...
                app_1 | 16:24:28.856 [main] WARN com.craigotis.myapp.core.lock.RedisLockService - ### Write lock
                acquired, acquiring read lock...
                app_1 | 16:24:38.862 [main] WARN com.craigotis.myapp.core.lock.RedisLockService - ### Lock success:
                false

                The "reentrancy" clause of the ReentrantReadWriteLock Javadoc states:

                Additionally, a writer can acquire the read lock, but not vice-versa.

                My use of redisson relies on these locks being reentrant (at least within the same Thread), however this
                does not seem to be the case - is this a bug, or a configuration issue on my end?
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.redisson.RedissonReadLock.java</file>
            <file type="M">org.redisson.RedissonWriteLock.java</file>
            <file type="M">org.redisson.RedissonReadWriteLockTest.java</file>
        </fixedFiles>
    </bug>
    <bug id="1966" opendate="2019-03-08 00:00:00" fixdate="2021-06-02 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>Deadlock after Redis timeout
            </summary>
            <description>Expected behavior
                After a Redis response timeout, Redisson needs to be resilient and back to normal operation after Redis
                timeout and allow to lock and unlock critical sections again.
                Actual behavior
                After a Redis response timeout between attempts, Redisson adds the lock to scheduled renew task and
                never remove them. So, the application enters on a deadlock due to a lock renewed even after unlock.
                The task can run on the same thread due to reentrant lock feature, but when the task runs on another
                thread a deadlock occurs.
                Steps to reproduce or test case

                Start Redis
                Run the test application (https://github.com/hmagarotto/redissonlock) with a simple locked task.
                Force a timeout on Redis runnning a client pause command: "CLIENT PAUSE 5000"

                Redis version
                4.0.10
                Redisson version
                3.10.3
                Redisson configuration
                Single server default
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.redisson.RedissonLock.java</file>
        </fixedFiles>
    </bug>
    <bug id="1433" opendate="2018-05-10 00:00:00" fixdate="2019-02-08 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>Concurrent calls to RemoteService should not result in an exception
            </summary>
            <description>Expected behavior
                Concurrent calls to RemoteService should not result in an exception
                Actual behavior
                Concurrent calls to RemoteService should result in an exception
                Steps to reproduce or test case
                public interface IService {
                Boolean addToThreadUnsafeList(Integer element);

                List
                Integer>
                getList();

                void clear();
                }


                public class ServiceImpl implements IService {
                private List
                Integer>
                list;

                public ServiceImpl() {
                list = new ArrayList();
                }

                public Boolean addToThreadUnsafeList(Integer element) {
                System.out.println(element);
                return list.add(element);
                }

                @Override
                public List
                Integer>
                getList() {
                return list;
                }

                @Override
                public void clear() {
                list.clear();
                }

                }


                private void parallelTestUsingExecutorServiceAndPoolSizeOf(int poolSize) {
                ExecutorService executorService = Executors.newFixedThreadPool(poolSize);
                IService service = redissonClient.getRemoteService().get(IService.class);

                List
                Future
                Boolean>> futures = new ArrayList();

                for (int i = 0; i iterations; i++) {
                final Integer element = i;
                futures.add(executorService.submit(() -> service.addToThreadUnsafeList(element)));
                }

                while (!futures.stream().allMatch(Future::isDone)) {}

                try {
                Thread.sleep(10000);
                } catch (InterruptedException e) {
                throw new RuntimeException(e);
                }
                assertThat(service.getList()).hasSize(iterations);
                }


                Redis version
                4.0.9
                Redisson version
                3.6.5
                Redisson configuration
                config.useSingleServer().setAddress("redis://127.0.0.1:6379");
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.redisson.RedissonRemoteServiceTest.java</file>
            <file type="M">org.redisson.remote.RemoteServiceResponse.java</file>
            <file type="M">org.redisson.BaseRemoteService.java</file>
        </fixedFiles>
    </bug>
    <bug id="2099" opendate="2019-05-09 00:00:00" fixdate="2019-08-27 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>RedissonFairLock timeout drift
            </summary>
            <description>We've discovered additional conditions that produce timeout drift as originally described in
                #1104. To summarize the issue, we see that the timeouts in the redisson_lock_timeout sorted set created
                by RedissonFairLock can gradually increase over time to be hours or days in the future despite the queue
                only containing less than 10 threads; this condition continues until the fair wait queue empties, or one
                of the threads dies and the rest of the threads are forced to wait for the far in the future timeout on
                the dead thread to lapse (this creates a dead lock-like situation).
                I'll provide a PR to follow shortly with test cases and proposed changes to fix the issue. We greatly
                appreciate feed back on any misunderstanding of the issue described here and will do the same for
                feedback on the PR.
                Expected behavior
                The expected behavior is that which is documented in the second paragraph of 8.2 Fair Lock:

                All waiting threads are queued and if some thread has died then Redisson waits its return for 5 seconds.
                For example, if 5 threads are died for some reason then delay will be 25 seconds.

                As an example that replicates this documentation, given we have 7 threads trying to take one lock, we
                expect the redis data structures to contain roughly the following after a 7 threads try to acquire the
                lock once using RLock.tryLock(threadId) in order with short delays between:
                > HGETALL example_fair_lock
                1) lock name of the 1st thread
                2) "1"

                > LRANGE redisson_lock_queue:{example_fair_lock} 0 -1
                1) lock name of the 2nd thread
                2) lock name of the 3rd thread
                3) lock name of the 4th thread
                4) lock name of the 5th thread
                5) lock name of the 6th thread
                6) lock name of the 7th thread

                > ZRANGE redisson_lock_timeout:{example_fair_lock} 0 -1 WITHSCORES
                1) lock name of the 2nd thread
                2) lock timeout + 5s
                3) lock name of the 3rd thread
                4) lock timeout + 10s
                5) lock name of the 4th thread
                6) lock timeout + 15s
                7) lock name of the 5th thread
                8) lock timeout + 20s
                9) lock name of the 6th thread
                10) lock timeout + 25s
                11) lock name of the 7th thread
                12) lock timeout + 30s

                In the above we see that the 7th thread will need to wait 25s after the lock expiration (based on the
                lease time) until it can acquire the lock if the 1st to 6th threads die.
                Additionally, we expect that when a thread leaves the queue voluntarily either by wait timeout or by
                acquiring the lock, that timeouts adjust regardless of where the thread was in the queue. For example,
                if the 3rd thread in the above example left the queue via a RedissonFairLock.acquireFailedAsync call due
                to wait timeout, the sorted set would then be expected to be:
                > ZRANGE redisson_lock_timeout:{example_fair_lock} 0 -1 WITHSCORES
                1) lock name of the 2nd thread
                2) lock timeout + 5s
                5) lock name of the 4th thread
                6) lock timeout + 10s
                7) lock name of the 5th thread
                8) lock timeout + 15s
                9) lock name of the 6th thread
                10) lock timeout + 20s
                11) lock name of the 7th thread
                12) lock timeout + 25s

                And the 7th thread would then only need to wait 20s after the lock expiration to acquire the lock if the
                other threads died.
                Actual behavior
                In one of our usages of redisson we use the fair lock using 3-6 servers using each a single instance of
                Redisson running roughly the following code in one thread per server:
                while (true) {
                RLock lock = redisson.getFairLock("worker_lock");
                if (lock.tryLock(5000, 5000, TimeUnit.MILLISECONDS)) {
                try {
                // do some "work" for 5s
                Thread.sleep(100);
                } finally {
                lock.unlock();
                }
                }
                }

                We observe that since the work done within the locked portion of the code can take only ~100ms that the
                6 server threads running this code quickly churn through the lock queue without issue, though a queue is
                always present since there is little time between when each thread unlocks and locks again. If we take
                periodic snapshots of the redisson_lock_timeout sorted set, we tend to see that the timeouts increase in
                increments of 5s over time until the timeouts are hours or days in the future. If one of the servers is
                killed, then we observe that the other servers stop doing work and are timing out trying to get the lock
                due to the dead server's lock holding the first position in redisson_lock_queue with a timeout in
                redisson_lock_timeout that may be hours or days in the future. We expect that under the case that we
                lose one server of 6, that the timeout values will be in the range of 10 to 35s in the future at any one
                time (5s lease time + 5s thread wait time * position in queue). It may be said that the above lock usage
                itself may be foolish, but its mostly working except for the chance of deadlock.
                Steps to reproduce or test case
                I'll provide a PR for this issue to follow with additional test cases added to RedissonFairLockTest.
                However, what we started with was a modification of the testTimeoutDrift where we changed the wait time
                from 500ms to 3s and changed the lock holding time from 30s to 100ms (see Thread.sleep(30000)); with
                this test case, instead of the tryLock failing due to wait timeout, the threads are able to lock and
                unlock the lock quickly. The new version of the test, the test fails with a timeout drift into the
                futre, in a similar way that the test failed in #1104.
                The PR will contain other test cases with the hope that we cover all code changes and produce the
                expected behavior described above.
                Redis version
                5.0.4
                Redisson version
                3.10.7
                Redisson configuration
                We run a single redis server for use exclusively by redisson
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.redisson.RedissonFairLock.java</file>
            <file type="M">org.redisson.RedissonFairLockTest.java</file>
        </fixedFiles>
    </bug>
    <bug id="2714" opendate="2020-04-21 00:00:00" fixdate="2022-06-15 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>RedissonLock.tryLock() interrupted and keep renew lock
            </summary>
            <description>Expected behavior
                when tryLock is interrupted, watchdog renew listener is cancelled too

                Actual behavior
                when tryLock is interrupted, watchdog keeps renewing lock, this makes ifinite lock

                Steps to reproduce or test case
                private volatile boolean threadTwoScheduled = false;
                @test
                public void lockThenInterruptNotCatchThenBroke() {
                RLock lock = redissonClient.getLock("concurrent-test2");
                Assert.assertFalse(lock.isLocked());

                Thread thread = new Thread(() -> {
                threadTwoScheduled = true;
                if (!lock.tryLock()) {
                return;
                }
                try {
                doBusiness();
                } finally {
                lock.unlock();
                }
                });
                thread.start();
                while(!threadTwoScheduled){}
                // let the tcp request be sent out
                for (int i = 0; i
                1000; i++) {
                new Object();
                }
                thread.interrupt();
                try {
                Thread.sleep(45000L); // longer than default watchdog time
                } catch (InterruptedException e) {
                e.printStackTrace();
                }
                // lock is still being renewed by watchdog
                Assert.assertTrue(lock.isLocked());
                }
                Redis version
                doesn't matter
                Redisson version
                3.11.2
                Redisson configuration
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.redisson.RedissonBaseLock.java</file>
            <file type="M">org.redisson.command.CommandAsyncService.java</file>
            <file type="M">org.redisson.RedissonLockTest.java</file>
        </fixedFiles>
    </bug>
    <bug id="67" opendate="2014-08-15 00:00:00" fixdate="2018-01-30 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>RedissonList concurrency problem.
            </summary>
            <description>How is it ever safe to rely on RedissonList.size() when each invocation can technically return
                a different value? This would mean that calling subList(0, size()), or even two consecutive calls to
                hasNext() on the iterator (which is normally what happens) can potentially cause an exception.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.redisson.RedissonList.java</file>
        </fixedFiles>
    </bug>
    <bug id="199" opendate="2015-07-29 00:00:00" fixdate="2015-08-03 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>Concurrency Issues
            </summary>
            <description>Today I tried some tests to experiment with Redisson's performance using a build from /master
                and discovered significant concurrency issues. I'm not sure what the state of this code is, so perhaps
                these issues are known ones, but wanted to make sure you guys are aware of them.

                I have a very simple app that a) Creates a Redisson instance with a connection pool of 50 to masters and
                50 to slaves, then b) creates a quick thread pool using Executors.newFixedThreadPool() and then c) feeds
                it Runnables, each of which generates a random string and adds it to the end of a Deque.

                Whenever I do any of these three things: 1) Add >1 millions of queued Runnables or 2) Increase the
                number of threads >20 or 3) add a .contains() call on the Deque (more on that in a sec), I get a ton of
                exceptions that don't include any of my code in the stack trace. Here's one such snippet:

                [main] INFO org.redisson.connection.ClusterConnectionManager - master: //127.0.0.1:7000 for slot range:
                0-5460 added
                [main] INFO org.redisson.connection.ClusterConnectionManager - master: //127.0.0.1:7004 for slot range:
                5461-10922 added
                [main] INFO org.redisson.connection.ClusterConnectionManager - master: //127.0.0.1:7005 for slot range:
                10923-16383 added
                [nioEventLoopGroup-2-9] WARN io.netty.channel.DefaultChannelPipeline - An exceptionCaught() event was
                fired, and it reached at the tail of the pipeline. It usually means the last handler in the pipeline did
                not handle the exception.
                io.netty.handler.codec.DecoderException: java.lang.IllegalStateException: complete already:
                DefaultPromise@57a2506d(failure(java.util.concurrent.CancellationException)
                at io.netty.handler.codec.ReplayingDecoder.callDecode(ReplayingDecoder.java:425)
                at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:230)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)
                at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
                at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
                at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
                at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
                at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
                at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
                at
                io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
                at java.lang.Thread.run(Thread.java:745)
                Caused by: java.lang.IllegalStateException: complete already:
                DefaultPromise@57a2506d(failure(java.util.concurrent.CancellationException)
                at io.netty.util.concurrent.DefaultPromise.setSuccess(DefaultPromise.java:400)
                at org.redisson.client.handler.CommandDecoder.handleResult(CommandDecoder.java:245)
                at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:162)
                at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:104)
                at io.netty.handler.codec.ReplayingDecoder.callDecode(ReplayingDecoder.java:370)
                ... 18 more
                [nioEventLoopGroup-2-16] WARN io.netty.channel.DefaultChannelPipeline - An exceptionCaught() event was
                fired, and it reached at the tail of the pipeline. It usually means the last handler in the pipeline did
                not handle the exception.
                io.netty.handler.codec.DecoderException: java.lang.IllegalStateException: complete already:
                DefaultPromise@635d00df(failure(java.util.concurrent.CancellationException)
                at io.netty.handler.codec.ReplayingDecoder.callDecode(ReplayingDecoder.java:425)
                at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:230)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)
                at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
                at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
                at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
                at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
                at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
                at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
                at
                io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
                at java.lang.Thread.run(Thread.java:745)
                Caused by: java.lang.IllegalStateException: complete already:
                DefaultPromise@635d00df(failure(java.util.concurrent.CancellationException)
                at io.netty.util.concurrent.DefaultPromise.setSuccess(DefaultPromise.java:400)
                at org.redisson.client.handler.CommandDecoder.handleResult(CommandDecoder.java:245)
                at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:162)
                at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:104)
                at io.netty.handler.codec.ReplayingDecoder.callDecode(ReplayingDecoder.java:370)
                ... 18 more
                [nioEventLoopGroup-2-5] WARN io.netty.channel.DefaultChannelPipeline - An exceptionCaught() event was
                fired, and it reached at the tail of the pipeline. It usually means the last handler in the pipeline did
                not handle the exception.
                [nioEventLoopGroup-2-11] WARN io.netty.channel.DefaultChannelPipeline - An exceptionCaught() event was
                fired, and it reached at the tail of the pipeline. It usually means the last handler in the pipeline did
                not handle the exception.
                io.netty.handler.codec.DecoderException: java.lang.IllegalStateException: complete already:
                DefaultPromise@469068c(failure(java.util.concurrent.CancellationException)
                at io.netty.handler.codec.ReplayingDecoder.callDecode(ReplayingDecoder.java:425)
                at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:230)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)
                at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
                at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
                at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
                at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
                at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
                at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
                at
                io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
                at java.lang.Thread.run(Thread.java:745)
                Caused by: java.lang.IllegalStateException: complete already:
                DefaultPromise@469068c(failure(java.util.concurrent.CancellationException)
                at io.netty.util.concurrent.DefaultPromise.setSuccess(DefaultPromise.java:400)
                at org.redisson.client.handler.CommandDecoder.handleResult(CommandDecoder.java:245)
                at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:162)
                at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:104)
                at io.netty.handler.codec.ReplayingDecoder.callDecode(ReplayingDecoder.java:370)
                ... 18 more
                [nioEventLoopGroup-2-7] WARN io.netty.channel.DefaultChannelPipeline - An exceptionCaught() event was
                fired, and it reached at the tail of the pipeline. It usually means the last handler in the pipeline did
                not handle the exception.
                [nioEventLoopGroup-2-12] WARN io.netty.channel.DefaultChannelPipeline - An exceptionCaught() event was
                fired, and it reached at the tail of the pipeline. It usually means the last handler in the pipeline did
                not handle the exception.
                [nioEventLoopGroup-2-3] WARN io.netty.channel.DefaultChannelPipeline - An exceptionCaught() event was
                fired, and it reached at the tail of the pipeline. It usually means the last handler in the pipeline did
                not handle the exception.
                io.netty.handler.codec.DecoderException: java.lang.IllegalStateException: complete already:
                DefaultPromise@3722d627(failure(java.util.concurrent.CancellationException)
                at io.netty.handler.codec.ReplayingDecoder.callDecode(ReplayingDecoder.java:425)
                at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:230)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)
                at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
                at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
                at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
                at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
                at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
                at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
                at
                io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
                at java.lang.Thread.run(Thread.java:745)
                Caused by: java.lang.IllegalStateException: complete already:
                DefaultPromise@3722d627(failure(java.util.concurrent.CancellationException)
                at io.netty.util.concurrent.DefaultPromise.setSuccess(DefaultPromise.java:400)
                at org.redisson.client.handler.CommandDecoder.handleResult(CommandDecoder.java:245)
                at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:162)
                at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:104)
                at io.netty.handler.codec.ReplayingDecoder.callDecode(ReplayingDecoder.java:370)
                ... 18 more
                [nioEventLoopGroup-2-1] WARN io.netty.channel.DefaultChannelPipeline - An exceptionCaught() event was
                fired, and it reached at the tail of the pipeline. It usually means the last handler in the pipeline did
                not handle the exception.
                io.netty.handler.codec.DecoderException: java.lang.IllegalStateException: complete already:
                DefaultPromise@4f07f76b(failure(java.util.concurrent.CancellationException)
                at io.netty.handler.codec.ReplayingDecoder.callDecode(ReplayingDecoder.java:425)
                at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:230)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)
                at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
                at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
                at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
                at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
                at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
                at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
                at
                io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
                at java.lang.Thread.run(Thread.java:745)
                Caused by: java.lang.IllegalStateException: complete already:
                DefaultPromise@4f07f76b(failure(java.util.concurrent.CancellationException)
                at io.netty.util.concurrent.DefaultPromise.setSuccess(DefaultPromise.java:400)
                at org.redisson.client.handler.CommandDecoder.handleResult(CommandDecoder.java:245)
                at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:162)
                at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:104)
                at io.netty.handler.codec.ReplayingDecoder.callDecode(ReplayingDecoder.java:370)
                ... 18 more
                io.netty.handler.codec.DecoderException: java.lang.IllegalStateException: complete already:
                DefaultPromise@2fdf959a(failure(java.util.concurrent.CancellationException)
                at io.netty.handler.codec.ReplayingDecoder.callDecode(ReplayingDecoder.java:425)
                at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:230)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)
                at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
                at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
                at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
                at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
                at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
                at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
                at
                io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
                at java.lang.Thread.run(Thread.java:745)
                Caused by: java.lang.IllegalStateException: complete already:
                DefaultPromise@2fdf959a(failure(java.util.concurrent.CancellationException)
                at io.netty.util.concurrent.DefaultPromise.setSuccess(DefaultPromise.java:400)
                at org.redisson.client.handler.CommandDecoder.handleResult(CommandDecoder.java:245)
                at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:162)
                at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:104)
                at io.netty.handler.codec.ReplayingDecoder.callDecode(ReplayingDecoder.java:370)
                ... 18 more
                io.netty.handler.codec.DecoderException: java.lang.IllegalStateException: complete already:
                DefaultPromise@7cdcbd37(failure(java.util.concurrent.CancellationException)
                at io.netty.handler.codec.ReplayingDecoder.callDecode(ReplayingDecoder.java:425)
                at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:230)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)
                at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
                at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
                at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
                at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
                at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
                at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
                at
                io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
                at java.lang.Thread.run(Thread.java:745)
                Caused by: java.lang.IllegalStateException: complete already:
                DefaultPromise@7cdcbd37(failure(java.util.concurrent.CancellationException)
                at io.netty.util.concurrent.DefaultPromise.setSuccess(DefaultPromise.java:400)
                at org.redisson.client.handler.CommandDecoder.handleResult(CommandDecoder.java:245)
                at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:162)
                at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:104)
                at io.netty.handler.codec.ReplayingDecoder.callDecode(ReplayingDecoder.java:370)
                ... 18 more
                io.netty.handler.codec.DecoderException: java.lang.IllegalStateException: complete already:
                DefaultPromise@48ce556d(failure(java.util.concurrent.CancellationException)
                at io.netty.handler.codec.ReplayingDecoder.callDecode(ReplayingDecoder.java:425)
                at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:230)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)
                at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
                at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
                at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
                at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
                at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
                at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
                at
                io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
                at java.lang.Thread.run(Thread.java:745)
                Caused by: java.lang.IllegalStateException: complete already:
                DefaultPromise@48ce556d(failure(java.util.concurrent.CancellationException)
                at io.netty.util.concurrent.DefaultPromise.setSuccess(DefaultPromise.java:400)
                at org.redisson.client.handler.CommandDecoder.handleResult(CommandDecoder.java:245)
                at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:162)
                at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:104)
                at io.netty.handler.codec.ReplayingDecoder.callDecode(ReplayingDecoder.java:370)
                ... 18 more
                [nioEventLoopGroup-2-12] WARN io.netty.channel.DefaultChannelPipeline - An exceptionCaught() event was
                fired, and it reached at the tail of the pipeline. It usually means the last handler in the pipeline did
                not handle the exception.
                io.netty.handler.codec.DecoderException: java.lang.IllegalStateException: complete already:
                DefaultPromise@33a1a4e7(failure(java.util.concurrent.CancellationException)
                at io.netty.handler.codec.ReplayingDecoder.callDecode(ReplayingDecoder.java:425)
                at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:230)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)
                at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
                at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
                at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
                at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
                at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
                at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
                at
                io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
                at java.lang.Thread.run(Thread.java:745)
                Caused by: java.lang.IllegalStateException: complete already:
                DefaultPromise@33a1a4e7(failure(java.util.concurrent.CancellationException)
                at io.netty.util.concurrent.DefaultPromise.setSuccess(DefaultPromise.java:400)
                at org.redisson.client.handler.CommandDecoder.handleResult(CommandDecoder.java:245)
                at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:162)
                at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:104)
                at io.netty.handler.codec.ReplayingDecoder.callDecode(ReplayingDecoder.java:370)
                ... 18 more
                [nioEventLoopGroup-2-13] WARN io.netty.channel.DefaultChannelPipeline - An exceptionCaught() event was
                fired, and it reached at the tail of the pipeline. It usually means the last handler in the pipeline did
                not handle the exception.
                io.netty.handler.codec.DecoderException: java.lang.IllegalStateException: complete already:
                DefaultPromise@10b9a25a(failure(java.util.concurrent.CancellationException)
                at io.netty.handler.codec.ReplayingDecoder.callDecode(ReplayingDecoder.java:425)
                at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:230)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)
                at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
                at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
                at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
                at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
                at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
                at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
                at
                io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
                at java.lang.Thread.run(Thread.java:745)
                Caused by: java.lang.IllegalStateException: complete already:
                DefaultPromise@10b9a25a(failure(java.util.concurrent.CancellationException)
                at io.netty.util.concurrent.DefaultPromise.setSuccess(DefaultPromise.java:400)
                at org.redisson.client.handler.CommandDecoder.handleResult(CommandDecoder.java:245)
                at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:162)
                at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:104)
                at io.netty.handler.codec.ReplayingDecoder.callDecode(ReplayingDecoder.java:370)
                ... 18 more
                [nioEventLoopGroup-2-10] WARN io.netty.channel.DefaultChannelPipeline - An exceptionCaught() event was
                fired, and it reached at the tail of the pipeline. It usually means the last handler in the pipeline did
                not handle the exception.
                io.netty.handler.codec.DecoderException: java.lang.IllegalStateException: complete already:
                DefaultPromise@12693591(failure(java.util.concurrent.CancellationException)
                at io.netty.handler.codec.ReplayingDecoder.callDecode(ReplayingDecoder.java:425)
                at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:230)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)
                at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
                at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
                at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
                at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
                at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
                at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
                at
                io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
                at java.lang.Thread.run(Thread.java:745)
                Caused by: java.lang.IllegalStateException: complete already:
                DefaultPromise@12693591(failure(java.util.concurrent.CancellationException)
                at io.netty.util.concurrent.DefaultPromise.setSuccess(DefaultPromise.java:400)
                at org.redisson.client.handler.CommandDecoder.handleResult(CommandDecoder.java:245)
                at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:162)
                at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:104)
                at io.netty.handler.codec.ReplayingDecoder.callDecode(ReplayingDecoder.java:370)
                ... 18 more
                [nioEventLoopGroup-2-10] WARN io.netty.channel.DefaultChannelPipeline - An exceptionCaught() event was
                fired, and it reached at the tail of the pipeline. It usually means the last handler in the pipeline did
                not handle the exception.
                io.netty.handler.codec.DecoderException: java.lang.IllegalStateException: complete already:
                DefaultPromise@a7eb2c2(failure(java.util.concurrent.CancellationException)
                at io.netty.handler.codec.ReplayingDecoder.callDecode(ReplayingDecoder.java:425)
                at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:230)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)
                at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
                at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
                at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
                at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
                at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
                at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
                at
                io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
                at java.lang.Thread.run(Thread.java:745)
                Caused by: java.lang.IllegalStateException: complete already:
                DefaultPromise@a7eb2c2(failure(java.util.concurrent.CancellationException)
                at io.netty.util.concurrent.DefaultPromise.setSuccess(DefaultPromise.java:400)
                at org.redisson.client.handler.CommandDecoder.handleResult(CommandDecoder.java:245)
                at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:162)
                at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:104)
                at io.netty.handler.codec.ReplayingDecoder.callDecode(ReplayingDecoder.java:370)
                ... 18 more
                [nioEventLoopGroup-2-10] WARN io.netty.channel.DefaultChannelPipeline - An exceptionCaught() event was
                fired, and it reached at the tail of the pipeline. It usually means the last handler in the pipeline did
                not handle the exception.
                io.netty.handler.codec.DecoderException: java.lang.IllegalStateException: complete already:
                DefaultPromise@71278849(failure(java.util.concurrent.CancellationException)
                at io.netty.handler.codec.ReplayingDecoder.callDecode(ReplayingDecoder.java:425)
                at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:230)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)
                at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
                at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
                at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
                at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
                at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
                at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
                at
                io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
                at java.lang.Thread.run(Thread.java:745)
                Caused by: java.lang.IllegalStateException: complete already:
                DefaultPromise@71278849(failure(java.util.concurrent.CancellationException)
                at io.netty.util.concurrent.DefaultPromise.setSuccess(DefaultPromise.java:400)
                at org.redisson.client.handler.CommandDecoder.handleResult(CommandDecoder.java:245)
                at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:162)
                at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:104)
                at io.netty.handler.codec.ReplayingDecoder.callDecode(ReplayingDecoder.java:370)
                ... 18 more
                [nioEventLoopGroup-2-8] WARN io.netty.channel.DefaultChannelPipeline - An exceptionCaught() event was
                fired, and it reached at the tail of the pipeline. It usually means the last handler in the pipeline did
                not handle the exception.
                io.netty.handler.codec.DecoderException: java.lang.IllegalStateException: complete already:
                DefaultPromise@6e7c07e9(failure(java.util.concurrent.CancellationException)
                at io.netty.handler.codec.ReplayingDecoder.callDecode(ReplayingDecoder.java:425)
                at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:230)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)
                at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
                at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
                at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
                at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
                at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
                at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
                at
                io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
                at java.lang.Thread.run(Thread.java:745)
                Caused by: java.lang.IllegalStateException: complete already:
                DefaultPromise@6e7c07e9(failure(java.util.concurrent.CancellationException)
                at io.netty.util.concurrent.DefaultPromise.setSuccess(DefaultPromise.java:400)
                at org.redisson.client.handler.CommandDecoder.handleResult(CommandDecoder.java:245)
                at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:162)
                at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:104)
                at io.netty.handler.codec.ReplayingDecoder.callDecode(ReplayingDecoder.java:370)
                ... 18 more
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.redisson.RedissonList.java</file>
            <file type="M">org.redisson.client.handler.CommandDecoder.java</file>
            <file type="M">org.redisson.connection.ClusterConnectionManager.java</file>
        </fixedFiles>
    </bug>
    <bug id="1626" opendate="2018-09-06 00:00:00" fixdate="2018-09-13 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>Deadlock with RedissonLock used by JCache.
            </summary>
            <description>We've ran into a number of issues with using the Redisson implementation of JCache via bucket4j
                in production for rate limiting.

                Actual behavior

                JCache.put(K key, V value) calls RedissonLock.lock() with no lease time, resulting in RedissonLock's
                creating a 'watchdog' thread that continuously renews the lease on the lock in Redis. We are using the
                default 'lockWatchdogTimeout' of 30 seconds so this thread runs every 10 seconds. What we've seen is in
                rare cases this watchdog thread never gets canceled, meaning it continues renewing the lease
                indefinitely until the instance containing the thread is restarted. This causes deadlock of all other
                threads trying to grab the lock and can ultimately bring down an application as threads build up.

                Threads using RedissonLock will wait forever for a lock to be released. There is no timeout when waiting
                for the lock.

                public void lockInterruptibly(long leaseTime, TimeUnit unit) throws InterruptedException {
                ...
                while (true) {
                ttl = tryAcquire(leaseTime, unit, threadId);
                // lock acquired
                if (ttl == null) {
                break;
                }

                // waiting for message
                if (ttl >= 0) {
                getEntry(threadId).getLatch().tryAcquire(ttl, TimeUnit.MILLISECONDS);
                } else {
                getEntry(threadId).getLatch().acquire();
                }
                }
                The 'unlock' method in RedissonLock can fail for a number of reasons -- e.g., timing out trying to
                unlock with Redis after 3 seconds (this happened in our case). When the unlock fails the watchdog thread
                is not canceled. This means the instance would have to be restarted to release the lock.
                public RFuture
                Void>
                unlockAsync(final long threadId) {
                final RPromise
                Void>
                result = newPromise();
                RFuture
                Boolean>
                future = unlockInnerAsync(threadId);

                future.addListener(new FutureListener#Boolean>() {
                @Override
                public void operationComplete(Future
                #Boolean>
                future) throws Exception {
                if (!future.isSuccess()) {
                result.tryFailure(future.cause());
                return;
                }

                Boolean opStatus = future.getNow();
                if (opStatus == null) {
                IllegalMonitorStateException cause = new IllegalMonitorStateException("attempt to unlock lock, not
                locked by current thread by node id: "
                + id + " thread-id: " + threadId);
                result.tryFailure(cause);
                return;
                }
                if (opStatus) {
                cancelExpirationRenewal();
                }
                result.trySuccess(null);
                }
                });

                return result;
                Expected behavior

                A 'watchdog' thread should not exist. This is prone to issues where it never gets destroyed and a lock
                is held forever. A lease time should be used to ensure the lock is never held forever and somehow it
                should be verified that the lock is still held by the caller when making updates with it (an atomic
                check at update time) in case the first thread doesn't execute its update within the lease time and
                another thread grabs the lock.

                Threads should not wait forever for the lock. There should be some configurable timeout.

                A timeout on 'unlock' should be retried, but the lease time in 1 should handle any failure to unlock. If
                for some reason the 'watchdog' thread is kept, the unlock needs to ensure it is canceled if it fails.

                Steps to reproduce or test case

                Issues verified via redisson code review.

                Can reproduce by forcing a timeout in redis on the 'unlock' call made by RedissonLock, leaving the
                'watchdog' thread around to keep holding the lock and observing that all other threads wait forever for
                the lock.

                Redis version

                Redis 3.2.4 via AWS ElastiCache

                Redisson version

                Redisson 3.5.4 but the same issues appear to be present in 3.8.0 as well

                Redisson configuration

                final Config redissonConfig = new Config();
                redissonConfig.setCodec( new SerializationCodec() );
                redissonConfig.setUseLinuxNativeEpoll( false );
                redissonConfig.useReplicatedServers()
                .addNodeAddress( redisNodes );
                Stacktraces

                Timeout on 'unlock':
                time 01:16:21.500

                javax.cache.processor.EntryProcessorException: org.redisson.client.RedisTimeoutException: Redis server
                response timeout (3000 ms) occured for command: (EVAL) with params:
                [if (redis.call('exists', KEYS[1]) == 0) then redis.call('publish', KEYS[2], ARGV[1]); return 1;
                end;..., 2, {buckets}:SiBJapgTpcxaB*******Sw:key,
                redisson_lock__channel:{buckets}:SiBJapgTpcxa*******Sw:key, 0, 30000,
                a6688b3a-07dc-4347-bd8c-060a3a86b32f:174] channel: [id: *****, L:/**********:******* -
                R:******.cache.amazonaws.com/*********:******]
                at org.redisson.jcache.JCache.invoke(JCache.java:2210) ~[redisson-3.5.4.jar!/:?]
                at io.github.bucket4j.grid.jcache.JCacheProxy.execute(JCacheProxy.java:39)
                ~[bucket4j-jcache-2.1.0.jar!/:?]
                at io.github.bucket4j.grid.GridBucket.execute(GridBucket.java:108) ~[bucket4j-core-2.1.0.jar!/:?]
                at io.github.bucket4j.grid.GridBucket.tryConsumeAndReturnRemainingTokensImpl(GridBucket.java:67)
                ~[bucket4j-core-2.1.0.jar!/:?]
                at io.github.bucket4j.AbstractBucket.tryConsumeAndReturnRemaining(AbstractBucket.java:113)
                ~[bucket4j-core-2.1.0.jar!/:?]
                ...
                Caused by: org.redisson.client.RedisTimeoutException: Redis server response timeout (3000 ms) occured
                for command: (EVAL) with params:
                [if (redis.call('exists', KEYS[1]) == 0) then redis.call('publish', KEYS[2], ARGV[1]); return 1;
                end;..., 2, {buckets}:SiBJapgTpcxaB************:key,
                redisson_lock__channel:{buckets}:SiBJapgTpcx***********qSw:key, 0, 30000,
                a6688b3a-07dc-4347-bd8c-060a3a86b32f:174] channel: [id:********8, L:/*****:*******-
                R:***********.cache.amazonaws.com/*****:****]
                at org.redisson.command.CommandAsyncService$11.run(CommandAsyncService.java:698)
                ~[redisson-3.5.4.jar!/:?]
                at io.netty.util.HashedWheelTimer$HashedWheelTimeout.expire(HashedWheelTimer.java:663)
                ~[netty-common-4.1.15.Final.jar!/:4.1.15.Final]
                at io.netty.util.HashedWheelTimer$HashedWheelBucket.expireTimeouts(HashedWheelTimer.java:738)
                ~[netty-common-4.1.15.Final.jar!/:4.1.15.Final]
                at io.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:466)
                ~[netty-common-4.1.15.Final.jar!/:4.1.15.Final]
                ... 1 more
                Subsequent 'watchdog' thread timing out, actually saving us in this case because when the 'watchdog'
                thread fails it doesn't reschedule itself and the lock got released:
                time 01:16:31.300

                Can't update lock {buckets}:SiBJapgTpcxaBHBLc5ZqSw:key expiration
                org.redisson.client.RedisTimeoutException: Redis server response timeout (3000 ms) occured for command:
                (EVAL) with params:
                [if (redis.call('hexists', KEYS[1], ARGV[2]) == 1) then redis.call('pexpire', KEYS[1], ARGV[1]);
                retu..., 1, {buckets}:SiBJapgTpcxaB*******qSw:key, 30000,
                a6688b3a-07dc-4347-bd8c-060a3a86b32f:174] channel: [id: ******, L:/*********:******* -
                R:***********.cache.amazonaws.com/*******:*****]
                at org.redisson.command.CommandAsyncService$11.run(CommandAsyncService.java:698)
                [redisson-3.5.4.jar!/:?]
                at io.netty.util.HashedWheelTimer$HashedWheelTimeout.expire(HashedWheelTimer.java:663)
                [netty-common-4.1.15.Final.jar!/:4.1.15.Final]
                at io.netty.util.HashedWheelTimer$HashedWheelBucket.expireTimeouts(HashedWheelTimer.java:738)
                [netty-common-4.1.15.Final.jar!/:4.1.15.Final]
                at io.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:466)
                [netty-common-4.1.15.Final.jar!/:4.1.15.Final]
                at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.redisson.RedissonFairLock.java</file>
            <file type="M">org.redisson.RedissonLock.java</file>
            <file type="M">org.redisson.RedissonReadLock.java</file>
            <file type="M">org.redisson.RedissonWriteLock.java</file>
        </fixedFiles>
    </bug>
    <bug id="775" opendate="2017-02-17 00:00:00" fixdate="2017-06-09 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>Indefinite lock lost during master failover
            </summary>
            <description>I've found my indefinitely held locks will sometimes disappear after a master/slave failover.
                The block here will not reschedule a renewal if the attempt of the update fails.
                The update can fail during a failover, in which case the lock is gone for good.
                The exception that gets thrown when the update fails:
                org.redisson.RedissonLock: Can't update lock {chaos:pixel-gun}:/2rQnzwnEeOcovlp1fBZCw:key expiration
                org.redisson.client.RedisTimeoutException: Redis server response timeout (3000 ms) occured for command:
                (EVAL) with params: [if (redis.call('hexists', KEYS[1], ARGV[2]) == 1) then redis.call('pexpire',
                KEYS[1], ARGV[1]); retu..., 1, {chaos:pixel-gun}:/2rQnzwnEeOcovlp1fBZCw:key, 30000,
                3edaecd7-8b5a-4c47-bdc6-fff775044369:82] channel: [id: 0x48ee8af7, 0.0.0.0/0.0.0.0:56264]
                at org.redisson.command.CommandAsyncService$10.run(CommandAsyncService.java:647)
                ~[redisson-3.2.3.jar:na]
                at io.netty.util.HashedWheelTimer$HashedWheelTimeout.expire(HashedWheelTimer.java:581)
                [netty-all-4.0.23.Final.jar:4.0.23.Final]
                at io.netty.util.HashedWheelTimer$HashedWheelBucket.expireTimeouts(HashedWheelTimer.java:656)
                [netty-all-4.0.23.Final.jar:4.0.23.Final]
                at io.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:367)
                [netty-all-4.0.23.Final.jar:4.0.23.Final]
                at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.redisson.RedissonLock.java</file>
            <file type="M">org.redisson.config.Config.java</file>
            <file type="M">org.redisson.RedissonFairLockTest.java</file>
            <file type="M">org.redisson.RedissonLockTest.java</file>
            <file type="M">org.redisson.RedissonReadWriteLockTest.java</file>
        </fixedFiles>
    </bug>
    <bug id="1950" opendate="2019-03-04 00:00:00" fixdate="2019-03-18 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>PriorityBlockingQueue doesn't release lock after reconnect on network
            </summary>
            <description>15:35:12.756 [redisson-netty-1-1] ERROR org.redisson.client.handler.CommandDecoder - Unable to
                decode data. reply: $-1
                , channel: [id: 0xf819abf5, L:/10.2.232.212:60019 - R:10.2.145.113/10.2.145.113:6379], command:
                CommandData [promise=RedissonPromise [promise=ImmediateEventExecutor$ImmediatePromise@7078d41d(failure:
                java.util.concurrent.CancellationException)], command=(EVAL), params=[if (redis.call('exists', KEYS[1])
                == 0) then redis.call('hset', KEYS[1], ARGV[2], 1); redis.call('pe..., 1,
                redisson_sortedset_lock:{testqueue}, 30000, c6d227cd-fb96-4350-b26f-9f47f2bd029c:19],
                codec=org.redisson.client.codec.LongCodec]
                java.util.concurrent.CancellationException
                at java.util.concurrent.CompletableFuture.cancel(CompletableFuture.java:2263)
                at org.redisson.misc.RedissonPromise.cancel(RedissonPromise.java:239)
                at org.redisson.command.CommandAsyncService$13.run(CommandAsyncService.java:941)
                at io.netty.util.HashedWheelTimer$HashedWheelTimeout.expire(HashedWheelTimer.java:682)
                at io.netty.util.HashedWheelTimer$HashedWheelBucket.expireTimeouts(HashedWheelTimer.java:757)
                at io.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:485)
                at java.lang.Thread.run(Thread.java:745)
                15:39:03.936 [redisson-netty-1-6] ERROR org.redisson.client.handler.CommandDecoder - Unable to decode
                data. reply: :1
                , channel: [id: 0x25aa8d2b, L:/10.2.232.212:60596 - R:10.2.145.113/10.2.145.113:6379], command:
                CommandData [promise=RedissonPromise [promise=ImmediateEventExecutor$ImmediatePromise@7e62623(failure:
                java.util.concurrent.CancellationException)], command=(EVAL), params=[if (redis.call('hexists', KEYS[1],
                ARGV[2]) == 1) then redis.call('pexpire', KEYS[1], ARGV[1]); retu..., 1,
                redisson_sortedset_lock:{testqueue}, 30000, c6d227cd-fb96-4350-b26f-9f47f2bd029c:19],
                codec=org.redisson.client.codec.LongCodec]
                java.util.concurrent.CancellationException
                at java.util.concurrent.CompletableFuture.cancel(CompletableFuture.java:2263)
                at org.redisson.misc.RedissonPromise.cancel(RedissonPromise.java:239)
                at org.redisson.command.CommandAsyncService$13.run(CommandAsyncService.java:941)
                at io.netty.util.HashedWheelTimer$HashedWheelTimeout.expire(HashedWheelTimer.java:682)
                at io.netty.util.HashedWheelTimer$HashedWheelBucket.expireTimeouts(HashedWheelTimer.java:757)
                at io.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:485)
                at java.lang.Thread.run(Thread.java:745)
                Expected behavior
                reconnect to network
                Actual behavior
                there is a lock name redisson_sortedset_lock:{testqueue} which dosen't release,it seem's like it can't
                release lock cause network boke,however it get a lock before network boke.
                Steps to reproduce or test case
                1.run code blow
                public class TestReleaseLock {
                public static void main(String[] args) {
                Config config = new Config();
                MasterSlaveServersConfig serverConfig = config.useMasterSlaveServers()
                .setTimeout(1000000)
                .setMasterAddress("redis://10.2.145.113:6379")
                .addSlaveAddress("redis://10.2.145.113:6379");

                serverConfig.setTimeout(100);
                serverConfig.setRetryAttempts(3);
                serverConfig.setRetryInterval(1000);
                serverConfig.setPingConnectionInterval(10000);
                serverConfig.setMasterConnectionMinimumIdleSize(5);
                serverConfig.setKeepAlive(true);

                RedissonClient redissonClient = Redisson.create(config);

                ExecutorService pullService = Executors.newFixedThreadPool(1);
                pullService.execute(new Runnable() {
                RPriorityBlockingQueue
                String>
                blockingQueue = redissonClient.getPriorityBlockingQueue("testqueue");

                @Override
                public void run() {
                while (true) {
                try{
                System.out.println("waiting...");
                String trunk = blockingQueue.take();
                Thread.sleep(1000*2);
                System.out.println(trunk);
                } catch (Exception e) {
                e.printStackTrace();
                }
                }
                }
                });
                }

                }
                2.disconnect your network
                3.wait a few minute,then reconnect your network
                Redis version
                3.2.3
                Redisson version
                3.10.1
                Redisson configuration
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.redisson.RedissonLock.java</file>
        </fixedFiles>
    </bug>
    <bug id="1602" opendate="2018-08-24 00:00:00" fixdate="2018-09-08 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>unlock fail and expirationRenewal still execute
            </summary>
            <description>Expected behavior
                unlock success
                Actual behavior
                unlock fail
                Steps to reproduce or test case
                unlock ()
                client -> redis service network problem
                unlock throw Exception
                network recovery
                expirationRenewal still execute
                Redis version
                Redisson version
                2.11.5
                Redisson configuration
                redisson.connectionPoolSize=5
                redisson.connectionMinimumIdleSize=2
                redisson.address=
                redisson.database=3
                redisson.password=
                redisson.connectTimeout=3000
                redisson.timeout=3000
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.redisson.RedissonFairLock.java</file>
            <file type="M">org.redisson.RedissonLock.java</file>
            <file type="M">org.redisson.RedissonReadLock.java</file>
            <file type="M">org.redisson.RedissonWriteLock.java</file>
        </fixedFiles>
    </bug>
    <bug id="2690" opendate="2020-04-13 00:00:00" fixdate="2020-04-14 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>RedissonSessionRepository topic listeners initialization has race condition
            </summary>
            <description>We’ve successfully migrated to using RedissonSessionRepository for our Spring sessions, but we
                get intermittent NPEs right after startup. It looks like the issue is that the first listener is added
                before all of the topics are initialized, and the conditional matching in onMessage doesn’t go in the
                same order as initialization.
                java.lang.NullPointerException
                at org.redisson.spring.session.RedissonSessionRepository.onMessage(RedissonSessionRepository.java:303)
                at org.redisson.spring.session.RedissonSessionRepository.onMessage(RedissonSessionRepository.java:58)
                at org.redisson.PubSubPatternMessageListener.onPatternMessage(PubSubPatternMessageListener.java:83)
                at org.redisson.client.RedisPubSubConnection.onMessage(RedisPubSubConnection.java:84)
                at
                org.redisson.client.handler.CommandPubSubDecoder.lambda$enqueueMessage$0(CommandPubSubDecoder.java:181)
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
                at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
                at java.lang.Thread.run(Thread.java:748)
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.redisson.spring.session.RedissonSessionRepository.java</file>
        </fixedFiles>
    </bug>
    <bug id="828" opendate="2017-03-30 00:00:00" fixdate="2017-04-20 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>URIBuilder seems not to be thread safe
            </summary>
            <description>We test this in case of a multi cluster setup and sometimes went the java.net.URL.factory to
                null instead of the original factory.

                Scenario:
                Thread 1
                replaceURLFactory
                currentFactory is original value
                start create URL

                Thread 2
                replaceURLFactory
                currentFactory == newFactory -> currentFactory = null
                start create URL

                Thread 1 and 2
                restoreURLFactory
                set URL.factory to null

                Next “new URL(…)” from different call cause to load new handlers.

                Best,
                ebersb
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.redisson.misc.URLBuilder.java</file>
        </fixedFiles>
    </bug>
    <bug id="40" opendate="2014-07-10 00:00:00" fixdate="2014-07-16 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>Lock TTL.
            </summary>
            <description>Any plan to add TTL to a Lock operation? Don't confuse with tryLock with TIME.
                I refer to a situation where a thread is dead and leave a resource locked (deadlock).
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">com.lambdaworks.redis.RedisAsyncConnection.java</file>
            <file type="M">com.lambdaworks.redis.RedisConnection.java</file>
            <file type="M">com.lambdaworks.redis.protocol.CommandType.java</file>
            <file type="M">org.redisson.RedissonLock.java</file>
            <file type="M">org.redisson.core.RLock.java</file>
            <file type="M">org.redisson.RedissonLockTest.java</file>
        </fixedFiles>
    </bug>
    <bug id="83" opendate="2014-10-16 00:00:00" fixdate="2014-11-18 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>Deadlock while obtaining lock
            </summary>
            <description>Thread gets stuck while obtaining lock. Running 'keys L*' in redis-cli returns an empty list.
                INFO | jvm 1 | 2014/10/16 17:41:57 | "http-8080-exec-156" daemon prio=10 tid=0x00000000022ab800
                nid=0x6535 in Object.wait() [0x00007f5799130000]
                INFO | jvm 1 | 2014/10/16 17:41:57 | java.lang.Thread.State: WAITING (on object monitor)
                INFO | jvm 1 | 2014/10/16 17:41:57 | at java.lang.Object.wait(Native Method)
                INFO | jvm 1 | 2014/10/16 17:41:57 | at java.lang.Object.wait(Object.java:503)
                INFO | jvm 1 | 2014/10/16 17:41:57 | at
                io.netty.util.concurrent.DefaultPromise.awaitUninterruptibly(DefaultPromise.java:286)
                INFO | jvm 1 | 2014/10/16 17:41:57 | - locked#0x00000006793b7dc0> (a
                io.netty.util.concurrent.DefaultPromise)
                INFO | jvm 1 | 2014/10/16 17:41:57 | at
                io.netty.util.concurrent.DefaultPromise.awaitUninterruptibly(DefaultPromise.java:32)
                INFO | jvm 1 | 2014/10/16 17:41:57 | at
                org.redisson.RedissonLock.lockInterruptibly(RedissonLock.java:233)
                INFO | jvm 1 | 2014/10/16 17:41:57 | at
                org.redisson.RedissonLock.lockInterruptibly(RedissonLock.java:226)
                INFO | jvm 1 | 2014/10/16 17:41:57 | at org.redisson.RedissonLock.lock(RedissonLock.java:206)

                The same lock name was likely concurrently obtained and held by another thread possibly on another jvm,
                and then released.
                Redisson version used: 1.1.5
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.redisson.RedissonLock.java</file>
        </fixedFiles>
    </bug>
    <bug id="169" opendate="2015-05-31 00:00:00" fixdate="2015-06-10 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>ConnectionManager call hangs forever if exception is thrown during Command processing
            </summary>
            <description>Bug found that can cause MasterSlaveConnectionManager to hang forever on get() call if
                exception is thrown anywhere in CommandHandler.
                To replicate the bug, you can use RedissonMap with JsonJacksonCodec to put instance of class that
                doesn't have default constructor. When you try to fetch that object by using RedissonMap.get() call,
                deserialization of object will fail in MapOutput because of missing appropriate constructor and thread
                calling RedissonMap.get() will block forever.
                In more details, this is happening because get() method awaits forever on Future object, which is
                released when Command.complete() is called. This complete() call is executed in decode() method of
                CommandHandler after RedisStateMachine processes Redis response. If, for example, RedisStateMachine
                throws an exception, complete() won't be called and result/exception will never be set to the Future
                object. This is causing calling thread to block forever in MasterSlaveConnectionManager.get() method.
                Pull request with test case that is proving this bug and bug fix proposition will be published shortly.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">com.lambdaworks.redis.protocol.Command.java</file>
            <file type="M">com.lambdaworks.redis.protocol.CommandOutput.java</file>
            <file type="M">com.lambdaworks.redis.protocol.RedisStateMachine.java</file>
            <file type="M">org.redisson.connection.MasterSlaveConnectionManager.java</file>
            <file type="M">org.redisson.RedissonMapTest.java</file>
        </fixedFiles>
    </bug>
    <bug id="2692" opendate="2020-04-13 00:00:00" fixdate="2020-04-15 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>RedissonSession#changeSessionId expiredKey race condition
            </summary>
            <description>We have a number of instances of our app in production, and we’ve encountered a race condition
                where the same session might have changeSessionId invoked on the same session separate servers. This
                means one server might receive -2 from PTTL, which isn’t currently handled.

                Stacktrace
                org.redisson.client.RedisException: ERR invalid expire time in psetex. channel: [id: 0xf8f93a9c,
                L:/[redacted]:53574 - R:[redacted]:6379] command: (PSETEX), params:
                [spring:session:sessions:expires:[session id], -2, PooledUnsafeDirectByteBuf(ridx: 0, widx: 2, cap:
                256)]
                at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:355)
                at org.redisson.client.handler.CommandDecoder.decodeCommandBatch(CommandDecoder.java:266)
                at org.redisson.client.handler.CommandDecoder.decodeCommand(CommandDecoder.java:207)
                at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:134)
                at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:104)
                at
                io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:498)
                at io.netty.handler.codec.ReplayingDecoder.callDecode(ReplayingDecoder.java:366)
                at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:276)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
                at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1486)
                at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1235)
                at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1282)
                at
                io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:498)
                at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:437)
                at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:276)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
                at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
                at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
                at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
                at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
                at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
                at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
                at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
                at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
                at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
                at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
                at java.lang.Thread.run(Thread.java:748)
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.redisson.spring.session.RedissonSessionRepository.java</file>
        </fixedFiles>
    </bug>
    <bug id="2278" opendate="2019-08-27 00:00:00" fixdate="2019-10-29 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>RedissonFairLock deadlock.
            </summary>
            <description>Seems related to the change on #2099.
                We have tried to upgrade to 3.11.2 from 3.10.7 and this behavior started occurring.

                Expected behavior

                The FairLock acquired on high load system without lease time defined.

                Actual behavior

                Redisson does not handle the case of Fair lock without lease time (on high load).
                when many different threads are asking for the same lock key the client is stuck and no thread acquires
                the lock.

                Steps to reproduce or test case

                `

                ExecutorService executorService = new ThreadPoolExecutor(0, Integer.MAX_VALUE,
                1000L, TimeUnit.MILLISECONDS,
                new SynchronousQueue#Runnable>());
                RedissonClient client = Redisson.create();
                IntStream.rangeClosed(0, 300).boxed().forEach(new Consumer#Integer>() {
                @Override
                public void accept(Integer integer) {
                executorService.submit(new Runnable() {
                @Override
                public void run() {
                ConnectionManager connectionManager = Redisson.class.cast(client).getConnectionManager();
                try {
                Field field = MasterSlaveConnectionManager.class.getDeclaredField("id");
                field.setAccessible(true);
                field.set(connectionManager, UUID.randomUUID());
                } catch (NoSuchFieldException | IllegalAccessException e) {
                e.printStackTrace();
                }
                RLock lock1 = client.getFairLock("fair:lock");
                try {
                lock1.lock();
                Thread.sleep(50);
                } catch (InterruptedException e) {
                e.printStackTrace();
                } finally {
                lock1.unlock();
                System.out.println("done = " + integer);
                }
                }
                });
                }
                });
                Thread.sleep(150000);`
                Redis version

                3.27 and 2.8

                Redisson version

                3.11.2

                Redisson configuration

                defaults
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.redisson.RedissonFairLock.java</file>
            <file type="M">org.redisson.RedissonFairLockTest.java</file>
        </fixedFiles>
    </bug>
    <bug id="4033" opendate="2021-12-19 00:00:00" fixdate="2022-04-05 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>multilock fail
            </summary>
            <description>Expected behavior
                lock successful
                Actual behavior
                lock fail when get first lock in multi lock case
                Steps to reproduce or test case
                when I set waitTime=0in multi lock , it fail, but I set waitTime=0 in RLock, it success, why?
                RLock lock1 = client.getLock("lock1");
                RLock lock2 = client.getLock("lock2");
                RLock lock3 = client.getLock("lock3");

                RedissonMultiLock lock = new RedissonMultiLock(lock1, lock2, lock3);

                boolean flag = lock.tryLock(0, TimeUnit.SECONDS);

                // flag is false


                then , debug RedissonMultiLock, until Line 413
                if (remainTime != -1) {
                remainTime -= System.currentTimeMillis() - time; // Here, remainTime is negative, so end the lock flow
                time = System.currentTimeMillis();
                if (remainTime= 0) {
                unlockInner(acquiredLocks);
                return false;
                }
                }

                I think it is a bug in RedissonMultiLock
                Redis version
                Redisson version
                3.16.6
                Redisson configuration
                single server
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.redisson.RedissonLock.java</file>
            <file type="M">org.redisson.RedissonMultiLock.java</file>
            <file type="M">org.redisson.RedissonFairLock.java</file>
            <file type="M">org.redisson.RedissonSpinLock.java</file>
        </fixedFiles>
    </bug>
    <bug id="757" opendate="2017-01-28 00:00:00" fixdate="2017-10-19 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>Unlock does not really unlock
            </summary>
            <description>If you unlock a FairLock from some thread, there is a delay until all threads actually see that
                lock as available.
                Example test clase
                import java.util.concurrent.ExecutorService;
                import java.util.concurrent.Executors;
                import java.util.concurrent.TimeUnit;

                import org.junit.Assert;
                import org.junit.Test;
                import org.redisson.Redisson;
                import org.redisson.api.RLock;
                import org.redisson.api.RedissonClient;
                import org.redisson.config.Config;
                import org.slf4j.Logger;
                import org.slf4j.LoggerFactory;


                public class RedissonLockTest {

                private static final String LOCK_NAME = "SOME_LOCK";
                private static final Logger log =
                LoggerFactory.getLogger(RedissonLockTest.class);

                @Test
                public void itDoesSomething() throws InterruptedException {

                ExecutorService executorService = Executors.newFixedThreadPool(2);

                Config config = new Config();
                config.useSingleServer().setAddress("127.0.0.1:6379");
                RedissonClient redisson = Redisson.create(config);

                executorService.execute(() -> {
                log.info("Attempting to get lock on first executor {}", Thread.currentThread().getId());
                RLock fairLock = redisson.getFairLock(LOCK_NAME);
                try {
                if (fairLock.tryLock(0, TimeUnit.SECONDS)) {
                try {
                log.info("Sleeping lock holder thread");
                Thread.sleep(1000L);
                }
                catch (InterruptedException e) {
                log.error("Thread sleep interrupted", e);
                }
                log.info("Releasing lock {} from first executor", LOCK_NAME);
                }
                else {
                Assert.fail("Unable to acquire lock for some reason");
                }
                }
                catch (InterruptedException e) {
                log.error("Interrupted", e);
                }
                finally {
                if (fairLock.isHeldByCurrentThread()) {
                fairLock.unlock();
                }
                }
                });

                executorService.execute(() -> {
                try {
                Thread.sleep(200L);
                }
                catch (InterruptedException e) {
                e.printStackTrace();
                }
                log.info("Attempting to get lock on second executor {}",
                Thread.currentThread().getId());
                RLock fairLock = redisson.getFairLock(LOCK_NAME);
                try {
                if (fairLock.tryLock(0, TimeUnit.SECONDS)) {
                log.info("Inside second block");
                Assert.fail("Should not be inside second block");
                log.info("Releasing lock {} from second executor", LOCK_NAME);
                }
                else {
                log.info("Second block could not get lock");
                }
                }
                catch (InterruptedException e) {
                log.error("Interrupted", e);
                }
                finally {
                if (fairLock.isHeldByCurrentThread()) {
                fairLock.unlock();
                }
                }
                });

                executorService.awaitTermination(3L, TimeUnit.SECONDS);

                log.info("Attempting to get lock again since it was unlocked");

                RLock fairLock = redisson.getFairLock(LOCK_NAME);
                try {
                if (fairLock.tryLock(0, TimeUnit.SECONDS)) {
                log.info("Lock successfully acquired");
                }
                else {
                Assert.fail("Could not get unlocked lock " + LOCK_NAME);
                }
                }
                catch (InterruptedException e) {
                log.error("Interrupted", e);
                }
                finally {
                if (fairLock.isHeldByCurrentThread()) {
                fairLock.unlock();
                }
                }

                }

                }
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.redisson.RedissonFairLock.java</file>
            <file type="M">org.redisson.RedissonLock.java</file>
            <file type="M">org.redisson.RedissonFairLockTest.java</file>
        </fixedFiles>
    </bug>
    <bug id="543" opendate="2016-06-30 00:00:00" fixdate="2016-08-06 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>RLock trylock blocks forever
            </summary>
            <description>Found this in 2.2.16, seems like this was not around in 2.2.10. But still verifying.
                When tryLock is called with 0 wait time, the thread blocks forever. I took a thread dump and it looks
                like the stack pasted below.
                A note on the environment: We are running this against an Elasticache cluster in AWS and accessing from
                4 EC2 instances. We have seen a lot of command timeouts. I am not sure if that is some way leading to
                this.
                "pool-4-thread-5" #86 prio=5 os_prio=0 tid=0x00007f15ca23f800 nid=0xcd4 waiting for monitor entry
                [0x00007f15b90e5000]
                java.lang.Thread.State: BLOCKED (on object monitor)
                at org.redisson.pubsub.PublishSubscribe.subscribe(PublishSubscribe.java:53)
                - waiting to lock#0x00000000dd405ea0> (a org.redisson.pubsub.LockPubSub)
                at org.redisson.pubsub.LockPubSub.subscribe(LockPubSub.java:22)
                at org.redisson.RedissonLock.subscribe(RedissonLock.java:311)
                at org.redisson.RedissonLock.tryLock(RedissonLock.java:264)
                at c.v.w.utils.caching.redis.RedisLock.tryLock(RedisLock.java:36)
                at c.v.w.utils.caching.LeasedLock.tryLock(LeasedLock.java:19)
                at c.v.w.eventprocessor.core.service.impl.JobProcessor.getProcessingLock(JobProcessor.java:130)
                at c.v.w.eventprocessor.core.service.impl.JobProcessor.access$000(JobProcessor.java:47)
                at c.v.w.webscan.eventprocessor.core.service.impl.JobProcessor$Processor.execute(JobProcessor.java:193)
                at c.v.w.service.integration.core.DataContextAwareTask.run(DataContextAwareTask.java:59)
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
                at java.lang.Thread.run(Thread.java:745)
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.redisson.RedissonPatternTopic.javav</file>
            <file type="M">org.redisson.RedissonTopic.java</file>
            <file type="M">org.redisson.connection.ConnectionManager.java</file>
            <file type="M">org.redisson.connection.MasterSlaveConnectionManager.java</file>
            <file type="M">org.redisson.connection.MasterSlaveEntry.java</file>
            <file type="M">org.redisson.connection.PubSubConnectionEntry.java</file>
            <file type="M">org.redisson.pubsub.PublishSubscribe.java</file>
            <file type="M">org.redisson.reactive.RedissonPatternTopicReactive.java</file>
            <file type="M">org.redisson.reactive.RedissonTopicReactive.java</file>
        </fixedFiles>
    </bug>
    <bug id="486" opendate="2016-05-05 00:00:00" fixdate="2016-06-11 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>Rlock performance issue.
            </summary>
            <description>Hi,
                I am using reddison Rlock with a cluster setup , and sometimes I see latency( up to 1000ms) when trying
                to acquire the lock or unlock.
                I saw this issue opened by zhxjouc (#455) with a similar problem and I am working with 2.2.13 but I am
                still getting latency when a thread is trying lock a key.
                My code is running with Java thread pool for accessing redis, I notice that if I work with pool of size
                1-2
                almost no latency when getting the lock, but working with 30-50 threads cause the lock delay.
                It could be thread overhead issue but I think that 1000ms is too long for that.
                Any help on how can I get better performance when locking an unlocking?.

                Thanks
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.redisson.connection.ConnectionManager.java</file>
            <file type="M">org.redisson.connection.MasterSlaveConnectionManager.java</file>
            <file type="M">org.redisson.connection.MasterSlaveEntry.java</file>
            <file type="M">org.redisson.pubsub.PublishSubscribe.java</file>
            <file type="M">org.redisson.RedissonLockHeavyTest.java</file>
        </fixedFiles>
    </bug>
    <bug id="2355" opendate="2019-10-10 00:00:00" fixdate="2020-12-22 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>RedissonLock.unlock() hangs infinitely waiting for RedissonPromise
            </summary>
            <description>Expected behavior
                Calls to RedissonLock.unlock() are expected to finish after the lock has been deleted from Redis.

                Actual behavior
                We have experienced several threads locked forever waiting inside calls to RedissonLock.unlock() in our
                production system within a few hours after upgrading to Redisson 3.11.4 from Redisson 3.8.0.

                Steps to reproduce or test case
                Unfortunately I am unable to reproduce the problem on my own computer. We have however experienced it on
                several of our production servers within just a few hours after starting our newer version of the
                application with updated Redisson version. We upgraded from 3.8.0 in order to receive fixes for bug
                #1966 - Deadlock after Redis timeout, which we also experienced a couple of times. However right after
                start we have found some (29) exceptions in our logs coming from internal redisson threads:

                [redisson-netty-5-24] WARN io.netty.util.concurrent.DefaultPromise - An exception was thrown by
                org.redisson.misc.RedissonPromise$$Lambda$50/265319658.operationComplete()
                java.lang.NullPointerException: null
                at org.redisson.RedissonLock.cancelExpirationRenewal(RedissonLock.java:330)
                ~[redisson-3.11.4.jar:3.11.4]
                at org.redisson.RedissonLock.lambda$unlockAsync$3(RedissonLock.java:583) ~[redisson-3.11.4.jar:3.11.4]
                at org.redisson.misc.RedissonPromise.lambda$onComplete$0(RedissonPromise.java:187)
                ~[redisson-3.11.4.jar:3.11.4]
                at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)
                ~[netty-common-4.1.41.Final.jar:4.1.41.Final]
                at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:570)
                ~[netty-common-4.1.41.Final.jar:4.1.41.Final]
                at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:549)
                ~[netty-common-4.1.41.Final.jar:4.1.41.Final]
                at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)
                ~[netty-common-4.1.41.Final.jar:4.1.41.Final]
                at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)
                ~[netty-common-4.1.41.Final.jar:4.1.41.Final]
                at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)
                ~[netty-common-4.1.41.Final.jar:4.1.41.Final]
                at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)
                ~[netty-common-4.1.41.Final.jar:4.1.41.Final]
                at org.redisson.misc.RedissonPromise.trySuccess(RedissonPromise.java:82) ~[redisson-3.11.4.jar:3.11.4]
                at org.redisson.command.RedisExecutor.handleReference(RedisExecutor.java:505)
                ~[redisson-3.11.4.jar:3.11.4]
                at org.redisson.command.RedisExecutor.handleSuccess(RedisExecutor.java:498)
                ~[redisson-3.11.4.jar:3.11.4]
                at org.redisson.command.RedisExecutor.handleResult(RedisExecutor.java:483) ~[redisson-3.11.4.jar:3.11.4]
                at org.redisson.command.RedisExecutor.checkAttemptPromise(RedisExecutor.java:469)
                ~[redisson-3.11.4.jar:3.11.4]
                at org.redisson.command.RedisExecutor.lambda$execute$3(RedisExecutor.java:187)
                ~[redisson-3.11.4.jar:3.11.4]
                at org.redisson.misc.RedissonPromise.lambda$onComplete$0(RedissonPromise.java:187)
                ~[redisson-3.11.4.jar:3.11.4]
                at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)
                [netty-common-4.1.41.Final.jar:4.1.41.Final]
                at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:570)
                [netty-common-4.1.41.Final.jar:4.1.41.Final]
                at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:549)
                [netty-common-4.1.41.Final.jar:4.1.41.Final]
                at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)
                [netty-common-4.1.41.Final.jar:4.1.41.Final]
                at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)
                [netty-common-4.1.41.Final.jar:4.1.41.Final]
                at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)
                [netty-common-4.1.41.Final.jar:4.1.41.Final]
                at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)
                [netty-common-4.1.41.Final.jar:4.1.41.Final]
                at org.redisson.misc.RedissonPromise.trySuccess(RedissonPromise.java:82) [redisson-3.11.4.jar:3.11.4]
                at org.redisson.client.handler.CommandDecoder.completeResponse(CommandDecoder.java:454)
                [redisson-3.11.4.jar:3.11.4]
                at org.redisson.client.handler.CommandDecoder.handleResult(CommandDecoder.java:449)
                [redisson-3.11.4.jar:3.11.4]
                at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:372)
                [redisson-3.11.4.jar:3.11.4]
                at org.redisson.client.handler.CommandDecoder.decodeCommand(CommandDecoder.java:209)
                [redisson-3.11.4.jar:3.11.4]
                at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:147)
                [redisson-3.11.4.jar:3.11.4]
                at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:117)
                [redisson-3.11.4.jar:3.11.4]
                at
                io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:505)
                [netty-codec-4.1.41.Final.jar:4.1.41.Final]
                at io.netty.handler.codec.ReplayingDecoder.callDecode(ReplayingDecoder.java:366)
                [netty-codec-4.1.41.Final.jar:4.1.41.Final]
                at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:283)
                [netty-codec-4.1.41.Final.jar:4.1.41.Final]
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:374)
                [netty-transport-4.1.41.Final.jar:4.1.41.Final]
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:360)
                [netty-transport-4.1.41.Final.jar:4.1.41.Final]
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:352)
                [netty-transport-4.1.41.Final.jar:4.1.41.Final]
                at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1422)
                [netty-transport-4.1.41.Final.jar:4.1.41.Final]
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:374)
                [netty-transport-4.1.41.Final.jar:4.1.41.Final]
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:360)
                [netty-transport-4.1.41.Final.jar:4.1.41.Final]
                at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:931)
                [netty-transport-4.1.41.Final.jar:4.1.41.Final]
                at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
                [netty-transport-4.1.41.Final.jar:4.1.41.Final]
                at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:700)
                [netty-transport-4.1.41.Final.jar:4.1.41.Final]
                at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:635)
                [netty-transport-4.1.41.Final.jar:4.1.41.Final]
                at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:552)
                [netty-transport-4.1.41.Final.jar:4.1.41.Final]
                at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:514)
                [netty-transport-4.1.41.Final.jar:4.1.41.Final]
                at io.netty.util.concurrent.SingleThreadEventExecutor$6.run(SingleThreadEventExecutor.java:1044)
                [netty-common-4.1.41.Final.jar:4.1.41.Final]
                at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
                [netty-common-4.1.41.Final.jar:4.1.41.Final]
                at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
                [netty-common-4.1.41.Final.jar:4.1.41.Final]
                at java.lang.Thread.run(Thread.java:748) [?:1.8.0_151]
                After that we were investigating issues with blocked threads by analyzing thread dumps. All stuck
                threads had exactly the same stack trace as below:

                rabbitmq-thread-1awaiting notification on [ 0x00000005e298c400 ] , holding [ 0x00000005cad3ebd0 ]
                at java.lang.Object.wait(Native Method)
                at java.lang.Object.wait(Object.java:502)
                at io.netty.util.concurrent.DefaultPromise.await(DefaultPromise.java:252)
                at org.redisson.misc.RedissonPromise.await(RedissonPromise.java:110)
                at org.redisson.misc.RedissonPromise.await(RedissonPromise.java:35)
                at org.redisson.command.CommandAsyncService.get(CommandAsyncService.java:132)
                at org.redisson.RedissonObject.get(RedissonObject.java:90)
                at org.redisson.RedissonLock.unlock(RedissonLock.java:453)
                at ... (our code goes below)
                After examining the source code of RedissonLock.java we have found that the NullPointerException occurs
                probably due to missing timeout object inside task:

                329: if (threadId == null || task.hasNoThreads()) {
                330: task.getTimeout().cancel();
                331: EXPIRATION_RENEWAL_MAP.remove(getEntryName());
                332: }
                I'm not sure if it would solve the root cause why task.getTimeout() returns null, but calls to
                cancelExpirationRenewal method should not fail with an exception, otherwise, e.g. in
                RedissonLock.unlockAsync method the future will never receive a result (or a failure):

                @Override
                public RFuture
                #Void>
                unlockAsync(long threadId) {
                RPromise
                #Void>
                result = new RedissonPromise#Void>();
                RFuture
                #Boolean>
                future = unlockInnerAsync(threadId);

                future.onComplete((opStatus, e) -> {
                if (e != null) {
                cancelExpirationRenewal(threadId);
                result.tryFailure(e);
                return;
                }

                if (opStatus == null) {
                IllegalMonitorStateException cause = new IllegalMonitorStateException("attempt to unlock lock, not
                locked by current thread by node id: "
                + id + " thread-id: " + threadId);
                result.tryFailure(cause);
                return;
                }

                cancelExpirationRenewal(threadId);
                result.trySuccess(null);
                });

                return result;
                }
                Please add null check for timeout inside the ExpirationEntry object.

                We had to return to 3.8.0 version because of this problem. I think it is quire severe.

                Redis version
                4.0.2

                Redisson version
                3.11.4

                Redisson configuration
                Default configuration with sentinel servers.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.redisson.RedissonLock.java</file>
        </fixedFiles>
    </bug>
    <bug id="530" opendate="2016-06-15 00:00:00" fixdate="2016-07-04 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>Deadlock on lock() and not only.
            </summary>
            <description>Hi,
                I want to raise the issue with deadlocks again.
                This bug is still exist and making big headache. As before, It present itself only on very heavy loaded
                tasks, but in this case I is happens only when client talks to the claster which is located remotely.
                With a single-local or claster-local servers I was unable to reproduce it, but with remote server it
                happens with rate 1 / 20 (means from 20 runs of "heavy-load" JUnit test it happens only once)
                I can see where thread is locked down, it always stuck in CommandAsyncService.get(Future), on l.await()
                line and never exits from it. As I understand something wrong with mainPromise object, it is staying in
                incomplete state... and nobody change it. I tried to understand the logics in
                CommandAsyncService.async(...) function, which is actually deals with connection, retry, redirections
                and at end should release (or fail) the mainPromise object, but it is nightmare. All these spagetty with
                promises and futures made the code difficult to read and impossible to analyse. For sure BUG is there,
                but I am near to give-up. Any thoughts?
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.redisson.pubsub.PublishSubscribe.java</file>
        </fixedFiles>
    </bug>
    <bug id="254" opendate="2015-10-02 00:00:00" fixdate="2015-10-02 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>EOFException when I use RLock with SerializationCodec
            </summary>
            <description>I configure Redisson to use SerializationCodec instead of default JsonJacksonCodec. Then I run
                my code in environment with concurrent threads and use Lock object to sync thread. After that I get
                exception and unlock only after expiration in 30 sec. Previous major version of Redisson does't contain
                this issues. Similar problem I have when I use CountDown.
                [nioEventLoopGroup-4-7] WARN io.netty.channel.DefaultChannelPipeline - An exceptionCaught() event was
                fired, and it reached at the tail of the pipeline. It usually means the last handler in the pipeline did
                not handle the exception.
                io.netty.handler.codec.DecoderException: java.io.EOFException
                at io.netty.handler.codec.ReplayingDecoder.callDecode(ReplayingDecoder.java:425)
                at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:230)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)
                at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)
                at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
                at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
                at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
                at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
                at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
                at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:110)
                at
                io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
                at java.lang.Thread.run(Thread.java:745)
                Caused by: java.io.EOFException
                at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2328)
                at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:2797)
                at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:802)
                at java.io.ObjectInputStream.#init>(ObjectInputStream.java:299)
                at org.redisson.codec.SerializationCodec$1.decode(SerializationCodec.java:42)
                at org.redisson.client.protocol.pubsub.PubSubMessageDecoder.decode(PubSubMessageDecoder.java:38)
                at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:179)
                at org.redisson.client.handler.CommandDecoder.decodeMulti(CommandDecoder.java:199)
                at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:189)
                at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:97)
                at io.netty.handler.codec.ReplayingDecoder.callDecode(ReplayingDecoder.java:370)
                ... 18 more
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.redisson.RedissonCountDownLatch.java</file>
            <file type="M">org.redisson.RedissonLock.java</file>
            <file type="M">org.redisson.client.protocol.RedisCommands.java</file>
            <file type="M">org.redisson.client.protocol.RedisStrictCommand.java</file>
            <file type="M">org.redisson.BaseTest.java</file>
            <file type="M">org.redisson.RedissonTwoLockedThread.java</file>
        </fixedFiles>
    </bug>
    <bug id="491" opendate="2016-05-16 00:00:00" fixdate="2016-06-16 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>Dead Locks Happen in lock() Process
            </summary>
            <description>when heavy concurrency happens in my application, a few lock requests will "sink" without any
                responses, even after the lock lease time has passed. All of these requests wait at
                RedissonLock.lockInterruptibly().The exact position is RedissonLock.get() after
                RedissonLock.subscribe().
                In my opinion, this may be due to a thread removes the netty listener which is used by another thread.
                It can happen in this way:

                Thread A is in the loop of getting the lock after subscription.
                Thread B has also applied subscription and waits for result.
                Thread A gets the lock very soon and enters RedissonLock.unsubscribe(). In this step, it possibly
                removes all the listeners on the same channel, which includes the listener used by Thread B. It causes
                Thread B can never get subscription response and hang on forever.

                The similar issue is at [https://github.com//pull/93]. But I think it is not solved completely.
                Also I suggest to apply the ttl algorithm to RedissonLock.get() because this step can cost some time.
                And if it has a timeout, dead lock can be prevented in a work-around way.
                This is the thread dump when dead lock happens:
                "http-nio-8001-exec-567" #5935 daemon prio=5 os_prio=0 tid=0x00007f15f81ca000 nid=0x6b3e waiting on
                condition [0x00007f154468a000] java.lang.Thread.State: WAITING (parking) at sun.misc.Unsafe.park(Native
                Method) - parking to wait for#0x00000000bf468fc8> (a java.util.concurrent.CountDownLatch$Sync) at
                java.util.concurrent.locks.LockSupport.park(LockSupport.java:175) at
                java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
                at
                java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)
                at
                java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
                at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231) at
                org.redisson.command.CommandAsyncService.get(CommandAsyncService.java:95) at
                org.redisson.RedissonObject.get(RedissonObject.java:55) at
                org.redisson.RedissonLock.lockInterruptibly(RedissonLock.java:113) at
                org.redisson.RedissonLock.lock(RedissonLock.java:92) ...
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.redisson.cluster.ClusterConnectionManager.java</file>
        </fixedFiles>
    </bug>
    <bug id="1104" opendate="2017-10-18 00:00:00" fixdate="2019-05-09 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>RFairLock dead lock issue.
            </summary>
            <description>Hi,
                I would like to report a locking issue we are seeing using redisson versions 3.3.2 to 3.5.4, using
                RFairLock.
                We have 3 instances of an application competing for a fair lock. After some time, the timeout of the
                fair lock queue gets set to a time many hours in the future, and no thread ever gets the lock.

                Here is what redis is reporting during one of the occurences - both commands ran at about 9:30AM PST on
                10/17/2017 (1508257800000):

                redis > zrange "redisson_lock_timeout:{mylock}" 0 99 WITHSCORES
                1) "3b9c4448-dfaa-4b4e-a691-cfee25458333:88"
                2) "1508278998141"
                3) "a8baa8c5-d42d-4912-9a1b-e72d0728ae88:90"
                4) "1508278998141"
                5) "17cd3d41-8ea1-44c7-b458-a530aff209f0:88"
                6) "1508279003141"
                redis > lrange "redisson_lock_queue:{mylock}" 0 99
                1) "17cd3d41-8ea1-44c7-b458-a530aff209f0:88"
                2) "3b9c4448-dfaa-4b4e-a691-cfee25458333:88"
                3) "a8baa8c5-d42d-4912-9a1b-e72d0728ae88:90"
                You can see that the timeout (zscore of the timeout keys) is set about 6 hours in the future.
                Current time = Tuesday, October 17, 2017 9:30:00.000 AM GMT-07:00 DST
                1508278998141 = Tuesday, October 17, 2017 3:23:18.141 PM GMT-07:00 DST
                1508279003141 = Tuesday, October 17, 2017 3:23:23.141 PM GMT-07:00 DST

                So far, we haven't been able to reproduce the issue locally, but it is happening daily in our production
                environment. We are suspecting the problem is happening randomly when one of the application gets killed
                by the operating system (kill -9), something that can happen quite often due to the way we manage the
                apps.

                We were using redisson version 3.2.3 before, and never experienced the issue.
                Earlier this year, we tried updating to version 3.3.2 and started seeing the problem within a day or so.
                We reverted to 3.2.3, and waited for an updated version.
                Two days ago, we decided to upgrade to 3.5.4 assuming the problem was fixed, but it happened again
                within a few hours.
                I will keep on trying to reproduce the issue, but any help is appreciated.
                Thanks,
                Michael
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.redisson.RedissonFairLock.java</file>
            <file type="M">org.redisson.RedissonFairLockTest.java</file>
        </fixedFiles>
    </bug>
    <bug id="889" opendate="2017-05-24 00:00:00" fixdate="2018-09-30 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>Cache ops taking too long
            </summary>
            <description>Currently i'm using Redisson 2.7.4 as JCache Provider, and some cache operations are taking a
                long time to execute, as can be seen in the log below:

                2017-06-05 11:44:18.103 WARN [com.contaazul.cache.cdi.interceptor.CacheResultInterceptor]
                (http-executor-threads - 89) Cache result for CompanyDemoFindService.find took 1669045 milliseconds
                in some cases it takes ~30 minutes

                Taking a thread dump I noticed that all EJB async threads (and some http-executor threads too) were in
                the same state, generating a huge queue in the async thread pool:

                captura de tela 2017-06-05 as 11 34 11

                Thread dump for some threads (Complete thread dump can be downloaded here):

                2017-06-05 11:03:55

                "EJB async - 2" - Thread t@1214
                java.lang.Thread.State: TIMED_WAITING
                at sun.misc.Unsafe.park(Native Method)
                - parking to wait for
                #b485fd6>
                (a java.util.concurrent.Semaphore$NonfairSync)
                at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
                at
                java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedNanos(AbstractQueuedSynchronizer.java:1033)
                at
                java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1326)
                at java.util.concurrent.Semaphore.tryAcquire(Semaphore.java:414)
                at org.redisson.RedissonLock.lockInterruptibly(RedissonLock.java:133)
                at org.redisson.RedissonLock.lock(RedissonLock.java:99)
                at org.redisson.jcache.JCache.getLockedLock(JCache.java:746)
                at org.redisson.jcache.JCache.get(JCache.java:194)
                at com.contaazul.cache.FailSafeCache.get(FailSafeCache.java:21)
                at
                org.jsr107.ri.annotations.AbstractCacheResultInterceptor.cacheResult(AbstractCacheResultInterceptor.java:71)
                at
                com.contaazul.cache.cdi.interceptor.CacheResultInterceptor.cacheResult(CacheResultInterceptor.java:28)
                at sun.reflect.GeneratedMethodAccessor141.invoke(Unknown Source)
                at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
                at java.lang.reflect.Method.invoke(Method.java:606)
                at org.jboss.weld.interceptor.proxy.SimpleMethodInvocation.invoke(SimpleMethodInvocation.java:30)
                at
                org.jboss.weld.interceptor.proxy.SimpleInterceptionChain.invokeNextInterceptor(SimpleInterceptionChain.java:69)
                at org.jboss.weld.bean.InterceptorImpl.intercept(InterceptorImpl.java:92)
                at
                org.jboss.as.weld.ejb.DelegatingInterceptorInvocationContext.proceed(DelegatingInterceptorInvocationContext.java:71)
                at
                org.jboss.as.weld.ejb.Jsr299BindingsInterceptor.delegateInterception(Jsr299BindingsInterceptor.java:72)
                at
                org.jboss.as.weld.ejb.Jsr299BindingsInterceptor.doMethodInterception(Jsr299BindingsInterceptor.java:84)
                at org.jboss.as.weld.ejb.Jsr299BindingsInterceptor.processInvocation(Jsr299BindingsInterceptor.java:97)
                at
                org.jboss.as.ee.component.interceptors.UserInterceptorFactory$1.processInvocation(UserInterceptorFactory.java:63)
                at org.jboss.invocation.InterceptorContext.proceed(InterceptorContext.java:288)
                at org.jboss.invocation.WeavedInterceptor.processInvocation(WeavedInterceptor.java:53)
                at
                org.jboss.as.ee.component.interceptors.UserInterceptorFactory$1.processInvocation(UserInterceptorFactory.java:63)
                at org.jboss.invocation.InterceptorContext.proceed(InterceptorContext.java:288)
                at
                org.jboss.as.ejb3.component.invocationmetrics.ExecutionTimeInterceptor.processInvocation(ExecutionTimeInterceptor.java:43)
                at org.jboss.invocation.InterceptorContext.proceed(InterceptorContext.java:288)
                at
                org.jboss.as.jpa.interceptor.SBInvocationInterceptor.processInvocation(SBInvocationInterceptor.java:47)
                at org.jboss.invocation.InterceptorContext.proceed(InterceptorContext.java:288)
                at
                org.jboss.as.weld.ejb.EjbRequestScopeActivationInterceptor.processInvocation(EjbRequestScopeActivationInterceptor.java:73)
                at org.jboss.invocation.InterceptorContext.proceed(InterceptorContext.java:288)
                at org.jboss.invocation.InitialInterceptor.processInvocation(InitialInterceptor.java:21)
                at org.jboss.invocation.InterceptorContext.proceed(InterceptorContext.java:288)
                at org.jboss.invocation.ChainedInterceptor.processInvocation(ChainedInterceptor.java:61)
                at
                org.jboss.as.ee.component.interceptors.ComponentDispatcherInterceptor.processInvocation(ComponentDispatcherInterceptor.java:53)
                at org.jboss.invocation.InterceptorContext.proceed(InterceptorContext.java:288)
                at
                org.jboss.as.ejb3.component.pool.PooledInstanceInterceptor.processInvocation(PooledInstanceInterceptor.java:51)
                at org.jboss.invocation.InterceptorContext.proceed(InterceptorContext.java:288)
                at org.jboss.as.ejb3.tx.CMTTxInterceptor.invokeInNoTx(CMTTxInterceptor.java:259)
                at org.jboss.as.ejb3.tx.CMTTxInterceptor.notSupported(CMTTxInterceptor.java:323)
                at org.jboss.as.ejb3.tx.CMTTxInterceptor.processInvocation(CMTTxInterceptor.java:236)
                at org.jboss.invocation.InterceptorContext.proceed(InterceptorContext.java:288)
                at
                org.jboss.as.ejb3.component.interceptors.CurrentInvocationContextInterceptor.processInvocation(CurrentInvocationContextInterceptor.java:41)
                at org.jboss.invocation.InterceptorContext.proceed(InterceptorContext.java:288)
                at
                org.jboss.as.ejb3.component.interceptors.ShutDownInterceptorFactory$1.processInvocation(ShutDownInterceptorFactory.java:64)
                at org.jboss.invocation.InterceptorContext.proceed(InterceptorContext.java:288)
                at
                org.jboss.as.ejb3.component.interceptors.LoggingInterceptor.processInvocation(LoggingInterceptor.java:59)
                at org.jboss.invocation.InterceptorContext.proceed(InterceptorContext.java:288)
                at
                org.jboss.as.ee.component.NamespaceContextInterceptor.processInvocation(NamespaceContextInterceptor.java:50)
                at org.jboss.invocation.InterceptorContext.proceed(InterceptorContext.java:288)
                at
                org.jboss.as.ejb3.component.interceptors.AdditionalSetupInterceptor.processInvocation(AdditionalSetupInterceptor.java:55)
                at org.jboss.invocation.InterceptorContext.proceed(InterceptorContext.java:288)
                at org.jboss.as.ee.component.TCCLInterceptor.processInvocation(TCCLInterceptor.java:45)
                at org.jboss.invocation.InterceptorContext.proceed(InterceptorContext.java:288)
                at org.jboss.invocation.ChainedInterceptor.processInvocation(ChainedInterceptor.java:61)
                at org.jboss.as.ee.component.ViewService$View.invoke(ViewService.java:185)
                at org.jboss.as.ee.component.ViewDescription$1.processInvocation(ViewDescription.java:182)
                at org.jboss.invocation.InterceptorContext.proceed(InterceptorContext.java:288)
                at org.jboss.invocation.ChainedInterceptor.processInvocation(ChainedInterceptor.java:61)
                at org.jboss.as.ee.component.ProxyInvocationHandler.invoke(ProxyInvocationHandler.java:73)
                at com.contaazul.company.CompanyDemoFindService$$$view659.find(Unknown Source)
                at sun.reflect.GeneratedMethodAccessor146.invoke(Unknown Source)
                at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
                at java.lang.reflect.Method.invoke(Method.java:606)
                at org.jboss.weld.util.reflection.SecureReflections$13.work(SecureReflections.java:267)
                at org.jboss.weld.util.reflection.SecureReflectionAccess.run(SecureReflectionAccess.java:52)
                at
                org.jboss.weld.util.reflection.SecureReflectionAccess.runAsInvocation(SecureReflectionAccess.java:137)
                at org.jboss.weld.util.reflection.SecureReflections.invoke(SecureReflections.java:263)
                at
                org.jboss.weld.bean.proxy.EnterpriseBeanProxyMethodHandler.invoke(EnterpriseBeanProxyMethodHandler.java:115)
                at org.jboss.weld.bean.proxy.EnterpriseTargetBeanInstance.invoke(EnterpriseTargetBeanInstance.java:56)
                at org.jboss.weld.bean.proxy.ProxyMethodHandler.invoke(ProxyMethodHandler.java:105)
                at
                com.contaazul.company.CompanyDemoFindService$Proxy$_$$_Weld$Proxy$.find(CompanyDemoFindService$Proxy$_$$_Weld$Proxy$.java)
                at com.contaazul.fanout.ApplicationTrackService.resolveOrigin(ApplicationTrackService.java:161)
                at com.contaazul.fanout.ApplicationTrackService.getContentDTO(ApplicationTrackService.java:131)
                at com.contaazul.fanout.ApplicationTrackService.sendTrack(ApplicationTrackService.java:104)
                at com.contaazul.fanout.ApplicationTrackService.sendTrackOperation(ApplicationTrackService.java:72)
                at com.contaazul.fanout.ApplicationTrackService.track(ApplicationTrackService.java:48)
                at sun.reflect.GeneratedMethodAccessor565.invoke(Unknown Source)
                at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
                at java.lang.reflect.Method.invoke(Method.java:606)
                at
                org.jboss.as.ee.component.ManagedReferenceMethodInterceptor.processInvocation(ManagedReferenceMethodInterceptor.java:52)
                at org.jboss.invocation.InterceptorContext.proceed(InterceptorContext.java:288)
                at org.jboss.invocation.WeavedInterceptor.processInvocation(WeavedInterceptor.java:53)
                at
                org.jboss.as.ee.component.interceptors.UserInterceptorFactory$1.processInvocation(UserInterceptorFactory.java:63)
                at org.jboss.invocation.InterceptorContext.proceed(InterceptorContext.java:288)
                at org.jboss.invocation.InterceptorContext$Invocation.proceed(InterceptorContext.java:374)
                at
                org.jboss.as.weld.ejb.Jsr299BindingsInterceptor.doMethodInterception(Jsr299BindingsInterceptor.java:86)
                at org.jboss.as.weld.ejb.Jsr299BindingsInterceptor.processInvocation(Jsr299BindingsInterceptor.java:97)
                at
                org.jboss.as.ee.component.interceptors.UserInterceptorFactory$1.processInvocation(UserInterceptorFactory.java:63)
                at org.jboss.invocation.InterceptorContext.proceed(InterceptorContext.java:288)
                at org.jboss.invocation.WeavedInterceptor.processInvocation(WeavedInterceptor.java:53)
                at
                org.jboss.as.ee.component.interceptors.UserInterceptorFactory$1.processInvocation(UserInterceptorFactory.java:63)
                at org.jboss.invocation.InterceptorContext.proceed(InterceptorContext.java:288)
                at
                org.jboss.as.ejb3.component.invocationmetrics.ExecutionTimeInterceptor.processInvocation(ExecutionTimeInterceptor.java:43)
                at org.jboss.invocation.InterceptorContext.proceed(InterceptorContext.java:288)
                at
                org.jboss.as.jpa.interceptor.SBInvocationInterceptor.processInvocation(SBInvocationInterceptor.java:47)
                at org.jboss.invocation.InterceptorContext.proceed(InterceptorContext.java:288)
                at
                org.jboss.as.weld.ejb.EjbRequestScopeActivationInterceptor.processInvocation(EjbRequestScopeActivationInterceptor.java:93)
                at org.jboss.invocation.InterceptorContext.proceed(InterceptorContext.java:288)
                at org.jboss.invocation.InitialInterceptor.processInvocation(InitialInterceptor.java:21)
                at org.jboss.invocation.InterceptorContext.proceed(InterceptorContext.java:288)
                at org.jboss.invocation.ChainedInterceptor.processInvocation(ChainedInterceptor.java:61)
                at
                org.jboss.as.ee.component.interceptors.ComponentDispatcherInterceptor.processInvocation(ComponentDispatcherInterceptor.java:53)
                at org.jboss.invocation.InterceptorContext.proceed(InterceptorContext.java:288)
                at
                org.jboss.as.ejb3.component.pool.PooledInstanceInterceptor.processInvocation(PooledInstanceInterceptor.java:51)
                at org.jboss.invocation.InterceptorContext.proceed(InterceptorContext.java:288)
                at org.jboss.as.ejb3.tx.CMTTxInterceptor.invokeInNoTx(CMTTxInterceptor.java:259)
                at org.jboss.as.ejb3.tx.CMTTxInterceptor.notSupported(CMTTxInterceptor.java:323)
                at org.jboss.as.ejb3.tx.CMTTxInterceptor.processInvocation(CMTTxInterceptor.java:236)
                at org.jboss.invocation.InterceptorContext.proceed(InterceptorContext.java:288)
                at
                org.jboss.as.ejb3.component.interceptors.CurrentInvocationContextInterceptor.processInvocation(CurrentInvocationContextInterceptor.java:41)
                at org.jboss.invocation.InterceptorContext.proceed(InterceptorContext.java:288)
                at
                org.jboss.as.ejb3.component.interceptors.ShutDownInterceptorFactory$1.processInvocation(ShutDownInterceptorFactory.java:64)
                at org.jboss.invocation.InterceptorContext.proceed(InterceptorContext.java:288)
                at
                org.jboss.as.ejb3.component.interceptors.LoggingInterceptor.processInvocation(LoggingInterceptor.java:59)
                at org.jboss.invocation.InterceptorContext.proceed(InterceptorContext.java:288)
                at
                org.jboss.as.ee.component.NamespaceContextInterceptor.processInvocation(NamespaceContextInterceptor.java:50)
                at org.jboss.invocation.InterceptorContext.proceed(InterceptorContext.java:288)
                at
                org.jboss.as.ejb3.component.interceptors.AdditionalSetupInterceptor.processInvocation(AdditionalSetupInterceptor.java:55)
                at org.jboss.invocation.InterceptorContext.proceed(InterceptorContext.java:288)
                at org.jboss.as.ee.component.TCCLInterceptor.processInvocation(TCCLInterceptor.java:45)
                at org.jboss.invocation.InterceptorContext.proceed(InterceptorContext.java:288)
                at org.jboss.invocation.ChainedInterceptor.processInvocation(ChainedInterceptor.java:61)
                at org.jboss.as.ee.component.ViewService$View.invoke(ViewService.java:185)
                at org.jboss.as.ee.component.ViewDescription$1.processInvocation(ViewDescription.java:182)
                at org.jboss.invocation.InterceptorContext.proceed(InterceptorContext.java:288)
                at
                org.jboss.as.ejb3.component.interceptors.AsyncFutureInterceptorFactory$1$1.runInvocation(AsyncFutureInterceptorFactory.java:89)
                at org.jboss.as.ejb3.component.interceptors.AsyncInvocationTask.run(AsyncInvocationTask.java:73)
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
                at java.lang.Thread.run(Thread.java:745)
                at org.jboss.threads.JBossThread.run(JBossThread.java:122)

                Locked ownable synchronizers:
                - locked
                #f75019a>
                (a java.util.concurrent.ThreadPoolExecutor$Worker)


                "redisson-3-9" - Thread t@1416
                java.lang.Thread.State: WAITING
                at sun.misc.Unsafe.park(Native Method)
                - parking to wait for#2d3b7977> (a
                java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
                at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
                at
                java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
                at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
                at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
                at
                io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
                at java.lang.Thread.run(Thread.java:745)

                Locked ownable synchronizers:
                - None


                "redisson-netty-1-9" - Thread t@1297
                java.lang.Thread.State: RUNNABLE
                at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
                at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
                at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
                at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
                - locked#7ed0d3e3> (a io.netty.channel.nio.SelectedSelectionKeySet)
                - locked#130940b7> (a java.util.Collections$UnmodifiableSet)
                - locked#4eab503d> (a sun.nio.ch.EPollSelectorImpl)
                at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
                at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:746)
                at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:391)
                at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
                at
                io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
                at java.lang.Thread.run(Thread.java:745)

                Locked ownable synchronizers:
                - None
                it looks like that is a lock issue, but i don't know how to solve it.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.redisson.RedissonMap.java</file>
            <file type="M">org.redisson.BaseMapTest.java</file>
        </fixedFiles>
    </bug>
    <bug id="533" opendate="2016-06-23 00:00:00" fixdate="2016-06-29 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>Exception in using RedissonMultiLock
            </summary>
            <description>I have 5 independent masters and I use single server config to create Redission instance.
                `
                import java.util.concurrent.TimeUnit;
                import org.redisson.Config;
                import org.redisson.Redisson;
                import org.redisson.RedissonClient;
                import org.redisson.core.RLock;
                import org.redisson.core.RedissonMultiLock;
                public class LockTest {
                static RedissonClient redisson1;
                static RedissonClient redisson2;
                static RedissonClient redisson3;
                static RedissonClient redisson4;
                static RedissonClient redisson5;

                public static void main(String[] args) {
                redisson1 = createClient("192.168.223.128", 8000);
                redisson2 = createClient("192.168.223.128", 8001);
                redisson3 = createClient("192.168.223.128", 8002);
                redisson4 = createClient("192.168.223.128", 8003);
                redisson5 = createClient("192.168.223.128", 8004);

                System.out.println(lock("lock1"));

                // shout down Redis instance with port 8000 and try again
                System.out.println("try to get another lock");
                System.out.println(lock("lock2"));
                }

                private static RedissonClient createClient(String ip, int port) {
                Config config = new Config();
                config.useSingleServer().setAddress(ip + ":" + port);
                RedissonClient redisson = Redisson.create(config);
                return redisson;
                }

                private static boolean lock(String lockKey) {
                RLock lock1 = redisson1.getLock(lockKey);
                RLock lock2 = redisson2.getLock(lockKey);
                RLock lock3 = redisson3.getLock(lockKey);
                RLock lock4 = redisson4.getLock(lockKey);
                RLock lock5 = redisson5.getLock(lockKey);

                RedissonMultiLock lock = new RedissonMultiLock(lock1, lock2, lock3, lock4, lock5);

                boolean lockResult = false;

                try {
                lockResult = lock.tryLock(5, 10 * 1000, TimeUnit.MILLISECONDS);
                } catch (InterruptedException e) {
                e.printStackTrace();
                }
                System.out.println("lock successfuly" + lockResult);
                return lockResult;
                }

                }`
                The RedissonMultiLock works fine if all the redis nodes are alive.
                If I shutdown one of the redis nodes, it will throw RedisConnectionException
                Exception in thread "main" org.redisson.client.RedisConnectionException: Can't init enough connections
                amount! Only 0 from 5 were initialized. Server: /192.168.223.128:8000
                According to The Redlock algorithm, it should try to lock another node but not throw exception. Do I use
                RedissonMultiLock correctly?
                It tries to acquire the lock in all the N instances sequentially, using the same key name and random
                value in all the instances. During step 2, when setting the lock in each instance, the client uses a
                timeout which is small compared to the total lock auto-release time in order to acquire it. For example
                if the auto-release time is 10 seconds, the timeout could be in the ~ 5-50 milliseconds range. This
                prevents the client from remaining blocked for a long time trying to talk with a Redis node which is
                down: if an instance is not available, we should try to talk with the next instance ASAP.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.redisson.RedissonFairLock.java</file>
            <file type="M">org.redisson.RedissonLock.java</file>
            <file type="M">org.redisson.RedissonReadLock.java</file>
            <file type="M">org.redisson.RedissonWriteLock.java</file>
            <file type="M">org.redisson.core.RLock.java</file>
            <file type="M">org.redisson.core.RedissonMultiLock.java</file>
            <file type="M">org.redisson.core.RedissonRedLock.java</file>
            <file type="M">org.redisson.RedissonMultiLockTest.java</file>
            <file type="M">org.redisson.RedissonRedLockTest.java</file>
        </fixedFiles>
    </bug>
    <bug id="1048" opendate="2017-09-12 00:00:00" fixdate="2017-10-09 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>Redisson hang on RBatch.execute()
            </summary>
            <description>
                Hi, we are using redisson client for our multi-threaded application.

                We use RBatch to batch two commands as below:

                RBatch batch = client.createBatch();
                batch.timeout(1000, TimeUnit.MILLISECONDS);
                RMapAsync#String, byte[]> map = batch.getMap(key, RedissonByteArrayMapValueCodec.INSTANCE);
                map.putAllAsync(hashData);
                map.expireAsync(ttlSeconds, TimeUnit.SECONDS);
                batch.execute();
                During the test, we noticed the hang several time.

                We first show this error first before the hang:

                VYM-748304795.us-west-2.elb.amazonaws.com/m3u8/lego262346.m3u8}]', anchor='PT59.832631279S',
                driftMillis='0'} 11 Sep 2017 18:09:43,195 [WARN] (redisson-netty-12-6)
                io.netty.util.concurrent.DefaultPromise: An exception was thrown by
                org.redisson.command.CommandBatchService$5.operationComplete()
                org.redisson.client.RedisNodeNotFoundException: Node: /127.0.0.1:30003 for slot: 10928 hasn't been
                discovered yet at
                org.redisson.connection.MasterSlaveConnectionManager.getEntry(MasterSlaveConnectionManager.java:684)
                ~[Redisson-3.x.jar:?] at
                org.redisson.connection.MasterSlaveConnectionManager.connectionWriteOp(MasterSlaveConnectionManager.java:674)
                ~[Redisson-3.x.jar:?] at org.redisson.command.CommandBatchService.execute(CommandBatchService.java:258)
                ~[Redisson-3.x.jar:?] at
                org.redisson.command.CommandBatchService.access$100(CommandBatchService.java:59) ~[Redisson-3.x.jar:?]
                at org.redisson.command.CommandBatchService$5.operationComplete(CommandBatchService.java:331)
                ~[Redisson-3.x.jar:?] at
                io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:507)
                ~[netty-all-4.1.jar:4.1.13.Final-SNAPSHOT] at
                io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:500)
                ~[netty-all-4.1.jar:4.1.13.Final-SNAPSHOT] at
                io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:479)
                ~[netty-all-4.1.jar:4.1.13.Final-SNAPSHOT] at
                io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:420)
                ~[netty-all-4.1.jar:4.1.13.Final-SNAPSHOT] at
                io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:122)
                ~[netty-all-4.1.jar:4.1.13.Final-SNAPSHOT] at
                org.redisson.misc.RedissonPromise.tryFailure(RedissonPromise.java:98) ~[Redisson-3.x.jar:?] at
                org.redisson.client.handler.CommandDecoder.decodeCommandBatch(CommandDecoder.java:207)
                ~[Redisson-3.x.jar:?] at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:135)
                ~[Redisson-3.x.jar:?] at
                io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:489)
                ~[netty-all-4.1.jar:4.1.13.Final-SNAPSHOT] at
                io.netty.handler.codec.ReplayingDecoder.callDecode(ReplayingDecoder.java:367)
                ~[netty-all-4.1.jar:4.1.13.Final-SNAPSHOT] at
                io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:265)
                ~[netty-all-4.1.jar:4.1.13.Final-SNAPSHOT] at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
                ~[netty-all-4.1.jar:4.1.13.Final-SNAPSHOT] at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
                ~[netty-all-4.1.jar:4.1.13.Final-SNAPSHOT] at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
                ~[netty-all-4.1.jar:4.1.13.Final-SNAPSHOT] at
                io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)
                ~[netty-all-4.1.jar:4.1.13.Final-SNAPSHOT] at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
                ~[netty-all-4.1.jar:4.1.13.Final-SNAPSHOT] at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
                ~[netty-all-4.1.jar:4.1.13.Final-SNAPSHOT] at
                io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
                ~[netty-all-4.1.jar:4.1.13.Final-SNAPSHOT] at
                io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1334)
                ~[netty-all-4.1.jar:4.1.13.Final-SNAPSHOT] at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
                ~[netty-all-4.1.jar:4.1.13.Final-SNAPSHOT] at
                io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
                ~[netty-all-4.1.jar:4.1.13.Final-SNAPSHOT] at
                io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:926)
                ~[netty-all-4.1.jar:4.1.13.Final-SNAPSHOT] at
                io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:134)
                ~[netty-all-4.1.jar:4.1.13.Final-SNAPSHOT] at
                io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:644)
                ~[netty-all-4.1.jar:4.1.13.Final-SNAPSHOT] at
                io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:579)
                ~[netty-all-4.1.jar:4.1.13.Final-SNAPSHOT] at
                io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:496)
                ~[netty-all-4.1.jar:4.1.13.Final-SNAPSHOT] at
                io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458) ~[netty-all-4.1.jar:4.1.13.Final-SNAPSHOT]
                at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
                ~[netty-all-4.1.jar:4.1.13.Final-SNAPSHOT] at
                io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
                ~[netty-all-4.1.jar:4.1.13.Final-SNAPSHOT] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_141]

                Then our thread calling the batch hung. There is the stacktrack:

                java.lang.Thread.State: WAITING (parking) at sun.misc.Unsafe.park(Native Method) - parking to wait for
                0x00000007bafd7438> (a java.util.concurrent.CountDownLatch$Sync) at
                java.util.concurrent.locks.LockSupport.park(LockSupport.java:175) at
                java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
                at
                java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)
                at
                java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
                at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231) at
                org.redisson.command.CommandAsyncService.get(CommandAsyncService.java:148) at
                org.redisson.command.CommandBatchService.execute(CommandBatchService.java:134) at
                org.redisson.RedissonBatch.execute(RedissonBatch.java:253)

                I tried to set the lower lockWatchdogTimeout and set the batch timeout but this did not help.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.redisson.connection.MasterSlaveConnectionManager.java</file>
        </fixedFiles>
    </bug>
    <bug id="455" opendate="2016-03-29 00:00:00" fixdate="2016-06-11 00:00:00" resolution="Fixed">
        <buginformation>
            <summary>Long latency issue while call Redis.
            </summary>
            <description>I use Redisson 2.2.10 to implement distributed lock in my project. And then I run load test for
                this distributed lock using SINGLE redis mode. But I found there are many long latency APIs and it's
                wired that almost all the long latency APIs are a little greater than 1000ms. The average latency and 99
                percent latecy is about 15ms and 30ms respectively. And then I used jstack to get the call stack and
                found most of the threads are WAITING at the same place just as the following

                "Processor-35" prio=10 tid=0x00007f61900e3800 nid=0x2614 in Object.wait() [0x00007f61027e5000]
                java.lang.Thread.State: WAITING (on object monitor)
                at java.lang.Object.wait(Native Method)
                at java.lang.Object.wait(Object.java:503)
                at io.netty.util.concurrent.DefaultPromise.awaitUninterruptibly(DefaultPromise.java:286)
                - locked#0x0000000600aa3e50> (a io.netty.util.concurrent.DefaultPromise)
                at io.netty.util.concurrent.DefaultPromise.awaitUninterruptibly(DefaultPromise.java:32)
                at org.redisson.command.CommandAsyncService.get(CommandAsyncService.java:84)
                at org.redisson.RedissonObject.get(RedissonObject.java:49)
                at org.redisson.RedissonLock.tryAcquire(RedissonLock.java:139)
                at org.redisson.RedissonLock.tryLock(RedissonLock.java:225)
                at com.mytest.lock.impl.redis.BaseRedisLockClientImpl.tryLock(BaseRedisLockClientImpl.java:72)
                Meanwhile, during the test, I dumped the network packages using tcpdump and found that it took about
                1000ms before the first package was sent to Redis. And in Redis server, I didn't find any slow query
                whose latency is more than 10ms. So I think, for the long latency API, most of the time might be cost in
                the project not the network. So can you help me to have a check or give me some suggestion about this
                issue? Many thanks.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.redisson.connection.ConnectionManager.java</file>
            <file type="M">org.redisson.connection.MasterSlaveConnectionManager.java</file>
            <file type="M">org.redisson.connection.MasterSlaveEntry.java</file>
            <file type="M">org.redisson.pubsub.PublishSubscribe.java</file>
            <file type="M">org.redisson.RedissonLockHeavyTest.java</file>
        </fixedFiles>
    </bug>
</bugrepository>