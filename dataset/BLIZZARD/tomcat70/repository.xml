<bugrepository name="tomcat70">
    <bug id="55582" opendate="2013-09-22 10:00:36" fixdate="2013-09-23 13:57:43" resolution="Fixed">
        <buginformation>
            <summary>Concurrent issue of TagFileProcessor.</summary>
            <description>Created attachment 30871 [details]
                Unsynchronized getting wrapper from RuntimeContext.
                The following code has concurrent issue.
                JspRuntimeContext rctxt = ctxt.getRuntimeContext();
                JspServletWrapper wrapper = rctxt.getWrapper(wrapperUri);
                synchronized (rctxt) {
                if (wrapper == null) {
                ....
                It creates duplicated JspServletWrapper in this scenario,
                A.jsp --> C.tag
                B.jsp --> C.tag
                A.jsp and B.jsp are both compiling and come to the given lines.
                Two threads all get null from JspRuntimeContext(JspServletWrapper == null).
                So two instances of JspServletWrapper was created.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.jasper.compiler.TagFileProcessor.java</file>
        </fixedFiles>
    </bug>
    <bug id="52777" opendate="2012-02-27 03:30:27" fixdate="2012-09-08 21:12:42" resolution="Fixed">
        <buginformation>
            <summary>Automatically shut down old versions in parallel deployment.
            </summary>
            <description>Under parallel deployment, when an older version in an application has zero sessions, shut it
                down automatically to release tomcat and JVM resources.
                Christopher Schultz suggested these approaches might be possible (on the Tomcat user list):
                1.
                Modify the parallel deployment code to register an MBean.
                NotificationListener that filters for useful events (such as expiring session notifications on the
                outgoing webapp).
                2.
                When the listener receives a notification, check the current state (e.g. session count=0; or, I suppose
                you could make this a part of your filter in step #1).
                If session count = 0, start a new thread that stops the outgoing webapp and de-registers the listener.
                Or.
                1.
                Install a SessionEventListener that counts-down the sessions (you'll have to get the count from JMX, I
                suppose) until they equal 0, then starts a new thread that ... etc.
                Or.
                1.
                Install a Timer thread that polls at intervals (1 minute?) to see if all the sessions are dead and then
                starts a thread ...
                etc.
                The first idea seems the cleanest, though Tomcat might not actually fire MBean events for things like
                session count changing.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.Host.java</file>
            <file type="M">org.apache.catalina.core.StandardHost.java</file>
            <file type="M">org.apache.catalina.startup.HostConfig.java</file>
        </fixedFiles>
    </bug>
    <bug id="52213" opendate="2011-11-18 23:29:32" fixdate="2011-12-23 21:16:31" resolution="Fixed">
        <buginformation>
            <summary>Field "org.apache.catalina.tribes.transport.bio.util.FastQueue.enabled" should be volatile.
            </summary>
            <description>The boolean flag "org.apache.catalina.tribes.transport.bio.util.FastQueue.enabled" may be
                read/written by multiple threads concurrently (Seehttp:
                //svn.apache.org/repos/asf/!svn/bc/1203897/tomcat/trunk/java/org/apache/catalina/tribes/group/interceptors/MessageDispatchInterceptor.java>).
                Therefore, accesses to it should be properly synchronized.
                It is sufficient to make this boolean flag volatile to protect access to it.
                Seehttps:
                //www.securecoding.cert.org/confluence/display/java/VNA00-J.+Ensure+visibility+when+accessing+shared+primitive+variables>
                for more information about this bug pattern.
                Keshmesh (https://keshmesh.cs.illinois.edu/) is an Eclipse plugin that analyzes the source code of Java
                programs for common concurrency bug patterns.
                We used Keshmesh to detect this concurrency bug pattern.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.tribes.transport.bio.util.FastQueue.java</file>
        </fixedFiles>
    </bug>
    <bug id="48249" opendate="2009-11-20 05:06:47" fixdate="2009-11-22 16:28:11" resolution="Fixed">
        <buginformation>
            <summary>org.apache.tomcat.util.net.NioBlockingSelector.BlockPoller.run = volatile.
            </summary>
            <description>org.apache.tomcat.util.net.NioBlockingSelector.BlockPoller.run should be volatile, as it is
                used to communicate between running threads.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.tomcat.util.net.NioBlockingSelector.java</file>
        </fixedFiles>
    </bug>
    <bug id="48895" opendate="2010-03-11 22:20:01" fixdate="2010-10-27 15:32:18" resolution="Fixed">
        <buginformation>
            <summary>WebAppClassLoader.clearThreadLocalMap() concurrency issues.
            </summary>
            <description>I think that the memory leak protection of WebAppClassLoader.clearReferencesThreadLocals()
                which detects and clears ThreadLocals that would prevent GC the WebAppClassLoader instance has issues
                regarding concurrency :
                - It enumerates Threads and looks into internal structures of the Thread class, but there are no "memory
                barrier" that would ensure a consistent state of the ThreadLocalMap being examined.
                So, it is theoretically possible that a ThreadLocal in Thread A was properly cleaned up by the
                application, but the current thread B (that is undeploying the application) does not see the up to date
                state because there's no synchronization between those threads.
                - Much more severe : after detecting such a leak, it invokes
                java.lang.ThreadLocal.ThreadLocalMap.remove(ThreadLocal) on Thread A's ThreadLocalMap instance but the
                invocation is done by Thread B (the thread that undeploys the app).
                The remove() method is not thread safe at all, and nor is the expungeStaleEntries() method which may
                also be invoked in clearThreadLocalMap().
                So, if a webapp is being undeployed while other applications continue to receive a heavy load of
                requests, this could corrupt the internal structures of the ThreadLocalMap instance !
                I propose to keep the detection of leaks as it is, but to make the actual clearing optional (and
                disabled by default) to avoid encountering big problems in production.
                Idea to improve the clearing in a safe way : if the thread that is "provoking" the leak is one of
                tomcat's worker threads, we could mark it as "dirty", and then have a background task that would end
                such threads (renew those threads in the pool).
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.core.StandardContext.java</file>
            <file type="M">org.apache.catalina.loader.WebappClassLoader.java</file>
            <file type="M">org.apache.catalina.loader.WebappLoader.java</file>
        </fixedFiles>
    </bug>
    <bug id="57779" opendate="2015-03-30 12:22:29" fixdate="2015-04-01 13:01:57" resolution="Fixed">
        <buginformation>
            <summary>Deadlock if using separate thread to write to ServletOutputStream.
            </summary>
            <description>If using a separate (non-container) thread to write to a ServletOutputStream, a deadlock can
                occur, if an io-error occurs during write.
                For example, if the client drops the connection.
                This is caused by a synchronized operation on the the underlying socket-object.
                The servlet handling thread is blocked, while waiting for the end of output from the separate thread,
                and holds a lock on the socket-object:
                "http-bio-25030-exec-10" daemon prio=10 tid=0x0000000001804800 nid=0x3fd0 waiting on condition
                [0x00007f767aeeb000]
                java.lang.Thread.State: WAITING (parking)
                at sun.misc.Unsafe.park(Native Method)
                - parking to wait for#x00000000c3b273b8> (a
                java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
                ...
                at org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run(JIoEndpoint.java:314)
                - locked#x00000000c38ef6d8> (a org.apache.tomcat.util.net.SocketWrapper)
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
                at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
                at java.lang.Thread.run(Thread.java:745)
                The separate thread trys to write, but an io-error occurs:
                "Thread-13" daemon prio=10 tid=0x00007f767c5c8000 nid=0x3fda waiting for monitor entry
                [0x00007f767a4e2000]
                java.lang.Thread.State: BLOCKED (on object monitor)
                at org.apache.tomcat.util.net.JIoEndpoint.processSocketAsync(JIoEndpoint.java:560)
                - waiting to lock#x00000000c38ef6d8> (a org.apache.tomcat.util.net.SocketWrapper)
                at org.apache.coyote.AbstractProcessor.setErrorState(AbstractProcessor.java:84)
                at org.apache.coyote.http11.AbstractHttp11Processor.action(AbstractHttp11Processor.java:802)
                at org.apache.coyote.Response.action(Response.java:172)
                at org.apache.catalina.connector.OutputBuffer.doFlush(OutputBuffer.java:363)
                at org.apache.catalina.connector.OutputBuffer.flush(OutputBuffer.java:331)
                at org.apache.catalina.connector.CoyoteOutputStream.flush(CoyoteOutputStream.java:101)
                Now it trys to get a lock on the socket-object and both threads are blocked forever.
                In the former tomcat version we used (7.0.53) this was not an issue.
                I had a look in the source code and saw that the failing code was invented later.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.coyote.AbstractProcessor.java</file>
        </fixedFiles>
    </bug>
    <bug id="56857" opendate="2014-08-14 21:42:28" fixdate="2014-09-03 11:06:32" resolution="Fixed">
        <buginformation>
            <summary>Thread safety issue in ApplicationContextFacade.invokeMethod.
            </summary>
            <description>Multiple subsequent Thread dumps were exhibiting lots of threads hanging in HashMap.get() while
                in RUNNABLE status, and consuming high CPU - a typical indicator of a thread safety issue with these
                non-threadsafe Maps.
                Thread stacks all as below.
                ApplicationContextFacade implements ServletContext, these objects are singletons per webapp, and get
                passed around among multiple threads.
                Their private member "objectCache" is a HashMap, i.e. not thread safe, but gets accessed AND modified
                without any synchronization in invokeMethod().
                Suggested remedy: change to a ConcurrentHashMap, which is thread-safe, and performs much better than
                synchronizing on every access.
                "connector-93: userId=_101_1, sessionId=C9EC1C59DD2244557BC6231A5476000E" daemon prio=10
                tid=0x00007fb1e40cf800 nid=0x7b90 runnable [0x00007fb1e82c3000]
                java.lang.Thread.State: RUNNABLE
                at java.util.HashMap.get(HashMap.java:326)
                at org.apache.catalina.core.ApplicationContextFacade.invokeMethod(ApplicationContextFacade.java:789)
                at org.apache.catalina.core.ApplicationContextFacade.doPrivileged(ApplicationContextFacade.java:767)
                at org.apache.catalina.core.ApplicationContextFacade.getContextPath(ApplicationContextFacade.java:428)
                at blackboard.portal.servlet.ModuleCustomizationServlet.service(ModuleCustomizationServlet.java:88)
                at javax.servlet.http.HttpServlet.service(HttpServlet.java:728)
                ...
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.core.ApplicationContextFacade.java</file>
        </fixedFiles>
    </bug>
    <bug id="48843" opendate="2010-03-02 17:43:49" fixdate="2010-06-22 04:55:08" resolution="Fixed">
        <buginformation>
            <summary>Tomcat Acceptor Thread goes into wait() and it will never come back.
            </summary>
            <description>Hi,
                I believe I've found a race condition in Tomcat that causes the http port to be non-responsive.
                It exists in 6.0 and also in 5.5 (although the code has been refactored).
                I could not find any reference to it in the Bug database or the mailing list archives.
                Consider a tomcat instance with maxThreads set to 2, i.e. you have 2 tomcat threads to service incoming
                requests.
                The sequence of events is as follows:
                1.
                Thread 1 and Thread 2 are both servicing a request each.
                2.
                A third request comes in.
                3.
                In class JIOEndpoint.java, the acceptor thread calls methods processSocket() which then calls
                getWorkerThread() which then calls createWorkerThread().
                4.
                createWorkerThread() returns null since both threads are busy processing the two requests.
                5.
                Here is the race condition in method getWorkerThread() in the code shown below protected Worker
                getWorkerThread(){
                ...
                Worker workerThread = createWorkerThread();
                while (workerThread == null) {
                try {
                synchronized (workers) {
                workers.wait();
                }
                }
                ...
                }
                The acceptor thread executes the "while(workerThread == null)" statement and is then switched out by the
                CPU.
                The two threads executing the two requests complete and go into Worker.await() waiting for the next job
                after executing method recycleWorkerThread().
                The acceptor thread is switched back into CPU and executes the synchronized block and goes into the
                wait().
                At this point, there aren't any Worker threads out there processing requestsand therefore there isn't
                any thread to wake up the acceptor thread.
                The application is non-responsive after this.
                A simple solution would be to check if curThreadsBusy > 0 in the synchronized block before going into
                wait() in method getWorkerThread() OR increase the scope of the critical section to include the while
                loop.
                Thanks,
                Harshad
                Stack Traces below:
                "bda19102143" id=1578 in WAITING on
                lock=org.apache.tomcat.util.net.jioendpoint$wor...@13aa4ee3^m
                at java.lang.Object.wait(Native Method)^M
                at java.lang.Object.wait(Object.java:485)^M
                at
                org.apache.tomcat.util.net.JIoEndpoint$Worker.await(JIoEndpoint.java:416)^M
                at
                org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:442)^M
                at java.lang.Thread.run(Thread.java:619)^M
                "http-8091-Acceptor-0" id=43 in WAITING on
                lock=org.apache.tomcat.util.net.jioendpoint$workerst...@13bd7b6a^m
                at java.lang.Object.wait(Native Method)^M
                at java.lang.Object.wait(Object.java:485)^M
                at
                org.apache.tomcat.util.net.JIoEndpoint.getWorkerThread(JIoEndpoint.java:700)^M
                at
                org.apache.tomcat.util.net.JIoEndpoint.processSocket(JIoEndpoint.java:731)^M
                at
                org.apache.tomcat.util.net.JIoEndpoint$Acceptor.run(JIoEndpoint.java:313)^M
                at java.lang.Thread.run(Thread.java:619)^M
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.tomcat.util.net.AprEndpoint.java</file>
        </fixedFiles>
    </bug>
    <bug id="57977" opendate="2015-06-01 07:40:43" fixdate="2015-06-09 11:58:22" resolution="Fixed">
        <buginformation>
            <summary>The original class loader isn't re-bound to the thread in PersistentValve.invoke().
            </summary>
            <description>Created attachment 32769 [details]
                patch against trunk
                In PersistentValve.invoke(Request, Response) method, The Webapp class loader has been bound to the
                current thread.
                However, the original class loader that had been used before bind isn't restored to the thread.
                The original class loader should be re-bound to the thread.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.valves.PersistentValve.java</file>
        </fixedFiles>
    </bug>
    <bug id="48248" opendate="2009-11-20 04:56:14" fixdate="2009-11-22 16:31:47" resolution="Fixed">
        <buginformation>
            <summary>tribes.group.interceptors.MessageDispatchInterceptor.run should be volatile.
            </summary>
            <description>org.apache.catalina.tribes.group.interceptors.MessageDispatchInterceptor.run should be
                volatile, as it is used to communicate between running threads.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.tribes.group.GroupChannel.java</file>
            <file type="M">org.apache.catalina.tribes.group.interceptors.MessageDispatchInterceptor.java</file>
            <file type="M">org.apache.catalina.tribes.group.interceptors.TcpPingInterceptor.java</file>
            <file type="M">org.apache.catalina.tribes.membership.McastServiceImpl.java</file>
        </fixedFiles>
    </bug>
    <bug id="57340" opendate="2014-12-11 11:20:44" fixdate="2015-10-25 18:40:12" resolution="Fixed">
        <buginformation>
            <summary>NioConnector caches get corrupted on concurrent comet close.
            </summary>
            <description>Configuration:
                Tomcat 7.0.47 NioConnector, nginx 1.6.2, atmosphere 2.2.3.
                It happens when nginx and atmosphere close the same comet connection concurrently.
                In NioEndpoint.Poller thread(A) the SocketChannel becomes ready for read when nginx closes it.
                Poller unregisters the channel for read and forks another thread(B) to handle close event.
                (see NioEndpoint:1239)
                Then atmosphere calls close on the connection in thread(C) and Tomcat receives internal action with code
                ActionCode.COMET_CLOSE and adds the channel to the Poller, which registers it for read again.
                (see Http11NioProcessor.java:462).
                The SocketChannel is still readable in case thread(B) hasn't invalidated the SelectionKey yet, so Poller
                in thread(A) initiates the closing process again and forks thread(D).
                Thread(B) completes the closing process and puts NioChannel and AttachmentKey into the corresponding
                caches.
                Then Thread(D) tries to close the channel again and realizes that it has already been closed (see
                AbstractProtocol.java:564) and puts the same NioChannel and AttachmentKey into caches.
                Caches become corrupted because they contain 2 references to the same object.
                Then any 2 subsequent requests may get the same NioChannel and AttachmentKey and some crazy stuff may
                happen (mixed up responses, etc).
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.tomcat.util.net.NioEndpoint.java</file>
        </fixedFiles>
    </bug>
    <bug id="50629" opendate="2011-01-21 07:23:54" fixdate="2011-01-21 08:17:49" resolution="Fixed">
        <buginformation>
            <summary>Make the bindThread/unbindThread method protected.
            </summary>
            <description>In the latest Tomcat codes, I saw that a new thread is created for listener start, filter start
                and loadOnStartup.
                But the two methods bindThread and unbindThread is marked as private, how about making those methods
                protected? So that other containers could use those two methods for their own initialization works.
                Thanks.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.core.StandardContext.java</file>
        </fixedFiles>
    </bug>
    <bug id="57959" opendate="2015-05-27 19:21:22" fixdate="2015-05-28 06:59:37" resolution="Fixed">
        <buginformation>
            <summary>Deadlock in FileHandler.java when log is rotated.
            </summary>
            <description>Tomcat 7 will deadlock when a log file is rotated.
                This issue has been fixed in latest tomcat 6 and tomcat 8 but not in tomcat 7.
                The issue is in following code (apache-tomcat-7.0.62-src/java/org/apache/juli/FileHandler.java:188)
                } finally {
                writerLock.writeLock().unlock();
                // Down grade to read-lock.
                This ensures the writer remains valid
                // until the log message is written
                writerLock.readLock().lock();
                }
                }
                Despite the comment the order is wrong, you have to acquire a read lock then release write lock to make
                sure the race condition does not occur.
                The correct way to do it is:
                } finally {
                writerLock.readLock().lock();
                writerLock.writeLock().unlock();
                }
                }
                In tomcat 6 (apache-tomcat-6.0.44-src/java/org/apache/juli/FileHandler.java:187) and tomcat 8
                (apache-tomcat-8.0.22-src/java/org/apache/juli/FileHandler.java:186) the order is correct.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.juli.FileHandler.java</file>
        </fixedFiles>
    </bug>
    <bug id="53450" opendate="2012-06-21 18:22:10" fixdate="2012-06-25 16:41:00" resolution="Fixed">
        <buginformation>
            <summary>Deployment of an application with 'ROOT' context hangs forever.
            </summary>
            <description>Hi,
                The scenario is the following:
                - I have running Tomcat 7.0.28
                - I deploy an application with 'ROOT' context
                - The deployment hangs forever.
                When taking a tread dump, the following is suspicious:
                "localhost-startStop-2" daemon prio=6 tid=0x0000000006607800 nid=0x2560 waiting on condition
                [0x0000000009ebf000]
                java.lang.Thread.State: WAITING (parking)
                at sun.misc.Unsafe.park(Native Method)
                - parking to wait for#x00000007d80f0108> (a
                java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync)
                at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
                at
                java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)
                at
                java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:842)
                at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1178)
                at java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock.lock(ReentrantReadWriteLock.java:807)
                at org.apache.catalina.core.ContainerBase.removeContainerListener(ContainerBase.java:1071)
                at org.apache.catalina.core.StandardEngine$AccessLogListener.uninstall(StandardEngine.java:463)
                at org.apache.catalina.core.StandardEngine$AccessLogListener.containerEvent(StandardEngine.java:505)
                at org.apache.catalina.core.ContainerBase.fireContainerEvent(ContainerBase.java:1431)
                at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:907)
                at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:875)
                at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:618)
                at org.apache.catalina.startup.HostConfig.deployDirectory(HostConfig.java:1100)
                at org.apache.catalina.startup.HostConfig$DeployDirectory.run(HostConfig.java:1618)
                at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
                at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
                at java.util.concurrent.FutureTask.run(FutureTask.java:138)
                at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
                at java.lang.Thread.run(Thread.java:662)
                As you can see
                - A read lock is acquired at
                org.apache.catalina.core.ContainerBase.fireContainerEvent(ContainerBase.java:1425)
                - Then every listener is invoked to process the container event
                - Then AccessLogListener is invoked
                org.apache.catalina.core.StandardEngine$AccessLogListener.uninstall(StandardEngine.java:463)
                - The latter causes a call for a write lock
                org.apache.catalina.core.ContainerBase.removeContainerListener(ContainerBase.java:1071)
                - Unfortunately in the ReentrantReadWriteLock it is stated that upgrade from read to write lock is not
                possible
                "Reentrancy also allows downgrading from the write lock to a read lock, by acquiring the write lock,
                then the read lock and then releasing the write lock.
                However, upgrading from a read lock to the write lock is not possible."
                Regards
                Violeta Georgieva
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.core.ContainerBase.java</file>
        </fixedFiles>
    </bug>
    <bug id="57420" opendate="2015-01-06 19:53:53" fixdate="2015-01-22 20:57:48" resolution="Fixed">
        <buginformation>
            <summary>Wrong class names generated since URL_ENCODER in DirContextURLConnection is not thread safe.
            </summary>
            <description>We've been debugging this for a long time.
                Sometimes when tomcat starts up, it will fail to deploy a webapp since it cannot find a certain class.
                That class name is always complete garbage.
                The error stack trace always is:
                SEVERE: Unable to process resource element
                [jndi:/localhost/testapp/WEB-INF/classes/ch/blabli/dNlewrP.lculgiansDisalog$3.class] for annotations
                java.io.FileNotFoundException:
                jndi:/localhost/testapp/WEB-INF/classes/ch/blabli/dNlewrP.lculgiansDisalog$3.class
                at org.apache.naming.resources.DirContextURLConnection.getInputStream(DirContextURLConnection.java:389)
                From time to time we see, that the referenced class name is actually a mix of the real class name and
                one or more other classes that exist in the vicinity of that class.
                Finally I was able to catch this exception with the debugger.
                What I see is that the problem stems from
                rg.apache.catalina.startup.ContextConfig.processAnnotationsJndi(ContextConfig.java:1986):
                - dcUrlConn (DirContextURLConnection) contains the correct entries of the WAR file
                - Enumeration
                #String>
                dirs = dcUrlConn.list() however does not.
                Calling list() again from the debugger yields the correct results.
                Here's the contents of that dirs variable:
                [lCsosn, oCrAopnpfliigcEadtiitoonr$AEpdpiltiocraMtoidoen.$c2l.acslsass, , tCioonnfi$gu4r.aticolnaLsosad,
                liconaftgiuroatni$o5n.LocadlLaisstsen, cCaotnifiognur$a6.clastsi, lCaonsfsig, s, cluaiss,
                CwoindgfetisegtEditorApplication$9.class, ConfigEditorApplication$ApplicationConfigLoadListener.class,
                ConfigEditorApplication$ConfigEditorToParameterHandlerIntegration.class,
                ConfigEditorApplication$EditorMode$1.class, ConfigEditorApplication$EditorMode$2.class,
                ConfigEditorApplication$EditorMode.class, ConfigEditorApplication.class, ConfigLoader$1.class,
                ConfigLoader.class, ConfigurationLoadListener$ConfigType.class,
                ConfigurationLoadListener$ConfigurationLoadedEvent.class,
                ConfigurationLoadListener$FailedToLoadConfigurationEvent.class, ConfigurationLoadListener.class,
                command, data, field, rendering, session, ui, util, widgetset]
                walking into the list() command leads to collection.list("/") which in turn is encoded using an
                URL_ENCODER (class UEncoder).
                This URL_ENCODER is *not* thread safe and can result in exactly such garbage if used concurrently.
                It is interesting to note that every single failure we had at this step was always caused by classes in
                the WEB-INF/classes directory, never in JAR files in WEB-INF/lib.
                To me it appears that there are two different DirContextURLConnections with rare concurrency issues when
                both use the same URL_ENCODER.
                Here's the full stacktrace when the error occurs:
                at org.apache.naming.resources.DirContextURLConnection.getInputStream(DirContextURLConnection.java:389)
                at org.apache.catalina.startup.ContextConfig.processAnnotationsJndi(ContextConfig.java:1994)
                at org.apache.catalina.startup.ContextConfig.processAnnotationsJndi(ContextConfig.java:1986)
                at org.apache.catalina.startup.ContextConfig.processAnnotationsJndi(ContextConfig.java:1986)
                at org.apache.catalina.startup.ContextConfig.processAnnotationsJndi(ContextConfig.java:1986)
                at org.apache.catalina.startup.ContextConfig.processAnnotationsJndi(ContextConfig.java:1986)
                at org.apache.catalina.startup.ContextConfig.processAnnotationsJndi(ContextConfig.java:1986)
                at org.apache.catalina.startup.ContextConfig.processAnnotationsJndi(ContextConfig.java:1986)
                at org.apache.catalina.startup.ContextConfig.processAnnotationsUrl(ContextConfig.java:1902)
                at org.apache.catalina.startup.ContextConfig.webConfig(ContextConfig.java:1298)
                at org.apache.catalina.startup.ContextConfig.configureStart(ContextConfig.java:876)
                at org.apache.catalina.startup.ContextConfig.lifecycleEvent(ContextConfig.java:374)
                at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:117)
                at org.apache.catalina.util.LifecycleBase.fireLifecycleEvent(LifecycleBase.java:90)
                at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:5378)
                at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:150)
                at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:901)
                at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:877)
                at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:649)
                at org.apache.catalina.startup.HostConfig.deployWAR(HostConfig.java:1083)
                at org.apache.catalina.startup.HostConfig$DeployWar.run(HostConfig.java:1880)
                at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
                at java.util.concurrent.FutureTask.run(FutureTask.java:262)
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
                at java.lang.Thread.run(Thread.java:744)
                Jan 06, 2015 3:43:44 PM org.apache.catalina.startup.ContextConfig processAnnotationsJndi
                SEVERE: Unable to process resource element
                [jndi:/localhost/medusa-config-editor-5.48.0/WEB-INF/classes/ch/ergon/medusa/configeditor/ui/dialog/TeerstRelgeaxsDialog.class]
                for annotations
                java.io.FileNotFoundException:
                jndi:/localhost/medusa-config-editor-5.48.0/WEB-INF/classes/ch/ergon/medusa/configeditor/ui/dialog/TeerstRelgeaxsDialog.class
                at org.apache.naming.resources.DirContextURLConnection.getInputStream(DirContextURLConnection.java:389)
                at org.apache.catalina.startup.ContextConfig.processAnnotationsJndi(ContextConfig.java:1994)
                at org.apache.catalina.startup.ContextConfig.processAnnotationsJndi(ContextConfig.java:1986)
                at org.apache.catalina.startup.ContextConfig.processAnnotationsJndi(ContextConfig.java:1986)
                at org.apache.catalina.startup.ContextConfig.processAnnotationsJndi(ContextConfig.java:1986)
                at org.apache.catalina.startup.ContextConfig.processAnnotationsJndi(ContextConfig.java:1986)
                at org.apache.catalina.startup.ContextConfig.processAnnotationsJndi(ContextConfig.java:1986)
                at org.apache.catalina.startup.ContextConfig.processAnnotationsJndi(ContextConfig.java:1986)
                at org.apache.catalina.startup.ContextConfig.processAnnotationsUrl(ContextConfig.java:1902)
                at org.apache.catalina.startup.ContextConfig.webConfig(ContextConfig.java:1298)
                at org.apache.catalina.startup.ContextConfig.configureStart(ContextConfig.java:876)
                at org.apache.catalina.startup.ContextConfig.lifecycleEvent(ContextConfig.java:374)
                at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:117)
                at org.apache.catalina.util.LifecycleBase.fireLifecycleEvent(LifecycleBase.java:90)
                at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:5378)
                at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:150)
                at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:901)
                at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:877)
                at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:649)
                at org.apache.catalina.startup.HostConfig.deployWAR(HostConfig.java:1083)
                at org.apache.catalina.startup.HostConfig$DeployWar.run(HostConfig.java:1880)
                at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
                at java.util.concurrent.FutureTask.run(FutureTask.java:262)
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
                at java.lang.Thread.run(Thread.java:744)
                Jan 06, 2015 3:43:44 PM org.apache.catalina.startup.ContextConfig processAnnotationsJndi
                SEVERE: Unable to process resource element
                [jndi:/localhost/medusa-config-editor-5.48.0/WEB-INF/classes/ch/ergon/medusa/configeditor/ui/dialog/aTsesxtAreaDialog$2.class]
                for annotations
                java.io.FileNotFoundException:
                jndi:/localhost/medusa-config-editor-5.48.0/WEB-INF/classes/ch/ergon/medusa/configeditor/ui/dialog/aTsesxtAreaDialog$2.class
                at org.apache.naming.resources.DirContextURLConnection.getInputStream(DirContextURLConnection.java:389)
                at org.apache.catalina.startup.ContextConfig.processAnnotationsJndi(ContextConfig.java:1994)
                at org.apache.catalina.startup.ContextConfig.processAnnotationsJndi(ContextConfig.java:1986)
                at org.apache.catalina.startup.ContextConfig.processAnnotationsJndi(ContextConfig.java:1986)
                at org.apache.catalina.startup.ContextConfig.processAnnotationsJndi(ContextConfig.java:1986)
                at org.apache.catalina.startup.ContextConfig.processAnnotationsJndi(ContextConfig.java:1986)
                at org.apache.catalina.startup.ContextConfig.processAnnotationsJndi(ContextConfig.java:1986)
                at org.apache.catalina.startup.ContextConfig.processAnnotationsJndi(ContextConfig.java:1986)
                at org.apache.catalina.startup.ContextConfig.processAnnotationsUrl(ContextConfig.java:1902)
                at org.apache.catalina.startup.ContextConfig.webConfig(ContextConfig.java:1298)
                at org.apache.catalina.startup.ContextConfig.configureStart(ContextConfig.java:876)
                at org.apache.catalina.startup.ContextConfig.lifecycleEvent(ContextConfig.java:374)
                at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:117)
                at org.apache.catalina.util.LifecycleBase.fireLifecycleEvent(LifecycleBase.java:90)
                at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:5378)
                at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:150)
                at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:901)
                at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:877)
                at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:649)
                at org.apache.catalina.startup.HostConfig.deployWAR(HostConfig.java:1083)
                at org.apache.catalina.startup.HostConfig$DeployWar.run(HostConfig.java:1880)
                at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
                at java.util.concurrent.FutureTask.run(FutureTask.java:262)
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
                at java.lang.Thread.run(Thread.java:744)
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.naming.resources.DirContextURLConnection.java</file>
        </fixedFiles>
    </bug>
    <bug id="49144" opendate="2010-04-16 21:47:20" fixdate="2010-04-23 10:00:45" resolution="Fixed">
        <buginformation>
            <summary>Incorrect lazy initialization and update of static fields?
            </summary>
            <description>Incorrect lazy initialization and update of static fields:
                org.apache.catalina.startup.Bootstrap.daemon
                org.apache.catalina.startup.ContextConfig.contextDigester
                Findbugs says:
                This method contains an unsynchronized lazy initialization of a static field.
                After the field is set, the object stored into that location is further updated or accessed.
                The setting of the field is visible to other threads as soon as it is set.
                If the futher accesses in the method that set the field serve to initialize the object, then you have a
                very serious multithreading bug, unless something else prevents any other thread from accessing the
                stored object until it is fully initialized.
                Even if you feel confident that the method is never called by multiple threads, it might be better to
                not set the static field until the value you are setting it to is fully populated/initialized.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.startup.Bootstrap.java</file>
        </fixedFiles>
    </bug>
    <bug id="52259" opendate="2011-11-28 19:32:12" fixdate="2011-12-02 08:58:03" resolution="Fixed">
        <buginformation>
            <summary>synchonization issues and dead lock if no realm is presented in configuration.
            </summary>
            <description>Problem:
                Tomcat 7.0.23 startup freezes at "INFO: Deploying web application directory ..."
                Nuances:
                - no errors are displayed in logs
                - tomcat process can't be shutted down with shutdown.sh script
                - http request on 80 port wait forever
                - it's a multi host configuration
                - the same setup works well on all previous build, including 7.0.22
                - when lib/*.jar files of tomcat-7.0.23 are replaced with lib/*.jar files of tomcat-7.0.23 - everything
                works just fine
                fix it asap!
                thank you!
                fill free to contact for details: a19596@mail.ru
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.core.ContainerBase.java</file>
            <file type="M">org.apache.catalina.core.StandardContext.java</file>
        </fixedFiles>
    </bug>
    <bug id="45453" opendate="2008-07-22 02:25:02" fixdate="2008-08-14 02:37:32" resolution="Fixed">
        <buginformation>
            <summary>JDBCRealm.getRoles bad synchronization causes hangs w/ DIGEST authentication.
            </summary>
            <description>JDBCRealm.getRoles bad synchronization causes hangs w/ DIGEST authentication.
                JDBCRealm caches PreparedStatement preparedRoles.
                That, and missing synchronization in JDBCRealm and/or DigestAuthenticator allow two threads to call
                getRoles simultaneously so that T1 will do stmt.executeQuery() while T2 does stmt.setString(1, userName)
                plus another .executeQuery() on the same PreparedStatement object.
                In the worst case, the JDBC driver gets confused by this, and blocks forever waiting for server
                response, causing all other threads that try to access DB hang.
                (This was observed with PostgreSQL 8.3-603-jdbc4 JDBC driver)
                org.postgresql.jdbc2.AbstractJdbc2Statement.execute(AbstractJdbc2Statement.java:451)
                org.postgresql.jdbc2.AbstractJdbc2Statement.executeWithFlags(AbstractJdbc2Statement.java:350)
                org.postgresql.jdbc2.AbstractJdbc2Statement.executeQuery(AbstractJdbc2Statement.java:254)
                org.apache.catalina.realm.JDBCRealm.getRoles(JDBCRealm.java:631)
                org.apache.catalina.realm.JDBCRealm.getPrincipal(JDBCRealm.java:596)
                org.apache.catalina.realm.RealmBase.authenticate(RealmBase.java:399)
                org.apache.catalina.authenticator.DigestAuthenticator.findPrincipal(DigestAuthenticator.java:283)
                org.apache.catalina.authenticator.DigestAuthenticator.authenticate(DigestAuthenticator.java:176)
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.realm.JDBCRealm.java</file>
        </fixedFiles>
    </bug>
    <bug id="49730" opendate="2010-08-09 16:16:04" fixdate="2010-11-04 10:28:20" resolution="Fixed">
        <buginformation>
            <summary>Race condition in StandardThreadExecutor : requests are sometimes enqueued instead of creating new
                threads.
            </summary>
            <description>In tomcat 6, I often configure an Executor with minSpareThreads=0 to work around memory leak
                issues upon redeployment.
                Sometimes (especially in development), when I refresh a page of my webapp with Safari, Chrome or
                Firefox, some resources of the page take several seconds (>10s) to be served though they are static
                resources and should come in less than 50ms.
                For instance, over 15 requests for a page (1 for html, the others for resources like js, css,
                images...), I sometimes have 1 or 2 that take >10s.
                After analysis, I found that in
                org.apache.catalina.core.StandardThreadExecutor.TaskQueue.offer(Runnable) the statement
                if (parent.getActiveCount()#(parent.getPoolSize())) is sometimes true unexpectedly.
                Here is the scenario :
                - ThreadPoolExecutor is empty
                - the user refreshes the page (or accesses it with an empty cache) in his web browser for a page that
                uses a more than 10-15 resources
                - the browser establishes one TCP connection and a new Thread is created
                - after the browser receives the response, it decides to load as many resources as possible in parallel.
                For this it establishes up to 6 TCP connections (in my tests)
                - The Acceptor thread calls StandardThreadExecutor.execute to process each incoming connection.
                - For each call, StandardThreadExecutor.TaskQueue.offer(Runnable) is being called
                - if you study the sources of Java 6 ThreadPoolExecutor, you can see that there's a small delay between
                the time a new Thread is created (thus increasing poolSize) and the time it starts working on its first
                task (increasing the activeCount)
                - Since in my case connections are established in a rapid burst, the calls to TaskQueue.offer() are
                sometimes faster than this small delay, so that we do have parent.getActiveCount()#parent.getPoolSize()
                and thus the task is enqueued instead of forcing the creation of a thread to serve it.
                - Since Keep-Alive is enabled and tomcat 6 threads take care of only one TCP connection at a time, the
                requests in the queue must wait for the keep-alive timeout so that a Thread is returned to the pool to
                serve pending tasks.
                - With 25s keepAliveTimeOut, it means that some requests take more than 25s to be served eventhough the
                pool was never full and the server quite idle!!!
                Other facts about this issue :
                - Although my test case starts with an empty pool, it can occur even on a loaded server.
                The thing that triggers the issue is the burst of new TCP connections.
                - The problem is less severe with a lower keepAliveTimeout, or if keepalive is disabled.
                In any case, it also depends on the time taken to serve current requests.
                - The issue also affects tomcat 7 but is less severe because tc7 threads are returned to the pool after
                each http request, even if the TCP connection is kept alive.
                The impact would the same as with tc6 with keepAlive disabled.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.tomcat.util.threads.TaskQueue.java</file>
            <file type="M">org.apache.tomcat.util.threads.ThreadPoolExecutor.java</file>
        </fixedFiles>
    </bug>
    <bug id="58946" opendate="2016-01-30 12:05:04" fixdate="2016-02-01 09:43:15" resolution="Fixed">
        <buginformation>
            <summary>ApplicationHttpRequest should enforce immutability of ParameterMap.
            </summary>
            <description>JavaDoc for ServletRequest.getParameterMap(), both ours and the one at Oracle site [1] says:
                "Returns: an immutable java.util.Map containing [...]"
                The problem is that this immutability is not enforced by org.apache.catalina.core.ApplicationHttpRequest
                class that is used to implement included or forwarded requests.
                Note that org.apache.catalina.util.ParameterMap class used for usual (not forwarded) requests does
                enforce immutability.
                An example of message from that class is shown below.
                Reproducible with current Tomcat 8.0-dev.
                Steps to reproduce
                --------------------
                1.
                Put the following two JSPs into ROOT web application:
                test.jsp
                [[[
                %@page contentType="text/plain;charset=UTF-8" import="java.util.*"%>
                %
                RequestDispatcher rd = request.getRequestDispatcher("test2.jsp");
                rd.forward(request, response);
                %>
                ]]]
                test2.jsp
                [[[
                %@page contentType="text/plain;charset=UTF-8" import="java.util.*"%>
                %
                Map map = request.getParameterMap();
                map.put("foo", "bar");
                %>
                %= map %>
                ]]]
                2.
                Call
                http://localhost:8080/test.jsp?z=a
                Actual:
                --------
                The following response is observed:
                [[[
                {foo=bar, z=[Ljava.lang.String;@3877a5}
                ]]]
                Expected:
                ---------
                If I call the test2.jsp application directly, the behaviour is as expected:
                http://localhost:8080/test2.jsp?z=a
                HTTP Status 500 - An exception occurred processing JSP page /test2.jsp at line 4
                root cause
                java.lang.IllegalStateException: No modifications are allowed to a locked ParameterMap
                org.apache.catalina.util.ParameterMap.put(ParameterMap.java:164)
                org.apache.jsp.test2_jsp._jspService(test2_jsp.java:115)
                ------------------------
                I noticed this issue while performing code review, inspired by a thread started 2016-01-19 on users
                mailing list [2].
                [1] http://docs.oracle.com/javaee/7/api/javax/servlet/ServletRequest.html#getParameterMap--
                [2] http://tomcat.markmail.org/thread/3hq4fghtoxcj44i5
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.core.ApplicationHttpRequest.java</file>
            <file type="M">org.apache.catalina.core.TestApplicationHttpRequest.java</file>
        </fixedFiles>
    </bug>
    <bug id="55521" opendate="2013-09-04 10:24:05" fixdate="2013-09-05 16:28:21" resolution="Fixed">
        <buginformation>
            <summary>Race Condition in HttpSession#invalidate() / HttpServletRequest#getSession(boolean).
            </summary>
            <description>Created attachment 30798 [details]
                code flow that exhibits the race condition
                For session fixation protection, we have to discard a user's session and create a new one whenever the
                user's login state changes.
                For this we rely on Spring Security's SessionFixationProtectionStrategy that, at its core, uses the
                following commands:
                session.invalidate();
                session = request.getSession(true);
                Yesterday, we had a message in the log that indicates the latter command returned the same session that
                was invalidated in the line before:
                "Your servlet container did not change the session ID when a new session was created.
                You will not be adequately protected against session-fixation attacks (catalina-exec-339,
                org.springframework.security.web.authentication.session.SessionFixationProtectionStrategy,
                SessionFixationProtectionStrategy.java:102)"
                When I investigated this issue, I found there is in fact a race condition if two threads (associated
                with requests from the same client) enter the session fixation protection code in parallel.
                I attached a TXT file that illustrates the code flow that leads to the race condition: When thread B
                calls session.invalidate(), the call returns immediately becuase the session is already in the
                "expiring" state.
                Since the session is not invalid yet, the call to request.getSession(true) won't create a new session,
                though.
                So in effect, thread B cannot obtain a new session.
                The documentation at http://tomcat.apache.org/tomcat-7.0-doc/servletapi/ has no indication that a
                session may not yet be invalid when session.invalidate() returns.
                The session interface neither provides a way to detect "expiring" session.
                The error message appears only once in the production log files that go some weeks back, so it seems to
                be an infrequent event.
                Nevertheless, it should be possible to implement session fixation without a race condition.
                Regards
                Christoph
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.ha.session.DeltaSession.java</file>
            <file type="M">org.apache.catalina.session.StandardSession.java</file>
        </fixedFiles>
    </bug>
    <bug id="50459" opendate="2010-12-11 19:06:16" fixdate="2011-01-05 09:18:11" resolution="Fixed">
        <buginformation>
            <summary>StandardContext.bindThread() and unbindThread() are not symmetrical and not limited to current
                thread.
            </summary>
            <description>As a side effect of my commit, I think I found a bug in StandardContext.bindThread() and
                unbindThread() methods in both tomcat 6 and 7 :
                - the methods should be symmetrical : unbindThread should restore the Thread CCL after calling
                DirContextURLStreamHandler.unbind() and ContextBindings.unbindThread()
                - StandardContext.bindThread() should call DirContextURLStreamHandler.bindThread() instead of
                DirContextURLStreamHandler.bind()
                - StandardContext.unbindThread() should call DirContextURLStreamHandler.unbindThread() instead of
                DirContextURLStreamHandler.unbind()
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.core.StandardContext.java</file>
        </fixedFiles>
    </bug>
    <bug id="51545" opendate="2011-07-23 09:07:02" fixdate="2011-07-26 09:43:49" resolution="Fixed">
        <buginformation>
            <summary>make threadname available in ExtendedAccessLogValve.
            </summary>
            <description>Created attachment 27309 [details]
                Threadname usable in ExtendedAccessLogValve.
                As discussed on the tomcat-users mailinglist.
                http://old.nabble.com/thread-name-in-extended-access-log-valve-tt32101677.html
                make the threadname of the thread, that served the request, available to the ExtendedAccessLogValve.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.valves.ExtendedAccessLogValve.java</file>
        </fixedFiles>
    </bug>
    <bug id="51197" opendate="2011-05-13 12:50:26" fixdate="2012-03-05 12:25:29" resolution="Fixed">
        <buginformation>
            <summary>sendError/sendRedirect don't work with AsyncContext.
            </summary>
            <description>For some reason, response.sendRedirect and response.sendError are not producing expected
                behavior when using an AsyncContext.
                On the server side, the async request completes normally, and everything looks hunky dory, but the
                client never gets ANY response from the server.
                It appears that Tomcat simply times out after the default 10 seconds and closes the connection.
                No headers are returned, no content, nothing.
                I've confirmed with thread dumps that the server isn't stuck.
                Yet...if you use response.setStatus and response.setHeader instead, it works absolutely fine.
                The client gets the response every time.
                I put together a very simple test that isolates the issue reliably:
                https://github.com/dcheckoway/async-test
                See the README in there for details.
                NOTE: the behavior for sendError changed slightly from 7.0.12 to 7.0.14.
                As of 7.0.14, sendError seems to close the connection immediately, but the client still never gets any
                response of any kind.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.valves.ErrorReportValve.java</file>
            <file type="M">org.apache.catalina.core.TestAsyncContextImpl.java</file>
            <file type="M">org.apache.catalina.connector.CoyoteAdapter.java</file>
            <file type="M">org.apache.catalina.core.TestAsyncContextImpl.java</file>
        </fixedFiles>
    </bug>
    <bug id="50306" opendate="2010-11-19 18:09:10" fixdate="2012-07-02 14:31:27" resolution="Fixed">
        <buginformation>
            <summary>Detect stuck threads.
            </summary>
            <description>Feature request :
                regularly scan worker threads and if one has been processing the same request for longer than a
                configurable delay, log a warning with the stack trace of that thread.
                This would allow to detect very long running threads, usually the ones that are stuck in a network call
                or in a deadlock.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.valves.LocalStrings_es.properties</file>
            <file type="M">org.apache.catalina.valves.StuckThreadDetectionValve.java</file>
            <file type="M">org.apache.catalina.valves.StuckThreadDetectionValve.java</file>
        </fixedFiles>
    </bug>
    <bug id="55524" opendate="2013-09-04 23:34:13" fixdate="2014-09-18 03:09:51" resolution="Fixed">
        <buginformation>
            <summary>Deadlock produced during Websocket write operation (org.apache.catalina.websocket.WsOutbound).
            </summary>
            <description>Created attachment 30800 [details]
                thread dump of the deadlock.
                I use Tomcat with an application that uses Websockets.
                The websockets are handled by the Atmosphere framework: https://github.com/Atmosphere/atmosphere
                A deadlock occurs during write operations to the websocket.
                I have attached a thread dump of the deadlock.
                This issue is also duscussed here:
                https://github.com/Atmosphere/atmosphere/issues/1264
                Here are the deadlocked threads:
                Found one Java-level deadlock:
                =============================
                "Atmosphere-Shared-AsyncOp-267":
                waiting to lock monitor 0x00007efebc0015f8 (object 0x00000005ef4c6988, a
                org.apache.catalina.websocket.WsOutbound),
                which is held by "Atmosphere-Scheduler-2"
                "Atmosphere-Scheduler-2":
                waiting to lock monitor 0x00007efe8c290ac0 (object 0x00000005ef4b77f8, a
                org.atmosphere.cpr.AtmosphereResourceImpl),
                which is held by "Atmosphere-Shared-AsyncOp-267"
                Java stack information for the threads listed above:
                ===================================================
                "Atmosphere-Shared-AsyncOp-267":
                at org.apache.catalina.websocket.WsOutbound.writeTextMessage(WsOutbound.java:165)
                - waiting to lock#x00000005ef4c6988> (a org.apache.catalina.websocket.WsOutbound)
                at org.atmosphere.container.version.TomcatWebSocket.write(TomcatWebSocket.java:49)
                at org.atmosphere.websocket.WebSocket.write(WebSocket.java:199)
                at org.atmosphere.websocket.WebSocket.write(WebSocket.java:168)
                at org.atmosphere.websocket.WebSocket.write(WebSocket.java:40)
                at org.atmosphere.cpr.AtmosphereResponse$2.write(AtmosphereResponse.java:574)
                at
                org.atmosphere.handler.AbstractReflectorAtmosphereHandler.onStateChange(AbstractReflectorAtmosphereHandler.java:169)
                at org.atmosphere.cpr.DefaultBroadcaster.invokeOnStateChange(DefaultBroadcaster.java:1027)
                at org.atmosphere.cpr.DefaultBroadcaster.prepareInvokeOnStateChange(DefaultBroadcaster.java:1047)
                at org.atmosphere.cpr.DefaultBroadcaster.executeAsyncWrite(DefaultBroadcaster.java:921)
                at org.atmosphere.cpr.DefaultBroadcaster$3.run(DefaultBroadcaster.java:580)
                - locked#x00000005ef4b77f8> (a org.atmosphere.cpr.AtmosphereResourceImpl)
                at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
                at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
                at java.util.concurrent.FutureTask.run(FutureTask.java:166)
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
                at java.lang.Thread.run(Thread.java:724)
                "Atmosphere-Scheduler-2":
                at org.atmosphere.cpr.AsynchronousProcessor.completeLifecycle(AsynchronousProcessor.java:476)
                - waiting to lock#x00000005ef4b77f8> (a org.atmosphere.cpr.AtmosphereResourceImpl)
                at org.atmosphere.cpr.AsynchronousProcessor.timedout(AsynchronousProcessor.java:437)
                at
                org.atmosphere.cpr.AsynchronousProcessor$AsynchronousProcessorHook.timedOut(AsynchronousProcessor.java:633)
                at org.atmosphere.websocket.DefaultWebSocketProcessor.close(DefaultWebSocketProcessor.java:483)
                at org.atmosphere.container.TomcatWebSocketHandler.onClose(TomcatWebSocketHandler.java:80)
                at org.apache.catalina.websocket.StreamInbound.doOnClose(StreamInbound.java:222)
                at org.apache.catalina.websocket.WsOutbound.doWriteBytes(WsOutbound.java:423)
                at org.apache.catalina.websocket.WsOutbound.doWriteText(WsOutbound.java:442)
                at org.apache.catalina.websocket.WsOutbound.writeTextMessage(WsOutbound.java:174)
                - locked#x00000005ef4c6988> (a org.apache.catalina.websocket.WsOutbound)
                at org.atmosphere.container.version.TomcatWebSocket.write(TomcatWebSocket.java:49)
                at org.atmosphere.websocket.WebSocket.write(WebSocket.java:199)
                at org.atmosphere.websocket.WebSocket.write(WebSocket.java:168)
                at org.atmosphere.websocket.WebSocket.write(WebSocket.java:40)
                at org.atmosphere.cpr.AtmosphereResponse$2.write(AtmosphereResponse.java:574)
                at org.atmosphere.cpr.AtmosphereResponse.write(AtmosphereResponse.java:992)
                at org.atmosphere.interceptor.HeartbeatInterceptor$1$1.call(HeartbeatInterceptor.java:104)
                at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
                at java.util.concurrent.FutureTask.run(FutureTask.java:166)
                at
                java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
                at
                java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
                at java.lang.Thread.run(Thread.java:724)
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.websocket.WsOutbound.java</file>
        </fixedFiles>
    </bug>
    <bug id="50138" opendate="2010-10-21 09:22:56" fixdate="2010-10-27 02:44:00" resolution="Fixed">
        <buginformation>
            <summary>Lack of synchronization in org.apache.catalina.security.SecurityUtil.
            </summary>
            <description>Symptom: all processor threads spin madly in:
                ==============
                "tomcat-processor-20" daemon prio=10 tid=0x09210800 nid=0x51fb runnable [0x61b76000]
                java.lang.Thread.State: RUNNABLE
                at java.util.HashMap.getEntry(HashMap.java:347)
                at java.util.HashMap.containsKey(HashMap.java:335)
                at org.apache.catalina.security.SecurityUtil.doAsPrivilege(SecurityUtil.java:227)
                at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:230)
                at org.apache.catalina.core.ApplicationFilterChain.access$000(ApplicationFilterChain.java:56)
                at org.apache.catalina.core.ApplicationFilterChain$1.run(ApplicationFilterChain.java:189)
                at java.security.AccessController.doPrivileged(Native Method)
                at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:185)
                ...
                ==============
                Cause: org.apache.catalina.security.SecurityUtil.objectCache is a HashMap, but access to it is not
                synchronized.
                The javadoc for HashMap says:
                =============
                Note that this implementation is not synchronized.
                If multiple threads access a hash map concurrently, and at least one of the threads modifies the map
                structurally, it must be synchronized externally.
                =============
                Proposed solution: change objectCache to ConcurrentHashMap;
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.security.SecurityUtil.java</file>
        </fixedFiles>
    </bug>
    <bug id="47524" opendate="2009-07-14 06:20:02" fixdate="2009-07-17 01:42:36" resolution="Fixed">
        <buginformation>
            <summary>McastServiceImpl executor is not dispatching events.
            </summary>
            <description>Created attachment 23977 [details]
                Fix proposal
                The class McastServiceImpl does not dispatch events, because the the executor is not creating threads.
                The class ReceiverBase has its own queue to address this issue.
                I have refactored the code a little to reuse the class.
                (attaching the diff file)
                I'm not sure if filing a bug in Bugzilla is the right procedure for Trunk branch.
                If not, could you please tell me how should I proceed?
                Regards,
                Ariel
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.tribes.membership.McastServiceImpl.java</file>
            <file type="M">org.apache.catalina.tribes.transport.ReceiverBase.java</file>
            <file type="M">org.apache.catalina.tribes.util.ExecutorFactory.java</file>
        </fixedFiles>
    </bug>
    <bug id="49905" opendate="2010-09-09 07:22:00" fixdate="2010-10-07 07:06:56" resolution="Fixed">
        <buginformation>
            <summary>In cluster, when using DeltaManager memory leak can occur.
            </summary>
            <description>Created attachment 26008 [details]
                Patch to remove context classloader from threads in ThreadPoolExecutor in tribes.
                Tested on apache-tomcat-6.0.29 running under jdk 1.6.0_18.
                When DeltaManager is instantiated and assigned in StandardContext.start(), is it done AFTER
                StandardContext.bindThreads().
                DeltaManager, in turn, during initalization, asks for sessions in other nodes, and this may result in
                creating threads in ThreadPoolExecutor in tribes.
                These threads created with contextClassLoader set to current webapplication WebAppClassLoader.
                This results in memory leak and error message during redeployment in tomcat log:
                09/09/2010 14:46:19 S - - WebappClassLoader.clearReferencesThreads:
                The web application [/creditdev] appears to have started a thread
                named [pool-1-thread-1] but has failed to stop it.
                This is very likely
                to create a memory leak.
                Stacktrace:
                at java.util.concurrent.ThreadPoolExecutor.addThread(Unknown Source)
                at java.util.concurrent.ThreadPoolExecutor.addIfUnderCorePoolSize(Unknown
                Source)
                at java.util.concurrent.ThreadPoolExecutor.execute(Unknown Source)
                at
                org.apache.catalina.tribes.group.interceptors.MessageDispatch15Interceptor.addToQueue(MessageDispatch15Interceptor.java:67)
                at
                org.apache.catalina.tribes.group.interceptors.MessageDispatchInterceptor.sendMessage(MessageDispatchInterceptor.java:68)
                at org.apache.catalina.tribes.group.ChannelInterceptorBase.sendMessage(ChannelInterceptorBase.java:75)
                at
                org.apache.catalina.tribes.group.interceptors.TcpFailureDetector.sendMessage(TcpFailureDetector.java:87)
                at org.apache.catalina.tribes.group.ChannelInterceptorBase.sendMessage(ChannelInterceptorBase.java:75)
                at org.apache.catalina.tribes.group.GroupChannel.send(GroupChannel.java:216)
                at org.apache.catalina.tribes.group.GroupChannel.send(GroupChannel.java:175)
                at org.apache.catalina.ha.tcp.SimpleTcpCluster.send(SimpleTcpCluster.java:813)
                at org.apache.catalina.ha.session.DeltaManager.getAllClusterSessions(DeltaManager.java:959)
                at org.apache.catalina.ha.session.DeltaManager.start(DeltaManager.java:930)
                at org.apache.catalina.core.ContainerBase.setManager(ContainerBase.java:438)
                at org.apache.catalina.core.StandardContext.start(StandardContext.java:4559)
                at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:791)
                at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:771)
                at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:546)
                at org.apache.catalina.startup.HostConfig.deployDescriptor(HostConfig.java:637)
                at org.apache.catalina.startup.HostConfig.deployDescriptors(HostConfig.java:563)
                at org.apache.catalina.startup.HostConfig.deployApps(HostConfig.java:498)
                at org.apache.catalina.startup.HostConfig.start(HostConfig.java:1277)
                at org.apache.catalina.startup.HostConfig.lifecycleEvent(HostConfig.java:321)
                at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:119)
                at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1053)
                at org.apache.catalina.core.StandardHost.start(StandardHost.java:785)
                at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1045)
                at org.apache.catalina.core.StandardEngine.start(StandardEngine.java:445)
                at org.apache.catalina.core.StandardService.start(StandardService.java:519)
                at org.apache.catalina.core.StandardServer.start(StandardServer.java:710)
                at org.apache.catalina.startup.Catalina.start(Catalina.java:581)
                at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
                at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
                at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
                at java.lang.reflect.Method.invoke(Unknown Source)
                at org.apache.catalina.startup.Bootstrap.start(Bootstrap.java:289)
                at org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:414)
                at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
                at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
                at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
                at java.lang.reflect.Method.invoke(Unknown Source)
                at org.tanukisoftware.wrapper.WrapperStartStopApp.run(WrapperStartStopApp.java:243)
                at java.lang.Thread.run(Unknown Source)
                Proposed solution - implement java.util.concurrent.ThreadFactory in
                MessageDispatch15Interceptor
                and pass instance on ThreadPoolExecutor executor creation.
                This instance must call setContextClassLoader(null) in newThread()
                overriden method.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.tribes.group.interceptors.MessageDispatch15Interceptor.java</file>
            <file type="M">org.apache.catalina.tribes.util.TcclThreadFactory.java</file>
        </fixedFiles>
    </bug>
    <bug id="48399" opendate="2009-12-16 19:20:55" fixdate="2009-12-19 19:46:09" resolution="Fixed">
        <buginformation>
            <summary>Lock fields should be final.
            </summary>
            <description>Created attachment 24715 [details]
                Patch lock field that should be final.
                A field that is used as a lock should be final.
                org.apache.catalina.ha.session.DeltaSession.diffLock is protected and mutable.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.ha.session.DeltaSession.java</file>
            <file type="M">org.apache.catalina.tribes.group.interceptors.OrderInterceptor.java</file>
            <file type="M">modules.jdbc-pool.java.org.apache.tomcat.jdbc.pool.MultiLockFairBlockingQueue.java</file>
        </fixedFiles>
    </bug>
    <bug id="55684" opendate="2013-10-21 16:30:04" fixdate="2013-10-29 09:56:13" resolution="Fixed">
        <buginformation>
            <summary>WebappClassLoader.getThread want an access to root ThreadGroup.
            </summary>
            <description>Tomcat may not have all permissions granted.
                The method WebappClassLoader.getThread try to access the root ThreadGroup in order to list all threads.
                You should either manage SecurityException or avoid to access the parent ThreadGroup of the thread which
                start Tomcat.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.loader.LocalStrings.properties</file>
            <file type="M">org.apache.catalina.loader.WebappClassLoader.java</file>
        </fixedFiles>
    </bug>
    <bug id="57681" opendate="2015-03-09 22:34:15" fixdate="2015-09-08 14:20:09" resolution="Fixed">
        <buginformation>
            <summary>Allow parallel class loading in web application class loader by synchronizing on class specific
                object.
            </summary>
            <description>Created attachment 32553 [details]
                Parallel classloading port from Tc8.0 into Tc7.0
                Related to fix in Tomcat8.0:
                Fix https://issues.apache.org/bugzilla/show_bug.cgi?id=56530
                Add a web application class loader implementation that supports the parallel loading of web application
                classes.
                Source file to patch:
                https://svn.apache.org/viewvc/tomcat/tc7.0.x/trunk/java/org/apache/catalina/loader/WebappClassLoader.java?revision=1661811
                view=markup
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.loader.ParallelWebappClassLoader.java</file>
            <file type="M">org.apache.catalina.loader.TestParallelWebappClassLoader.java</file>
            <file type="M">org.apache.tomcat.util.compat.JreCompat.java</file>
        </fixedFiles>
    </bug>
    <bug id="53624" opendate="2012-07-30 19:07:39" fixdate="2012-08-14 22:50:45" resolution="Fixed">
        <buginformation>
            <summary>sendRedirect doesn't work after a dispatch through the AsyncContext.
            </summary>
            <description>A call to sendRedirect after AsyncContext.dispatch doesn't redirect and returns with a 200
                status code instead.
                A project demonstrating the issue:
                https://github.com/rstoyanchev/dispatch-test
                Build and deploy the source in the 'tomcat-issue-repro'.
                From the home page of the deployed application, select the "Redirect" scenario.
                The scenario involves Servlet "RedirectA" that creates a thread and dispatches to Servlet "RedirectB",
                which in turn redirects to Servlet "RedirectC" and that forwards to a simple JSP page.
                Instead of the JSP page rendering, you should see a blank page and the log output will show that
                processing ended at ServletC.
                For details on the setup see WebAppInitializer.setupRedirectScenario(ServletContext).
                ---
                FWIW if a forward is used instead of a dispatch from the async thread, the scenario works.
                To do that, modify the line that creates servlet "RedirectA" to be ForwardingAsyncServlet instead of
                DispatchingAsyncServlet.
                ---
                This issue is very similar to 51197, which was marked resolved in 7.0.25.
                The current issue was tested against 7.0.29.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.core.ApplicationDispatcher.java</file>
        </fixedFiles>
    </bug>
    <bug id="57683" opendate="2015-03-10 14:54:40" fixdate="2015-03-11 22:19:18" resolution="Fixed">
        <buginformation>
            <summary>Crash of stockticket async example caused by an aborted client request.
            </summary>
            <description>I mentioned this issue in "Time for 7.0.60" thread on dev@.
                It is a bug with error handling in example webapp.
                It is not a regression from recent changes.
                It is reproducible with 7.0.59.
                Steps to reproduce:
                Using Tomcat 7.0.59, JDK 6u45.
                1.
                Start Tomcat
                2.
                Visit stockicker example,
                http://localhost:8080/examples/async/stockticker
                3.
                Abort the request while the page is being loaded (Press "Esc" key on keyboard)
                4.
                Re-visit the example
                http://localhost:8080/examples/async/stockticker
                Expected: Working stockicker example.
                Actual:
                1) Browser waits for a response.
                After several seconds the progress indicator stops.
                A blank page is displayed.
                1) Access log shows response status 200, but byte counter is zero ("-").
                127.0.0.1 - - [10/Mar/2015:17:45:31 +0300] "GET /examples/async/stockticker HTTP/1.1" 200 -
                1) Looking into catalina.2015-03-10.log, there is the following exception:
                [[[
                10.03.2015 17:36:58 org.apache.coyote.AbstractProcessor setErrorState
                INFO: An error occurred in processing while on a non-container thread.
                The connection will be closed immediately
                java.net.SocketException: Software caused connection abort: socket write error
                at java.net.SocketOutputStream.socketWrite0(Native Method)
                at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:92)
                at java.net.SocketOutputStream.write(SocketOutputStream.java:136)
                at org.apache.coyote.http11.InternalOutputBuffer.realWriteBytes(InternalOutputBuffer.java:215)
                at org.apache.tomcat.util.buf.ByteChunk.flushBuffer(ByteChunk.java:480)
                at org.apache.coyote.http11.InternalOutputBuffer.flush(InternalOutputBuffer.java:119)
                at org.apache.coyote.http11.AbstractHttp11Processor.action(AbstractHttp11Processor.java:800)
                at org.apache.coyote.Response.action(Response.java:172)
                at org.apache.catalina.connector.OutputBuffer.doFlush(OutputBuffer.java:363)
                at org.apache.catalina.connector.OutputBuffer.flush(OutputBuffer.java:331)
                at org.apache.catalina.connector.CoyoteWriter.flush(CoyoteWriter.java:98)
                at async.AsyncStockServlet.writeStock(AsyncStockServlet.java:98)
                at async.AsyncStockServlet.tick(AsyncStockServlet.java:81)
                at async.Stockticker.run(Stockticker.java:84)
                at java.lang.Thread.run(Thread.java:662)
                ]]]
                1) Looking at console,
                There is the same exception as in "3)",
                followed by the following exception:
                [[[
                java.lang.IllegalStateException: The request associated with the AsyncContext has already completed
                processing.
                at org.apache.catalina.core.AsyncContextImpl.check(AsyncContextImpl.java:553)
                at org.apache.catalina.core.AsyncContextImpl.getResponse(AsyncContextImpl.java:265)
                at async.AsyncStockServlet.writeStock(AsyncStockServlet.java:86)
                at async.AsyncStockServlet.tick(AsyncStockServlet.java:81)
                at async.Stockticker.run(Stockticker.java:84)
                at java.lang.Thread.run(Thread.java:662)
                ]]]
                Essentially, the async.Stockticker thread crashed due to a non handled ISE.
                This explains the behaviour.
                1) The exception in "4)" is logged to the console only.
                It is not logged into Tomcat log files.
            </description>
            <version>1.1.0</version>
            <fixedVersion>7.0.59</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">webapps.docs.changelog.xml</file>
            <file type="M">webapps.examples.WEB-INF.classes.async.AsyncStockServlet.java</file>
        </fixedFiles>
    </bug>
    <bug id="48172" opendate="2009-11-11 03:23:50" fixdate="2009-11-18 15:35:24" resolution="Fixed">
        <buginformation>
            <summary>JspRuntimeContext synch. problems.
            </summary>
            <description>JspRuntimeContext.jspReloadCount is synchronised when updated, but not when read by
                getJspReloadCount().
                If the instance is accessed from multiple threads, then the returned value may not be the current value,
                it could be arbitrarily stale.
                If this is acceptable, then the Javadoc should say so; otherwise the getter needs to be synch. (or the
                field needs to be volatile) Or just use AtomicInteger.
                Various other fields are neither final nor synchronized, e.g. classpath codeSource etc.
                As far as I can tell, these could easily be made final as they are only written by the constructor.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.jasper.compiler.JspRuntimeContext.java</file>
        </fixedFiles>
    </bug>
    <bug id="55309" opendate="2013-07-26 04:38:23" fixdate="2013-07-29 21:04:00" resolution="Fixed">
        <buginformation>
            <summary>Concurrent issue of TagPluginManager.
            </summary>
            <description>Created attachment 30629 [details]
                Patch on TagPluginManager.
                TagPluginManager is shared within ServletContext.
                But it contains a variable "PageInfo pageInfo".
                PageInfo should be aligned with specified page.
                PageInfo is used for holding page information, such as "Import".
                So when there are two pages are compiled in parallel.
                It encounters concurrent issue.
                One of the page can't be compiled unless the server is restarted.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.jasper.compiler.TagPluginManager.java</file>
        </fixedFiles>
    </bug>
    <bug id="51185" opendate="2011-05-11 10:21:44" fixdate="2011-05-20 22:03:36" resolution="Fixed">
        <buginformation>
            <summary>Performance: DataSourceProxy#createPool should use more fine grained synchronisation
            </summary>
            <description>Hello,
                I don't know if this is the right component.
                My Enhancement concerns new Tomcat JDBC POOL.
                Looking at code I think that DataSourceProxy#createPool should not be synchronized but use another
                private method that is synchronized and used only when creation is needed, something like that:
                /**
                * Sets up the connection pool, by creating a pooling driver.
                * @return Driver
                * @throws SQLException
                */
                public ConnectionPool createPool() throws SQLException {
                if (pool != null) {
                return pool;
                } else {
                return pCreatePool();
                }
                }
                /**
                *
                * @return
                * @throws SQLException
                */
                private synchronized ConnectionPool pCreatePool() throws SQLException {
                if (pool != null) {
                return pool;
                } else {
                pool = new ConnectionPool(poolProperties);
                return pool;
                }
                }
                Currently since createPool() is called for lots of getters we get this synchro IMPACT without really
                needing it.
                Regards
                Philippe
                http://www.ubik-ingenierie.com
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.tomcat.jdbc.pool.DataSourceProxy.java</file>
        </fixedFiles>
    </bug>
    <bug id="56577" opendate="2014-05-29 19:17:17" fixdate="2014-06-05 20:32:02" resolution="Fixed">
        <buginformation>
            <summary>Inappropriate executor in WsServerContainer.
            </summary>
            <description>The executor service to process SendHandler for sendAsync calls is initialized in
                WsServerContainer, which is using an unbounded queue.
                Thus, no more than corePoolSize threads will ever be created.
                (And the value of the maximumPoolSize therefore doesn't have any effect.) [1]
                The corePoolSize has a default value to be 1, though it could be changed by context parameter, it's
                still hard to find an optimized value.
                This will create an issue if using the SendHandler to close the session after sending the last message.
                An example stack trace is like the following:
                "WebSocketServer-/spring-websocket-test-1" daemon prio=10 tid=0x00007f9f040ec000 nid=0x7499 waiting on
                condition [0x00007f9f73af8000]
                java.lang.Thread.State: TIMED_WAITING (parking)
                at sun.misc.Unsafe.park(Native Method)
                - parking to wait for#x000000075d52f018> (a java.util.concurrent.CountDownLatch$Sync)
                at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
                at
                java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedNanos(AbstractQueuedSynchronizer.java:1033)
                at
                java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1326)
                at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:282)
                at org.apache.tomcat.websocket.FutureToSendHandler.get(FutureToSendHandler.java:93)
                at
                org.apache.tomcat.websocket.WsRemoteEndpointImplBase.startMessageBlock(WsRemoteEndpointImplBase.java:238)
                at org.apache.tomcat.websocket.WsSession.sendCloseMessage(WsSession.java:487)
                at org.apache.tomcat.websocket.WsSession.doClose(WsSession.java:418)
                - locked#x000000075d7dc388> (a java.lang.Object)
                at org.apache.tomcat.websocket.WsSession.close(WsSession.java:395)
                at org.apache.tomcat.websocket.WsSession.close(WsSession.java:389)
                at
                com.tango.test.spring.test.service.DefaultSessionManager$CloseSessionHandler.onResult(DefaultSessionManager.java:133)
                at
                org.apache.tomcat.websocket.WsRemoteEndpointImplBase$StateUpdateSendHandler.onResult(WsRemoteEndpointImplBase.java:1083)
                at org.apache.tomcat.websocket.WsRemoteEndpointImplBase.endMessage(WsRemoteEndpointImplBase.java:320)
                at
                org.apache.tomcat.websocket.WsRemoteEndpointImplBase$EndMessageHandler.onResult(WsRemoteEndpointImplBase.java:468)
                at
                org.apache.tomcat.websocket.server.WsRemoteEndpointImplServer$OnResultRunnable.run(WsRemoteEndpointImplServer.java:234)
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
                at java.lang.Thread.run(Thread.java:745)
                If all the threads are in this state, there will be no available threads to clear any SendHandler and
                all Remote.send methods would throw TimeoutException though actually the clients could receive the
                messages.
                [1] http://docs.oracle.com/javase/7/docs/api/java/util/concurrent/ThreadPoolExecutor.html
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.tomcat.websocket.server.WsServerContainer.java</file>
        </fixedFiles>
    </bug>
    <bug id="48234" opendate="2009-11-18 17:41:42" fixdate="2009-11-18 18:26:18" resolution="Fixed">
        <buginformation>
            <summary>org.apache.catalina.core.ContainerBase.getLogger() not thread-safe?
            </summary>
            <description>org.apache.catalina.core.ContainerBase.getLogger() is not synchronised, yet all the other
                methods which access org.apache.catalina.core.ContainerBase.logger are synch.
                This seems wrong.
                org.apache.catalina.core.ContainerBase.logger should probably be private to prevent unsynch.
                access by subclasses.
                Similar considerations apply to logName and logName().
                The Javadoc for getLogger() states that it can return null - that does not appear to be possible.
                [Not sure if the method ever returns the parent logger either.]
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.core.ContainerBase.java</file>
        </fixedFiles>
    </bug>
    <bug id="52091" opendate="2011-10-26 10:39:01" fixdate="2011-11-10 11:18:43" resolution="Fixed">
        <buginformation>
            <summary>TagHandlerPool is slow when high concurrently.
            </summary>
            <description>TagHandlerPool is slow when I ran stress test my web application using tomcat7.
                It occured in 7.0.19.
                Profiler said that it causes are followings:
                1.
                Lock of log instance at org.apache.jasper.runtime.TagHandlerPool.
                2.
                Lock of parameters field at org.apache.catalina.core.StandardWrapper.getInitParameter.
                An attached patch resolve this issue in my environment.
                It makes following changes:
                1.
                Makes log field to static.
                2.
                At StandardWrapper, changes lock policy from synchronized block to ReentrantReadWriteLock.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.core.StandardWrapper.java</file>
            <file type="M">org.apache.jasper.runtime.TagHandlerPool.java</file>
        </fixedFiles>
    </bug>
    <bug id="53173" opendate="2012-05-01 13:54:22" fixdate="2013-11-09 03:56:51" resolution="Fixed">
        <buginformation>
            <summary>maxConnections feature hangs the system.
            </summary>
            <description>Created attachment 28704 [details]
                fix missing count down for maxConnections latch
                We've run into a scenario where the JIO Acceptor thread hangs as connections are not counted down
                properly.
                Executor name="tomcatThreadPool"
                          namePrefix="tomcat-8080-"
                          minSpareThreads="50"
                          maxThreads="300"/>
                Connector port="8080"
                           redirectPort="${bio.https.port}"
                           protocol="org.apache.coyote.http11.Http11Protocol"
                           maxKeepAliveRequests="15"
                           executor="tomcatThreadPool"
                           connectionTimeout="20000"
                           acceptCount="100"/>
                Thread dump yields
                "http-bio-8080-Acceptor-0" daemon prio=3 tid=0xXXXXXXXX nid=0xXX waiting on condition
                [0xXXXXXXXX..0xXXXXXXXX]
                java.lang.Thread.State: WAITING (parking)
                at sun.misc.Unsafe.park(Native Method)
                - parking to wait for#xXXXXXXXX> (a org.apache.tomcat.util.threads.LimitLatch$Sync)
                at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
                at
                java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:747)
                at
                java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:905)
                at
                java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1217)
                at org.apache.tomcat.util.threads.LimitLatch.countUpOrAwait(LimitLatch.java:99)
                at org.apache.tomcat.util.net.AbstractEndpoint.countUpOrAwaitConnection(AbstractEndpoint.java:660)
                at org.apache.tomcat.util.net.JIoEndpoint$Acceptor.run(JIoEndpoint.java:210)
                at java.lang.Thread.run(Thread.java:619)
                This, as you may imagine, is a fairly hard use case to reproduce into a simple test case.
                The easiest way to reproduce it is to create the following configuration
                Executor name="tomcatThreadPool"
                          namePrefix="catalina-exec-"
                          maxThreads="5"
                          minSpareThreads="0"
                          maxQueueSize="15"/>
                Connector port="8080"
                           protocol="HTTP/1.1" executor="tomcatThreadPool"
                           connectionTimeout="10000"
                           redirectPort="8443"
                           maxConnections="30"/>
                This reproduces one test case, where the state machine is not taking into account that connections may
                be rejected by the queue, but it doesn't count down the latch.
                I'm attaching a patch to fix this specific use case, but it may not be a complete fix.
                As a workaround, the patch also introduces the maxConnections="-1" configuration that disables the usage
                of maxConnections.
                The -1 setting is important to give administrator a workaround while the other edge cases are tracked
                down.
                I have not been able to reproduce this error with NIO connector.
                There is one more place in the JioEndpoint that requires handling of RejectedExecutionException in the
                public boolean processSocketAsync(SocketWrapper
                #Socket>
                socket,SocketStatus status)
                This is currently unhandled.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.tomcat.util.net.AprEndpoint.java</file>
            <file type="M">org.apache.coyote.AbstractProtocol.java</file>
            <file type="M">org.apache.tomcat.util.net.AbstractEndpoint.java</file>
            <file type="M">org.apache.tomcat.util.net.AprEndpoint.java</file>
            <file type="M">org.apache.tomcat.util.net.JIoEndpoint.java</file>
            <file type="M">org.apache.tomcat.util.net.NioEndpoint.java</file>
            <file type="M">org.apache.tomcat.util.threads.LimitLatch.java</file>
        </fixedFiles>
    </bug>
    <bug id="49129" opendate="2010-04-14 19:09:31" fixdate="2010-04-14 19:13:50" resolution="Fixed">
        <buginformation>
            <summary>DigestAuthenticator.md5Helper - possible incorrect lazy initialisation.
            </summary>
            <description>Findbugs says:
                "This method contains an unsynchronized lazy initialization of a non-volatile static field.
                Because the compiler or processor may reorder instructions, threads are not guaranteed to see a
                completely initialized object, if the method can be called by multiple threads.
                You can make the field volatile to correct the problem."
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.authenticator.DigestAuthenticator.java</file>
        </fixedFiles>
    </bug>
    <bug id="59138" opendate="2016-03-07 17:48:07" fixdate="2016-03-08 20:18:29" resolution="Fixed">
        <buginformation>
            <summary>checkThreadLocalMapForLeaks has false positives.
            </summary>
            <description>ThreadLocal$ThreadLocalMap weakly references keys but strongly references values.
                However, it appears the checkThreadLocalMapForLeaks checking reports false positives if the key is a
                ThreadLocal subclass (e.g., anonymous class) but the value does not strongly reference the class loader
                (e.g., Integer, int[] List#SimpleDateFormatter>, etc.).
                Example output:
                07-Mar-2016 11:27:08.258 SEVERE [localhost-startStop-2]
                org.apache.catalina.loader.WebappClassLoaderBase.checkThreadLocalMapForLeaks The web application
                [servlettest-0.1] created a ThreadLocal with key of type [servlettest.TestServlet$1] (value
                [servlettest.TestServlet$1@40d92399]) and a value of type [java.lang.Integer] (value [1]) but failed to
                remove it when the web application was stopped.
                Threads are going to be renewed over time to try and avoid a probable memory leak.
                For large web applications with many such false positives, this output makes tracking down (or even
                noticing new) real issues more difficult.
                Third party libraries refuse to adjust their use of ThreadLocal because they believe (IMO rightly) their
                code is not causing leaks.
                I have read bug 50175 comment 6, but given that the current heuristic has false positives, can some
                compromise be reached? Perhaps some configuration for stifling the warning on a per key class name basis
                could be added? It would even be acceptable for us if that configuration hid the per-instance message
                but issued a single overall "suppressing N ThreadLocal warnings based on config" info/warning message.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.loader.WebappClassLoaderBase.java</file>
        </fixedFiles>
    </bug>
    <bug id="42530" opendate="2007-05-27 21:26:55" fixdate="2008-10-08 07:15:05" resolution="Fixed">
        <buginformation>
            <summary>ManagerBase.backgroundProcess throws NullPointerException.
            </summary>
            <description>ManagerBase.backgroundProcess throws NullPointerException.
                java.lang.NullPointerException
                at org.apache.catalina.session.ManagerBase.processExpires(ManagerBase.java:682)
                at org.apache.catalina.session.ManagerBase.backgroundProcess(ManagerBase.java:667)
                at org.apache.catalina.core.ContainerBase.backgroundProcess(ContainerBase.java:1316)
                at
                org.apache.catalina.core.ContainerBase$ContainerBackgroundProcessor.processChildren(ContainerBase.java:1601)
                at
                org.apache.catalina.core.ContainerBase$ContainerBackgroundProcessor.processChildren(ContainerBase.java:1610)
                at
                org.apache.catalina.core.ContainerBase$ContainerBackgroundProcessor.processChildren(ContainerBase.java:1610)
                at org.apache.catalina.core.ContainerBase$ContainerBackgroundProcessor.run(ContainerBase.java:1590)
                at java.lang.Thread.run(Thread.java:595)
                Environment:
                Tomcat6.0.13
                JDK1.5.11
                TestServletF
                protected void doGet(HttpServletRequest request,
                HttpServletResponse response) throws ServletException, IOException {
                HttpSession session = request.getSession();
                session.invalidate();
                }
                Test:
                ab -c 3000 -n 300000 http://localhost:8080/contextName/TestServlet
                It is as follows in org.apache.catalina.session.ManagerBase:(findSessions()
                that is called by processExpires()).
                public Session[] findSessions() {
                Session results[] = null;
                synchronized (sessions) {
                results = new Session[sessions.size()]; ----- (A)
                results = (Session[]) sessions.values().toArray(results); ----- (B)
                }
                return (results);
                }
                In Tomcat6, sessions of ManagerBase has been changed from HashMap to ConcurrentHashMap.
                At the same time, synchronized operations of sessions have been removed from the methods such as
                findSession(String is), add(Session session), and remove(Session session).
                However, if the lock of sessions is not acquired by add(Session session) and remove(Session session),
                Servlets can execute session.invalidate() between (A) and (B). (see above.)
                As a result, NullpointerException is thrown by processing the 682 line of ManagerBase.processExpires().
                if (!sessions[i].isValid()) {
                I think add(Session session) and remove(Session session) should manipulate sessions in synchronized
                manner like Tomcat5.5.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.session.ManagerBase.java</file>
        </fixedFiles>
    </bug>
    <bug id="48790" opendate="2010-02-22 03:41:21" fixdate="2010-04-11 09:49:24" resolution="Fixed">
        <buginformation>
            <summary>Race condition in org.apache.catalina.session.ManagerBase:maxActive.
            </summary>
            <description>We are running tomcat 6.0.18 with a race detector (http://www.alphaworks.ibm.com/tech/msdk),
                and find one race condition in class
                org.apache.catalina.session.ManagerBase field maxActive.
                Data Race 1 : org.apache.catalina.session.ManagerBase : maxActive
                Thread "http-8080-3" : Tid 34 : WRITE
                Lock Set : [ ]
                [org.apache.catalina.session.ManagerBase : add(Lorg/apache/catalina/Session;)V : : 741]
                [org.apache.catalina.session.StandardSession : setId(Ljava/lang/String;)V : : 368]
                [org.apache.catalina.session.ManagerBase :
                createSession(Ljava/lang/String;)Lorg/apache/catalina/Session; : : 827]
                [org.apache.catalina.session.StandardManager :
                createSession(Ljava/lang/String;)Lorg/apache/catalina/Session; : : 291]
                [org.apache.catalina.connector.Request : doGetSession(Z)Lorg/apache/catalina/Session; : : 2324]
                [org.apache.catalina.connector.Request : getSession(Z)Ljavax/servlet/http/HttpSession; : : 2074]
                [org.apache.catalina.connector.RequestFacade : getSession(Z)Ljavax/servlet/http/HttpSession; : : 833]
                [org.apache.catalina.connector.RequestFacade : getSession()Ljavax/servlet/http/HttpSession; : : 844]
                [com.ecyrd.jspwiki.auth.AuthenticationManager : login(Ljavax/servlet/http/HttpServletRequest;)Z : : 270]
                ........
                Thread "http-8080-1" : Tid 32 : READ
                Lock Set : [ ]
                [org.apache.catalina.session.ManagerBase : add(Lorg/apache/catalina/Session;)V : : 740]
                [org.apache.catalina.session.StandardSession : setId(Ljava/lang/String;)V : : 368]
                [org.apache.catalina.session.ManagerBase :
                createSession(Ljava/lang/String;)Lorg/apache/catalina/Session; : : 827]
                [org.apache.catalina.session.StandardManager :
                createSession(Ljava/lang/String;)Lorg/apache/catalina/Session; : : 291]
                ........
                Here is the code snip of ManagerBase.java
                public void add(Session session) {
                sessions.put(session.getIdInternal(), session);
                int size = sessions.size();
                1-> if( size > maxActive ) {
                2-> maxActive = size;
                }
                }
                Statement 1 and statement 2 should executed in an atomic manner.
                One fix to this problem is to move statement 1 and statement 2 into a synchronized block.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.ha.session.DeltaManager.java</file>
            <file type="M">org.apache.catalina.session.ManagerBase.java</file>
        </fixedFiles>
    </bug>
    <bug id="58179" opendate="2015-07-24 14:54:37" fixdate="2015-07-27 10:25:45" resolution="Fixed">
        <buginformation>
            <summary>Atomicity violation.
            </summary>
            <description>My name is Bai Guangdong, a research fellow from National University of Singapore.
                I find an atomicity violation similar to bug 53498.
                The problem occurs in the same file java/org/apache/catalina/core/ApplicationContext.java.
                Look at the code snippet below.
                L791 oldValue = attributes.get(name);
                L792 if (oldValue != null)
                L793 replaced = true;
                L794 attributes.put(name, value);
                ...
                ...
                L801 if (replaced)
                L802 event =
                L803 new ServletContextAttributeEvent(context.getServletContext(),
                L804 name, oldValue);
                L805 else
                L806 event =
                L807 new ServletContextAttributeEvent(context.getServletContext(),
                L808 name, value);
                ...
                ...
                L816 if (replaced) {
                L817 context.fireContainerEvent
                L818 ("beforeContextAttributeReplaced", listener);
                L819 listener.attributeReplaced(event);
                L820 context.fireContainerEvent("afterContextAttributeReplaced",
                L821 listener);
                L822 } else {
                L823 context.fireContainerEvent("beforeContextAttributeAdded",
                L824 listener);
                L825 listener.attributeAdded(event);
                L826 context.fireContainerEvent("afterContextAttributeAdded",
                L827 listener);
                Suppose two threads T1 and T2 executes this code snippet with the same key ("name").
                Initially, "attributes" is empty.
                T1 executes line 791 and "oldValue" in T1 becomes null.
                Before T1 executes line 794, T2 executes 791 and "oldValue" in T2 becomes null as well.
                Then T1 executes line 794, and later T2 replaces T1's "value" at line 794.
                Afterwards, both T1 and T2 fire the "beforeContextAttributeAdded" event at line 823.
                However, in the above situation, "replaced" in T2 should be true and "beforeContextAttributeReplaced"
                should be fired at line 817.
            </description>
            <version>1.1.0</version>
            <fixedVersion>7.0.63</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.core.ApplicationContext.java</file>
        </fixedFiles>
    </bug>
    <bug id="56746" opendate="2014-07-19 00:06:34" fixdate="2014-07-28 21:53:43" resolution="Fixed">
        <buginformation>
            <summary>Webssocket secure client thread cannot access webapp resources.
            </summary>
            <description>The new SecureIOThreadFactory method in org.apache.tomcat.websocket.AsyncChannelWrapperSecure
                sets the context class loader of the secure websocket client threads to it's own standard class loader -
                thus preventing the them from accessing webapp resources.
                Removing this line restores access, as the new threads then have the classloader of the calling (webapp)
                thread.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.tomcat.websocket.AsyncChannelWrapperSecure.java</file>
        </fixedFiles>
    </bug>
    <bug id="55267" opendate="2013-07-15 11:38:25" fixdate="2013-07-24 15:02:59" resolution="Fixed">
        <buginformation>
            <summary>NIO thread locked.
            </summary>
            <description>Did some load test and thread got stuck, see dump.
                Basically I just simulate 100 users that connect and disconnect.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.coyote.ajp.AjpNioProcessor.java</file>
            <file type="M">org.apache.coyote.http11.InternalNioOutputBuffer.java</file>
            <file type="M">org.apache.tomcat.util.net.NioEndpoint.java</file>
        </fixedFiles>
    </bug>
    <bug id="51212" opendate="2011-05-17 16:25:52" fixdate="2011-05-18 17:46:16" resolution="Fixed">
        <buginformation>
            <summary>QueryStats has synchornisation issues.
            </summary>
            <description>Created attachment 27017 [details]
                Patch with Atomic approach
                Hello,
                I don't know if it is volontary but I think QueryStats fields should be Atomic because they are accessed
                by multiple threads at same time.
                I made a Load test with heavy load and collected results of SlowQueryReport and I get Sql queries with 0
                as number of executions which is wrong as they have been executed at least once.
                By the way I don't know which approach will perform better:
                - synchronize add/failure
                - Use Atomic
                Maybe you did this for performance reasons.
                Regards
                Philippe Mouawad
                http://www.ubik-ingenierie.com
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.tomcat.jdbc.pool.interceptor.SlowQueryReport.java</file>
        </fixedFiles>
    </bug>
    <bug id="52055" opendate="2011-10-18 21:53:21" fixdate="2012-10-06 14:50:06" resolution="Fixed">
        <buginformation>
            <summary>ChunkedInputFilter is not recycled for servlet 3.0 asynchronous request.
            </summary>
            <description>I'm using the standard servlet 3.0 async APIs with tomcat 7.0.22.
                The server side code is something like the following:
                final AsyncContext asyncContext = request.startAsync();
                ...
                // in a processing thread
                asyncContext.getRequest().getInputStream();
                ...
                read the input stream
                asyncContext.complete();
                The InputStream returns 0 bytes for the HTTP post with chunking.
                After debugging, I found that the ChunkedInputFilter is reused by
                org.apache.coyote.http11.AbstractInputBuffer.
                But it has never been recycled (nextRequest()?) before the reuse for another request.
                As a result, the endChunk flag is always true after the first request.
                And it always return immediately without reading more from the buffer.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.coyote.http11.filters.ChunkedInputFilter.java</file>
            <file type="M">org.apache.coyote.http11.AbstractInputBuffer.java</file>
            <file type="M">org.apache.coyote.http11.InternalNioInputBuffer.java</file>
            <file type="M">org.apache.coyote.http11.AbstractHttp11Processor.java</file>
        </fixedFiles>
    </bug>
    <bug id="39769" opendate="2006-06-09 20:09:27" fixdate="2006-06-15 22:21:01" resolution="Fixed">
        <buginformation>
            <summary>Wrong class loader when the myServlet.destroy() called if myServlet is in the T5 shared directory.
            </summary>
            <description>Hi all,
                My web app needs to have Thread.currentThread().getContextClassLoader() always right.
                When the application is unload it seem not be the case.
                My classes are under the T5's shared directory (so all my servlet.getClass().getClassLoader() == the
                shared classloader i.e.: StandardCalssLoader.
                My understanding is that when executing the code in my web app.
                Thread.currentThread().getContextClassLoader() should always give the WebappLoader used in the
                StandardContext.
                The problem occurs in StandardWrapper.unload() before calling the servlet.destroy() method
                ....
                ClassLoader oldCtxClassLoader =
                Thread.currentThread().getContextClassLoader();
                ClassLoader classLoader = instance.getClass().getClassLoader();
                PrintStream out = System.out;
                if (swallowOutput) {
                SystemLogHandler.startCapture();
                }
                // Call the servlet destroy() method
                try {
                instanceSupport.fireInstanceEvent
                (InstanceEvent.BEFORE_DESTROY_EVENT, instance);
                Thread.currentThread().setContextClassLoader(classLoader);
                if( System.getSecurityManager() != null) {
                SecurityUtil.doAsPrivilege("destroy",
                instance);
                SecurityUtil.remove(instance);
                } else {
                instance.destroy();
                }
                ....
                In the code, the INSTANCE variable is my servlet and it have been loaded by the SHARED classloader.
                So the classloader set in the thread is not the right one.
                It Should be the classloader related to StandardContext.getLoader()
                This problem occurs when I try to close my T5 dos windows and I suppose will occurs when I will
                uninstall any webApp from my T5.
                I notice the problem in T5 5.5.16 but I verified and it's the same code in 5.5.17.
                So probably the problem is still there Best Regards
                /David Gagnon
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.core.StandardWrapper.java</file>
        </fixedFiles>
    </bug>
    <bug id="57265" opendate="2014-11-26 11:24:30" fixdate="2015-09-23 12:37:07" resolution="Fixed">
        <buginformation>
            <summary>Tomcat 8 hiden behind NGINX fails to send file when using NIO connector.
            </summary>
            <description>We have moved Tomcat 8 server behind the nginx balancing server and have started experiencing
                this problem:
                org.apache.tomcat.util.net.NioEndpoint$NioBufferHandler@2001a157
                26-Nov-2014 11:37:04.476 SEVERE [http-nio-8443-ClientPoller-0]
                org.apache.tomcat.util.net.NioEndpoint$Poller.processSendfile
                java.lang.IllegalArgumentException: You can only read using the application read buffer provided by the
                handler.
                at org.apache.tomcat.util.net.SecureNioChannel.write(SecureNioChannel.java:489)
                at sun.nio.ch.FileChannelImpl.transferToArbitraryChannel(FileChannelImpl.java:534)
                at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:583)
                at org.apache.tomcat.util.net.NioEndpoint$Poller.processSendfile(NioEndpoint.java:1200)
                at org.apache.tomcat.util.net.NioEndpoint$Poller.processKey(NioEndpoint.java:1122)
                at org.apache.tomcat.util.net.NioEndpoint$Poller.run(NioEndpoint.java:1087)
                at java.lang.Thread.run(Thread.java:745)
                Problem occurres irregularly when loading lots of scripts refencenced by homepage.
                It seems to be ok with useSendfile=false.
                I have tried to add some slow logging (with flushing output) to code and it lowers occurrence rate, so
                it looks like some race condition problem.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.tomcat.util.net.NioEndpoint.java</file>
            <file type="M">org.apache.tomcat.util.net.NioChannel.java</file>
        </fixedFiles>
    </bug>
    <bug id="50293" opendate="2010-11-18 16:58:18" fixdate="2012-02-05 19:18:05" resolution="Fixed">
        <buginformation>
            <summary>javax.el.CompositeELResolver synchronization issue.
            </summary>
            <description>Created attachment 26310 [details]
                Synchronize change in CompositeELResolver.add.
                The javax.el.CompositeELResolver.add method needs to synchronize around the update to the resolvers.
                If two threads call add, then it is possible for both threads get past the size check, then one thread
                updates the size, and when the next thread tries to update the size, it fails with an
                ArrayIndexOutOfBoundsException.
                Also, it makes sense to increase the default size of the ELResolver array since even the jasper EL adds
                5 ELResolvers.
                I propose a default of 8.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">javax.el.CompositeELResolver.java</file>
        </fixedFiles>
    </bug>
    <bug id="57338" opendate="2014-12-10 17:58:51" fixdate="2015-04-22 10:36:46" resolution="Fixed">
        <buginformation>
            <summary>SingleSignOnEntry cache of ClusterSingleSignOn valve is not synchronized on Tomcat startup.
            </summary>
            <description>When using the ClusterSingleSignOn valve, it looks like the single sign on session state is not
                synchronized to cluster nodes when they start.
                The state is instead only replicated at the moment that an action is performed, so any nodes that come
                online after that action are out of sync.
                See mailing list discussion here: http://www.mail-archive.com/users@tomcat.apache.org/msg115472.html
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.authenticator.SingleSignOn.java</file>
            <file type="M">org.apache.catalina.authenticator.SingleSignOnEntry.java</file>
            <file type="M">org.apache.catalina.authenticator.SingleSignOnSessionKey.java</file>
            <file type="M">org.apache.catalina.ha.authenticator.ClusterSingleSignOn.java</file>
            <file type="M">org.apache.catalina.ha.authenticator.ClusterSingleSignOnListener.java</file>
            <file type="M">org.apache.catalina.ha.authenticator.SingleSignOnMessage.java</file>
        </fixedFiles>
    </bug>
    <bug id="47158" opendate="2009-05-05 15:12:45" fixdate="2009-11-02 13:58:48" resolution="Fixed">
        <buginformation>
            <summary>I think AccessLogValve has race condition problem.
            </summary>
            <description>"Double-Checked Locking" pattern is heavily used in this class, but the usage of it in
                AccessLogValve has some issues of race condition judging by this article:
                http://www.cs.umd.edu/~pugh/java/memoryModel/DoubleCheckedLocking.html.
                Based by this article, I think these variables in this class should be declared as volatile to get rid
                of race condition:
                private volatile long currentMillis;
                private volatile Date currentDate;
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.valves.AccessLogValve.java</file>
        </fixedFiles>
    </bug>
    <bug id="56082" opendate="2014-01-29 00:11:33" fixdate="2014-03-18 09:42:52" resolution="Fixed">
        <buginformation>
            <summary>ConcurrentModificationException with org.apache.juli.ClassLoaderLogManager.
            </summary>
            <description>Created attachment 31267 [details]
                Source to reproduce
                In Tomcat, java.util.logging.LogManager.getLogManager() is in general an instance of
                org.apache.juli.ClassLoaderLogManager.
                In a webapp, when iterating over the result of LogManager.getLogManager().getLoggerNames(), a
                ConcurrentModification can sometimes occur.
                To reproduce :
                - download attached test-1.0.zip
                - run "mvn clean package" using Maven
                - copy "target/test-1.0.war" into tomcat/webapps
                - start Tomcat
                - see a lot of "Issue reproduced: java.util.ConcurrentModificationException" in the System output
                This webapp uses simple Logger.getLogger(String) and LogManager.getLoggerNames() in threads.
                I my case, this is reproduced using Tomcat 7.0.42 and JDK 1.7.
                I suppose that a copy of the result could be made in
                org.apache.juli.ClassLoaderLogManager.getLoggerNames()
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.juli.TestClassLoaderLogManager.java</file>
            <file type="M">org.apache.juli.ClassLoaderLogManager.java</file>
        </fixedFiles>
    </bug>
    <bug id="41059" opendate="2006-11-28 11:53:02" fixdate="2009-07-20 03:18:23" resolution="Fixed">
        <buginformation>
            <summary>WebAppClassLoader clearReferences code break running threads.
            </summary>
            <description>When the WebAppClassLoader stops it manually sets all static and final variables to null in the
                clearReferences method.
                This results in NPEs for any thread that was not stopped and not expecting its final or static variables
                to be null.
                This is particularly a problem with hot deploys since the unload fails and terminating the deploy.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.loader.WebappClassLoader.java</file>
        </fixedFiles>
    </bug>
    <bug id="56653" opendate="2014-06-21 02:19:53" fixdate="2017-01-01 11:15:50" resolution="Fixed">
        <buginformation>
            <summary>Concurrency issue with Mapper$ContextList when stopping Contexts.
            </summary>
            <description>I noticed this issue while reviewing the code of Mapper.removeContextVersion() of the current
                trunk (@1604217).
                The same code exists in Tomcat 7 and 6.
                In Mapper.removeContextVersion() (Mapper.removeContext() in Tomcat 6) it does the following:
                [[[
                host.contextList.contexts = newContexts;
                // Recalculate nesting
                host.contextList.nesting = 0;
                for (int i = 0; i#newContexts.length; i++) {
                int slashCount = slashCount(newContexts[i].name);
                if (slashCount > host.contextList.nesting) {
                host.contextList.nesting = slashCount;
                }
                }
                ]]]
                The problem is there is a delay between when the list of contexts is updated (contextList.contexts) and
                the contextList.nesting field is updated.
                The "nesting" field is used when mapping contexts.
                For example,
                1.
                If there are the following contexts:
                ROOT
                foo
                foo#bar
                2.
                Context foo#bar is being stopped.
                3.
                A request for "foo" comes in, e.g.
                http://localhost/foo/index.html
                Expected behaviour: Map the context to foo application.
                Actual behaviour:
                It may be that the request will be erroneously mapped to the ROOT webapp instead of "foo".
                I have a test case.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.tomcat.util.http.mapper.Mapper.java</file>
            <file type="M">org.apache.tomcat.util.http.mapper.TestMapper.java</file>
        </fixedFiles>
    </bug>
    <bug id="54086" opendate="2012-11-01 13:32:28" fixdate="2013-07-01 13:09:45" resolution="Fixed">
        <buginformation>
            <summary>ConcurrentModificationException in NioReceiver on shutdown.
            </summary>
            <description>Solaris 10 x86, jdk 1.7.
                We use tomcat clustering for session replication with 4 nodes and sometimes 8 nodes.
                We get a ConcurrentModificationException occasionally on shutdown.
                I have been unable to reliably reproduce the exception.
                In the log, I see "Unable to close cluster receiver selector." with the exception below:
                java.util.ConcurrentModificationException
                java.util.HashMap$HashIterator.nextEntry(HashMap.java:894)
                java.util.HashMap$KeyIterator.next(HashMap.java:928)
                java.util.Collections$UnmodifiableCollection$1.next(Collections.java:1067)
                org.apache.catalina.tribes.transport.nio.NioReceiver.closeSelector(NioReceiver.java:382)
                org.apache.catalina.tribes.transport.nio.NioReceiver.stopListening(NioReceiver.java:365)
                org.apache.catalina.tribes.transport.nio.NioReceiver.stop(NioReceiver.java:86)
                org.apache.catalina.tribes.group.ChannelCoordinator.internalStop(ChannelCoordinator.java:203)
                org.apache.catalina.tribes.group.ChannelCoordinator.stop(ChannelCoordinator.java:115)
                org.apache.catalina.tribes.group.ChannelInterceptorBase.stop(ChannelInterceptorBase.java:178)
                org.apache.catalina.tribes.group.ChannelInterceptorBase.stop(ChannelInterceptorBase.java:178)
                org.apache.catalina.tribes.group.ChannelInterceptorBase.stop(ChannelInterceptorBase.java:178)
                org.apache.catalina.tribes.group.interceptors.MessageDispatchInterceptor.stop(MessageDispatchInterceptor.java:172)
                org.apache.catalina.tribes.group.ChannelInterceptorBase.stop(ChannelInterceptorBase.java:178)
                org.apache.catalina.tribes.group.ChannelInterceptorBase.stop(ChannelInterceptorBase.java:178)
                org.apache.catalina.tribes.group.GroupChannel.stop(GroupChannel.java:438)
                org.apache.catalina.ha.tcp.SimpleTcpCluster.stopInternal(SimpleTcpCluster.java:744)
                org.apache.catalina.util.LifecycleBase.stop(LifecycleBase.java:232)
                org.apache.catalina.core.ContainerBase.stopInternal(ContainerBase.java:1199)
                org.apache.catalina.util.LifecycleBase.stop(LifecycleBase.java:232)
                org.apache.catalina.core.StandardService.stopInternal(StandardService.java:502)
                org.apache.catalina.util.LifecycleBase.stop(LifecycleBase.java:232)
                org.apache.catalina.core.StandardServer.stopInternal(StandardServer.java:753)
                org.apache.catalina.util.LifecycleBase.stop(LifecycleBase.java:232)
                org.apache.catalina.startup.Catalina.stop(Catalina.java:751)
                org.apache.catalina.startup.Catalina.start(Catalina.java:713)
                sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
                sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
                sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
                java.lang.reflect.Method.invoke(Method.java:601)
                org.apache.catalina.startup.Bootstrap.start(Bootstrap.java:322)
                org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:451)
                I looked at the code briefly and noticed the use of a SelectionKey Iterator.
                I have not dug deep enough to find any issue in the NioReceiver code, but I did find this potentially
                relevant text in the Selector javadocs
                (http://docs.oracle.com/javase/7/docs/api/java/nio/channels/Selector.html):
                "A selector's key and selected-key sets are not, in general, safe for use by multiple concurrent
                threads.
                If such a thread might modify one of these sets directly then access should be controlled by
                synchronizing on the set itself.
                The iterators returned by these sets' iterator methods are fail-fast: If the set is modified after the
                iterator is created, in any way except by invoking the iterator's own remove method, then a
                ConcurrentModificationException will be thrown."
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.tribes.transport.nio.NioReceiver.java</file>
            <file type="M">org.apache.catalina.tribes.transport.ReceiverBase.java</file>
            <file type="M">org.apache.catalina.tribes.transport.nio.NioReceiver.java</file>
        </fixedFiles>
    </bug>
    <bug id="54045" opendate="2012-10-23 16:03:53" fixdate="2013-02-15 20:27:16" resolution="Fixed">
        <buginformation>
            <summary>ReplicatedMap don't like TcpFailureDetector in static configuration.
            </summary>
            <description>Tribes stack using:
                * TcpPingInterceptor
                * TcpFailureDetector
                * MessageDispatchInterceptor
                * StaticMembershipInterceptor
                Do not work well in static cluster.
                First side (ie one thread):
                * call to TcpFailureDetector.heartbeat()
                * call to checkMembers(false)
                * call to performBasicCheck() in synchronized(membership)
                * in performBasicCheck, for a missing static node:
                * add "missing" member to membership with membership.memberAlive(m)
                * HERE THE SECOND THREAD HAVE SOME TIME TO WORK
                * check it with memberAlive(m)
                * remove it since if it doesn't exist
                Second side (ie another thread):
                * some call to channel.getMembers() like what the done by AbstractReplicatedMap
                * this call will call the TcpFailureDetector.getMembers()
                * this one could return a wrong value since it can contains unavailable nodes
                Note:
                * synchronize on membership isn't use by TcpFailureDetector in getMember(), getMembers(), hasMembers(),
                neither in Membership equivalent method (maybe because it's too heavy to lock every thread while the
                TcpFailureDetector check if node are alive).
                It must not be an issue for AbstractReplicatedMap since with or without TcpFailureDetector a node could
                disapear while replicated map try to use it.
                But ReplicatedMap use always Channel.SEND_OPTIONS_DEFAULT where the value is
                Channel.SEND_OPTIONS_USE_ACK.
                So a message sent to a missing node will fail with an exception.
                Personnaly I override TcpFailureDetector.heartbeat() to avoid performBasicCheck() if I use a static
                configuration (TcpPingInterceptor call performForcedCheck()).
                But this doesn't fix ReplicatedMap issue.
                Better fix could avoid adding missing member to membership list:
                * Add a method like memberAlive(MemberImpl) to Membership without side effect (add the member)
                * in TcpFailureDetector.performBasicCheck(): check this new method before adding the node
                This doesn't fix the AbstractReplicatedMap issue which work always with acknoledge from other nodes.
                Same code for Tomcat 6.
                best regards
                F.Arnoud
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.tribes.group.interceptors.TcpFailureDetector.java</file>
        </fixedFiles>
    </bug>
    <bug id="54521" opendate="2013-02-04 10:54:16" fixdate="2013-02-08 08:41:43" resolution="Fixed">
        <buginformation>
            <summary>DigestAuthenticator: nonceCountValid() fails in case of simultaneous asynchronous requests of the
                same client.
            </summary>
            <description>Concerned components:
                org.apache.catalina.authenticator.DigestAuthenticator
                org.apache.catalina.authenticator.DigestAuthenticator.NonceInfo
                Scenario:
                A multithreaded client sends two requests within one millisecond.
                The DigestAuthenticator creates for each request a nonce as well as a NonceInfo instance.
                The two nonces are equal, as they were created within the same millisecond.
                When writing data into the cache (method generateNonce()), the second NonceInfo instance overwrites the
                first one (same key!).
                Problem:
                The two client threads then send a second request with a digest authentication header.
                In both requests, nc? (nonce count) is equal 1?, as the nonce has been newly created.
                In the NonceInfo of the first request, array seen? is set to true? for index ((nonceCount +
                offset) % seen.length).
                In the second request the same NonceInfo instance is used, as the instance is retrieved from map
                nonces? using nonce? as a key, that is, in both requests the same key is used.
                Consequently, method nonceCountValid() returns žfalse, as seen[(nonceCount + offset) %
                seen.length)] has already been set to žtrue.
                Therefore the authentication fails, although the client has sent a valid digest authentication header.
                Conclusion:
                Working with multi-threaded clients with many requests, digest authentication does not function
                reliably.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.authenticator.DigestAuthenticator.java</file>
            <file type="M">org.apache.catalina.authenticator.TestDigestAuthenticator.java</file>
        </fixedFiles>
    </bug>
    <bug id="56518" opendate="2014-05-13 10:47:48" fixdate="2014-06-16 10:08:55" resolution="Fixed">
        <buginformation>
            <summary>NIO async servlet limit latch leak.
            </summary>
            <description>Created attachment 31613 [details]
                the sample webapp to reproduce the bug
                we have encouter this bug in a real product webapp.
                I have tested this in linux x86, oracle jdk jdk1.7.0_55, tomcat 7.0.53 and tomcat 8.0.5.
                CONFIG:
                we change HTTP Connector to NIO in "server.xml",
                e.g.
                protocol="org.apache.coyote.http11.Http11NioProtocol"
                WEBAPP LOGIC:
                the simplified situation:
                1.
                call "req.startAsync()" to start async serlvet, then execute the async logic in our user thread.
                2.
                sometimes the user thread be interrupted (by some timeout logic of our code).
                3.
                some user code call "resp.flushBuffer()" to send response to client
                PROBLEM:
                in the situation descibed above, the "LimitLatch.countDown()" is not called.
                when the connections limit latch count up to max ( default "10000" ), tomcat DO not accept any
                connection, all incoming client hangs.
                REPRODUCER:
                in a clean tomcat-7.0.53 installation:
                1.
                change the default "server.xml" Connector config.
                (1) change protocol="org.apache.coyote.http11.Http11NioProtocol"
                (2) Optional, add maxConnections="100" to reproduce the bug faster.
                2.
                copy the sample webapp in the attachment to "webapps/ROOT.war"
                3.
                start tomcat.
                4.
                make plenty request to "/async.html",
                for (( i = 0; i
                15000; ++i )) ; do echo $i; curl localhost:8080/async.html; done
                each request is likely cause a limit latch leak.
                when the requests reaches maxConnections (100 as we set above) or some more, the client ( curl ) hangs.
                TECHNIC-DETAILS:
                after some debug, wo found these:
                1.
                when the thread was interrupted, when the user code call "resp.flushBuffer()", the NioChannel was Closed
                by jdk NIO code, and a ClosedByInterruptException is thrown.
                2.
                when the channel closed, the SelectionKey was removed by Poller thread,
                stack trace:
                Daemon Thread [http-nio-8080-ClientPoller-0] (Suspended)
                owns: Object (id=3346)
                owns: HashSet
                #E>
                (id=3354)
                owns: EPollSelectorImpl (id=82)
                owns: Collections$UnmodifiableSet
                E>
                (id=3355)
                owns: Util$2 (id=3356)
                SocketChannelImpl(AbstractSelectableChannel).removeKey(SelectionKey) line: 114
                EPollSelectorImpl(AbstractSelector).deregister(AbstractSelectionKey) line: 168
                EPollSelectorImpl.implDereg(SelectionKeyImpl) line: 162
                EPollSelectorImpl(SelectorImpl).processDeregisterQueue() line: 131
                EPollSelectorImpl.doSelect(long) line: 69
                EPollSelectorImpl(SelectorImpl).lockAndDoSelect(long) line: 69
                EPollSelectorImpl(SelectorImpl).select(long) line: 80
                NioEndpoint$Poller.run() line: 1163
                Thread.run() line: 662
                3.
                when we call "ctx.complete()", it run to
                "org.apache.tomcat.util.net.NioEndpoint.processSocket(NioChannel, SocketStatus, boolean)", code is
                below:
                public boolean processSocket(NioChannel socket, SocketStatus status, boolean dispatch) {
                try {
                KeyAttachment attachment = (KeyAttachment)socket.getAttachment(false);
                if (attachment == null) {
                return false;
                }
                since the SelectionKey was removed, the "attachment" returns null.
                the logic is break, "AbstractEndpoint.countDownConnection()" is not called, a limit latch leak happens.
                WORK-AROUND:
                some work-around:
                1.
                switch to the stable BIO connector.
                2.
                avoid call "resp.flushBuffer()" in the user thread.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.connector.CoyoteAdapter.java</file>
            <file type="M">org.apache.catalina.connector.CoyoteAdapter.java</file>
            <file type="M">org.apache.coyote.AbstractProcessor.java</file>
            <file type="M">org.apache.coyote.Adapter.java</file>
            <file type="M">org.apache.coyote.Processor.java</file>
            <file type="M">org.apache.coyote.ajp.AbstractAjpProcessor.java</file>
            <file type="M">org.apache.coyote.ajp.AjpAprProcessor.java</file>
            <file type="M">org.apache.coyote.ajp.AjpNioProcessor.java</file>
            <file type="M">org.apache.coyote.ajp.AjpProcessor.java</file>
            <file type="M">org.apache.coyote.http11.AbstractHttp11Processor.java</file>
            <file type="M">org.apache.coyote.http11.Http11AprProcessor.java</file>
            <file type="M">org.apache.coyote.http11.Http11NioProcessor.java</file>
            <file type="M">org.apache.coyote.http11.upgrade.AbstractProcessor.java</file>
            <file type="M">org.apache.coyote.http11.upgrade.UpgradeProcessor.java</file>
            <file type="M">org.apache.tomcat.util.net.AbstractEndpoint.java</file>
            <file type="M">org.apache.tomcat.util.net.NioChannel.java</file>
            <file type="M">org.apache.tomcat.util.net.NioEndpoint.java</file>
            <file type="M">org.apache.tomcat.util.net.SecureNioChannel.java</file>
            <file type="M">org.apache.tomcat.util.net.SocketStatus.java</file>
        </fixedFiles>
    </bug>
    <bug id="56042" opendate="2014-01-21 02:26:34" fixdate="2014-01-23 22:49:30" resolution="Fixed">
        <buginformation>
            <summary>lang.IllegalStateException: Calling [asyncComplete()] is not valid for a request with Async
                state [MUST_DISPATCH].
            </summary>
            <description>The issue appears if the response has been set before startAsync:
                response.setStatus(HttpServletResponse.SC_BAD_REQUEST);
                AsyncContext asyncContext = request.startAsync(request, response);
                asyncContext.dispatch();
                You might wonder why would anyone do that? The actual scenario is a bit more complex.
                It involves a separate thread that completes very fast, even before startAsync is called.
                The resulting stack trace:
                java.lang.IllegalStateException: Calling [asyncComplete()] is not valid for a request with Async state
                [MUST_DISPATCH]
                at org.apache.coyote.AsyncStateMachine.asyncComplete(AsyncStateMachine.java:227)
                at org.apache.coyote.http11.Http11Processor.actionInternal(Http11Processor.java:358)
                at org.apache.coyote.http11.AbstractHttp11Processor.action(AbstractHttp11Processor.java:871)
                at org.apache.coyote.Request.action(Request.java:344)
                at org.apache.catalina.core.AsyncContextImpl.complete(AsyncContextImpl.java:92)
                at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:140)
                at org.apache.catalina.valves.AccessLogValve.invoke(AccessLogValve.java:953)
                at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:118)
                at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:409)
                at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1044)
                at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:607)
                at org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run(JIoEndpoint.java:313)
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
                at java.lang.Thread.run(Thread.java:744)
                The Servlet spec says: "It is illegal to call startAsync if ..., or if the response has been committed
                and closed, ...".
                If that is indeed the reason, a clear error should be raised, and startAsync not be allowed to proceed.
                Or perhaps it is an issue that can be fixed? For what it's worth it actually works in Jetty even though
                the spec says it is illegal.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.valves.ErrorReportValve.java</file>
            <file type="M">org.apache.catalina.valves.TestErrorReportValve.java</file>
        </fixedFiles>
    </bug>
    <bug id="56724" opendate="2014-07-15 12:29:23" fixdate="2019-08-09 17:28:28" resolution="Fixed">
        <buginformation>
            <summary>Restart Container background thread if it died unexpectedly.
            </summary>
            <description>To address the issue that has been raised several times on the mailing lists, e.g.
                http://tomcat.markmail.org/thread/xooxcq56ehki63dh
                "ContainerBackgroundProcessor and compounding OOMEs"
                http://tomcat.markmail.org/thread/f6b6vicg7kusckra
                "Background thread died; no errors in log; invoking backgroundProcess via JMX has no effect"
                I think it is OK to start a new background thread after some delay.
                If the start succeeds, it will be a new thread with its own (clean) stack.
                It may help for StackOverflowError.
                It might partially help with OutOfMemoryError thread death if nothing else is available, but a better
                strategy for an admin to handle an OutOfMemoryError is to start JVM with -XX:OnOutOfMemoryError flag
                with a script that shuts down (and restarts) Tomcat.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.core.ContainerBase.java</file>
        </fixedFiles>
    </bug>
    <bug id="51467" opendate="2011-07-03 10:19:46" fixdate="2011-07-04 18:50:13" resolution="Fixed">
        <buginformation>
            <summary>usage of method run instead of start to start a thread.
            </summary>
            <description>In StandardContext method Thread#run is used.
                This is most probably a mistake, since a few lines below Thread#join is called to wait for the
                completion of the thread.
                So we could either remove the join and get rid of the thread by using just a runnable, or start the
                thread.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.core.StandardContext.java</file>
        </fixedFiles>
    </bug>
    <bug id="57943" opendate="2015-05-22 06:21:27" fixdate="2015-10-25 18:36:02" resolution="Fixed">
        <buginformation>
            <summary>NioEndpoint, one poller thread died.
            </summary>
            <description>when i restart tomcat,the tomcat can not process request soon find an error log.
                Exception in thread "http-nio-7001-ClientPoller-1" java.util.ConcurrentModificationException
                at java.util.HashMap$HashIterator.nextEntry(HashMap.java:793)
                at java.util.HashMap$KeyIterator.next(HashMap.java:828)
                at java.util.Collections$UnmodifiableCollection$1.next(Collections.java:1010)
                at org.apache.tomcat.util.net.NioEndpoint$Poller.timeout(NioEndpoint.java:1421)
                at org.apache.tomcat.util.net.NioEndpoint$Poller.run(NioEndpoint.java:1215)
                at java.lang.Thread.run(Thread.java:662)
                tomcat poller thread is not catch this exception,so this thread is died.
                connection is accept but has no poller thread to process.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.tomcat.util.net.NioEndpoint.java</file>
            <file type="M">org.apache.tomcat.util.net.res.LocalStrings.properties</file>
        </fixedFiles>
    </bug>
    <bug id="50547" opendate="2011-01-06 03:48:03" fixdate="2011-01-20 14:24:04" resolution="Fixed">
        <buginformation>
            <summary>The CHANGE_SESSION_ID message that received during cluster session synchronization is dropped.
            </summary>
            <description>Neither CHANGE_SESSION_ID message nor SESSION_EXPIRED message set timestamp.
                If these messages are received during cluster sessions synchronization in DeltaManager startup, because
                the timestamp is not set, they are dropped.
                I made a patch that add timestamp for CHANGE_SESSION_ID and SESSION_EXPIRED message.
                Best Regards.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.ha.session.DeltaManager.java</file>
            <file type="M">webapps.docs.changelog.xml</file>
        </fixedFiles>
    </bug>
    <bug id="49567" opendate="2010-07-07 09:22:22" fixdate="2010-10-10 17:11:27" resolution="Fixed">
        <buginformation>
            <summary>when starting a new thread from a startAsync Runnable, an infinite amount of doPosts is generated.
            </summary>
            <description>Created attachment 25728 [details]
                testcase demonstrating the bug when starting a new thread from a startAsync Runnable, an infinite amount
                of doPosts is generated.
                Also, the GET status is: 500 Internal Server Error, however, there is no exception thrown by tomcat.
                The attached code demonstrates this, by printing "Start async()" an infinite amount of times.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.connector.CoyoteAdapter.java</file>
            <file type="M">org.apache.catalina.connector.Request.java</file>
            <file type="M">org.apache.catalina.core.AsyncContextImpl.java</file>
            <file type="M">org.apache.coyote.http11.Http11Processor.java</file>
            <file type="M">org.apache.catalina.core.TestAsyncContextImpl.java</file>
        </fixedFiles>
    </bug>
    <bug id="52999" opendate="2012-03-28 21:31:42" fixdate="2012-06-12 12:40:15" resolution="Fixed">
        <buginformation>
            <summary>Performance issue with locking in ContainerBase.fireContainerEvent().
            </summary>
            <description>This was reported on dev list [1]:
                > 2) org.apache.catalina.core.ContainerBase.fireContainerEvent;
                > That method contains critical section:
                > synchronized (listeners) {
                > list = listeners.toArray(list);
                > }
                >
                > Is is called pretty often with every put operation into request or
                > session map.
                That code in tomcat looks like a candidate for
                > CopyOnWriteArrayList
                >
                I confirm that I see fireContainerEvent() calls in many places in StandardSession.
                Moreover those are two nested loops: a loop in StandardSession over array of
                context.getApplicationEventListeners(); x copying the list of container listeners inside into array in
                context.fireContainerEvent().
                I cannot confirm reported problem with request attributes - I do not see anything in the code that would
                send events from that access.
                Is it possible to solve it with a ReadWriteLock?
                Or it would be better to have a helper class that avoids copying the array over on every access (the
                said copy-on-write one)?
                I classify this as an enhancement request.
                [1] Thread "Two performance problems (found during myfaces testing)" on dev list, starting on
                2012-03-08,
                - http://tomcat.markmail.org/thread/7bbvzmkvyvryvn44
                - http://marc.info/?t=133124021200002r=1w=2
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.core.ContainerBase.java</file>
        </fixedFiles>
    </bug>
    <bug id="58522" opendate="2015-10-22 19:54:03" fixdate="2015-10-23 18:40:47" resolution="Fixed">
        <buginformation>
            <summary>concurrency problem corrupts WARDirContext.Entry children.
            </summary>
            <description>We discovered a problem where calling ServletContext.getResourcePaths from a jsp may corrupt
                WARDirContext.Entry.children.
                The WARDirContext.list(Entry) method performs an Arrays.sort(children) call which is not thread-safe.
                Calling this from multiple request threads may result in the children array losing some entries and
                duplicating others.
                When entries representing directories in the war are lost, Tomcat cannot load resources from there.
                Static resource requests result in http error code 404 and jsp compliation can throw JasperException
                when included files are not found.
                We have seen this on CentOS 6.5 (and newer versions) with Tomcat 7.0.52 and Java 1.7.0_51.
                The following are attached in the zip:
                * Test case project which demonstrates this corruption.
                It usually requires multiple runs.
                * A stack trace illustrating the problematic flow.
                * A patch file for WARDirContext.java.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.naming.resources.WARDirContext.java</file>
        </fixedFiles>
    </bug>
    <bug id="53498" opendate="2012-07-02 21:19:45" fixdate="2012-08-03 18:02:06" resolution="Fixed">
        <buginformation>
            <summary>Atomicity violation bugs because of misusing concurrent collections.
            </summary>
            <description>Created attachment 29021 [details]
                The patch that may fix the atomicity violation bugs.
                My name is Yu Lin.
                I'm a Ph.D.
                student in the CS department at
                UIUC.
                I'm currently doing research on mining Java concurrent library misusages.
                I found some misusages of ConcurrentHashMap in Tomcat 7.0.28, which may result in potential atomicity
                violation bugs or harm the performance.
                The code below is a snapshot of the code in file java/org/apache/catalina/core/ApplicationContext.java
                from line 761 to 767 and line 1262 to 1266
                L761 found = attributes.containsKey(name);
                L762 if (found) {
                L763 value = attributes.get(name);
                L764 attributes.remove(name);
                L765 } else {
                L766 return;
                L767 }
                ...
                L1262 if (parameters.containsKey(name)) {
                L1263 return false;
                L1264 }
                L1265
                L1266 parameters.put(name, value);
                In the code above, an atomicity violation may occur between lines 762 and 763.
                Suppose thread T1 executes line 761 and finds that the concurrent hashmap "attributes" contains the key
                "name".
                Before thread T1 executes line 763, another thread T2 removes the "name" key from "attributes".
                Now thread T1 resumes execution at line 763 and will get a null value for "name".
                Then the next line will throw a NullPointerException when invoking the method on "name".
                Second, the snapshot above has another atomicity violation.
                Let's look at lines 1262 and 1266.
                Suppose a thread T1 executes line 1262 and finds out the concurrent hashmap dose not contain the key
                "name".
                Before it gets to execute line 1266, another thread T2 puts a pair#name, v> in the concurrent hashmap
                "parameters".
                Now thread T1 resumes execution and it will overwrite the value written by thread T2.
                Thus, the code no longer preserves the "put-if-absent" semantics.
                I found some similar misusages in other files:
                In java/org/apache/catalina/ha/context/ReplicatedContext.java, similar atomicity violation may occur
                when another thread T2 remove the key "name" from concurrent hashmap "tomcatAttributes" before thread T1
                executes line 172.
                In java/org/apache/catalina/startup/HostConfig.java, suppose thread T1 executes line 1480 and finds out
                the concurrent hashmap dose not contain the key "contextName".
                Before it executes line 1509, another thread T2 puts a pair#contextName, v> in the concurrent hashmap
                "deployed".
                Now thread T1 resumes execution and it will overwrite the value written by thread T2.
                Indeed, the putIfAbsent method shoule be used rather than put method at line 1509.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.core.ApplicationContext.java</file>
            <file type="M">org.apache.catalina.ha.context.ReplicatedContext.java</file>
        </fixedFiles>
    </bug>
    <bug id="49985" opendate="2010-09-23 08:55:42" fixdate="2010-10-04 17:16:33" resolution="Fixed">
        <buginformation>
            <summary>Lazy initialization without any synchronization - data race in AstInteger, AstFloatingPoint,
                AstString.
            </summary>
            <description>r998053
                http://svn.apache.org/repos/asf/tomcat/tc6.0.x/trunk/java/org/apache/el/parser/AstInteger.java
                http://svn.apache.org/repos/asf/tomcat/tc6.0.x/trunk/java/org/apache/el/parser/AstFloatingPoint.java
                http://svn.apache.org/repos/asf/tomcat/tc6.0.x/trunk/java/org/apache/el/parser/AstString.java
                Use lazy init without any synchronization in methods getInteger(), getFloatingPoint(), getString(),
                respectively.
                Consider AstInteger:
                private Number number;
                protected Number getInteger() {
                if (this.number == null) {
                try {
                this.number = new Long(this.image);
                } catch (ArithmeticException e1) {
                this.number = new BigInteger(this.image);
                }
                }
                return number;
                }
                Data races on variable number :37
                in method getInteger() :39
                concurrent read on line 40
                concurrent write on lines 42, 44
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.el.parser.AstString.java</file>
            <file type="M">org.apache.el.parser.AstInteger.java</file>
            <file type="M">org.apache.el.parser.AstFloatingPoint.java</file>
        </fixedFiles>
    </bug>
    <bug id="56907" opendate="2014-09-02 07:40:48" fixdate="2014-09-03 12:22:37" resolution="Fixed">
        <buginformation>
            <summary>Threads leak.
            </summary>
            <description>Hello.
                While using WebSocket implementation of versions 7.0.55 and 8.0.11 leaking threads were noticed.
                Use method WsWebSocketContainer.connection (endpoint, clientEndpointConfiguration, path) to reproduce
                this issue.
                The URL to specify a non-existent page.
                As a result, the connection will return an error "404 Not Found", but the threads that have been created
                in the class AsyncChannelWrapperSecure will stay alive for a long time (source of threads creation:
                private final ExecutorService executor = Executors.newFixedThreadPool (2, new SecureIOThreadFactory
                ());).
                Count of threads are increasing with every reconnect attempt to non-existent URL.
                Those will live for a very long time, until you disable the application that calls the connection
                WsWebSocketContainer.connection (...).
                Leakage threads can be observed using the jvisualvm program.
                Please, make it possible to deal with such kind of situations and destroy unused threads.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.tomcat.websocket.WsWebSocketContainer.java</file>
        </fixedFiles>
    </bug>
    <bug id="53843" opendate="2012-09-08 00:42:57" fixdate="2012-09-20 09:57:23" resolution="Fixed">
        <buginformation>
            <summary>isAsyncStarted may return false in thread that started async processing.
            </summary>
            <description>After async processing starts in some container thread A, subsequent calls to
                request.isAsyncStarted() return true as expected.
                However as soon as an application thread B calls asyncContext.dispatch(), isAsyncStarted() begins to
                return false even if the actual dispatch has not yet occurred.
                For this case the spec says: "the dispatch operation will be delayed until after the container-initiated
                dispatch has returned to the container."
                For the brief period after dispatch() is called but before the actual dispatch begins (and
                DispatcherType becomes ASYNC), isAsyncStarted should return true.
                Without that, the return value of isAsyncStarted() may suddenly change while thread A is still exiting
                causing it to not recognize that async processing started and that it needs to exit as quickly as
                possible.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.coyote.AsyncStateMachine.java</file>
            <file type="M">org.apache.catalina.core.TestAsyncContextImpl.java</file>
        </fixedFiles>
    </bug>
    <bug id="50353" opendate="2010-11-27 17:45:41" fixdate="2011-08-15 18:53:33" resolution="Fixed">
        <buginformation>
            <summary>Calling asyncContext.getResponse() returns null after async timeout.
            </summary>
            <description>If the async thread calls asyncContext.getResponse() after the async timeout, it gets a null
                reference.
                In the following example, it leads to a NPE.
                The servlet spec is not very clear on the behavior to adopt after a timeout, but I don't think null
                should be returned.
                Maybe an IllegalStateException instead? It seems to be the case if complete() is called after the
                timeout.
                package test;
                import java.io.IOException;
                import java.io.PrintWriter;
                import javax.servlet.AsyncContext;
                import javax.servlet.AsyncEvent;
                import javax.servlet.AsyncListener;
                import javax.servlet.ServletException;
                import javax.servlet.ServletResponse;
                import javax.servlet.annotation.WebServlet;
                import javax.servlet.http.HttpServlet;
                import javax.servlet.http.HttpServletRequest;
                import javax.servlet.http.HttpServletResponse;
                /**
                * Servlet implementation class MyServlet
                */
                @WebServlet(value = "/MyServlet", asyncSupported = true)
                public class MyServlet extends HttpServlet implements AsyncListener {
                private static final long serialVersionUID = 1L;
                protected void doGet(HttpServletRequest request,
                HttpServletResponse response) throws ServletException, IOException {
                final AsyncContext asyncContext = request.startAsync(request, response);
                asyncContext.addListener(this);
                asyncContext.setTimeout(10*1000);
                asyncContext.start(new Runnable() {
                @Override
                public void run() {
                System.out.println("Entering async thread");
                try {
                Thread.sleep(20 * 1000);
                System.out.println("
                #Asyncthread>
                about to write response");
                ServletResponse response2 = asyncContext.getResponse();
                PrintWriter writer = response2.getWriter();
                writer.write("Hello world");
                System.out.println("
                #Asyncthread>
                about to complete");
                asyncContext.complete();
                } catch (Exception e) {
                e.printStackTrace();
                }
                }
                });
                }
                @Override
                public void onComplete(AsyncEvent evt) throws IOException {
                System.out.println("onComplete " + evt);
                }
                @Override
                public void onError(AsyncEvent evt) throws IOException {
                System.out.println("onError " + evt);
                }
                @Override
                public void onStartAsync(AsyncEvent evt) throws IOException {
                System.out.println("onStartAsync " + evt);
                }
                @Override
                public void onTimeout(AsyncEvent evt) throws IOException {
                System.out.println("onTimeout " + evt);
                evt.getAsyncContext().getResponse().getWriter().write("Timed out");
                evt.getAsyncContext().complete();
                }
                }
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.core.AsyncContextImpl.java</file>
        </fixedFiles>
    </bug>
    <bug id="52356" opendate="2011-12-18 23:41:25" fixdate="2011-12-24 00:08:34" resolution="Fixed">
        <buginformation>
            <summary>Prevent potential data races on "org.apache.catalina.tribes.transport.bio.util.FastQueue.size".
            </summary>
            <description>Even though accesses to "org.apache.catalina.tribes.transport.bio.util.FastQueue.size" are
                synchronized inside "org.apache.catalina.tribes.transport.bio.util.FastQueue.add(ChannelMessage,
                Member[], InterceptorPayload)" and "org.apache.catalina.tribes.transport.bio.util.FastQueue.remove()",
                the read access in "org.apache.catalina.tribes.transport.bio.util.FastQueue.getSize()" is not properly
                synchronized and thus can lead to a race condition.
                However, "org.apache.catalina.tribes.transport.bio.util.FastQueue.getSize()" is not used, so this bug
                never manifests.
                Nevertheless, we suggest that Tomcat developers either remove
                "org.apache.catalina.tribes.transport.bio.util.FastQueue.getSize()" or make it access the shared field
                safely.
                Field "org.apache.catalina.tribes.transport.bio.util.FastQueue.size" is declared at
                http:
                //svn.apache.org/repos/asf/!svn/bc/1220560/tomcat/trunk/java/org/apache/catalina/tribes/transport/bio/util/FastQueue.java>.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.tribes.transport.bio.util.FastQueue.java</file>
        </fixedFiles>
    </bug>
    <bug id="51688" opendate="2011-08-19 14:49:17" fixdate="2011-08-25 13:39:54" resolution="Fixed">
        <buginformation>
            <summary>JreMemoryLeakPreventionListener should protect against AWT thread creation.
            </summary>
            <description>Any webapp that calls java.awt.Toolkit.getDefaultToolkit will launch a new thread (AWT-Windows,
                AWT-XAWT, etc.) which will capture the ContextClassLoader, pinning the webapp in memory after an
                undeploy/redeploy.
                A simple addition to JreMemoryLeakPreventionListener can alleviate this condition.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.core.JreMemoryLeakPreventionListener.java</file>
        </fixedFiles>
    </bug>
    <bug id="49987" opendate="2010-09-23 10:16:06" fixdate="2010-10-13 10:42:37" resolution="Fixed">
        <buginformation>
            <summary>Data race in ApplicationContext.
            </summary>
            <description>r998053
                http://svn.apache.org/repos/asf/tomcat/tc6.0.x/trunk/java/org/apache/catalina/core/ApplicationContext.java
                Data race on variable.
                private Map parameters
                In method
                private void mergeParameters() {
                if (parameters != null) // concurrent read : 881
                return;
                Map results = new ConcurrentHashMap();
                ...
                parameters = results; // concurrent write : 897
                }
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.core.StandardContext.java</file>
            <file type="M">org.apache.catalina.core.ApplicationContext.java</file>
        </fixedFiles>
    </bug>
    <bug id="50554" opendate="2011-01-06 14:38:25" fixdate="2011-01-07 05:32:45" resolution="Fixed">
        <buginformation>
            <summary>Bad use of getClass() could potentially lead to concurrency bugs in future.
            </summary>
            <description>The method org.apache.naming.java.javaURLContextFactory.getInitialContext(Hashtable#?,?>) has a
                synchronized block on getClass().
                Using getClass() as lock is a bug pattern because if some class subclasses javaURLContextFactory and
                getInitialContext gets called on an instance of such a subclass, getClass() will return a different
                Class object for that instance.
                Locking different objects for different classes is usually not the expected behavior.
                Therefore, I suggest to remove this bug pattern by simply replacing getClass() by
                javaURLContextFactory.class.
                See
                https://www.securecoding.cert.org/confluence/display/java/LCK02-J.+Do+not+synchronize+on+the+class+object+returned+by+getClass%28%29
                for more information about this bug pattern.
                Keshmesh (https://github.com/reprogrammer/keshmesh/) is an Eclipse plugin that analyzes the source code
                of Java programs to detect concurrency bug patterns such the one I reported.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.naming.java.javaURLContextFactory.java</file>
        </fixedFiles>
    </bug>
    <bug id="40380" opendate="2006-08-31 18:57:30" fixdate="2009-07-17 04:16:21" resolution="Fixed">
        <buginformation>
            <summary>Potential syncro problem in StandardSession.expire(boolean).
            </summary>
            <description>public void expire(boolean notify) {
                // Mark this session as "being expired" if needed
                if (expiring)
                return;
                // No man's land here
                synchronized (this) {
                if (manager == null)
                return;
                expiring = true;
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.session.StandardSession.java</file>
        </fixedFiles>
    </bug>
    <bug id="55799" opendate="2013-11-20 00:01:34" fixdate="2013-11-29 02:01:59" resolution="Fixed">
        <buginformation>
            <summary>Stability issues when concurrently sending large messages.
            </summary>
            <description>Created attachment 31055 [details]
                Test case (modifications to the Chat example)
                Hi, a thread on the Users List [1] described that problems can occur when sending large messages over a
                WebSocket connection using getBasicRemote().sendText(String).
                The ChatAnnotation class does not synchronize when using this method, which means that multiple threads
                could call RemoteEndpoint.Basic#sendText() concurrently.
                The JavaDoc of RemoteEndpoint.Basic says:
                "If the websocket connection underlying this RemoteEndpoint is busy sending a message when a call is
                made to send another one, for example if two threads attempt to call a send method concurrently, or if a
                developer attempts to send a new message while in the middle of sending an existing one, the send method
                called while the connection is already busy may throw an IllegalStateException."
                (I thought I had read earlier that the implementation should synchronize calls to methods of
                RemoteEndpoint.Basic instead of throwing an ISE, but maybe that has changed).
                When sending large Messages over Websocket using RemoteEndpoint.Basic from different threads without or
                with synchronization, some problems happen like:
                a) The WebSocket connection is suddenly closed (I guess the browser actually aborts the connections due
                to data corruption or Timeout errors, but I have not examined the raw data sent over TCP)
                b) Various Exceptions occur (see below)
                c) Sometimes when I open the chat.xhtml example in my browser, it shows what seems to be a raw WebSocket
                response instead of the .xhtml file (see added screenshots)
                These issues also happen after synchronizing calls to RemoteEndpoint.Basic#sendText(), but are then
                harder to reproduce.
                To reproduce:
                1) Checkout Tomcat 8 trunk (r1543467) and apply the attached patch.
                It applies some modifications to the Chat Websocket Example, so that the Javascript sends messages in a
                regular interval (50 ms), and the ChatAnnotation modifies the message to be 256 times as large as the
                original message, and sends it back using session.getBasicRemote()#sendText(msg).
                1) Build Tomcat and run it on a Windows machine (I used Windows 8.1 x64, Java 1.7.0_45 x64), using the
                NIO HTTP connector (default configuration).
                2) Open Firefox and IE 11.
                With both browsers, open the Chat example (http://localhost:8080/examples/websocket/chat.xhtml).
                1) Repeat the following actions in a regular interval:
                a) Wait several seconds (it might be that Tomcat already closes one of the two WebSocket connections in
                that time).
                b) On one of the browsers (e.g. IE), press F5 several times.
                1) After some time, you can see that in one of the browsers, the WebSocket connection is suddenly
                closed.
                Tomcat will show one or more of the following exceptions (I think the IOException and
                ClosedChannelException are expected if the browser aborts the connection):
                19-Nov-2013 23:18:39.809 SEVERE [http-nio-8080-ClientPoller-0]
                org.apache.tomcat.util.net.NioEndpoint.processSocket Error allocating socket processor
                java.lang.NullPointerException
                at org.apache.tomcat.util.net.NioEndpoint.processSocket(NioEndpoint.java:624)
                at org.apache.tomcat.util.net.NioEndpoint$Poller.processKey(NioEndpoint.java:1165)
                at org.apache.tomcat.util.net.NioEndpoint$Poller.run(NioEndpoint.java:1122)
                at java.lang.Thread.run(Thread.java:744)
                19-Nov-2013 23:32:16.601 SEVERE [http-nio-8080-exec-3] websocket.chat.ChatAnnotation.onError Chat Error:
                java.nio.channels.ClosedChannelException
                java.nio.channels.ClosedChannelException
                at sun.nio.ch.SocketChannelImpl.ensureReadOpen(SocketChannelImpl.java:252)
                at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:295)
                at org.apache.tomcat.util.net.NioChannel.read(NioChannel.java:136)
                at org.apache.coyote.http11.upgrade.NioServletInputStream.fillReadBuffer(NioServletInputStream.java:136)
                at org.apache.coyote.http11.upgrade.NioServletInputStream.doIsReady(NioServletInputStream.java:49)
                at
                org.apache.coyote.http11.upgrade.AbstractServletInputStream.isReady(AbstractServletInputStream.java:62)
                at org.apache.tomcat.websocket.server.WsFrameServer.onDataAvailable(WsFrameServer.java:44)
                at
                org.apache.tomcat.websocket.server.WsHttpUpgradeHandler$WsReadListener.onDataAvailable(WsHttpUpgradeHandler.java:192)
                at
                org.apache.coyote.http11.upgrade.AbstractServletInputStream.onDataAvailable(AbstractServletInputStream.java:180)
                at org.apache.coyote.http11.upgrade.AbstractProcessor.upgradeDispatch(AbstractProcessor.java:95)
                at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:640)
                at
                org.apache.coyote.http11.Http11NioProtocol$Http11ConnectionHandler.process(Http11NioProtocol.java:223)
                at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1597)
                at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1555)
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
                at java.lang.Thread.run(Thread.java:744)
                19-Nov-2013 23:32:19.658 SEVERE [http-nio-8080-exec-2] websocket.chat.ChatAnnotation.onError Chat Error:
                java.lang.IllegalArgumentException: java.lang.reflect.InvocationTargetException
                java.lang.IllegalArgumentException: java.lang.reflect.InvocationTargetException
                at
                org.apache.tomcat.websocket.pojo.PojoMessageHandlerWholeBase.onMessage(PojoMessageHandlerWholeBase.java:82)
                at org.apache.tomcat.websocket.WsFrameBase.sendMessageText(WsFrameBase.java:369)
                at org.apache.tomcat.websocket.WsFrameBase.processDataText(WsFrameBase.java:468)
                at org.apache.tomcat.websocket.WsFrameBase.processData(WsFrameBase.java:272)
                at org.apache.tomcat.websocket.WsFrameBase.processInputBuffer(WsFrameBase.java:116)
                at org.apache.tomcat.websocket.server.WsFrameServer.onDataAvailable(WsFrameServer.java:55)
                at
                org.apache.tomcat.websocket.server.WsHttpUpgradeHandler$WsReadListener.onDataAvailable(WsHttpUpgradeHandler.java:192)
                at
                org.apache.coyote.http11.upgrade.AbstractServletInputStream.onDataAvailable(AbstractServletInputStream.java:180)
                at org.apache.coyote.http11.upgrade.AbstractProcessor.upgradeDispatch(AbstractProcessor.java:95)
                at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:640)
                at
                org.apache.coyote.http11.Http11NioProtocol$Http11ConnectionHandler.process(Http11NioProtocol.java:223)
                at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1597)
                at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1555)
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
                at java.lang.Thread.run(Thread.java:744)
                Caused by: java.lang.reflect.InvocationTargetException
                at sun.reflect.GeneratedMethodAccessor38.invoke(Unknown Source)
                at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
                at java.lang.reflect.Method.invoke(Method.java:606)
                at
                org.apache.tomcat.websocket.pojo.PojoMessageHandlerWholeBase.onMessage(PojoMessageHandlerWholeBase.java:80)
                ...
                15 more
                Caused by: java.nio.charset.CoderMalfunctionError: java.nio.BufferOverflowException
                at java.nio.charset.CharsetEncoder.encode(CharsetEncoder.java:565)
                at
                org.apache.tomcat.websocket.WsRemoteEndpointImplBase$TextMessageSendHandler.write(WsRemoteEndpointImplBase.java:624)
                at
                org.apache.tomcat.websocket.WsRemoteEndpointImplBase.sendPartialString(WsRemoteEndpointImplBase.java:197)
                at org.apache.tomcat.websocket.WsRemoteEndpointImplBase.sendString(WsRemoteEndpointImplBase.java:154)
                at org.apache.tomcat.websocket.WsRemoteEndpointBasic.sendText(WsRemoteEndpointBasic.java:37)
                at websocket.chat.ChatAnnotation.broadcast(ChatAnnotation.java:96)
                at websocket.chat.ChatAnnotation.incoming(ChatAnnotation.java:83)
                ...
                19 more
                Caused by: java.nio.BufferOverflowException
                at java.nio.Buffer.nextPutIndex(Buffer.java:513)
                at java.nio.HeapByteBuffer.put(HeapByteBuffer.java:163)
                at org.apache.tomcat.util.buf.Utf8Encoder.encodeNotHasArray(Utf8Encoder.java:177)
                at org.apache.tomcat.util.buf.Utf8Encoder.encodeLoop(Utf8Encoder.java:40)
                at java.nio.charset.CharsetEncoder.encode(CharsetEncoder.java:561)
                ...
                25 more
                19-Nov-2013 23:32:23.353 SEVERE [http-nio-8080-exec-10] websocket.chat.ChatAnnotation.onError Chat
                Error: java.io.IOException: Eine vorhandene Verbindung wurde vom Remotehost geschlossen
                java.io.IOException: Eine vorhandene Verbindung wurde vom Remotehost geschlossen
                at sun.nio.ch.SocketDispatcher.read0(Native Method)
                at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
                at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
                at sun.nio.ch.IOUtil.read(IOUtil.java:197)
                at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
                at org.apache.tomcat.util.net.NioChannel.read(NioChannel.java:136)
                at org.apache.coyote.http11.upgrade.NioServletInputStream.fillReadBuffer(NioServletInputStream.java:136)
                at org.apache.coyote.http11.upgrade.NioServletInputStream.doRead(NioServletInputStream.java:80)
                at org.apache.coyote.http11.upgrade.AbstractServletInputStream.read(AbstractServletInputStream.java:124)
                at org.apache.tomcat.websocket.server.WsFrameServer.onDataAvailable(WsFrameServer.java:46)
                at
                org.apache.tomcat.websocket.server.WsHttpUpgradeHandler$WsReadListener.onDataAvailable(WsHttpUpgradeHandler.java:192)
                at
                org.apache.coyote.http11.upgrade.AbstractServletInputStream.onDataAvailable(AbstractServletInputStream.java:180)
                at org.apache.coyote.http11.upgrade.AbstractProcessor.upgradeDispatch(AbstractProcessor.java:95)
                at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:640)
                at
                org.apache.coyote.http11.Http11NioProtocol$Http11ConnectionHandler.process(Http11NioProtocol.java:223)
                at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1597)
                at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1555)
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
                at java.lang.Thread.run(Thread.java:744)
                If you try to press F5, then it might be that the Websocket connection is closed as soon as it was
                opened, or that the browser doesn't get a response for the request to chat.xhtml.
                Now, add synchronization by modifying ChatAnnotation's broadcast() method:
                private static void broadcast(String msg) {
                for (ChatAnnotation client : connections) {
                synchronized (client) {
                try {
                client.session.getBasicRemote().sendText(msg);
                } catch (Exception e) {
                }
                }
                }
                }
                and repeat the above steps.
                Now, if you open chat.xhtml with both IE and Firefox and do nothing, the WebSocket connection will not
                be closed.
                Even if you start to repeatedly press F5, most of the time everything will appear normal (besides
                getting IOExceptions and ClosedChannelExceptions).
                However, after I tried this several minutes, I still got the problems that the WebSocket connections are
                closed just after opening it (or after some time), or that the browser didn't get a response to its HTTP
                request, or that the browser got a raw WebSocket reply instead of the XHTML page reply (see added
                screenshots).
                I also got these exceptions:
                20-Nov-2013 00:18:20.037 SEVERE [http-nio-8080-exec-9] websocket.chat.ChatAnnotation.onError Chat Error:
                java.io.IOException: java.util.concurrent.ExecutionException: java.io.IOException: Key must be cancelled
                java.io.IOException: java.util.concurrent.ExecutionException: java.io.IOException: Key must be cancelled
                at
                org.apache.tomcat.websocket.WsRemoteEndpointImplBase.startMessageBlock(WsRemoteEndpointImplBase.java:226)
                at org.apache.tomcat.websocket.WsSession.sendCloseMessage(WsSession.java:476)
                at org.apache.tomcat.websocket.WsSession.onClose(WsSession.java:439)
                at org.apache.tomcat.websocket.server.WsHttpUpgradeHandler.close(WsHttpUpgradeHandler.java:172)
                at org.apache.tomcat.websocket.server.WsHttpUpgradeHandler.access$200(WsHttpUpgradeHandler.java:45)
                at
                org.apache.tomcat.websocket.server.WsHttpUpgradeHandler$WsReadListener.onDataAvailable(WsHttpUpgradeHandler.java:194)
                at
                org.apache.coyote.http11.upgrade.AbstractServletInputStream.onDataAvailable(AbstractServletInputStream.java:180)
                at org.apache.coyote.http11.upgrade.AbstractProcessor.upgradeDispatch(AbstractProcessor.java:95)
                at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:640)
                at
                org.apache.coyote.http11.Http11NioProtocol$Http11ConnectionHandler.process(Http11NioProtocol.java:223)
                at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1597)
                at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1555)
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
                at java.lang.Thread.run(Thread.java:744)
                Caused by: java.util.concurrent.ExecutionException: java.io.IOException: Key must be cancelled
                at org.apache.tomcat.websocket.FutureToSendHandler.get(FutureToSendHandler.java:102)
                at
                org.apache.tomcat.websocket.WsRemoteEndpointImplBase.startMessageBlock(WsRemoteEndpointImplBase.java:222)
                ...
                14 more
                Caused by: java.io.IOException: Key must be cancelled
                at
                org.apache.coyote.http11.upgrade.NioServletOutputStream.doWriteInternal(NioServletOutputStream.java:83)
                at org.apache.coyote.http11.upgrade.NioServletOutputStream.doWrite(NioServletOutputStream.java:60)
                at
                org.apache.coyote.http11.upgrade.AbstractServletOutputStream.writeInternal(AbstractServletOutputStream.java:118)
                at
                org.apache.coyote.http11.upgrade.AbstractServletOutputStream.write(AbstractServletOutputStream.java:85)
                at
                org.apache.tomcat.websocket.server.WsRemoteEndpointImplServer.onWritePossible(WsRemoteEndpointImplServer.java:94)
                at
                org.apache.tomcat.websocket.server.WsRemoteEndpointImplServer.doWrite(WsRemoteEndpointImplServer.java:81)
                at
                org.apache.tomcat.websocket.WsRemoteEndpointImplBase.writeMessagePart(WsRemoteEndpointImplBase.java:362)
                at org.apache.tomcat.websocket.WsRemoteEndpointImplBase.startMessage(WsRemoteEndpointImplBase.java:259)
                at
                org.apache.tomcat.websocket.WsRemoteEndpointImplBase.startMessageBlock(WsRemoteEndpointImplBase.java:217)
                ...
                14 more
                20-Nov-2013 00:32:53.483 SEVERE [http-nio-8080-exec-3]
                org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun
                java.lang.NullPointerException
                at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:593)
                at
                org.apache.coyote.http11.Http11NioProtocol$Http11ConnectionHandler.process(Http11NioProtocol.java:223)
                at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1597)
                at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1555)
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
                at java.lang.Thread.run(Thread.java:744)
                20-Nov-2013 00:35:19.333 SEVERE [http-nio-8080-exec-15]
                org.apache.tomcat.websocket.server.WsHttpUpgradeHandler.destroy Failed to close WebConnection while
                destroying the WebSocket HttpUpgradeHandler
                java.lang.NullPointerException
                at org.apache.tomcat.websocket.server.WsHttpUpgradeHandler.destroy(WsHttpUpgradeHandler.java:143)
                at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:715)
                at
                org.apache.coyote.http11.Http11NioProtocol$Http11ConnectionHandler.process(Http11NioProtocol.java:223)
                at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1597)
                at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1555)
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
                at java.lang.Thread.run(Thread.java:744)
                20-Nov-2013 00:35:19.327 SEVERE [http-nio-8080-exec-15]
                org.apache.coyote.http11.AbstractHttp11Processor.process Error processing request
                java.lang.IllegalArgumentException
                at java.nio.Buffer.position(Buffer.java:236)
                at sun.nio.ch.IOUtil.write(IOUtil.java:68)
                at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:487)
                at org.apache.tomcat.util.net.NioChannel.write(NioChannel.java:123)
                at org.apache.tomcat.util.net.NioBlockingSelector.write(NioBlockingSelector.java:101)
                at org.apache.tomcat.util.net.NioSelectorPool.write(NioSelectorPool.java:174)
                at org.apache.coyote.http11.InternalNioOutputBuffer.writeToSocket(InternalNioOutputBuffer.java:140)
                at org.apache.coyote.http11.InternalNioOutputBuffer.addToBB(InternalNioOutputBuffer.java:198)
                at org.apache.coyote.http11.InternalNioOutputBuffer.commit(InternalNioOutputBuffer.java:178)
                at org.apache.coyote.http11.AbstractHttp11Processor.action(AbstractHttp11Processor.java:739)
                at org.apache.coyote.Response.action(Response.java:180)
                at org.apache.coyote.Response.sendHeaders(Response.java:368)
                at org.apache.catalina.connector.OutputBuffer.doFlush(OutputBuffer.java:335)
                at org.apache.catalina.connector.OutputBuffer.close(OutputBuffer.java:290)
                at org.apache.catalina.connector.Response.finishResponse(Response.java:411)
                at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:560)
                at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1015)
                at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:642)
                at
                org.apache.coyote.http11.Http11NioProtocol$Http11ConnectionHandler.process(Http11NioProtocol.java:223)
                at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1597)
                at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1555)
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
                at java.lang.Thread.run(Thread.java:744)
                [1] http://markmail.org/message/ee3jch4zj2orltzs
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.tomcat.websocket.WsRemoteEndpointImplBase.java</file>
            <file type="M">org.apache.tomcat.websocket.TestWsRemoteEndpoint.java</file>
        </fixedFiles>
    </bug>
    <bug id="55996" opendate="2014-01-13 06:12:37" fixdate="2014-01-14 19:18:58" resolution="Fixed">
        <buginformation>
            <summary>Async context does not timeout with HTTP NIO connector.
            </summary>
            <description>Created attachment 31200 [details]
                Servlet with Async processing and Java Based client
                I created a ProblemServlet which receives request via a Java based client.
                The Servlet starts an Async processing for each request.
                Within the Async processing run() method there is a while loop which cyclically sends String messages to
                the client.
                The implementation of run method is shown.
                public void run()
                {
                try
                {
                String msg = "";
                ServletOutputStream outputStream = publisherAsyncCtx.getResponse().getOutputStream();
                boolean continu = true;
                while (continu)
                {
                msg = "";
                msg = "|" + " " + new Date();
                System.out.println("publishing message...
                " + msg);
                outputStream.println(msg);
                publisherAsyncCtx.getResponse().flushBuffer();
                try
                {
                Thread.sleep(1000);
                }
                catch (InterruptedException e)
                {
                System.out.println("sleep InterruptedException: " + e.getMessage());
                e.printStackTrace();
                }
                }
                }
                When a Java based console application client hits this servlet and reads its output stream, for around
                10 sec the messages arrive.
                But after 10 seconds the connection is closed by the server.
                Logs on the Server side:
                Starting the Async Context.
                publishing message...
                | Mon Jan 13 11:28:30 IST 2014
                publishing message...
                | Mon Jan 13 11:28:31 IST 2014
                publishing message...
                | Mon Jan 13 11:28:32 IST 2014
                publishing message...
                | Mon Jan 13 11:28:33 IST 2014
                publishing message...
                | Mon Jan 13 11:28:34 IST 2014
                publishing message...
                | Mon Jan 13 11:28:35 IST 2014
                publishing message...
                | Mon Jan 13 11:28:36 IST 2014
                publishing message...
                | Mon Jan 13 11:28:37 IST 2014
                publishing message...
                | Mon Jan 13 11:28:38 IST 2014
                publishing message...
                | Mon Jan 13 11:28:39 IST 2014
                publishing message...
                | Mon Jan 13 11:28:40 IST 2014
                publishing message...
                | Mon Jan 13 11:28:41 IST 2014
                Exception in thread "http-bio-8080-exec-6" java.lang.IllegalStateException: The request associated with
                the AsyncContext has already completed processing.
                at org.apache.catalina.core.AsyncContextImpl.check(AsyncContextImpl.java:521)
                at org.apache.catalina.core.AsyncContextImpl.getResponse(AsyncContextImpl.java:245)
                at com.pg.orion.artcloopcheck.ProblemServlet$AsynRunnable.run(ProblemServlet.java:67)
                at org.apache.catalina.core.AsyncContextImpl$RunnableWrapper.run(AsyncContextImpl.java:557)
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
                at java.lang.Thread.run(Thread.java:722)
                Issue is seen with "apache-tomcat-7.0.50", "apache-tomcat-7.0.47".
                Not tested with other release 7 variants.
                The same codebase when run on "apache-tomcat-8.0.0-RC10" there are no issues.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.tomcat.util.net.SocketWrapper.java</file>
        </fixedFiles>
    </bug>
    <bug id="49159" opendate="2010-04-20 13:33:43" fixdate="2019-01-25 04:07:54" resolution="Fixed">
        <buginformation>
            <summary>Improve ThreadLocal memory leak clean-up.
            </summary>
            <description>Doing this in a thread-safe way means performing the clean-up in the thread where the
                ThreadLocal exists.
                A likely point is just before the Thread gets returned to the pool.
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.tomcat.util.threads.ThreadPoolExecutor.java</file>
            <file type="M">org.apache.catalina.core.StandardContext.java</file>
            <file type="M">org.apache.tomcat.util.threads.DedicatedThreadExecutor.java</file>
            <file type="M">org.apache.tomcat.util.threads.DedicatedThreadExecutorTest.java</file>
            <file type="M">org.apache.catalina.core.ThreadLocalLeakPreventionListener.java</file>
            <file type="M">org.apache.catalina.core.StandardContext.java</file>
            <file type="M">org.apache.catalina.core.StandardThreadExecutor.java</file>
            <file type="M">org.apache.catalina.core.ThreadLocalLeakPreventionListener.java</file>
            <file type="M">org.apache.catalina.loader.WebappClassLoader.java</file>
            <file type="M">org.apache.tomcat.util.threads.Constants.java</file>
            <file type="M">org.apache.tomcat.util.threads.TaskQueue.java</file>
            <file type="M">org.apache.tomcat.util.threads.TaskThread.java</file>
            <file type="M">org.apache.tomcat.util.threads.ThreadPoolExecutor.java</file>
            <file type="M">org.apache.catalina.core.StandardContext.java</file>
            <file type="M">org.apache.catalina.core.StandardThreadExecutor.java</file>
            <file type="M">org.apache.catalina.core.ThreadLocalLeakPreventionListener.java</file>
            <file type="M">org.apache.catalina.loader.WebappClassLoader.java</file>
            <file type="M">org.apache.catalina.loader.WebappLoader.java</file>
            <file type="M">org.apache.tomcat.util.threads.Constants.java</file>
            <file type="M">org.apache.tomcat.util.threads.TaskQueue.java</file>
            <file type="M">org.apache.tomcat.util.threads.TaskThread.java</file>
            <file type="M">org.apache.tomcat.util.threads.TaskThreadFactory.java</file>
            <file type="M">org.apache.tomcat.util.threads.ThreadPoolExecutor.java</file>
        </fixedFiles>
    </bug>
    <bug id="50352" opendate="2010-11-27 16:30:59" fixdate="2010-11-29 12:00:30" resolution="Fixed">
        <buginformation>
            <summary>AsyncListener.onComplete is not called after AsyncContext.complete() is called.
            </summary>
            <description>Using servlet 3 async features, when asyncContext.complete(); is called from an async thread,
                the AsyncListener onComplete() method is not called though it should be.
                Example Servlet :
                package test;
                import java.io.IOException;
                import javax.servlet.AsyncContext;
                import javax.servlet.AsyncEvent;
                import javax.servlet.AsyncListener;
                import javax.servlet.ServletException;
                import javax.servlet.annotation.WebServlet;
                import javax.servlet.http.HttpServlet;
                import javax.servlet.http.HttpServletRequest;
                import javax.servlet.http.HttpServletResponse;
                /**
                * Servlet implementation class MyServlet
                */
                @WebServlet(value = "/MyServlet", asyncSupported = true)
                public class MyServlet extends HttpServlet implements AsyncListener {
                private static final long serialVersionUID = 1L;
                protected void doGet(HttpServletRequest request,
                HttpServletResponse response) throws ServletException, IOException {
                final AsyncContext asyncContext = request.startAsync(request, response);
                asyncContext.addListener(this);
                asyncContext.start(new Runnable() {
                @Override
                public void run() {
                try {
                Thread.sleep(5 * 1000);
                asyncContext.getResponse().getWriter().write("Hello world");
                asyncContext.complete();
                } catch (Exception e) {
                e.printStackTrace();
                }
                }
                });
                }
                @Override
                public void onComplete(AsyncEvent arg0) throws IOException {
                System.out.println("onComplete " + arg0);
                }
                @Override
                public void onError(AsyncEvent arg0) throws IOException {
                System.out.println("onError " + arg0);
                }
                @Override
                public void onStartAsync(AsyncEvent arg0) throws IOException {
                System.out.println("onStartAsync " + arg0);
                }
                @Override
                public void onTimeout(AsyncEvent arg0) throws IOException {
                System.out.println("onTimeout " + arg0);
                }
                }
            </description>
            <version>1.1.0</version>
            <fixedVersion>1.1.0</fixedVersion>
            <type>Bug</type>
        </buginformation>
        <fixedFiles>
            <file type="M">org.apache.catalina.core.TestAsyncContextImpl.java</file>
            <file type="M">org.apache.coyote.AsyncStateMachine.java</file>
        </fixedFiles>
    </bug>
</bugrepository>