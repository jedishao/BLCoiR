<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sat Dec 03 23:52:49 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HIVE-5317/HIVE-5317.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HIVE-5317] Implement insert, update, and delete in Hive with full ACID support</title>
                <link>https://issues.apache.org/jira/browse/HIVE-5317</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;Many customers want to be able to insert, update and delete rows from Hive tables with full ACID support. The use cases are varied, but the form of the queries that should be supported are:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;INSERT INTO tbl SELECT &#8230;&lt;/li&gt;
	&lt;li&gt;INSERT INTO tbl VALUES ...&lt;/li&gt;
	&lt;li&gt;UPDATE tbl SET &#8230; WHERE &#8230;&lt;/li&gt;
	&lt;li&gt;DELETE FROM tbl WHERE &#8230;&lt;/li&gt;
	&lt;li&gt;MERGE INTO tbl USING src ON &#8230; WHEN MATCHED THEN ... WHEN NOT MATCHED THEN ...&lt;/li&gt;
	&lt;li&gt;SET TRANSACTION LEVEL &#8230;&lt;/li&gt;
	&lt;li&gt;BEGIN/END TRANSACTION&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Use Cases&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Once an hour, a set of inserts and updates (up to 500k rows) for various dimension tables (eg. customer, inventory, stores) needs to be processed. The dimension tables have primary keys and are typically bucketed and sorted on those keys.&lt;/li&gt;
	&lt;li&gt;Once a day a small set (up to 100k rows) of records need to be deleted for regulatory compliance.&lt;/li&gt;
	&lt;li&gt;Once an hour a log of transactions is exported from a RDBS and the fact tables need to be updated (up to 1m rows)  to reflect the new data. The transactions are a combination of inserts, updates, and deletes. The table is partitioned and bucketed.&lt;/li&gt;
&lt;/ul&gt;
</description>
                <environment></environment>
        <key id="12669392">HIVE-5317</key>
            <summary>Implement insert, update, and delete in Hive with full ACID support</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="owen.omalley">Owen O&apos;Malley</assignee>
                                    <reporter username="owen.omalley">Owen O&apos;Malley</reporter>
                        <labels>
                    </labels>
                <created>Thu, 19 Sep 2013 04:57:06 +0000</created>
                <updated>Thu, 29 Sep 2016 18:45:08 +0000</updated>
                            <resolved>Thu, 25 Sep 2014 16:18:17 +0000</resolved>
                                                    <fixVersion>0.14.0</fixVersion>
                                    <component>Transactions</component>
                        <due></due>
                            <votes>34</votes>
                                    <watches>182</watches>
                                                                                                            <comments>
                            <comment id="13771598" author="cwsteinbach" created="Thu, 19 Sep 2013 05:29:06 +0000"  >&lt;p&gt;Will these features place any limitations on which storage formats you can use? Also, I don&apos;t think it&apos;s possible to support ACID guarantees and HCatalog (i.e. file permission based authorization) simultaneously on top of the same Hive warehouse. Is there a plan in place for fixing that?&lt;/p&gt;</comment>
                            <comment id="13771894" author="alangates" created="Thu, 19 Sep 2013 13:41:51 +0000"  >&lt;p&gt;The only requirement is that the file format must be able to support a rowid.  With things like text and sequence file this can be done via a byte offset.&lt;/p&gt;

&lt;p&gt;I&apos;m not seeing why this falls apart in the file based authorization.  Are you worried that different users will own the base and delta files?  It&apos;s no different than the current case where different users may own different partitions.  We will need to make sure the compactions can still happen in this case, that is that the compaction can be run as the user who owns the table, not as Hive.&lt;/p&gt;</comment>
                            <comment id="13771983" author="owen.omalley" created="Thu, 19 Sep 2013 15:44:32 +0000"  >&lt;p&gt;Here are my thoughts about how it can be approached.&lt;/p&gt;</comment>
                            <comment id="13772005" author="brocknoland" created="Thu, 19 Sep 2013 16:30:05 +0000"  >&lt;p&gt;Just curious, I was surprised I didn&apos;t see adding transactions to HBase + support in the hbase storage handler as a potential alternative implementation. Could you speak to why your approach is superior to that approach?  Also, it&apos;d be great if you posted design document on the design document section of the wiki: &lt;a href=&quot;https://cwiki.apache.org/confluence/display/Hive/DesignDocs&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://cwiki.apache.org/confluence/display/Hive/DesignDocs&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13772021" author="alangates" created="Thu, 19 Sep 2013 16:47:50 +0000"  >&lt;p&gt;Brock, we did look at that.  We didn&apos;t go that route for a couple of reasons:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Adding transactions to HBase is a fair amount of work.  See Google&apos;s Percolator paper on one approach to that.&lt;/li&gt;
	&lt;li&gt;HBase can&apos;t offer the same scan speed as HDFS.  Since we&apos;re choosing to focus this on updates done in the OLAP style work loads HBase isn&apos;t going to be a great storage mechanism for the data.  I agree it might make sense to have transactions on HBase for a more OLTP style workload.&lt;/li&gt;
&lt;/ol&gt;
</comment>
                            <comment id="13772028" author="owen.omalley" created="Thu, 19 Sep 2013 16:53:58 +0000"  >&lt;p&gt;Expanding on Alan&apos;s comments:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;The HBase scan rate is much lower than HDFS, especially with short-circuit reads.&lt;/li&gt;
	&lt;li&gt;HBase is tuned for a write-heavy workloads.&lt;/li&gt;
	&lt;li&gt;HBase doesn&apos;t have a columnar format and can&apos;t support column projection.&lt;/li&gt;
	&lt;li&gt;HBase doesn&apos;t have predicate pushdown into the file format.&lt;/li&gt;
	&lt;li&gt;HBase doesn&apos;t have the equivalent of partitions or buckets.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13772046" author="stack" created="Thu, 19 Sep 2013 17:09:07 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=alangates&quot; class=&quot;user-hover&quot; rel=&quot;alangates&quot;&gt;Alan Gates&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Looks like a bunch of hbase primitives done as mapreduce jobs.&lt;/p&gt;

&lt;p&gt;At first blush, on 1., percolator would be a bunch of work but looks less than what is proposed here (would you need percolator given you write the transaction id into the row?).  On 2., if hbase were made write ORC, couldn&apos;t you MR the files hbase writes after asking hbase to snapshot.&lt;/p&gt;</comment>
                            <comment id="13772077" author="stack" created="Thu, 19 Sep 2013 17:35:28 +0000"  >&lt;blockquote&gt;&lt;p&gt;The HBase scan rate is much lower than HDFS, especially with short-circuit reads.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;What kinda of numbers are you talking Owen?  Would be interested in knowing what they are.  Implication would be also that it cannot be improved?  Or scanning the files written by hbase offline from a snapshot wouldn&apos;t work from you (snapshots are cheap in hbase.  Going by your use cases, you&apos;d be doing these runs infrequently enough).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;HBase is tuned for a write-heavy workloads.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Funny.  Often we&apos;re accused of the other extreme.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;HBase doesn&apos;t have a columnar format and can&apos;t support column projection.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It doesn&apos;t. Too much work to add a storage engine that wrote columnar?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;HBase doesn&apos;t have the equivalent of partitions or buckets.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;In hbase we call them &apos;Regions&apos;.&lt;/p&gt;</comment>
                            <comment id="13772086" author="bikassaha" created="Thu, 19 Sep 2013 17:41:37 +0000"  >&lt;p&gt;Some questions which I am sure have been considered but are not clear in the document.&lt;br/&gt;
Should metastore heartbeat be in the job itself and not the client since the job is the source of truth and the client can disappear. What happens if the client disappears but the job completes with success and manages to promote the output files?&lt;br/&gt;
Is transaction id per file or per metastore? Where does the metastore recover the last transaction id(s) from after restart?&lt;/p&gt;</comment>
                            <comment id="13772115" author="owen.omalley" created="Thu, 19 Sep 2013 17:59:45 +0000"  >&lt;p&gt;Bikas,&lt;br/&gt;
  In Hive if the client disappears, the query fails, because the final work (output promotion, display to the user) is done by the client. Also don&apos;t forget that a single query may be composed on many MR jobs, although obviously that changes on Tez. &lt;/p&gt;

&lt;p&gt;The transaction id is global for all of the tasks working on the same query. &lt;/p&gt;

&lt;p&gt;The metastore&apos;s data in stored in an underlying SQL database, so the transaction information will need to be there also.&lt;/p&gt;</comment>
                            <comment id="13772192" author="ehans" created="Thu, 19 Sep 2013 19:29:30 +0000"  >&lt;p&gt;Overall this looks like a workable approach give the use cases described (mostly coarse grained updates with a low transaction rate), and it has the benefit that it doesn&apos;t take a dependency on another large piece of software like an update-aware DBMS or NoSQL store.&lt;/p&gt;

&lt;p&gt;Regarding use cases, it appears that this design won&apos;t be able to have fast performance for fine-grained inserts. E.g. there might be scenarios where you want to insert one row into a fact table every 10 milliseconds in a separate transaction and have the rows immediately visible to readers. Are you willing to forgo that use case? It sounds like yes. This may be reasonable. If you want to handle it then a different design for the delta insert file information is probably needed, i.e. a store that&apos;s optimized for short write transactions.&lt;/p&gt;

&lt;p&gt;I didn&apos;t see any obvious problem, due to the versioned scans, but is this design safe from the Halloween problem? That&apos;s the problem where an update scan sees its own updates again, causing an infinite loop or incorrect update. An argument that the design is safe from this would be good.&lt;/p&gt;

&lt;p&gt;You mention that you will have one type of delta file that encodes updates directly, for sorted files. Is this really necessary, or can you make updates illegal for sorted files? If updates can always be modelled as insert plus deleted, that simplifies things.&lt;/p&gt;

&lt;p&gt;How do you ensure that the delta files are fully written (committed) to the storage system before the metastore treats the transaction that created the delta file as committed?&lt;/p&gt;

&lt;p&gt;It&apos;s not completely clear why you need exactly the transaction ID information specified in the delta file names. E.g. would just the transaction ID (start timestamp) be enough? A precise specification of how they are used would be useful.&lt;/p&gt;

&lt;p&gt;Explicitly explaining what happens when a transaction aborts and how its delta files get ignored and then cleaned up would be useful.&lt;/p&gt;

&lt;p&gt;Is there any issue with correctness of task retry in the presence of updates if a task fails? It appears that it is safe due to the snapshot isolation. Explicitly addressing this in the specification would be good.&lt;/p&gt;
</comment>
                            <comment id="13773242" author="alangates" created="Fri, 20 Sep 2013 17:59:20 +0000"  >&lt;p&gt;One thing that might help people understand the design, take a look at &lt;a href=&quot;http://research.microsoft.com/pubs/193599/Apollo3%20-%20Sigmod%202013%20-%20final.pdf&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://research.microsoft.com/pubs/193599/Apollo3%20-%20Sigmod%202013%20-%20final.pdf&lt;/a&gt; a paper that influenced our thinking and design.&lt;/p&gt;</comment>
                            <comment id="13773322" author="alangates" created="Fri, 20 Sep 2013 19:13:21 +0000"  >&lt;blockquote&gt;&lt;p&gt;Regarding use cases, it appears that this design won&apos;t be able to have fast performance for fine-grained inserts. ...&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Agreed, this will fail badly in a one insert at a time situation.  That isn&apos;t what we&apos;re going after.  We would like to be able to handle a batch inserts every minute, but for the moment that seems like the floor.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I didn&apos;t see any obvious problem, due to the versioned scans, but is this design safe from the Halloween problem?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;As a rule Hive jobs always define their input up front and then scan only once.  So even though an update is writing a new record, the delta file it&apos;s writing into shouldn&apos;t be defined as part of it&apos;s input.  In the future when we move to having one delta file rather than one per write (more details on that to follow), this may be more of an issue, and we&apos;ll need to think about how to avoid it.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;How do you ensure that the delta files are fully written (committed) to the storage system before the metastore treats the transaction that created the delta file as committed?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The OutputCommitter will move the new delta files from a temp directory to the directory of the base file (as is standard in Hadoop apps).  Only after this will the Hive client communicate to the metastore that the transaction is committed.  If there is a failure between moving the files from temp to base dir, readers will still ignore these files as they will have a transaction id that is listed as aborted.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;It&apos;s not completely clear why you need exactly the transaction ID information specified in the delta file names. E.g. would just the transaction ID (start timestamp) be enough?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The reason for including the end id is so that readers can quickly decide whether they need to scan that file at all, and potentially prune files from their scans.  Does that answer the question?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Is there any issue with correctness of task retry in the presence of updates if a task fails? &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;As in standard Hadoop practice, output from tasks will be written to a temp directory.  Failed or killed tasks&apos; output will never be promoted to the base file directory and thus will never be seen by readers.&lt;/p&gt;

&lt;p&gt;I&apos;m working on updating the doc with answers to these.  One of us will post the updated doc soon.&lt;/p&gt;</comment>
                            <comment id="13773361" author="ehans" created="Fri, 20 Sep 2013 20:14:13 +0000"  >&lt;p&gt;Okay, thanks for the response!&lt;/p&gt;</comment>
                            <comment id="13794803" author="kstirman" created="Tue, 15 Oct 2013 02:27:30 +0000"  >&lt;p&gt;I&apos;m curious - why not use ZK to maintain transactional state.&lt;/p&gt;

&lt;p&gt;Hive metastore, if I&apos;m not mistaken, is not HA by default, and it imposes the associated complexity of HA (for MySQL and PG at least) on the user.&lt;/p&gt;
</comment>
                            <comment id="13800912" author="owen.omalley" created="Mon, 21 Oct 2013 18:16:57 +0000"  >&lt;p&gt;Hive already depends on the metastore being up, so it isn&apos;t adding a new SPoF. Zookeeper adds additional semantic complexity, especially for highly dynamic data.&lt;/p&gt;</comment>
                            <comment id="13825611" author="appodictic" created="Mon, 18 Nov 2013 18:50:05 +0000"  >&lt;p&gt;I have two fundamental problems with this concept.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The only requirement is that the file format must be able to support a rowid. With things like text and sequence file this can be done via a byte offset.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This is a good reason not to do this. Things that  only work for some formats create fragmentation. What about format&apos;s that do not have a row id? What if the user is already using the key for something else like data?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Once an hour a log of transactions is exported from a RDBS and the fact tables need to be updated (up to 1m rows) to reflect the new data. The transactions are a combination of inserts, updates, and deletes. The table is partitioned and bucketed.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;What this ticket describes seems like a bad use case for hive. Why would the user not simply create a new table partitioned by hour? What is the need to transaction ally in-place update a table? &lt;/p&gt;

&lt;p&gt;It seems like the better solution would be for the user to log these updates themselves and then export the table with a tool like squoop periodically.  &lt;/p&gt;

&lt;p&gt;I see this as a really complicated piece of work, for a narrow use case, and I have a very difficult time believing adding transactions to hive to support this is the right answer.&lt;/p&gt;</comment>
                            <comment id="13825619" author="appodictic" created="Mon, 18 Nov 2013 18:55:39 +0000"  >&lt;p&gt;By the way. I do work like this very often, and having tables that update periodically cause a lot of problems. The first is when you have to re-compute a result 4 days later.&lt;/p&gt;

&lt;p&gt;You do not want a fresh up-to-date table, you want the table as it existed 4 days ago. When you want to troubleshoot a result you do not want your intermediate tables trampled over. When you want to rebuild a months worth of results you want to launch 31 jobs in parallel not 31 jobs in series. &lt;/p&gt;

&lt;p&gt;In fact in programming hive I suggest ALWAYS partitioning this dimension tables by time and NOT doing what this ticket is describing for the reasons above (and more)&lt;/p&gt;</comment>
                            <comment id="13826009" author="owen.omalley" created="Tue, 19 Nov 2013 00:48:22 +0000"  >&lt;p&gt;Ed,&lt;br/&gt;
   If you don&apos;t use the insert, update, and delete commands, they won&apos;t impact your use of Hive. On the other hand, there are a wide number of users who need ACID and updates.&lt;/p&gt;</comment>
                            <comment id="13826137" author="thejas" created="Tue, 19 Nov 2013 03:28:07 +0000"  >&lt;p&gt;Ed, For the data re-processing use case, this approach is not what is recommended. This approach is meant to be used for use cases where your changes to a partition are small fraction of the existing number of rows.&lt;br/&gt;
Even with this approach, it still would make sense to partition your data by time for &apos;fact tables&apos;. Your dimension table has &lt;b&gt;new&lt;/b&gt; records being added periodically, making it more like the &apos;fact table&apos; use case. This approach will also work with tables partitioned by time.&lt;/p&gt;</comment>
                            <comment id="13826191" author="appodictic" created="Tue, 19 Nov 2013 04:34:54 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Ed,&lt;br/&gt;
If you don&apos;t use the insert, update, and delete commands, they won&apos;t impact your use of Hive. On the other hand, there are a wide number of users who need ACID and updates.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Why don&apos;t those users just use an acid database?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The dimension tables have primary keys and are typically bucketed and sorted on those keys.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;All the use cases defined seem to be exactly what hive is not built for.&lt;br/&gt;
1) Hive does not do much/any optimization of a table when it is sorted.&lt;br/&gt;
2) Hive tables do not have primary keys&lt;br/&gt;
3) Hive is not made to play with tables of only a few rows&lt;/p&gt;

&lt;p&gt;It seems like the idea is to turn hive and hive metastore into a once shot database for processes that can easily be done differently. &lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Once a day a small set (up to 100k rows) of records need to be deleted for regulatory compliance.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;1. squoop export to rdbms&lt;br/&gt;
2. run query on rdbms&lt;br/&gt;
3. write back to hive.&lt;/p&gt;

&lt;p&gt;I am not ready to vote -1, but I am struggling to understand why anyone would want to use hive to solve the use cases described. This seems like a square peg in a round hole solution. It feels like something that belongs outside of hive.&lt;/p&gt;

&lt;p&gt;It feels a lot like this:&lt;br/&gt;
&lt;a href=&quot;http://db.cs.yale.edu/hadoopdb/hadoopdb.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://db.cs.yale.edu/hadoopdb/hadoopdb.html&lt;/a&gt;&lt;/p&gt;





</comment>
                            <comment id="13826207" author="appodictic" created="Tue, 19 Nov 2013 05:06:03 +0000"  >&lt;blockquote&gt;
&lt;p&gt;&quot;In theory the base can be in any format, but ORC will be required for v1&quot;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;This is exactly what I talk about when I talk about fragmentation. Hive can not be a system where features only work when using a specific input format. The feature must be applicable to more then just the single file format. Taging &quot;other file formats&quot; in the &quot;LATER&quot; bothers me. Wouldn&apos;t the community have more utility of something that worked against a TextFormat was written first, then later against other formats. I know about the &quot;stinger initiative&quot;, developing features that only work with specific input formats does not seem like the correct course of action. It goes against our core design principals:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/Hive/Home&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://cwiki.apache.org/confluence/display/Hive/Home&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&quot;Hive does not mandate read or written data be in the &quot;Hive format&quot;---there is no such thing. Hive works equally well on Thrift, control delimited, or your specialized data formats. Please see File Format and SerDe in the Developer Guide for details.&quot;&lt;/p&gt;</comment>
                            <comment id="13832042" author="sershe" created="Mon, 25 Nov 2013 23:09:56 +0000"  >&lt;p&gt;I think &quot;the small number of rows&quot; meant above was for the update, not the entire partition.&lt;br/&gt;
So, large dataset, small number of rows updated. Exporting entire dataset to rdbms to perform a query seems excessive in this case&lt;/p&gt;</comment>
                            <comment id="13905932" author="lefty@hortonworks.com" created="Wed, 19 Feb 2014 19:29:38 +0000"  >&lt;p&gt;Off topic:  This ticket has 100 watchers.  Is that a record?&lt;/p&gt;</comment>
                            <comment id="13906059" author="alangates" created="Wed, 19 Feb 2014 20:52:13 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/MAPREDUCE-279&quot; title=&quot;Map-Reduce 2.0&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAPREDUCE-279&quot;&gt;&lt;del&gt;MAPREDUCE-279&lt;/del&gt;&lt;/a&gt;, at 109, currently out scores us.  There may be others, but it would be cool to have more watchers than Yarn. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13920000" author="vinodkv" created="Tue, 4 Mar 2014 20:53:03 +0000"  >&lt;blockquote&gt;&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/MAPREDUCE-279&quot; title=&quot;Map-Reduce 2.0&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAPREDUCE-279&quot;&gt;&lt;del&gt;MAPREDUCE-279&lt;/del&gt;&lt;/a&gt;, at 109, currently out scores us. There may be others, but it would be cool to have more watchers than Yarn.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Hehe, looks like we have a race. I&apos;ll go ask some of us YARN folks who are also watching this JIRA to stop watching this one &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/biggrin.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13968696" author="codebunker" created="Mon, 14 Apr 2014 19:11:25 +0000"  >&lt;p&gt;Vinod.. it is very much obvious.. ACID, updates and Deletes are one of the most awaited features of Hive and many people like me are waiting for the same.. &lt;/p&gt;</comment>
                            <comment id="13968698" author="codebunker" created="Mon, 14 Apr 2014 19:13:33 +0000"  >&lt;p&gt;Will these features be supported on all Hive file formats i.e. Sequencefile, Text, ORC, RC etc.&lt;/p&gt;</comment>
                            <comment id="13968789" author="alangates" created="Mon, 14 Apr 2014 20:20:55 +0000"  >&lt;p&gt;Currently they are being supported in ORC.  It is done in such a way that it could be extended to any file format that can support a row id, though there is some code to write to make it happen.  It could be extended to support text or sequence file by using offset in the base file as the surrogate for rowid.  I&apos;m not sure if this would work for RC file or not. &lt;/p&gt;</comment>
                            <comment id="13984506" author="codebunker" created="Tue, 29 Apr 2014 17:12:11 +0000"  >&lt;p&gt;Got a quick question from our interested customer - Will Hive be supporting commit and rollback too? And if yes, how will it be done?&lt;/p&gt;</comment>
                            <comment id="13984600" author="alangates" created="Tue, 29 Apr 2014 18:06:11 +0000"  >&lt;p&gt;The intention is to support begin/commit/rollback, hopefully in the next release.  See &lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12614488/HiveTransactionManagerDetailedDesign%20%281%29.pdf&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12614488/HiveTransactionManagerDetailedDesign%20%281%29.pdf&lt;/a&gt; for the design of the transaction manager which will be handling this.  Note that with Hive 0.13 we already do transactions in the streaming ingest interface, they just aren&apos;t available through SQL yet.&lt;/p&gt;</comment>
                            <comment id="14012722" author="satishthumar" created="Thu, 29 May 2014 18:59:01 +0000"  >&lt;p&gt;Can someone put a example to update records on Hive ORC table?&lt;/p&gt;</comment>
                            <comment id="14032718" author="ankamv" created="Mon, 16 Jun 2014 18:17:36 +0000"  >&lt;p&gt;Eagerly waiting for this ACID support...Do you know which release and by when this will be implemented in?&lt;/p&gt;

&lt;p&gt;Regards,&lt;br/&gt;
Venkat&lt;/p&gt;</comment>
                            <comment id="14032720" author="a.schroeder@bigpoint.net" created="Mon, 16 Jun 2014 18:19:05 +0000"  >&lt;p&gt;Thanks for your e-mail.&lt;/p&gt;

&lt;p&gt;Unfortunately, I&#8217;ll be away until June 25th, 2014.&lt;br/&gt;
Please note that your mail will not be forwarded.&lt;/p&gt;

&lt;p&gt;If you have urgent requests, please contact Nils Hofmeister.&lt;/p&gt;

&lt;p&gt;Best regards,&lt;br/&gt;
   Alain Schr&#246;der&lt;/p&gt;</comment>
                            <comment id="14032791" author="alangates" created="Mon, 16 Jun 2014 19:04:17 +0000"  >&lt;p&gt;My hope is to have the INSERT, UPDATE, DELETE functionality working by the next release of Hive.&lt;/p&gt;</comment>
                            <comment id="14032826" author="ankamv" created="Mon, 16 Jun 2014 19:33:39 +0000"  >&lt;p&gt;Thanks Alan. When the next release is scheduled? &lt;/p&gt;</comment>
                            <comment id="14065627" author="ankamv" created="Thu, 17 Jul 2014 21:52:56 +0000"  >&lt;p&gt;Any update on the next release of Hive with this feature?&lt;/p&gt;</comment>
                            <comment id="14141743" author="muthu" created="Sat, 20 Sep 2014 04:41:57 +0000"  >&lt;p&gt;Waiting for this Hive release, when we can expect for this release.&lt;/p&gt;</comment>
                            <comment id="14142013" author="alangates" created="Sat, 20 Sep 2014 14:48:39 +0000"  >&lt;p&gt;The discussion of when to branch for this has been going on on dev@hive.apache.org for a bit now, see &lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/hive-dev/201408.mbox/%3CCAKjA-pyhnHhxjaCYhWibX3o-RfQ7g2Sk9fyLYBN%3DFx6UofJ33A%40mail.gmail.com%3E&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://mail-archives.apache.org/mod_mbox/hive-dev/201408.mbox/%3CCAKjA-pyhnHhxjaCYhWibX3o-RfQ7g2Sk9fyLYBN%3DFx6UofJ33A%40mail.gmail.com%3E&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The summary is, any day now.  Once we branch it&apos;s usually ~4 weeks for stabilization and release mechanics before the release.&lt;/p&gt;</comment>
                            <comment id="14147916" author="alangates" created="Thu, 25 Sep 2014 16:18:17 +0000"  >&lt;p&gt;All the sub-tasks have been completed.&lt;/p&gt;</comment>
                            <comment id="14152764" author="dapengsun" created="Tue, 30 Sep 2014 03:40:23 +0000"  >&lt;p&gt;Hi Owen &amp;amp; Alan&lt;br/&gt;
The feature is great!&lt;br/&gt;
I have a question, if the cluster enable security(Kerberos), does &lt;tt&gt;ZooKeeperHiveLockManager&lt;/tt&gt; support a security Zookeeper? &lt;/p&gt;</comment>
                            <comment id="14210878" author="thejas" created="Thu, 13 Nov 2014 19:43:02 +0000"  >&lt;p&gt;This has been fixed in 0.14 release. Please open new jira if you see any issues.&lt;/p&gt;</comment>
                            <comment id="14212326" author="joyoungzhang@gmail.com" created="Fri, 14 Nov 2014 14:53:24 +0000"  >&lt;p&gt;An error occurred when I use hive  release 0.14.0&lt;/p&gt;

&lt;p&gt;delete from pokes where foo=97;&lt;/p&gt;

&lt;p&gt;&quot;FAILED: SemanticException &lt;span class=&quot;error&quot;&gt;&amp;#91;Error 10294&amp;#93;&lt;/span&gt;: Attempt to do update or delete using transaction manager that does not support these operations.&quot;&lt;/p&gt;
</comment>
                            <comment id="14212402" author="alangates" created="Fri, 14 Nov 2014 15:47:29 +0000"  >&lt;p&gt;See &lt;a href=&quot;https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DML&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DML&lt;/a&gt; and &lt;a href=&quot;https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions&lt;/a&gt; for information on how to set up your cluster and tables to use update and delete.&lt;/p&gt;</comment>
                            <comment id="14235328" author="sanjiv singh" created="Fri, 5 Dec 2014 10:12:07 +0000"  >&lt;p&gt;I am looking for named insertion in hive. Seems it is not supoorted as you can&apos;t explicitly specify column order. &lt;/p&gt;

&lt;p&gt;INSERT INTO tbl(colm1, colm2...) VALUES....&lt;/p&gt;

&lt;p&gt;is NAMED INSERTION supported in hive ?  If No, then Can we know rational behind it ? and Any future plan for it?&lt;/p&gt;

&lt;p&gt;Thanks,&lt;br/&gt;
Sanjiv &lt;/p&gt;</comment>
                            <comment id="14235330" author="anamaria" created="Fri, 5 Dec 2014 10:13:40 +0000"  >&lt;p&gt;&#65279;&lt;br/&gt;
Hello,&lt;br/&gt;
I am currently out of the office on an extended leave. During my absence, I will have no access to e-mail.&lt;br/&gt;
Please contact Fernanda Tavares, Director, Software Development, at ftavares@syncsort.com.&lt;/p&gt;

&lt;p&gt;Regards,&lt;/p&gt;

&lt;p&gt;Ana-Maria Badulescu&lt;br/&gt;
Senior Manager - Software Development&lt;/p&gt;

&lt;p&gt;Syncsort Incorporated&lt;br/&gt;
P: 201-930-8246&lt;br/&gt;
E: abadulescu@syncsort.com&amp;lt;&lt;a href=&quot;https://mail.syncsort.com/ecp/Organize/lrabin@syncsort.com&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://mail.syncsort.com/ecp/Organize/lrabin@syncsort.com&lt;/a&gt;&amp;gt;&lt;br/&gt;
www.syncsort.com&amp;lt;&lt;a href=&quot;http://www.syncsort.com/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.syncsort.com/&lt;/a&gt;&amp;gt;&lt;/p&gt;

&lt;p&gt;Integrating BIG data&#8230; Smarter&lt;/p&gt;


</comment>
                            <comment id="14238284" author="alangates" created="Mon, 8 Dec 2014 19:11:50 +0000"  >&lt;blockquote&gt;&lt;p&gt;is NAMED INSERTION supported in hive ? If No, then Can we know rational behind it ? and Any future plan for it?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;This is a larger issue than just ACID, as Hive doesn&apos;t support named insertion.  I hope to add it soon for all insert, not just ACID inserts.  I don&apos;t have a planned release for that yet.&lt;/p&gt;</comment>
                            <comment id="14252900" author="madhunaveen" created="Fri, 19 Dec 2014 04:16:18 +0000"  >&lt;p&gt;Did u get your issue resolved....I have modified all the config changes recommended...insert values works but update and delete error is showing the same error... Appreciate if u can help&lt;/p&gt;</comment>
                            <comment id="14252908" author="madhunaveen" created="Fri, 19 Dec 2014 04:23:33 +0000"  >&lt;p&gt;I followed all the config recommendations and created table in ACID but delete and update is still throwing error 10294 in hive .14..can someone help in fixing it&lt;/p&gt;</comment>
                            <comment id="14253591" author="alangates" created="Fri, 19 Dec 2014 16:12:54 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=madhunaveen&quot; class=&quot;user-hover&quot; rel=&quot;madhunaveen&quot;&gt;Madhu&lt;/a&gt;, the place to ask for help is on hive&apos;s dev list.  Please post the error logs you are getting from both your client and your metastore instance.  The DDL you used to create the table will also be very helpful.&lt;/p&gt;</comment>
                            <comment id="14358084" author="lifanhong" created="Thu, 12 Mar 2015 04:28:00 +0000"  >&lt;p&gt;insert into table values() when UTF-8 character is not correct&lt;/p&gt;



&lt;p&gt;insert into table test_acid partition(pt=&apos;pt_2&apos;)&lt;br/&gt;
 values( 2, &apos;&#20013;&#25991;_2&apos; , &apos;city_2&apos; )&lt;br/&gt;
 ;&lt;/p&gt;

&lt;p&gt;hive&amp;gt; select *&lt;br/&gt;
 &amp;gt; from test_acid &lt;br/&gt;
 &amp;gt; ;&lt;br/&gt;
 OK&lt;br/&gt;
 2 -&#65533;_2 city_2 pt_2&lt;br/&gt;
 Time taken: 0.237 seconds, Fetched: 1 row(s)&lt;br/&gt;
 hive&amp;gt; &lt;/p&gt;

&lt;p&gt;CREATE TABLE test_acid(id INT, &lt;br/&gt;
 name STRING, &lt;br/&gt;
 city STRING) &lt;br/&gt;
 PARTITIONED BY (pt STRING)&lt;br/&gt;
 clustered by (id) into 1 buckets&lt;br/&gt;
 stored as ORCFILE&lt;br/&gt;
 TBLPROPERTIES(&apos;transactional&apos;=&apos;true&apos;)&lt;br/&gt;
 ;&lt;/p&gt;</comment>
                            <comment id="14564551" author="saurabh santhosh" created="Fri, 29 May 2015 10:40:34 +0000"  >&lt;p&gt;is the merge functionality (MERGE INTO tbl USING src ON &#8230; WHEN MATCHED THEN ... WHEN NOT MATCHED THEN ...) available as well?&lt;br/&gt;
Or is there any other JIRA ticket for it?&lt;/p&gt;</comment>
                            <comment id="14615278" author="pfosse" created="Mon, 6 Jul 2015 16:36:01 +0000"  >&lt;p&gt;It was moved into issue 10924.  I don&apos;t know why. &lt;/p&gt;</comment>
                            <comment id="14615279" author="pfosse" created="Mon, 6 Jul 2015 16:36:43 +0000"  >&lt;p&gt;Merge command seems to be needed to do the first use case of the ACID feature.&lt;/p&gt;

&lt;p&gt;&quot;Once an hour, a set of inserts and updates (up to 500k rows) for various dimension tables (eg. customer, inventory, stores) needs to be processed. The dimension tables have primary keys and are typically bucketed and sorted on those keys.&quot;&lt;/p&gt;

&lt;p&gt;Typically we will load the updates to a hive table and just want to merge that table to the existing dimension. We are either using the old way of doing this (ingest, reconcile, compact &amp;amp; purge) or we are writing a Python script to process the updates. But we can&apos;t do 500K update statements an hour, so it doesn&apos;t seem the ACID does us any good for this use case until we have merge&lt;/p&gt;</comment>
                            <comment id="14615283" author="pfosse" created="Mon, 6 Jul 2015 16:38:09 +0000"  >&lt;p&gt;By it, I mean Merge. &lt;/p&gt;</comment>
                            <comment id="14615433" author="alangates" created="Mon, 6 Jul 2015 18:20:54 +0000"  >&lt;p&gt;Yes, agreed that the merge command is needed, and hence is being worked on &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-10924&quot; title=&quot;add support for MERGE statement&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-10924&quot;&gt;HIVE-10924&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="14984131" author="koert" created="Sat, 31 Oct 2015 20:10:13 +0000"  >&lt;p&gt;yikes&lt;/p&gt;</comment>
                            <comment id="14984270" author="koert" created="Sun, 1 Nov 2015 05:07:29 +0000"  >&lt;p&gt;i agree with edward capriolo that this is a bad idea&lt;/p&gt;

&lt;p&gt;this is just giving all those users that think they need insert/update (but probably dont if they design it right) a gun to shoot themselves in the foot with&lt;/p&gt;</comment>
                            <comment id="15006206" author="scwf" created="Mon, 16 Nov 2015 03:47:21 +0000"  >&lt;p&gt;have hive already finished all the work of phase1, phase2 in the design doc? &lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="12644147">HIVE-4402</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="12310010">
                    <name>Incorporates</name>
                                            <outwardlinks description="incorporates">
                                        <issuelink>
            <issuekey id="12676510">HIVE-5687</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12835256">HIVE-10924</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12743801">HIVE-8244</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12708352">HIVE-6905</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12774712">HIVE-9675</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="12310051">
                    <name>Supercedes</name>
                                            <outwardlinks description="supercedes">
                                        <issuelink>
            <issuekey id="12637360">HIVE-4196</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12604051" name="InsertUpdatesinHive.pdf" size="190839" author="owen.omalley" created="Thu, 19 Sep 2013 15:44:32 +0000"/>
                    </attachments>
                <subtasks>
                            <subtask id="12685629">HIVE-6060</subtask>
                            <subtask id="12679785">HIVE-5843</subtask>
                            <subtask id="12691516">HIVE-6319</subtask>
                            <subtask id="12695798">HIVE-6460</subtask>
                            <subtask id="12698527">HIVE-6541</subtask>
                            <subtask id="12676510">HIVE-5687</subtask>
                            <subtask id="12699817">HIVE-6604</subtask>
                            <subtask id="12701603">HIVE-6675</subtask>
                            <subtask id="12702491">HIVE-6699</subtask>
                            <subtask id="12703845">HIVE-6759</subtask>
                            <subtask id="12714851">HIVE-7078</subtask>
                            <subtask id="12715510">HIVE-7098</subtask>
                            <subtask id="12729631">HIVE-7513</subtask>
                            <subtask id="12731094">HIVE-7571</subtask>
                            <subtask id="12732633">HIVE-7646</subtask>
                            <subtask id="12732891">HIVE-7663</subtask>
                            <subtask id="12735238">HIVE-7788</subtask>
                            <subtask id="12735252">HIVE-7790</subtask>
                            <subtask id="12735441">HIVE-7802</subtask>
                            <subtask id="12735574">HIVE-7811</subtask>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 19 Sep 2013 05:29:06 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>349324</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            1 year, 2 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1o8ev:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>349622</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>