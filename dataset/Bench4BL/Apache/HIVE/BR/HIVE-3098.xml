<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sat Dec 03 23:01:09 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HIVE-3098/HIVE-3098.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HIVE-3098] Memory leak from large number of FileSystem instances in FileSystem.CACHE</title>
                <link>https://issues.apache.org/jira/browse/HIVE-3098</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;The problem manifested from stress-testing HCatalog 0.4.1 (as part of testing the Oracle backend).&lt;/p&gt;

&lt;p&gt;The HCatalog server ran out of memory (-Xmx2048m) when pounded by 60-threads, in under 24 hours. The heap-dump indicates that hadoop::FileSystem.CACHE had 1000000 instances of FileSystem, whose combined retained-mem consumed the entire heap.&lt;/p&gt;

&lt;p&gt;It boiled down to hadoop::UserGroupInformation::equals() being implemented such that the &quot;Subject&quot; member is compared for equality (&quot;==&quot;), and not equivalence (&quot;.equals()&quot;). This causes equivalent UGI instances to compare as unequal, and causes a new FileSystem instance to be created and cached.&lt;/p&gt;

&lt;p&gt;The UGI.equals() is so implemented, incidentally, as a fix for yet another problem (&lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-6670&quot; title=&quot;UserGroupInformation doesn&amp;#39;t support use in hash tables&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-6670&quot;&gt;&lt;del&gt;HADOOP-6670&lt;/del&gt;&lt;/a&gt;); so it is unlikely that that implementation can be modified.&lt;/p&gt;

&lt;p&gt;The solution for this is to check for UGI equivalence in HCatalog (i.e. in the Hive metastore), using an cache for UGI instances in the shims.&lt;/p&gt;

&lt;p&gt;I have a patch to fix this. I&apos;ll upload it shortly. I just ran an overnight test to confirm that the memory-leak has been arrested.&lt;/p&gt;</description>
                <environment>&lt;p&gt;Running with Hadoop 20.205.0.3+ / 1.0.x with security turned on.&lt;/p&gt;</environment>
        <key id="12559587">HIVE-3098</key>
            <summary>Memory leak from large number of FileSystem instances in FileSystem.CACHE</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="mithun">Mithun Radhakrishnan</assignee>
                                    <reporter username="mithun">Mithun Radhakrishnan</reporter>
                        <labels>
                    </labels>
                <created>Wed, 6 Jun 2012 18:34:56 +0000</created>
                <updated>Wed, 8 Apr 2015 20:44:29 +0000</updated>
                            <resolved>Fri, 7 Sep 2012 14:23:08 +0000</resolved>
                                    <version>0.9.0</version>
                                    <fixVersion>0.9.1</fixVersion>
                    <fixVersion>0.10.0</fixVersion>
                                    <component>Shims</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>17</watches>
                                                                <comments>
                            <comment id="13290328" author="mithun" created="Wed, 6 Jun 2012 18:38:04 +0000"  >&lt;p&gt;Caching for UGI instances.&lt;/p&gt;</comment>
                            <comment id="13291106" author="rohini" created="Thu, 7 Jun 2012 16:17:34 +0000"  >&lt;p&gt;Looks good. But depending on toString() of UGI might cause problems if it changes.&lt;/p&gt;

&lt;p&gt;&amp;lt;code&amp;gt;&lt;br/&gt;
private static String getKey(String user, UserGroupInformation realUgi) &lt;/p&gt;
{
         return user + &quot; via &quot; + realUgi.toString();
       }
&lt;p&gt;&amp;lt;/code&amp;gt;&lt;/p&gt;

&lt;p&gt; Can we construct it instead of calling toString(). Also the authentication method returned in the toString() is not required. Removing that will ensure we cache only one ugi for Kerberos(GSS-API) and delegation token(DIGEST-MD5) auth schemes cutting down on the number of FileSystem objects by half.&lt;/p&gt;</comment>
                            <comment id="13291234" author="mithun" created="Thu, 7 Jun 2012 19:24:06 +0000"  >&lt;p&gt;Must incorporate Rohini&apos;s review comments.&lt;/p&gt;</comment>
                            <comment id="13291235" author="mithun" created="Thu, 7 Jun 2012 19:25:07 +0000"  >&lt;p&gt;Removed dependency on UGI::toString(). Using a custom implementation in UGICache.&lt;/p&gt;</comment>
                            <comment id="13291236" author="mithun" created="Thu, 7 Jun 2012 19:25:40 +0000"  >&lt;p&gt;@Rohini: Point taken. Here&apos;s an update.&lt;/p&gt;</comment>
                            <comment id="13291373" author="rohini" created="Thu, 7 Jun 2012 22:11:58 +0000"  >&lt;p&gt;+1. Looks good.&lt;/p&gt;</comment>
                            <comment id="13291419" author="cwsteinbach" created="Thu, 7 Jun 2012 23:35:20 +0000"  >&lt;p&gt;@Mithun: Please post a review request on reviewboard or phabricator. Thanks.&lt;/p&gt;</comment>
                            <comment id="13291523" author="mithun" created="Fri, 8 Jun 2012 03:08:14 +0000"  >&lt;p&gt;Sure thing, Carl. Here it is:&lt;br/&gt;
&lt;a href=&quot;https://reviews.facebook.net/D3543&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.facebook.net/D3543&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13396939" author="mithun" created="Tue, 19 Jun 2012 17:20:11 +0000"  >&lt;p&gt;Hello, folks. Does the patch look good?&lt;/p&gt;</comment>
                            <comment id="13397717" author="ashutoshc" created="Wed, 20 Jun 2012 18:23:11 +0000"  >&lt;p&gt;@Mithun,&lt;br/&gt;
is this similar to &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-3155&quot; title=&quot;Memory leak in Hive&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-3155&quot;&gt;&lt;del&gt;HIVE-3155&lt;/del&gt;&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-3545&quot; title=&quot;DFSClient leak due to malfunctioning of FileSystem Cache&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-3545&quot;&gt;HDFS-3545&lt;/a&gt; ? &lt;/p&gt;</comment>
                            <comment id="13397749" author="mithun" created="Wed, 20 Jun 2012 18:40:26 +0000"  >&lt;p&gt;Hey, Ashutosh.&lt;/p&gt;

&lt;p&gt;I&apos;m not sure about &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-3155&quot; title=&quot;Memory leak in Hive&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-3155&quot;&gt;&lt;del&gt;HIVE-3155&lt;/del&gt;&lt;/a&gt; (since it talks of the LeaseRenewer), but &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-3545&quot; title=&quot;DFSClient leak due to malfunctioning of FileSystem Cache&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-3545&quot;&gt;HDFS-3545&lt;/a&gt; is definitely related. The leak mentioned in this JIRA is seen in FileSystem.CACHE.&lt;/p&gt;

&lt;p&gt;Mithun&lt;/p&gt;</comment>
                            <comment id="13397757" author="mithun" created="Wed, 20 Jun 2012 18:45:29 +0000"  >&lt;p&gt;@Ashutosh: Thanks for the heads-up. I&apos;ve linked this to &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-3545&quot; title=&quot;DFSClient leak due to malfunctioning of FileSystem Cache&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-3545&quot;&gt;HDFS-3545&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As per Daryn&apos;s last comment, it&apos;s likely not solvable in HDFS, since a check for UGI/Subject-equivalence will not suffice, since they are mutable. That explains why Subjects are compared for equality/identity.&lt;/p&gt;</comment>
                            <comment id="13398040" author="rohini" created="Thu, 21 Jun 2012 01:07:01 +0000"  >&lt;p&gt;This patch will fix &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-3155&quot; title=&quot;Memory leak in Hive&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-3155&quot;&gt;&lt;del&gt;HIVE-3155&lt;/del&gt;&lt;/a&gt; too.&lt;/p&gt;</comment>
                            <comment id="13398058" author="ashutoshc" created="Thu, 21 Jun 2012 01:26:37 +0000"  >&lt;p&gt;Rohini,&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-3155&quot; title=&quot;Memory leak in Hive&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-3155&quot;&gt;&lt;del&gt;HIVE-3155&lt;/del&gt;&lt;/a&gt; is talking about leak in HiveServer (not HiveMetaStore). Further, they haven&apos;t specified if they are running in secure mode (I will ask on that ticket now).&lt;/p&gt;</comment>
                            <comment id="13398508" author="ashutoshc" created="Thu, 21 Jun 2012 15:52:18 +0000"  >&lt;p&gt;@Mithun,&lt;br/&gt;
According to analysis done by Daryn &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-3545?focusedCommentId=13398502&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13398502&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HDFS-3545?focusedCommentId=13398502&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13398502&lt;/a&gt; proper fix for application(Hive) is to call FileSystem.closeForAll(ugi) once ugi is no longer in use. &lt;/p&gt;</comment>
                            <comment id="13398711" author="rohini" created="Thu, 21 Jun 2012 18:31:36 +0000"  >&lt;p&gt;@Ashutosh,&lt;br/&gt;
   We would not hit the wrong token issue mentioned by Daryn, because hive metastore proxies as the user requesting the operation. The actual ugi with which the operation would be done is &quot;user via metastoreuser&quot;. The metastoreuser UGI is the RealUser and the kerberos TGT of it is used so there is never a issue of expired or wrong tokens. We have had the same fix in hdfsproxy and Oozie for 2 years now in production and we have not had issues with it. Hdfsproxy/Oozie UGI cache is slightly more advanced and they close unused filesystem objects from the cache after 30 mins or some configured time. Closing the FileSystem object immediately and not taking advantage of the cache is a bad thing as the fs init operation is costly. Not closing the filesystem and retaining it in the cache with this fix is not going to cause memory leak unless you have 10000s of users accessing the hivemetastore. Oozie was doing it for sometime before the cache expiry logic was added.&lt;/p&gt;</comment>
                            <comment id="13398868" author="daryn" created="Thu, 21 Jun 2012 21:21:30 +0000"  >&lt;p&gt;If I understand correctly, you essentially intend to only use the cached UGI in a &lt;tt&gt;doAs&lt;/tt&gt; to get a &lt;tt&gt;FileSystem&lt;/tt&gt;?  That should generally work so long as tokens are never allowed to be added to the cached UGIs.&lt;/p&gt;

&lt;p&gt;Even after the caching is fixed, you will have to age out and close fs instances.  Some like hftp and webhdfs will implicitly acquire tokens.  When those tokens eventually expire, the fs instance becomes useless and must be re-created.&lt;/p&gt;</comment>
                            <comment id="13398979" author="rohini" created="Thu, 21 Jun 2012 23:09:29 +0000"  >&lt;p&gt;@Daryn&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
 &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; UserGroupInformation createProxyUser(&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; user,
      UserGroupInformation realUser) {
    Set&amp;lt;Principal&amp;gt; principals = subject.getPrincipals();
    principals.add(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; User(user));
    principals.add(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; RealUser(realUser));
    UserGroupInformation result =&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; UserGroupInformation(subject);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;  Since the new UGI refers to the realUser (which is UserGroupInformation.getLoginUser()), the tokens used for authentication would be that of the logged in user. Since TGT will be part of the logged in users tokens, it will always be used to do SASL authentication. Currently the filesystem fetched inside doAs in hive metastore is only hdfs:// and so only SASL will be done. &lt;/p&gt;

&lt;p&gt;The problem you mentioned with hftp:// and webhdfs:// is there. UGI does not get modified with addition of tokens, but the FileSystem class itself becomes unusable because they fetch the delegation tokens  in their constructors and store in private variables of the class. DelegationTokenRenewer in those classes take care of renewing the token. But after the renewal limit is over(7 days default), the local variable is not updated and so the FileSystem class becomes unusable. One way to get it working would be to fix the DelegationTokenRenewer to get a new token and update the local variable after token expiry.&lt;/p&gt;

&lt;p&gt;@Ashutosh,&lt;br/&gt;
   We need to add closing of the filesystem along with the cache if we are going to access hftp:// or webhdfs:// in hive metastore. hftp:// I am not sure as it readonly, but users might start specify partition location using webhdfs:// soon.&lt;/p&gt;
</comment>
                            <comment id="13401872" author="ashutoshc" created="Wed, 27 Jun 2012 01:51:25 +0000"  >&lt;p&gt;@Rohini,&lt;/p&gt;

&lt;p&gt;Another possibility is to disable fs cache altogether via &lt;tt&gt;fs.hdfs.impl.disable.cache&lt;/tt&gt; config parameter. I get that it will add to latency, but once we switch to &lt;tt&gt;FileContext&lt;/tt&gt; api in 2.0 line, this problem itself will no longer be there (since FileContext has no cache). So, instead of adding code to workaround underlying fs problem, we can live with the latency till we make that move. Does that sound reasonable?&lt;/p&gt;</comment>
                            <comment id="13401876" author="mithun" created="Wed, 27 Jun 2012 02:00:54 +0000"  >&lt;p&gt;As I remember it, testing with fs.cache disabled didn&apos;t help, for some odd reason. It&apos;s worth testing again.&lt;/p&gt;</comment>
                            <comment id="13402246" author="daryn" created="Wed, 27 Jun 2012 14:20:46 +0000"  >&lt;p&gt;I believe neither disabling the fs cache nor switching to &lt;tt&gt;FileContext&lt;/tt&gt; will help since Rohini stated earlier the desire to take advantage of the cache for performance, albeit w/o leaking.&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;FileContext&lt;/tt&gt; isn&apos;t going to be a panacea for this issue.  I think it only implements hdfs, view, and ftp (not hftp).  It provides no &lt;tt&gt;close()&lt;/tt&gt; method, so there&apos;s no way to cleanup or shutdown clients until jvm shutdown, ie. aborting streams, deleting tmp files, closing the dfs client, etc.  The latter will lead to leaks such as the dfs socket cache leaks, dfs lease renewer threads, etc.&lt;/p&gt;

&lt;p&gt;Even with the fs cache disabled, leaks such as the aforementioned dfs leaks will still occur unless &lt;em&gt;all&lt;/em&gt; fs instances are explicitly closed.&lt;/p&gt;

&lt;p&gt;I&apos;d suggest either &lt;tt&gt;closeAllForUGI&lt;/tt&gt; which provides a cache boost for each request, but degrades performance across multiple requests.  Or do the oozie style UGI caching with a periodic cache purging.&lt;/p&gt;</comment>
                            <comment id="13402394" author="tucu00" created="Wed, 27 Jun 2012 17:45:11 +0000"  >&lt;p&gt;Or FIX Hadoop FileSystem caching, this issue is gremlin-ing all over as we have long running/multiuser systems that use Hadoop FileSystem.&lt;/p&gt;</comment>
                            <comment id="13402448" author="ashutoshc" created="Wed, 27 Jun 2012 18:39:39 +0000"  >&lt;p&gt;I think we are mixing two things here: Performance Vs Memory Leak.&lt;/p&gt;

&lt;p&gt;Patch is originally intended to plug memory leak, not the performance. It fixes the original leak, but will introduce a new one. It fixes the case when a limited number of users are contacting metastore(which was the original Mithun&apos;s test where same 60 clients keep hitting the metastore). But, if different users hit the metastore, leak is still there, rather more acute with a patch, since there are now two caches (ugi and fs) which both will grow, instead of just one (fs). &lt;/p&gt;

&lt;p&gt;Problem stems from the fact that there is no expiration policy either in fs or ugi cache. We need to design for UGI cache eviction policy. There, when we are expiring stale ugi&apos;s from ugi-cache we can do &lt;tt&gt;closeAllForUGI&lt;/tt&gt; for evicting ugi to evict cached FS objects from fs-cache.&lt;/p&gt;

</comment>
                            <comment id="13402525" author="daryn" created="Wed, 27 Jun 2012 20:15:52 +0000"  >&lt;blockquote&gt;&lt;p&gt;Or FIX Hadoop FileSystem caching&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I wanted to &quot;fix&quot; the cache for the NM, but after digging around, I think the cache is working as designed.  UGIs are mutable, so if two different requests share the same cached UGI, then tokens from one request will be shared with another request.  This contamination may lead to security issues and will cause bugs.&lt;/p&gt;</comment>
                            <comment id="13402530" author="tucu00" created="Wed, 27 Jun 2012 20:27:33 +0000"  >&lt;p&gt;But we have a bug, that not only affects clients creating UGIs on the fly for the same user and if caching is not off will choke the NN with open sockets. And the more clients doing that the more likely for the NM to choke. Could we make UGIs immutable (which they should have been in the first place)?&lt;/p&gt;</comment>
                            <comment id="13402716" author="daryn" created="Thu, 28 Jun 2012 00:28:30 +0000"  >&lt;p&gt;Unfortunately, no.  If you make a &lt;tt&gt;UGI&lt;/tt&gt; immutable, it really means marking the &lt;tt&gt;Subject&lt;/tt&gt; immutable.  Everything token-based breaks because they can&apos;t be added to the &lt;tt&gt;Subject&lt;/tt&gt;, you can&apos;t login from a keytab or relogin because you can&apos;t update the TGT in the &lt;tt&gt;Subject&lt;/tt&gt;, proxy users (any maybe spnego?) won&apos;t work because service tickets can&apos;t be added to the &lt;tt&gt;Subject&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;A &lt;tt&gt;Subject&lt;/tt&gt;/&lt;tt&gt;UGI&lt;/tt&gt; is an execution context with specific privileges.  Those contexts cannot be cached and shared w/o risking escalated privileges. Think of it this way:  If I entrust you with the keys to my home to pickup a delivery, I don&apos;t want you to make a copy of the keys and have the ability to enter anytime you want w/o my explicit permission.&lt;/p&gt;

&lt;p&gt;Without knowing the intricacies, I recommend: leaving the fs cache on, create a new ugi for connections, and &lt;tt&gt;closeAllForUGI&lt;/tt&gt; when the request is complete. &lt;/p&gt;</comment>
                            <comment id="13403290" author="jnp" created="Thu, 28 Jun 2012 17:48:57 +0000"  >&lt;blockquote&gt;&lt;p&gt;Problem stems from the fact that there is no expiration policy either in fs or ugi cache. We need to design for UGI cache eviction policy. There, when we are expiring stale ugi&apos;s from ugi-cache we can do closeAllForUGI for evicting ugi to evict cached FS objects from fs-cache.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1. It may be more tractable to have a cache expiration policy in ugi-cache based on the semantics of this particular use case. In FS-cache it gets trickier because of the general purpose nature of the file system.&lt;/p&gt;</comment>
                            <comment id="13403563" author="tucu00" created="Thu, 28 Jun 2012 23:07:50 +0000"  >&lt;p&gt;@Daryn,&lt;/p&gt;

&lt;p&gt;Your solution of closeAllForUGI means you have to keep the original UGI, if you keep recreating them you are back to square one.&lt;/p&gt;

&lt;p&gt;Thanks for the UGI mutability explanation. Still, I&apos;d argue that we could achieve UGI immutability if we create a new UGI everytime you add credentials to it by composing the old UGI and the new credentials. But this still would not solve the caching problem if we want to do it by user ID.&lt;/p&gt;

&lt;p&gt;Leaving UGI alone, it seem that one thing it would help is hadoop-common providing an (&amp;lt;KEY&amp;gt;, FileSystem) ExpirationCache implementation for others to use. This cache should return a FileSystemProxy wrapping the original filesystem instance which in turn wraps the IO stream returned by open()/create() to be able to detect streams in use to not start the eviction timer.&lt;/p&gt;

&lt;p&gt;thx&lt;/p&gt;</comment>
                            <comment id="13403579" author="owen.omalley" created="Thu, 28 Jun 2012 23:28:10 +0000"  >&lt;p&gt;Alejandro,&lt;br/&gt;
  Daryn is absolutely right that we can&apos;t make the Subjects immutable. We need to be able to update a Subject with update Kerberos tickets and Tokens and changing that would break a lot of other code.&lt;/p&gt;

&lt;p&gt;It would probably make sense to make a UGI.doAsAndCleanup that does a doAs and then removes all filesystems based on the ugi, since clearly most of the Hadoop ecosystem servers have related problems.&lt;/p&gt;</comment>
                            <comment id="13403592" author="tucu00" created="Thu, 28 Jun 2012 23:50:18 +0000"  >&lt;p&gt;Hi Owen, yeah, I do understand we are already hosed on how UGI works and we cannot change things. Not sure how you suggestion of doAsAndCleanup() would work as the usecase is a server based system that does not keep state for a user, not even the UGI but wants to benefit from an ExpirationCache of FS instances for performance. If the system creates a new UGI for the same user every time it needs to do something, then there will be a new FS instance every time and the doAsAndCleanup() will miss all the FS instances created before. In the case Oozie and HttpFS we need a cache based on the username, we don&apos;t playaround adding/removing credentials to the Subject.&lt;/p&gt;</comment>
                            <comment id="13403970" author="daryn" created="Fri, 29 Jun 2012 15:29:41 +0000"  >&lt;p&gt;You can&apos;t reasonably expect or know that any code called, now or in the future, by the request isn&apos;t going to implicitly alter the &lt;tt&gt;Subject&lt;/tt&gt; with the addition of tokens/TGT/service tickets/etc.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Your solution of closeAllForUGI means you have to keep the original UGI, if you keep recreating them you are back to square one.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The UGI is basically a wrapper around the javax security context&apos;s &lt;tt&gt;Subject&lt;/tt&gt; which is created by the authentication process.  By design, the UGI/Subject is a context for a specific request/connection &amp;#8211; not for an indeterminate number of future requests.  A UGI should be created when a request comes in and used throughout the request rather than a new UGI created multiple times during the request.  If the request operates in a &lt;tt&gt;doAs&lt;/tt&gt; block, you don&apos;t have to explicitly remember the UGI.  You can simply do a &lt;tt&gt;FileSystem.closeAllForUGI(UserGroupInformation.currentUser()&lt;/tt&gt; when the request is done.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I&apos;d argue that we could achieve UGI immutability if we create a new UGI everytime you add credentials to it by composing the old UGI and the new credentials.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I admire the thought, but it won&apos;t work.  If you mean creating a new UGI:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Login/relogin is completely broken.  It&apos;s not possible to propagate the new UGI to everything holding the old UGI reference in every thread.&lt;/li&gt;
	&lt;li&gt;Code operating in a &lt;tt&gt;doAs&lt;/tt&gt; block would have to add another nested &lt;tt&gt;doAs&lt;/tt&gt; to use the tokens/tickets it just acquired.&lt;/li&gt;
	&lt;li&gt;Similarly, code currently expects to create a UGI, perform a &lt;tt&gt;doAs&lt;/tt&gt; which may update the &lt;tt&gt;Subject&lt;/tt&gt; with tickets or tokens, exit the &lt;tt&gt;doAs&lt;/tt&gt; block, then call &lt;tt&gt;doAs&lt;/tt&gt; on the same UGI and expect the tokens to be in the UGI.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;If you mean swapping out the UGI&apos;s subject, it won&apos;t work either.  The hash code is based on the &lt;tt&gt;Subject&lt;/tt&gt;&apos;s identity hash.  Swapping out the &lt;tt&gt;Subject&lt;/tt&gt; when tokens are added fouls collections using the UGI.&lt;/p&gt;

&lt;p&gt;There might be some interesting things that could be done by moving the fs cache into the &lt;tt&gt;Subject&lt;/tt&gt;&apos;s credentials.  A finalizer could ensure that all &lt;tt&gt;Closable&lt;/tt&gt; objects (such as fs instances) in the &lt;tt&gt;Subject&lt;/tt&gt;&apos;s credentials are closed.  That still requires a UGI to be scoped to a request but could better ensure that everything is cleaned up for the security context.&lt;/p&gt;</comment>
                            <comment id="13411287" author="ashutoshc" created="Wed, 11 Jul 2012 07:08:39 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rohini&quot; class=&quot;user-hover&quot; rel=&quot;rohini&quot;&gt;Rohini Palaniswamy&lt;/a&gt; / &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mithun&quot; class=&quot;user-hover&quot; rel=&quot;mithun&quot;&gt;Mithun Radhakrishnan&lt;/a&gt; : After reading through all the comments, my recommendation is to instead of adding more code (and complexity) in Hive to workaround underlying Filesystem issue, lets just disable the fs.cache via config parameter because:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;By disabling cache we are not making anything worse then its today, since we are already unable to reuse FS objects without this patch.&lt;/li&gt;
	&lt;li&gt;Disabling cache will plug the memory leak by not filling FS cache.&lt;/li&gt;
	&lt;li&gt;Once FSContext apis are declared stable for other projects to consume, we can switch over to those where the promise is that underlying problem is fixed.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13411712" author="daryn" created="Wed, 11 Jul 2012 16:45:33 +0000"  >&lt;p&gt;Possibly a reasonable approach.  Just make sure you aren&apos;t doing a lot of &lt;tt&gt;FileSystem.get&lt;/tt&gt; or &lt;tt&gt;path.getFileSystem&lt;/tt&gt; calls to avoid creating a bunch of fs &amp;amp; client objects.  I think &lt;tt&gt;DFSClient&lt;/tt&gt; and its renewal thread might have a cyclic dependency.  If so, all dfs instances will need to be explicitly closed to avoid another leak.&lt;/p&gt;

&lt;p&gt;According to another jira, &lt;tt&gt;FileContext&lt;/tt&gt; isn&apos;t going to offer a &lt;tt&gt;close&lt;/tt&gt; method so it may still leak &lt;tt&gt;DFSClient&lt;/tt&gt; instances.&lt;/p&gt;</comment>
                            <comment id="13411714" author="daryn" created="Wed, 11 Jul 2012 16:49:18 +0000"  >&lt;p&gt;Or, as mentioned earlier, if you are entirely operating within a &lt;tt&gt;doAs&lt;/tt&gt; (I don&apos;t know if you are), just add a single line of &lt;tt&gt;FileSystem.closeAllForUGI(UserGroupInformation.currentUser()&lt;/tt&gt; before you exit the &lt;tt&gt;doAs&lt;/tt&gt;.&lt;/p&gt;</comment>
                            <comment id="13411777" author="tucu00" created="Wed, 11 Jul 2012 18:00:51 +0000"  >&lt;p&gt;Then I guess that for a system like HttpFS which is all about proxying to a FS, the solution is to disable the Hadoop FS caching and do its own app level caching as proposed in &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-3513&quot; title=&quot;HttpFS should cache filesystems&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-3513&quot;&gt;&lt;del&gt;HDFS-3513&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="13411778" author="rohini" created="Wed, 11 Jul 2012 18:00:55 +0000"  >&lt;p&gt;@Ashutosh&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;to workaround underlying Filesystem issue, lets just disable the fs.cache via config parameter. Disabling cache will plug the memory leak by not filling FS cache.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;  Disabling fs.cache is not going to matter. We are already getting a new FileSystem object for every request and the fs.cache is not used at all. The newly created FileSystem objects are still going to be in memory until garbage collected. And they don&apos;t get garbage collected for some reason (We have not analyzed what else is holding reference to them) as the experiments Mithun ran with fs.cache disabled did not make any difference. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Once FSContext apis are declared stable for other projects to consume, we can switch over to those where the promise is that underlying problem is fixed.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;   As Daryn had mentioned in previous comments, this is not going to solve the problem at all.&lt;/p&gt;

&lt;p&gt;We need to fix this. It is not good to have to restart the metastore server every week or two. I see two possible interim fixes until it is solved in core hadoop itself for all applications.&lt;br/&gt;
 1) Disable fs.cache and do fs.close() after every filesystem call in the code.  If there is a place to put the fs.close() after every request is executed it is easy. Or you might have to put fs.close() in too many places. Anyways it is not going to be good for performance.&lt;br/&gt;
  2) Add the fs.close() logic after a timeout to the current patch. Mithun was already working on this. I can&apos;t understand why you think it adds too much complexity. Other applications are already doing this. It will be difficult to answer to the Ops guys why we have a software that needs to be restarted often.&lt;/p&gt;</comment>
                            <comment id="13411808" author="mithun" created="Wed, 11 Jul 2012 18:20:37 +0000"  >&lt;p&gt;I should have the aging-cache +  FS.closeAllForUGI() code in a patch shortly. The approach is to have a clean-up thread toss out UGIs from the UGICache that are &quot;too old&quot;, after calling FS.closeAllForUGI(). The assumption is that a FileSystem instance is likely to be done with if it&apos;s been idle for 5 minutes. (The FS operations would have completed well before, given that they&apos;re largely directory-creation/listing operations.)&lt;/p&gt;

&lt;p&gt;It is probably worth a shot testing again with the FS.CACHE disabled, but as Rohini mentions, the last time I tried this, the problem persisted.&lt;/p&gt;</comment>
                            <comment id="13411828" author="tucu00" created="Wed, 11 Jul 2012 18:34:01 +0000"  >&lt;p&gt;@Mithun, if you are reading/writing a large file, you are still using the FS instance through the corresponding IN/OUT stream, you&apos;ll have to wrap those as well to check for FS activity. Futhermore, you should not close a FS is a stream is not closed.&lt;/p&gt;
</comment>
                            <comment id="13411835" author="ashutoshc" created="Wed, 11 Jul 2012 18:43:36 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tucu00&quot; class=&quot;user-hover&quot; rel=&quot;tucu00&quot;&gt;Alejandro Abdelnur&lt;/a&gt; : This is a metastore. So, in this context we only perform metadata operations, like create/rm dir/file. We never open/read/write files in this use-case.&lt;br/&gt;
Similarly, Daryn&apos;s point about cyclic dependency between dfsclient and renewal thread is moot here, since there are no write calls that we do.&lt;/p&gt;</comment>
                            <comment id="13411850" author="tucu00" created="Wed, 11 Jul 2012 18:53:59 +0000"  >&lt;p&gt;that makes sense, again this reinforces the point that apps have to do their own caching of FS as the one provided by Hadoop FS is not really useful &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13411872" author="rohini" created="Wed, 11 Jul 2012 19:08:55 +0000"  >&lt;p&gt;You are right. Since there is no way to track if a filesystem is actually still in use within hadoop (unless they start implementing reference counters for open IO and making a lot of things synchronized) and the knowledge lies with the application, the FSCache is not fully beneficial though its still very useful for client applications. Only server applications have this problem. Not having to track requests (unlike oozie and hdfsproxy) makes it a little more easy for metastore. &lt;/p&gt;</comment>
                            <comment id="13411908" author="daryn" created="Wed, 11 Jul 2012 19:58:43 +0000"  >&lt;blockquote&gt;&lt;p&gt;This is a metastore. So, in this context we only perform metadata operations, like create/rm dir/file. We never open/read/write files in this use-case.  Similarly, Daryn&apos;s point about cyclic dependency between dfsclient and renewal thread is moot here, since there are no write calls that we do.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don&apos;t think you can create a file in dfs w/o creating a stream which starts the lease renewer.  I believe creation of an empty file involves an open/close.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;the FSCache is not fully beneficial though its still very useful for client applications. Only server applications have this problem&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The cache is actually useful for both clients and servers if used as designed.  In fact, &lt;tt&gt;closeAllForUGI&lt;/tt&gt; is specifically for servers.  Servers like the JT use it to avoid leaking filesystems like a sieve.  The general idea is to service a request something like this:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
UserGroupInformation remoteUser = &amp;lt;get from somewhere or create it&amp;gt;;
remoteUser.doAs(...);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;followed by, or included at the end of the &lt;tt&gt;doAs&lt;/tt&gt;:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;FileSystem.closeAllForUGI(remoteUser);&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This pattern allows arbitrary code to freely obtain cached fs instances w/o requiring an explicit close.  When the request is finished, you know it&apos;s safe to close all the request&apos;s cached fs instances.  There&apos;s no need to track activity, etc.&lt;/p&gt;

&lt;p&gt;I&apos;m not going to tell anyone what to do.  I just think trying to roll a custom ugi/fs shared cache will add a lot of complexity and subtle edge-cases.&lt;/p&gt;</comment>
                            <comment id="13411960" author="rohini" created="Wed, 11 Jul 2012 20:30:37 +0000"  >&lt;p&gt;@Daryn&lt;br/&gt;
   The metastore only created directories. Does not create files. It only deletes them. So that should be ok. &lt;/p&gt;

&lt;p&gt;   When you have multiple parallel requests from same user, you cannot easily do a FileSystem.closeAllForUGI(remoteUser); The same FileSystem instance will be fetched by other threads from the cache and could still be in use. Only when you are certain that no other threads have reference to the FileSystem object, you can go ahead and close the filesystems.&lt;/p&gt;</comment>
                            <comment id="13412011" author="daryn" created="Wed, 11 Jul 2012 21:25:05 +0000"  >&lt;blockquote&gt;&lt;p&gt;When you have multiple parallel requests from same user, you cannot easily do a FileSystem.closeAllForUGI(remoteUser); The same FileSystem instance will be fetched by other threads from the cache and could still be in use&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Correct, but it&apos;s only an issue when attempting to cache the ugi.  If you allow each request to continue to have its own ugi, then its completely safe to call &lt;tt&gt;FileSystem.closeAllForUGI&lt;/tt&gt; when the request is finished.  That one-liner should solve the leak.&lt;/p&gt;</comment>
                            <comment id="13413263" author="sanjay.radia" created="Thu, 12 Jul 2012 21:47:12 +0000"  >&lt;p&gt;&amp;gt; .. FileContext isn&apos;t going to offer a close method so it may still leak DFSClient instances.&lt;br/&gt;
We eliminated the cache completely in FileContext because the cache is a mess to deal with - however FileContext is not in active use and hence it still remains to be seen if it all works out in practice.&lt;/p&gt;</comment>
                            <comment id="13413726" author="daryn" created="Fri, 13 Jul 2012 13:32:32 +0000"  >&lt;p&gt;It&apos;s not the lack of a cache in &lt;tt&gt;FileContext&lt;/tt&gt; that causes the problem, but rather that it holds a reference to a fs, and possibly creates new fs instances on the fly, and there is no means to close them.  In the case of &lt;tt&gt;DFSClient&lt;/tt&gt; there are cyclic hard references that prevent the client from being gc-ed unless explicitly closed.  Even if those hard references are broken, the fs doesn&apos;t get a chance to abort streams and delete tmp files.&lt;/p&gt;</comment>
                            <comment id="13417698" author="mithun" created="Wed, 18 Jul 2012 21:27:32 +0000"  >&lt;p&gt;Here&apos;s an extension of the old patch, complete with FileSystem.closeAllForUGI() and cache-aging. This should address concerns about cache-cleanup and further memory-leaks.&lt;/p&gt;</comment>
                            <comment id="13417700" author="mithun" created="Wed, 18 Jul 2012 21:28:31 +0000"  >&lt;p&gt;(Re-upload, fixing the ASF license.)&lt;/p&gt;</comment>
                            <comment id="13417728" author="ashutoshc" created="Wed, 18 Jul 2012 21:54:23 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mithun&quot; class=&quot;user-hover&quot; rel=&quot;mithun&quot;&gt;Mithun Radhakrishnan&lt;/a&gt; Thanks mithun for addressing the concerns. I will take a look shortly. Will it be easy for you to also update the phabricator link ?&lt;/p&gt;</comment>
                            <comment id="13417753" author="rohini" created="Wed, 18 Jul 2012 22:20:56 +0000"  >&lt;p&gt;Few comments: &lt;/p&gt;

&lt;p&gt;1) Better to have AGE_THRESHOLD configurable instead of a constant.&lt;/p&gt;

&lt;p&gt;2) LOG.debug(&quot;Cleaning up file-system handles for: &quot; + ugi); - Can be info&lt;/p&gt;

&lt;p&gt;3) if (ugi == null) ugi = newUgi; is redundant. There is no way ugi can be null after putIfAbsent call.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
ugi = ugiCache.putIfAbsent(key, newUgi);
+           &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (ugi == &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) &lt;span class=&quot;code-comment&quot;&gt;// New entry.
&lt;/span&gt;+             ugi = newUgi;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;4) Would be good to move UGICache to a separate class.&lt;/p&gt;</comment>
                            <comment id="13417757" author="rohini" created="Wed, 18 Jul 2012 22:25:09 +0000"  >&lt;p&gt;Sorry. Ignore 3). That is valid. &lt;/p&gt;</comment>
                            <comment id="13418294" author="daryn" created="Thu, 19 Jul 2012 13:57:07 +0000"  >&lt;p&gt;It&apos;s probably safer to use the iterator&apos;s remove instead of a direct remove from the map.  Ie. change &lt;tt&gt;trashedUGIs.add(ugiCache.remove(entry.getKey())&lt;/tt&gt; to &lt;tt&gt;entry.remove(); trashedUGIs.add(entry.getKey)&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;How does this protect against requests using trashed UGIs having their filesystems closed before the request is complete?  Is it assuming all requests will complete within the time interval between which the ugi is trashed and then purged?  If true, it seems fragile esp. when considering rpc retries, HA failover delays, etc.&lt;/p&gt;

&lt;p&gt;Has anyone benchmarked cached ugis to ensure this isn&apos;t a premature optimization?&lt;/p&gt;</comment>
                            <comment id="13418441" author="mithun" created="Thu, 19 Jul 2012 16:57:32 +0000"  >&lt;p&gt;Hey, All. First off, I&apos;m considering a rewrite, essentially identical but implemented via Guava caching. This will save on lines-of-code.&lt;/p&gt;

&lt;p&gt;@Ashutosh: Sure. I&apos;ll put a new patch on Phabricator, as soon as I&apos;ve incorporated Rohini&apos;s comments. The code could stand to be refactored.&lt;/p&gt;

&lt;p&gt;@Daryn: Thank you for reviewing.&lt;br/&gt;
1. If the concern is over ConcurrentModificationExceptions, that&apos;s not a problem since ugiCache is a ConcurrentHashMap. :]&lt;br/&gt;
2. Yes, that was the assumption, but you have a point. Perhaps a better way to deal with this is through WeakReferences (or some such).&lt;br/&gt;
3. I&apos;m not sure I follow your concern about premature-optimization. We&apos;re caching to avoid the creation of multiple FS instances. We&apos;ve seen from tests that this solves that problem. Would you please elaborate?&lt;/p&gt;</comment>
                            <comment id="13418485" author="daryn" created="Thu, 19 Jul 2012 17:40:41 +0000"  >&lt;p&gt;I think the large discussion has obscured the fact that there&apos;s nothing wrong with the fs api.  The leak is a result of hive misusing the fs api.  Simply calling &lt;tt&gt;FileSystem.closeAllForUGI(remoteUser)&lt;/tt&gt; at the end of a request will stop hive from leaking.  My guess, and I could be wrong, is that trying to cache UGIs will provide a negligible performance increase.  Hence why I question if this is premature optimization.&lt;/p&gt;</comment>
                            <comment id="13419908" author="ashutoshc" created="Sat, 21 Jul 2012 18:51:12 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mithun&quot; class=&quot;user-hover&quot; rel=&quot;mithun&quot;&gt;Mithun Radhakrishnan&lt;/a&gt; Few comments on current patch.&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;As Rohini suggested its a good idea to extract UGI in its own class, since HadoopThrift20SAuthBridge is already getting too big.&lt;/li&gt;
	&lt;li&gt;+1 on using Guava for this. &lt;tt&gt;CacheBuilder&lt;/tt&gt; class provides appropriate functionality for it.&lt;/li&gt;
	&lt;li&gt;I think configuring the size of cache with CacheBuilder::maximumSize(long size) might be better idea then time-based expiration. This way we can limit the cache size by memory rather time-based. What do you think?&lt;/li&gt;
	&lt;li&gt;Also, update HadoopShimsSecure::createRemoteUser() to do UGICache.getRemoteUser() from current UGI.createRemoteUser() so that it also benefits from ugi-cache.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13425007" author="mithun" created="Mon, 30 Jul 2012 17:01:27 +0000"  >&lt;p&gt;@Daryn: I was about to reply that calling FS.closeAllForUGI(remoteUser) would be dangerous if the UGI is shared across multiple requests, but that clearly can&apos;t be the case. :] Your suggestion warrants trying out. I&apos;ll post results as soon as I&apos;m done.&lt;/p&gt;

&lt;p&gt;@Ashutosh:&lt;br/&gt;
1. Yep. UGICache ballooned up more than expected. :]&lt;br/&gt;
2. Guava should be interesting.&lt;br/&gt;
3. You&apos;re right. We were considering the same, if only to do away with the cleanup thread.&lt;br/&gt;
4. I&apos;ll keep HadoopShimsSecure in mind. There&apos;s also HdfsAuthorizationProvider to be considered, in HCatalog code.&lt;/p&gt;

&lt;p&gt;I&apos;ll carry on with implementation iff Daryn&apos;s suggestion doesn&apos;t work out.&lt;/p&gt;</comment>
                            <comment id="13426796" author="mithun" created="Wed, 1 Aug 2012 18:28:14 +0000"  >&lt;p&gt;Good news.&lt;/p&gt;

&lt;p&gt;After applying the following change (i.e. to use FS.closeAllForUGI(), as was intended), I see that the memory footprint doesn&apos;t grow as before. The cleanup of FS handles is better. &lt;/p&gt;

&lt;p&gt;I left my stress-test running overnight. I see that the memory-footprint does grow when compared to running with the UGICache, (22MB vs 75MB). But it doesn&apos;t grow to the ludicrous proportions of the past (2GB). I&apos;m going to put the increase down to heap-fragmentation because of new object-creation (avoidable with the cache).&lt;/p&gt;

&lt;p&gt;Review, please?&lt;/p&gt;</comment>
                            <comment id="13427298" author="daryn" created="Thu, 2 Aug 2012 13:15:31 +0000"  >&lt;p&gt;+1 Looks great!  I see that 20S is in the class name.  The same behavior is needed for all releases so I&apos;m not sure if you need to add it to other files as well?&lt;/p&gt;</comment>
                            <comment id="13428381" author="mithun" created="Fri, 3 Aug 2012 21:03:14 +0000"  >&lt;p&gt;Thanks for reviewing, Daryn. Unless I&apos;m terribly mistaken, this is the only class that needs changing. &lt;/p&gt;

&lt;p&gt;Ashutosh, if this solution is acceptable, could this please be checked in?&lt;/p&gt;</comment>
                            <comment id="13430748" author="rohini" created="Tue, 7 Aug 2012 23:57:31 +0000"  >&lt;p&gt;Daryn,&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;trying to cache UGIs will provide a negligible performance increase. Hence why I question if this is premature optimization.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;   Ran a small test. For a 10 node cluster with kerberos: &lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;First FileSystem.get() took 350-700ms. On one node it was ~350ms and other ~700ms. Did not spend time trying to analyze the reason behind the difference. Some of the time I assume is spent on fetching service ticket to NN, opening socket, etc.&lt;/li&gt;
	&lt;li&gt;Further FileSystem.get() for the same namenode took ~75ms (With fs.cache disabled or different UGIs with cache enabled).&lt;/li&gt;
	&lt;li&gt;If FileSystem is fetched from cache because of same UGI, it is 0-1 ms.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;  Time might be slightly higher for a bigger cluster with namenode under heavy usage.&lt;/p&gt;

&lt;p&gt;  ~75ms is negligible at the moment. But as we move towards using hive for more realtime queries and we look at reducing the response times of the hive metastore operations, we can consider the UGI-FSCache patch as 75ms is ~25-40% of the response time for some hive metastore operations.&lt;/p&gt;

&lt;p&gt;  Hope this data is useful for &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-7973&quot; title=&quot;DistributedFileSystem close has severe consequences&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-7973&quot;&gt;&lt;del&gt;HADOOP-7973&lt;/del&gt;&lt;/a&gt; too.   &lt;/p&gt;</comment>
                            <comment id="13444267" author="ashutoshc" created="Wed, 29 Aug 2012 18:05:55 +0000"  >&lt;p&gt;Mithun:&lt;/p&gt;

&lt;p&gt;Sorry for being late in getting back on this one. Patch looks good. There is a similar leak (and thus same fix) for unsecure case as well. It will be good to get that in too.&lt;/p&gt;</comment>
                            <comment id="13449930" author="mithun" created="Thu, 6 Sep 2012 18:45:23 +0000"  >&lt;p&gt;Posting updated patch for unsecure-Hadoop.&lt;/p&gt;</comment>
                            <comment id="13449934" author="mithun" created="Thu, 6 Sep 2012 18:48:37 +0000"  >&lt;p&gt;Updated patch that fixes the leak in TUGIBasedProcessor (alongside the fix in HadoopThriftAuthBridge20S.)&lt;/p&gt;</comment>
                            <comment id="13449937" author="mithun" created="Thu, 6 Sep 2012 18:49:16 +0000"  >&lt;p&gt;Thanks, Ashutosh and Alan. The new patch looks good.&lt;/p&gt;</comment>
                            <comment id="13450302" author="ashutoshc" created="Fri, 7 Sep 2012 03:37:39 +0000"  >&lt;p&gt;+1 will commit if tests pass.&lt;/p&gt;</comment>
                            <comment id="13450656" author="ashutoshc" created="Fri, 7 Sep 2012 14:23:08 +0000"  >&lt;p&gt;Committed to trunk. Thanks, Mithun &amp;amp; Rohini for your persistence on this one!&lt;/p&gt;</comment>
                            <comment id="13450978" author="hudson" created="Fri, 7 Sep 2012 21:02:53 +0000"  >&lt;p&gt;Integrated in Hive-trunk-h0.21 #1654 (See &lt;a href=&quot;https://builds.apache.org/job/Hive-trunk-h0.21/1654/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/Hive-trunk-h0.21/1654/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-3098&quot; title=&quot;Memory leak from large number of FileSystem instances in FileSystem.CACHE&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-3098&quot;&gt;&lt;del&gt;HIVE-3098&lt;/del&gt;&lt;/a&gt; : Memory leak from large number of FileSystem instances in FileSystem.CACHE (Mithun R via Ashutosh Chauhan) (Revision 1382040)&lt;/p&gt;

&lt;p&gt;     Result = FAILURE&lt;br/&gt;
hashutosh : &lt;a href=&quot;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1382040&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1382040&lt;/a&gt;&lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hive/trunk/metastore/src/java/org/apache/hadoop/hive/metastore/TUGIBasedProcessor.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/shims/src/0.20/java/org/apache/hadoop/hive/shims/Hadoop20Shims.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/shims/src/common-secure/java/org/apache/hadoop/hive/shims/HadoopShimsSecure.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/shims/src/common-secure/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge20S.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/shims/src/common/java/org/apache/hadoop/hive/shims/HadoopShims.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13452481" author="mithun" created="Mon, 10 Sep 2012 21:46:35 +0000"  >&lt;p&gt;Gentlemen, would it be agreeable to get this fix into branch-0.9/? That would really help us.&lt;/p&gt;</comment>
                            <comment id="13455313" author="ashutoshc" created="Thu, 13 Sep 2012 21:21:57 +0000"  >&lt;p&gt;Committed to 0.9 branch.&lt;/p&gt;</comment>
                            <comment id="13455465" author="hudson" created="Thu, 13 Sep 2012 23:57:15 +0000"  >&lt;p&gt;Integrated in Hive-0.9.1-SNAPSHOT-h0.21-keepgoing=false #137 (See &lt;a href=&quot;https://builds.apache.org/job/Hive-0.9.1-SNAPSHOT-h0.21-keepgoing=false/137/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/Hive-0.9.1-SNAPSHOT-h0.21-keepgoing=false/137/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-3098&quot; title=&quot;Memory leak from large number of FileSystem instances in FileSystem.CACHE&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-3098&quot;&gt;&lt;del&gt;HIVE-3098&lt;/del&gt;&lt;/a&gt; : Memory leak from large number of FileSystem instances in FileSystem.CACHE (Mithun Radhakrishnan via Ashutosh Chauhan) (Revision 1384541)&lt;/p&gt;

&lt;p&gt;     Result = FAILURE&lt;br/&gt;
hashutosh : &lt;a href=&quot;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1384541&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1384541&lt;/a&gt;&lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hive/branches/branch-0.9/metastore/src/java/org/apache/hadoop/hive/metastore/TUGIBasedProcessor.java&lt;/li&gt;
	&lt;li&gt;/hive/branches/branch-0.9/shims/src/0.20/java/org/apache/hadoop/hive/shims/Hadoop20Shims.java&lt;/li&gt;
	&lt;li&gt;/hive/branches/branch-0.9/shims/src/common-secure/java/org/apache/hadoop/hive/shims/HadoopShimsSecure.java&lt;/li&gt;
	&lt;li&gt;/hive/branches/branch-0.9/shims/src/common-secure/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge20S.java&lt;/li&gt;
	&lt;li&gt;/hive/branches/branch-0.9/shims/src/common/java/org/apache/hadoop/hive/shims/HadoopShims.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13455572" author="hudson" created="Fri, 14 Sep 2012 04:46:06 +0000"  >&lt;p&gt;Integrated in Hive-0.9.1-SNAPSHOT-h0.21 #137 (See &lt;a href=&quot;https://builds.apache.org/job/Hive-0.9.1-SNAPSHOT-h0.21/137/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/Hive-0.9.1-SNAPSHOT-h0.21/137/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-3098&quot; title=&quot;Memory leak from large number of FileSystem instances in FileSystem.CACHE&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-3098&quot;&gt;&lt;del&gt;HIVE-3098&lt;/del&gt;&lt;/a&gt; : Memory leak from large number of FileSystem instances in FileSystem.CACHE (Mithun Radhakrishnan via Ashutosh Chauhan) (Revision 1384541)&lt;/p&gt;

&lt;p&gt;     Result = SUCCESS&lt;br/&gt;
hashutosh : &lt;a href=&quot;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1384541&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1384541&lt;/a&gt;&lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hive/branches/branch-0.9/metastore/src/java/org/apache/hadoop/hive/metastore/TUGIBasedProcessor.java&lt;/li&gt;
	&lt;li&gt;/hive/branches/branch-0.9/shims/src/0.20/java/org/apache/hadoop/hive/shims/Hadoop20Shims.java&lt;/li&gt;
	&lt;li&gt;/hive/branches/branch-0.9/shims/src/common-secure/java/org/apache/hadoop/hive/shims/HadoopShimsSecure.java&lt;/li&gt;
	&lt;li&gt;/hive/branches/branch-0.9/shims/src/common-secure/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge20S.java&lt;/li&gt;
	&lt;li&gt;/hive/branches/branch-0.9/shims/src/common/java/org/apache/hadoop/hive/shims/HadoopShims.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13548112" author="hudson" created="Wed, 9 Jan 2013 10:24:26 +0000"  >&lt;p&gt;Integrated in Hive-trunk-hadoop2 #54 (See &lt;a href=&quot;https://builds.apache.org/job/Hive-trunk-hadoop2/54/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/Hive-trunk-hadoop2/54/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-3098&quot; title=&quot;Memory leak from large number of FileSystem instances in FileSystem.CACHE&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-3098&quot;&gt;&lt;del&gt;HIVE-3098&lt;/del&gt;&lt;/a&gt; : Memory leak from large number of FileSystem instances in FileSystem.CACHE (Mithun R via Ashutosh Chauhan) (Revision 1382040)&lt;/p&gt;

&lt;p&gt;     Result = ABORTED&lt;br/&gt;
hashutosh : &lt;a href=&quot;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1382040&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1382040&lt;/a&gt;&lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hive/trunk/metastore/src/java/org/apache/hadoop/hive/metastore/TUGIBasedProcessor.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/shims/src/0.20/java/org/apache/hadoop/hive/shims/Hadoop20Shims.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/shims/src/common-secure/java/org/apache/hadoop/hive/shims/HadoopShimsSecure.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/shims/src/common-secure/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge20S.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/shims/src/common/java/org/apache/hadoop/hive/shims/HadoopShims.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13550290" author="ashutoshc" created="Thu, 10 Jan 2013 19:54:07 +0000"  >&lt;p&gt;This issue is fixed and released as part of 0.10.0 release. If you find an issue which seems to be related to this one, please create a new jira and link this one with new jira.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12595088">HDFS-3545</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12646151">HIVE-4501</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12668772">HIVE-5296</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12559588">HDFS-3513</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12764307">HIVE-9234</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12538815" name="Hive-3098_(FS_closeAllForUGI()).patch" size="1906" author="mithun" created="Wed, 1 Aug 2012 18:28:14 +0000"/>
                            <attachment id="12537071" name="Hive_3098.patch" size="7257" author="mithun" created="Wed, 18 Jul 2012 21:28:31 +0000"/>
                            <attachment id="12544092" name="hive-3098.patch" size="5023" author="mithun" created="Thu, 6 Sep 2012 18:48:37 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 7 Jun 2012 16:17:34 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>255575</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            3 years, 47 weeks, 2 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0fs9r:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>90177</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>