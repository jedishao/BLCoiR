<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 02 11:49:05 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HIVE-13749/HIVE-13749.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HIVE-13749] Memory leak in Hive Metastore</title>
                <link>https://issues.apache.org/jira/browse/HIVE-13749</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;Looking a heap dump of 10GB, a large number of Configuration objects(&amp;gt; 66k instances) are being retained. These objects along with its retained set is occupying about 95% of the heap space. This leads to HMS crashes every few days.&lt;/p&gt;

&lt;p&gt;I will attach an exported snapshot from the eclipse MAT.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12969057">HIVE-13749</key>
            <summary>Memory leak in Hive Metastore</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="ngangam">Naveen Gangam</assignee>
                                    <reporter username="ngangam">Naveen Gangam</reporter>
                        <labels>
                    </labels>
                <created>Thu, 12 May 2016 16:31:29 +0000</created>
                <updated>Wed, 28 Sep 2016 21:20:22 +0000</updated>
                            <resolved>Fri, 8 Jul 2016 13:56:19 +0000</resolved>
                                    <version>1.1.0</version>
                                    <fixVersion>2.2.0</fixVersion>
                    <fixVersion>2.1.1</fixVersion>
                                    <component>Metastore</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>10</watches>
                                                                <comments>
                            <comment id="15281918" author="thejas" created="Thu, 12 May 2016 18:58:50 +0000"  >&lt;p&gt;What is retaining them as per MAT ?&lt;/p&gt;</comment>
                            <comment id="15282359" author="ngangam" created="Fri, 13 May 2016 02:56:54 +0000"  >&lt;p&gt;I am still analyzing the heap, but appears like they are all stashed away in HashMap, perhaps in a threadlocal. I do not have the allocation stack for these objects so I cannot tell what part of the code creates these instances.&lt;/p&gt;

&lt;p&gt;Just running a simple query iteratively via beeline where it connects + disconnects every iteration, I observe the leak. Not sure if it is the same workload as in the environment where the heap dump was generated from.&lt;/p&gt;

&lt;p&gt;I am currently running some tests with a change to remove it from the threadlocal at&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/hive/blob/master/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java#L811&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/hive/blob/master/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java#L811&lt;/a&gt;&lt;br/&gt;
and&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/hive/blob/master/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java#L483&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/hive/blob/master/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java#L483&lt;/a&gt;&lt;/p&gt;
</comment>
                            <comment id="15296454" author="ngangam" created="Mon, 23 May 2016 14:43:55 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=thejas&quot; class=&quot;user-hover&quot; rel=&quot;thejas&quot;&gt;Thejas M Nair&lt;/a&gt; I have a better understanding of what is causing this issue. It appears that FileSystem.Cache (hadoop APIs) is retaining the instances of Configuration in its cache. &lt;br/&gt;
Anytime we call a FileSystem.get(conf), like so&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/hive/blob/master/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java#L1685&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/hive/blob/master/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java#L1685&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;the conf object becomes the part of the key for the map entry. Its meant to improve performance so we dont have to re-create these FileSystem objects, but doesnt appear that Hive&apos;s use of these APIs is using the cache efficiently. &lt;br/&gt;
There are other areas in the code that contribute, like Path.getFileSystem() under the covers could add to this cache.&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/hive/blob/master/metastore/src/java/org/apache/hadoop/hive/metastore/Warehouse.java#L104&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/hive/blob/master/metastore/src/java/org/apache/hadoop/hive/metastore/Warehouse.java#L104&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Caching can be turned off entirely by using &lt;br/&gt;
fs.%s.impl.disable.cache=true where %s is the caching scheme (ex: hdfs or s3) which might make this problem go away but has a performance overhead. (I havent measured it though).&lt;/p&gt;

&lt;p&gt;Unfortunately, there is no means to selectively turn off the caching on a per call basis. So we have to fix this in the hive code. fs.close() would remove the entry from the cache. But we cannot call it every time we use this API, as it would be the same as disabling the cache entirely. So its easy choice to add fs.close() here&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/hive/blob/master/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java#L1685&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/hive/blob/master/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java#L1685&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;But for the other code in Warehouse, we need more data around the cache hits and misses. I am working on instrumenting the FileSystem code to provide this info.&lt;/p&gt;

&lt;p&gt;Alternate thought, (I am not sure how feasible it is though), since the FileSystem code does not appear to be using the properties within this Configuration object itself, it may be safe to use a static instance of HiveConf on most calls to FileSystem, like mkdirs(), get() etc. This way we use the cache efficiently too. However, I am not sure if there will be session specific properties that get used across all calls to the FileSystem APIs. &lt;/p&gt;

&lt;p&gt;Thoughts? Thanks in advance.&lt;/p&gt;</comment>
                            <comment id="15296771" author="thejas" created="Mon, 23 May 2016 18:07:13 +0000"  >&lt;p&gt;This FileSystem.CACHE leak is an issue that shows up in several places across projects. Its not a cache in the traditional sense, that it doesn&apos;t have an eviction policy.&lt;br/&gt;
The peculiar thing about this cache is that the underlying Map has a key that contains UGI object, and the key comparison uses object equality (== operator) instead of .equals method for comparison. This is apparently by design for some security aspects.&lt;/p&gt;

&lt;p&gt;So you have one entry in the cache for every UGI object. &lt;/p&gt;

&lt;p&gt;See the final patch in &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-3098&quot; title=&quot;Memory leak from large number of FileSystem instances in FileSystem.CACHE&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-3098&quot;&gt;&lt;del&gt;HIVE-3098&lt;/del&gt;&lt;/a&gt; for example. The usual way to solve this is to call FileSystem.closeAllForUGI once the UGI use is over.&lt;br/&gt;
Other related leak jiras - &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-4501&quot; title=&quot;HS2 memory leak - FileSystem objects in FileSystem.CACHE&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-4501&quot;&gt;&lt;del&gt;HIVE-4501&lt;/del&gt;&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-9234&quot; title=&quot;HiveServer2 leaks FileSystem objects in FileSystem.CACHE&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-9234&quot;&gt;&lt;del&gt;HIVE-9234&lt;/del&gt;&lt;/a&gt; .&lt;/p&gt;

&lt;p&gt;You might want to check if this is happening in latest trunk as well. There might be other patches that have fixed the problem.&lt;/p&gt;


</comment>
                            <comment id="15302889" author="ngangam" created="Thu, 26 May 2016 20:47:31 +0000"  >&lt;p&gt;perhaps in the HiveMetaStore.shutdown() we clear the cache for the current UGI. Make sense?  Could you please review the patch when you have a chance ?&lt;br/&gt;
I have had the customer disable the FileSystem caching by adding &lt;tt&gt;fs.hdfs.impl.disable.cache=true&lt;/tt&gt; to the HMS configuration, the re-run the workloads. The same site that had 66000+ Configuration instances in their heapdump now has 80 instances and none of them are in Cache. So it is clear that the FileSystem.CACHE is the problem. &lt;br/&gt;
Thanks&lt;/p&gt;</comment>
                            <comment id="15302925" author="thejas" created="Thu, 26 May 2016 21:02:33 +0000"  >&lt;p&gt;Regarding the patch&lt;br/&gt;
1. How do you make sure that files created by this ugi are not in use in other parts ? We need do the closing only after we are sure that the ugi object is no longer going to be used. &lt;br/&gt;
2. I am not sure if this would fix the leak. As you can see we have patches that deal with the closing when UGI object is no longer used.&lt;br/&gt;
Are you able to reproduce this in your environment ? If not, you might want to add some debugging around code that adds entries in the cache, and see if the closing of files generated from those places is happening.&lt;br/&gt;
You might also want to see if the user is some some plugins that might be creating new UGI objects.&lt;/p&gt;
</comment>
                            <comment id="15302965" author="ngangam" created="Thu, 26 May 2016 21:28:34 +0000"  >&lt;p&gt;Oops just posted the patch to RB (&lt;a href=&quot;https://reviews.apache.org/r/47918/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.apache.org/r/47918/&lt;/a&gt;) at the same time as this comment.&lt;br/&gt;
1) Isnt the shutdown() called when a HMS request is fulfilled and the executor thread is being released back to the pool? So any new calls would potentially have a new UGI and a new instance of HiveConf. Also, calling closeAll() just removes the cached element. At worst, the FileSystem object is re-cached on a miss.&lt;br/&gt;
2) The other fixes are to address a similar issue on the HS2 side where using the FileSystem APIs causes the Cache to grow. This issue is on the HMS side.&lt;br/&gt;
Regarding reproducing this locally, yes and no. I ran 100&apos;s of iterations of beeline executing a script that create a table and then drops it while randomly toggling the value of a hive conf property. For 300 iterations, I have gotten it to retain 60 instances which is not quite the same success as the customer is having. I think because of my test being run as a single user. Re-running the test with this fix, I have 8 instances retained but none in this particular cache.&lt;br/&gt;
I have run with debug around this code and during the drop table command, I can see an element being added to the cache. I am also waiting for logs from this customer who is running with some instrumentation + fix. I can confirm that from those logs too.&lt;/p&gt;

&lt;p&gt;Alternatively, in checkTrashPurgeCombination() we could add a close() to this FileSystem. In my testcase, this has been the primary reason for the retained instances.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
          HadoopShims.HdfsEncryptionShim shim =
            ShimLoader.getHadoopShims().createHdfsEncryptionShim(FileSystem.get(hiveConf), hiveConf);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Thoughts? Thanks&lt;/p&gt;</comment>
                            <comment id="15303049" author="thejas" created="Thu, 26 May 2016 22:09:16 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-3098&quot; title=&quot;Memory leak from large number of FileSystem instances in FileSystem.CACHE&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-3098&quot;&gt;&lt;del&gt;HIVE-3098&lt;/del&gt;&lt;/a&gt; fixes it for metastore . &lt;br/&gt;
This fix can be dangerous for the embedded metastore use case.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I think because of my test being run as a single user. &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Single user shouldn&apos;t matter as the cache is based on the UGI object as I mentioned earlier. Testing using hive-cli might be better that would ensure creation on new metastore connection each time as well.&lt;/p&gt;

&lt;p&gt;I assume you haven&apos;t seen this in other user environments. I suspect there is something unique about their environment that would be triggering this. You might want to check if their are using any specific plugins.&lt;br/&gt;
Is this with kerberos enabled ?&lt;/p&gt;</comment>
                            <comment id="15304074" author="aihuaxu" created="Fri, 27 May 2016 13:43:39 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ngangam&quot; class=&quot;user-hover&quot; rel=&quot;ngangam&quot;&gt;Naveen Gangam&lt;/a&gt; Just try to understand the issue. FileSystem will cache the key (containing conf). But seems it doesn&apos;t make copies of conf but uses the reference? Are we seeing the conf is held by cache object? &lt;/p&gt;</comment>
                            <comment id="15304139" author="ngangam" created="Fri, 27 May 2016 14:41:56 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=thejas&quot; class=&quot;user-hover&quot; rel=&quot;thejas&quot;&gt;Thejas M Nair&lt;/a&gt; Thanks for continued discussion on this. However, I am not entirely sure I understand your comment about it being dangerous for scenarios with embedded metastore. Could you please elaborate and help me understand? &lt;br/&gt;
I will lay out what I found so far so that we are all on the same page. Helps answer &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aihuaxu&quot; class=&quot;user-hover&quot; rel=&quot;aihuaxu&quot;&gt;Aihua Xu&lt;/a&gt; questions too.&lt;br/&gt;
1) In a HMS heap dump from a customer, there were 66000 instances of Configuration class stored in a HashMap (FileSystem.Cache.CACHE). These instances along with the set of its retained objects accounted for 95% of the total heap. Thats huge.&lt;br/&gt;
2) FileSystem.Cache use a map that uses a composite key with (UGI + Configuration) objects. Because of the way, these keys are compared or because Configuration objects used in Hive are different (because of session params and what not), or a combination of both, we have a lot of cache misses which result in new entries being added to this cache.&lt;br/&gt;
3) This Cache is storing FileSystem objects (an abstraction to the underlying DFS), which, I am told by Hadoop engineer, are a bit expensive to instantiate.  &lt;br/&gt;
4) HMS uses APIs like, FileSystem.get() (in HiveMetaStore) and Path.getFileSystem()(In Warehouse.java) that cause the cache to grow. &lt;br/&gt;
5) I have had the customer disable this cache entirely using a hadoop property, and they have neither seen any issues functionally nor observed any severe performance regression while the HMS heap size remained &amp;lt;350MB compared to 10GB with the cache enabled.&lt;/p&gt;

&lt;p&gt;Since disabling the cache entirely causes no functional regression, wouldn&apos;t adding a call to delete elements from this cache would be fine too, functionally? Thank you&lt;/p&gt;</comment>
                            <comment id="15304155" author="ngangam" created="Fri, 27 May 2016 15:00:26 +0000"  >&lt;p&gt;Unfortunately, my test environment got auto-destroyed. I will set it up again to re-run with hive CLI instead. I wasnt using kerberos the first time. Did you want me to use kerberos this time? Thanks&lt;/p&gt;</comment>
                            <comment id="15304165" author="aihuaxu" created="Fri, 27 May 2016 15:12:03 +0000"  >&lt;p&gt;OK. So it&apos;s not the objects of Conf themselves occupy the memory, but the instances of the conf (too many of them) as I read from above and &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-3098&quot; title=&quot;Memory leak from large number of FileSystem instances in FileSystem.CACHE&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-3098&quot;&gt;&lt;del&gt;HIVE-3098&lt;/del&gt;&lt;/a&gt;?&lt;/p&gt;</comment>
                            <comment id="15304219" author="ngangam" created="Fri, 27 May 2016 15:47:08 +0000"  >&lt;p&gt;The retained size for each Configuration instance is roughly 100KB. ~66k instances x ~100KB ~= 6+GB. Thats what we are trying to address.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=thejas&quot; class=&quot;user-hover&quot; rel=&quot;thejas&quot;&gt;Thejas M Nair&lt;/a&gt; We have seen several issues around HMS stability (sudden death crashes) in the past, a good % of them were OOMs, some connections leaks, some incorrectly sized thread pools etc. We have asked customers to increase the max memory for the HMS process. Some customers running with 40GB memory for HMS and that too at times is not enough. 16GB to 24GB is quite common amongst other customers too. &lt;br/&gt;
In the past, we have not received heap dumps from the process prior to the crash. So we did not get a chance to analyze it.&lt;/p&gt;</comment>
                            <comment id="15304586" author="thejas" created="Fri, 27 May 2016 19:09:45 +0000"  >&lt;blockquote&gt;&lt;p&gt;Since disabling the cache entirely causes no functional regression, wouldn&apos;t adding a call to delete elements from this cache would be fine too, functionally&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;No, that method is closeAllForUGI, not deleteAllForUGIFromCache, ie it closes the FileSystem instances. In case of metastore being used in embedded mode (-hiveconf hive.metastore.uris=&apos; &apos;) , if there are references to the FileSystem object in other parts of the code, they will be suddenly closed instances.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aihuaxu&quot; class=&quot;user-hover&quot; rel=&quot;aihuaxu&quot;&gt;Aihua Xu&lt;/a&gt; The FileSystem.CACHE keeps the references to the FileSystem objects around even if most of them aren&apos;t being used and GC will not free them as a result.&lt;/p&gt;

&lt;p&gt;The changes in &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-3098&quot; title=&quot;Memory leak from large number of FileSystem instances in FileSystem.CACHE&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-3098&quot;&gt;&lt;del&gt;HIVE-3098&lt;/del&gt;&lt;/a&gt; should be addressing this, we need to understand why it wouldn&apos;t do that before adding yet another call to closeAllForUGI.&lt;br/&gt;
We haven&apos;t seen this OOM in metastore with hive 1.2.0 users, except for an issue when ACID was enabled, which &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=wzheng&quot; class=&quot;user-hover&quot; rel=&quot;wzheng&quot;&gt;Wei Zheng&lt;/a&gt; worked on. So one thing to analyze is if any plugins are creating new UGI object and that is causing this OOM. &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=daijy&quot; class=&quot;user-hover&quot; rel=&quot;daijy&quot;&gt;Daniel Dai&lt;/a&gt; has some experience with dealing with this type of leak. He can also advise on this.&lt;/p&gt;</comment>
                            <comment id="15317001" author="daijy" created="Mon, 6 Jun 2016 19:01:13 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ngangam&quot; class=&quot;user-hover&quot; rel=&quot;ngangam&quot;&gt;Naveen Gangam&lt;/a&gt;, what I used to do before to diagnose is to use a patched hadoop client libraries to catch the stack of every invocation of FileSystem.get, and understand exactly where the leak coming from. I don&apos;t want to blindly remove it in shutdown, plus, UGI object might already get lost at that time and you might not able to remove it.&lt;/p&gt;

&lt;p&gt;Here is how I patch Hadoop:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java
+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java
@@ -20,6 +20,8 @@
 &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; java.io.Closeable;
 &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; java.io.FileNotFoundException;
 &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; java.io.IOException;
+&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; java.io.StringWriter;
+&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; java.io.PrintWriter;
 &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; java.lang.ref.WeakReference;
 &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; java.net.URI;
 &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; java.net.URISyntaxException;
@@ -2699,6 +2701,10 @@ &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; FileSystem getInternal(URI uri, Configuration conf, Key key) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOEx
         }
         fs.key = key;
         map.put(key, fs);
+        StringWriter sw = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; StringWriter();
+        &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Throwable(&quot;&quot;).printStackTrace(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; PrintWriter(sw));
+        LOG.info(&lt;span class=&quot;code-quote&quot;&gt;&quot;calling context &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; getInternal:&quot;&lt;/span&gt; + sw.toString());
+        LOG.info(&lt;span class=&quot;code-quote&quot;&gt;&quot;# of maps:&quot;&lt;/span&gt; + map.size());
         &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (conf.getBoolean(&lt;span class=&quot;code-quote&quot;&gt;&quot;fs.automatic.close&quot;&lt;/span&gt;, &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;)) {
           toAutoClose.add(key);
         }
@@ -2752,6 +2758,7 @@ &lt;span class=&quot;code-keyword&quot;&gt;synchronized&lt;/span&gt; void closeAll(&lt;span class=&quot;code-object&quot;&gt;boolean&lt;/span&gt; onlyAutomatic) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
       &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (!exceptions.isEmpty()) {
         &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; MultipleIOException.createIOException(exceptions);
       }
+      LOG.info(&lt;span class=&quot;code-quote&quot;&gt;&quot;map size after closeAll:&quot;&lt;/span&gt; + map.size());
     }

     &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; class ClientFinalizer &lt;span class=&quot;code-keyword&quot;&gt;implements&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;Runnable&lt;/span&gt; {
@@ -2789,6 +2796,7 @@ &lt;span class=&quot;code-keyword&quot;&gt;synchronized&lt;/span&gt; void closeAll(UserGroupInformation ugi) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
       &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (!exceptions.isEmpty()) {
         &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; MultipleIOException.createIOException(exceptions);
       }
+      LOG.info(&lt;span class=&quot;code-quote&quot;&gt;&quot;map size after closeAll:&quot;&lt;/span&gt; + map.size());
     }

     /** FileSystem.Cache.Key */
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here is how to instruct Hive to use this jar:&lt;br/&gt;
1. Put attached hadoop-common.jar into $HIVE_HOME/lib&lt;br/&gt;
2. export HADOOP_USER_CLASSPATH_FIRST=true&lt;br/&gt;
3. Make sure fs cache is enabled&lt;br/&gt;
4. Restart hivemetastore&lt;br/&gt;
5. Collect hivemetastore.log&lt;/p&gt;</comment>
                            <comment id="15317100" author="ngangam" created="Mon, 6 Jun 2016 19:58:53 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=daijy&quot; class=&quot;user-hover&quot; rel=&quot;daijy&quot;&gt;Daniel Dai&lt;/a&gt; &lt;br/&gt;
I have been running with some added instrumentation in the HMS code to figure out the cache sizes before and after. But your idea seems better, seeking info from the hadoop&apos;s end.&lt;br/&gt;
There are 3 general areas that seem to be adding objects to the cache.&lt;br/&gt;
1) The compactor.Initiator and CompactorThread create about ~420k objects. These seem to be addressed in &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-13151&quot; title=&quot;Clean up UGI objects in FileSystem cache for transactions&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-13151&quot;&gt;&lt;del&gt;HIVE-13151&lt;/del&gt;&lt;/a&gt;. This environment is not running with this fix.&lt;br/&gt;
2) The Warehouse.getFs() and Warehouse.getFileStatusesForLocation() are invoked about ~900k times, but not all calls result in new object in the cache.&lt;br/&gt;
3) A small % of the calls are from drop_table_core. &lt;/p&gt;

&lt;p&gt;I will try to see other areas that use these FS apis that could be adding to this cache.&lt;/p&gt;

&lt;p&gt;Thejas, the fix from &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-3098&quot; title=&quot;Memory leak from large number of FileSystem instances in FileSystem.CACHE&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-3098&quot;&gt;&lt;del&gt;HIVE-3098&lt;/del&gt;&lt;/a&gt; no longer exists in the codebase. It has been replaced by the fix in &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-8228&quot; title=&quot;CBO: fix couple of issues with partition pruning&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-8228&quot;&gt;&lt;del&gt;HIVE-8228&lt;/del&gt;&lt;/a&gt; (simliar intent). The root cause could very well be the initiator thread. I will check their configuration to affirm this and use &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-13151&quot; title=&quot;Clean up UGI objects in FileSystem cache for transactions&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-13151&quot;&gt;&lt;del&gt;HIVE-13151&lt;/del&gt;&lt;/a&gt; if needed. Thanks&lt;/p&gt;</comment>
                            <comment id="15321265" author="ngangam" created="Wed, 8 Jun 2016 19:12:31 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=thejas&quot; class=&quot;user-hover&quot; rel=&quot;thejas&quot;&gt;Thejas M Nair&lt;/a&gt; After disabling the compactor.Initiator and the Compactor threads, (because this customer is not using the fix from &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-13151&quot; title=&quot;Clean up UGI objects in FileSystem cache for transactions&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-13151&quot;&gt;&lt;del&gt;HIVE-13151&lt;/del&gt;&lt;/a&gt;), there appear to be no more leaks. &lt;br/&gt;
However, there are still about 400 instances of Configuration objects in memory (about 80MB of retained objects, 12% in this case), about 11 of them from static initializers in *Writable classes and the remaining of them stashed in thread locals, 1 per thread. So HMS roughly has 390 threads, each has 1 instance of Configuration set in its threadlocals. These references should be re-set when the thread gets re-assigned but they would be retained until this occurs. Would it make sense to do this cleanup sooner. Something like this&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
         &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
           ms.shutdown();
         } &lt;span class=&quot;code-keyword&quot;&gt;finally&lt;/span&gt; {
           threadLocalConf.remove();
           threadLocalMS.remove();
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;As always, thank you for your input in advance. &lt;/p&gt;</comment>
                            <comment id="15359603" author="ngangam" created="Fri, 1 Jul 2016 20:46:22 +0000"  >&lt;p&gt;Have a new heap dump from a customer that has been running for an extended period of time, with the following properties removed.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&amp;lt;property&amp;gt;&amp;lt;name&amp;gt;hive.compactor.initiator.on&amp;lt;/name&amp;gt;&amp;lt;value&amp;gt;&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;&amp;lt;name&amp;gt;hive.compactor.worker.threads&amp;lt;/name&amp;gt;&amp;lt;value&amp;gt;1&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The leak from the above compactor thread was later addressed in &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-13151&quot; title=&quot;Clean up UGI objects in FileSystem cache for transactions&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-13151&quot;&gt;&lt;del&gt;HIVE-13151&lt;/del&gt;&lt;/a&gt; that the customer was not running with. So I had them remove these properties.&lt;/p&gt;

&lt;p&gt;HMS has been stable, no OOM crashes yet. There are about 214 instances of Configuration occupying 66MB of heap space, roughly 18% of the total active heap. Each of this instance was from an individual thread holding on to the object in the threadlocals. I think we can save this unnecessary reference and do better by cleaning up the threadlocal before the thread is returning to the pool.&lt;/p&gt;

&lt;p&gt;I have uploaded a new patch that does just that. Let me know of any thoughts. Thanks&lt;/p&gt;</comment>
                            <comment id="15360253" author="hiveqa" created="Sat, 2 Jul 2016 16:55:00 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12815813/HIVE-13749.1.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12815813/HIVE-13749.1.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to no test(s) being added or modified.&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to 6 failed/errored test(s), 10287 tests executed&lt;br/&gt;
&lt;b&gt;Failed tests:&lt;/b&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_multiinsert
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver_vector_complex_all
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver_vector_complex_join
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_schemeAuthority
org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskSchedulerService.testDelayedLocalityNodeCommErrorImmediateAllocation
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-MASTER-Build/350/testReport&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-MASTER-Build/350/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-MASTER-Build/350/console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-MASTER-Build/350/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://ec2-50-18-27-0.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-MASTER-Build-350/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://ec2-50-18-27-0.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-MASTER-Build-350/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 6 tests failed
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12815813 - PreCommit-HIVE-MASTER-Build&lt;/p&gt;</comment>
                            <comment id="15362557" author="ngangam" created="Tue, 5 Jul 2016 14:28:54 +0000"  >&lt;p&gt;The test failures above do not appear to be related to the patch. So +1 for me.&lt;/p&gt;</comment>
                            <comment id="15362576" author="aihuaxu" created="Tue, 5 Jul 2016 14:45:25 +0000"  >&lt;p&gt;+1. The change makes sense to me.&lt;/p&gt;</comment>
                            <comment id="15363705" author="thejas" created="Wed, 6 Jul 2016 03:46:27 +0000"  >&lt;p&gt;+1 looks good.&lt;br/&gt;
Thanks for chasing this down!&lt;/p&gt;</comment>
                            <comment id="15367699" author="aihuaxu" created="Fri, 8 Jul 2016 13:56:19 +0000"  >&lt;p&gt;Pushed to master. Thanks Naveen for the contribution.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12815813" name="HIVE-13749.1.patch" size="532" author="ngangam" created="Fri, 1 Jul 2016 20:35:44 +0000"/>
                            <attachment id="12806472" name="HIVE-13749.patch" size="829" author="ngangam" created="Thu, 26 May 2016 20:48:36 +0000"/>
                            <attachment id="12803698" name="Top_Consumers7.html" size="4990" author="ngangam" created="Thu, 12 May 2016 16:31:59 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 12 May 2016 18:58:50 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            20 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2xusf:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12310320" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Target Version/s</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue>12335838</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>