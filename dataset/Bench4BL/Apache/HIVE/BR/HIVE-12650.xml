<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 02 06:10:35 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HIVE-12650/HIVE-12650.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HIVE-12650] Improve error messages for Hive on Spark in case the cluster has no resources available</title>
                <link>https://issues.apache.org/jira/browse/HIVE-12650</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;I think hive.spark.client.server.connect.timeout should be set greater than spark.yarn.am.waitTime. The default value for &lt;br/&gt;
spark.yarn.am.waitTime is 100s, and the default value for hive.spark.client.server.connect.timeout is 90s, which is not good. We can increase it to a larger value such as 120s.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12920958">HIVE-12650</key>
            <summary>Improve error messages for Hive on Spark in case the cluster has no resources available</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="lirui">Rui Li</assignee>
                                    <reporter username="JoyoungZhang@gmail.com">JoneZhang</reporter>
                        <labels>
                    </labels>
                <created>Fri, 11 Dec 2015 03:58:44 +0000</created>
                <updated>Tue, 21 Jun 2016 15:53:13 +0000</updated>
                            <resolved>Fri, 1 Apr 2016 06:42:41 +0000</resolved>
                                    <version>1.1.1</version>
                    <version>1.2.1</version>
                                    <fixVersion>2.1.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                                                <comments>
                            <comment id="15126219" author="lirui" created="Mon, 1 Feb 2016 13:08:53 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vanzin&quot; class=&quot;user-hover&quot; rel=&quot;vanzin&quot;&gt;Marcelo Vanzin&lt;/a&gt;, any idea on this?&lt;/p&gt;</comment>
                            <comment id="15126293" author="xuefuz" created="Mon, 1 Feb 2016 14:26:08 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;Rui Li&lt;/a&gt;, since application master in the context of Hive on Spark takes a container from yarn. In a busy cluster, spark-submit may wait up to spark.yarn.am.waitTime to launch the master. On the other hand, Hive waits for  hive.spark.client.server.connect.timeout  before declaring that the remote driver is not connecting back. If the latter is less than the former, it&apos;s possible that Hive prematurely disconnects, causing an unstable condition. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=JoyoungZhang%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;JoyoungZhang@gmail.com&quot;&gt;JoneZhang&lt;/a&gt; had a description of the problem in the user list.&lt;/p&gt;

&lt;p&gt;I think we need at least to make hive.spark.client.server.connect.timeout greater than spark.yarn.am.waitTime by default. To further guard against the problem, Hive can increase hive.spark.client.server.connect.timeout automatically based on the value of spark.yarn.am.waitTime;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vanzin&quot; class=&quot;user-hover&quot; rel=&quot;vanzin&quot;&gt;Marcelo Vanzin&lt;/a&gt;, please share your thoughts as well.&lt;/p&gt;</comment>
                            <comment id="15126779" author="vanzin" created="Mon, 1 Feb 2016 18:47:59 +0000"  >&lt;p&gt;&lt;tt&gt;spark.yarn.am.waitTime&lt;/tt&gt; is not the time Spark waits for the master to launch. It&apos;s the time the Spark AM waits for the SparkContext to be created after the AM has been launched.&lt;/p&gt;

&lt;p&gt;That being said, it&apos;s ok for the Hive timeout to be larger. 90s already seems like a really long time to wait, so I doubt the extra 30s will help, but it won&apos;t hurt.&lt;/p&gt;</comment>
                            <comment id="15126866" author="xuefuz" created="Mon, 1 Feb 2016 19:29:24 +0000"  >&lt;p&gt;Thanks for the clarification, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vanzin&quot; class=&quot;user-hover&quot; rel=&quot;vanzin&quot;&gt;Marcelo Vanzin&lt;/a&gt;. I agree with you. Do you know what factors (such as a lack of available executors) might make Spark AM wait for SparkContext to be initialized for longer period of time (say, a minute)? The problem seems to be that Hive times out first while the AM still appears running, waiting for the context to be initialized. It will eventually fail either the context gets initialized for timeout occurs. This might look a bit confusing. I&apos;m think if we make Hive waits longer than that, then we can avoid the scenario. Any further thoughts?&lt;/p&gt;</comment>
                            <comment id="15126881" author="vanzin" created="Mon, 1 Feb 2016 19:38:12 +0000"  >&lt;blockquote&gt;&lt;p&gt;Do you know what factors (such as a lack of available executors) might make Spark AM wait for SparkContext to be initialized for longer period of time (say, a minute)?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The only factor is possible problems in the user&apos;s &lt;tt&gt;main&lt;/tt&gt; method, since that&apos;s the code that creates the SparkContext. The AM container is &lt;b&gt;already running&lt;/b&gt; at that time, so it can&apos;t really fail for not being able to allocate the container...&lt;/p&gt;</comment>
                            <comment id="15127492" author="lirui" created="Tue, 2 Feb 2016 02:41:30 +0000"  >&lt;p&gt;Thanks guys for your inputs. My understanding is that &lt;tt&gt;hive.spark.client.server.connect.timeout&lt;/tt&gt; is the timeout between RPC server and client handshake. In &lt;tt&gt;RemoteDriver&lt;/tt&gt;, RPC client is created before SparkContext. And if &lt;tt&gt;spark.yarn.am.waitTime&lt;/tt&gt; is the timeout waiting for SparkContext to be created, maybe it won&apos;t help here. I mean we can try increasing &lt;tt&gt;hive.spark.client.server.connect.timeout&lt;/tt&gt;, but according to something else.&lt;br/&gt;
BTW, is it possible the timeout is caused by the schedule delay within yarn? Is the issue only encountered with yarn-cluster?&lt;/p&gt;</comment>
                            <comment id="15127507" author="xuefuz" created="Tue, 2 Feb 2016 02:47:55 +0000"  >&lt;p&gt;Here is the log that provided the the JIRA creator:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Logs of Application_1448873753366_121022 as follows(same as application_1448873753366_121055):
Container: container_1448873753366_121022_03_000001 on 10.226.136.122_8041
============================================================================
LogType: stderr
LogLength: 4664
Log Contents:
Please use CMSClassUnloadingEnabled in place of CMSPermGenSweepingEnabled in the &lt;span class=&quot;code-keyword&quot;&gt;future&lt;/span&gt;
Please use CMSClassUnloadingEnabled in place of CMSPermGenSweepingEnabled in the &lt;span class=&quot;code-keyword&quot;&gt;future&lt;/span&gt;
15/12/09 16:29:45 INFO yarn.ApplicationMaster: Registered signal handlers &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; [TERM, HUP, INT]
15/12/09 16:29:46 INFO yarn.ApplicationMaster: ApplicationAttemptId: appattempt_1448873753366_121022_000003
15/12/09 16:29:47 INFO spark.&lt;span class=&quot;code-object&quot;&gt;SecurityManager&lt;/span&gt;: Changing view acls to: mqq
15/12/09 16:29:47 INFO spark.&lt;span class=&quot;code-object&quot;&gt;SecurityManager&lt;/span&gt;: Changing modify acls to: mqq
15/12/09 16:29:47 INFO spark.&lt;span class=&quot;code-object&quot;&gt;SecurityManager&lt;/span&gt;: &lt;span class=&quot;code-object&quot;&gt;SecurityManager&lt;/span&gt;: authentication disabled; ui acls disabled; users with view permissions: Set(mqq); users with modify permissions: Set(mqq)
15/12/09 16:29:47 INFO yarn.ApplicationMaster: Starting the user application in a separate &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;
15/12/09 16:29:47 INFO yarn.ApplicationMaster: Waiting &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; spark context initialization
15/12/09 16:29:47 INFO yarn.ApplicationMaster: Waiting &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; spark context initialization ... 
15/12/09 16:29:47 INFO client.RemoteDriver: Connecting to: 10.179.12.140:38842
15/12/09 16:29:48 WARN rpc.Rpc: Invalid log level &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;, reverting to &lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;.
15/12/09 16:29:48 ERROR yarn.ApplicationMaster: User class threw exception: java.util.concurrent.ExecutionException: javax.security.sasl.SaslException: Client closed before SASL negotiation finished.
java.util.concurrent.ExecutionException: javax.security.sasl.SaslException: Client closed before SASL negotiation finished.
        at io.netty.util.concurrent.AbstractFuture.get(AbstractFuture.java:37)
        at org.apache.hive.spark.client.RemoteDriver.&amp;lt;init&amp;gt;(RemoteDriver.java:156)
        at org.apache.hive.spark.client.RemoteDriver.main(RemoteDriver.java:556)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:483)
Caused by: javax.security.sasl.SaslException: Client closed before SASL negotiation finished.
        at org.apache.hive.spark.client.rpc.Rpc$SaslClientHandler.dispose(Rpc.java:449)
        at org.apache.hive.spark.client.rpc.SaslHandler.channelInactive(SaslHandler.java:90)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:233)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:219)
        at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75)
        at org.apache.hive.spark.client.rpc.KryoMessageCodec.channelInactive(KryoMessageCodec.java:127)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:233)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:219)
        at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:233)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:219)
        at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:769)
        at io.netty.channel.AbstractChannel$AbstractUnsafe$5.run(AbstractChannel.java:567)
        at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:380)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
        at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
15/12/09 16:29:48 INFO yarn.ApplicationMaster: Final app status: FAILED, exitCode: 15, (reason: User class threw exception: java.util.concurrent.ExecutionException: javax.security.sasl.SaslException: Client closed before SASL negotiation finished.)
15/12/09 16:29:57 ERROR yarn.ApplicationMaster: SparkContext did not initialize after waiting &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 150000 ms. Please check earlier log output &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; errors. Failing the application.
15/12/09 16:29:57 INFO yarn.ApplicationMaster: Unregistering ApplicationMaster with FAILED (diag message: User class threw exception: java.util.concurrent.ExecutionException: javax.security.sasl.SaslException: Client closed before SASL negotiation finished.)
15/12/09 16:29:57 INFO yarn.ApplicationMaster: Deleting staging directory .sparkStaging/application_1448873753366_121022
15/12/09 16:29:57 INFO util.Utils: Shutdown hook called

LogType: stdout
LogLength: 216
Log Contents:
Java HotSpot(TM) 64-Bit Server VM warning: ignoring option UseCompressedStrings; support was removed in 7.0
Java HotSpot(TM) 64-Bit Server VM warning: ignoring option UseCompressedStrings; support was removed in 7.0
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The interesting part of the log is:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
15/12/09 16:29:57 ERROR yarn.ApplicationMaster: SparkContext did not initialize after waiting &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 150000 ms. Please check earlier log output &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; errors. Failing the application.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I have shared with the thread in the user mailing list for reference via email.&lt;/p&gt;</comment>
                            <comment id="15127564" author="xuefuz" created="Tue, 2 Feb 2016 03:09:46 +0000"  >&lt;p&gt;I&apos;m specially interested in case where Hive calls spark-submit to submit the application while there is no container available. I&apos;m not sure if spark-submit will wait. If it does, then Hive can time out first before the AM starts to run.&lt;/p&gt;</comment>
                            <comment id="15127747" author="lirui" created="Tue, 2 Feb 2016 06:23:45 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xuefuz&quot; class=&quot;user-hover&quot; rel=&quot;xuefuz&quot;&gt;Xuefu Zhang&lt;/a&gt;, the exception you posted doesn&apos;t seem to be a timeout, at least it&apos;s not related to &lt;tt&gt;hive.spark.client.server.connect.timeout&lt;/tt&gt;, because the elapsed time is much less than 90s. I found the code that prints the log you mentioned:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
      &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; (sparkContextRef.get() == &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt; &amp;amp;&amp;amp; &lt;span class=&quot;code-object&quot;&gt;System&lt;/span&gt;.currentTimeMillis &amp;lt; deadline &amp;amp;&amp;amp; !finished) {
        logInfo(&lt;span class=&quot;code-quote&quot;&gt;&quot;Waiting &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; spark context initialization ... &quot;&lt;/span&gt;)
        sparkContextRef.wait(10000L)
      }

      val sparkContext = sparkContextRef.get()
      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (sparkContext == &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) {
        logError((&lt;span class=&quot;code-quote&quot;&gt;&quot;SparkContext did not initialize after waiting &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; %d ms. Please check earlier&quot;&lt;/span&gt;
          + &lt;span class=&quot;code-quote&quot;&gt;&quot; log output &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; errors. Failing the application.&quot;&lt;/span&gt;).format(totalWaitTime))
      }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;You can see the while loop can exit either on timeout or finished being set to true. Since time elapsed is short, it must because user thread (RemoteDriver) has finished abnormally:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    val userThread = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt; {
      override def run() {
        &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
          mainMethod.invoke(&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;, userArgs.toArray)
          finish(FinalApplicationStatus.SUCCEEDED, ApplicationMaster.EXIT_SUCCESS)
          logDebug(&lt;span class=&quot;code-quote&quot;&gt;&quot;Done running users class&quot;&lt;/span&gt;)
        } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; {
          &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; e: InvocationTargetException =&amp;gt;
            e.getCause match {
              &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; _: InterruptedException =&amp;gt;
                &lt;span class=&quot;code-comment&quot;&gt;// Reporter thread can interrupt to stop user class
&lt;/span&gt;              &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; SparkUserAppException(exitCode) =&amp;gt;
                val msg = s&lt;span class=&quot;code-quote&quot;&gt;&quot;User application exited with status $exitCode&quot;&lt;/span&gt;
                logError(msg)
                finish(FinalApplicationStatus.FAILED, exitCode, msg)
              &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; cause: Throwable =&amp;gt;
                logError(&lt;span class=&quot;code-quote&quot;&gt;&quot;User class threw exception: &quot;&lt;/span&gt; + cause, cause)
                finish(FinalApplicationStatus.FAILED,
                  ApplicationMaster.EXIT_EXCEPTION_USER_CLASS,
                  &lt;span class=&quot;code-quote&quot;&gt;&quot;User class threw exception: &quot;&lt;/span&gt; + cause)
            }
        }
      }
    }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;In conclusion, the problem here is not we timed out creating SparkContext. My guess is that something goes wrong before we create SparkContext (you can refer to the constructor of RemoteDriver). Also found another property &lt;tt&gt;hive.spark.client.connect.timeout&lt;/tt&gt; which defaults to 1000ms. It&apos;s used when RemoteDriver creates RPC client so it could be related, although I&apos;m a little confused about the difference between the 2 configurations.&lt;/p&gt;

&lt;p&gt;Regarding your last question, I tried submitting application when no container is available. Spark-submit will wait until timeout (90s).&lt;/p&gt;</comment>
                            <comment id="15128361" author="xuefuz" created="Tue, 2 Feb 2016 14:55:45 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;Rui Li&lt;/a&gt;, thanks for your analysis. Yeah, I saw the actually elapsed time is very short, while the message says timeout 150s, which is very confusing.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vanzin&quot; class=&quot;user-hover&quot; rel=&quot;vanzin&quot;&gt;Marcelo Vanzin&lt;/a&gt;, could you please explain a little bit the use of the two timeout? Also, what timeout value does spark-submit use if the application cannot be submitted?&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=JoyoungZhang%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;JoyoungZhang@gmail.com&quot;&gt;JoneZhang&lt;/a&gt;, could you please reproduce the problem and provide more info such as hive.log?&lt;/p&gt;

&lt;p&gt;Thanks, folks!&lt;/p&gt;</comment>
                            <comment id="15128913" author="vanzin" created="Tue, 2 Feb 2016 20:00:38 +0000"  >&lt;blockquote&gt;&lt;p&gt;could you please explain a little bit the use of the two timeout?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;There&apos;s nothing complicated about them.&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;RSC timeout: time between the RSC launching the Spark app and the Spark driver connecting back.&lt;/li&gt;
	&lt;li&gt;Spark AM timeout: time between Spark AM launching the user&apos;s &quot;main&quot; method and a SparkContext being created.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Both overlap but one is not necessarily contained in the other.&lt;/p&gt;</comment>
                            <comment id="15128949" author="xuefuz" created="Tue, 2 Feb 2016 20:19:26 +0000"  >&lt;p&gt;Thanks, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vanzin&quot; class=&quot;user-hover&quot; rel=&quot;vanzin&quot;&gt;Marcelo Vanzin&lt;/a&gt;. I guess the question is the difference between the follow two (both defined in Hive):&lt;br/&gt;
1. hive.spark.client.connect.timeout&lt;br/&gt;
2. hive.spark.client.server.connect.timeout&lt;/p&gt;

&lt;p&gt;The second question is: what&apos;s the timeout value that spark-submit uses in case of no available containers?&lt;/p&gt;</comment>
                            <comment id="15128979" author="vanzin" created="Tue, 2 Feb 2016 20:37:19 +0000"  >&lt;ul&gt;
	&lt;li&gt;hive.spark.client.connect.timeout&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;That&apos;s the socket connect timeout when the driver connects to the RSC server. Equivalent to this:&lt;br/&gt;
&lt;a href=&quot;http://docs.oracle.com/javase/7/docs/api/java/net/Socket.html#connect(java.net.SocketAddress,%20int&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://docs.oracle.com/javase/7/docs/api/java/net/Socket.html#connect(java.net.SocketAddress,%20int&lt;/a&gt;)&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;hive.spark.client.server.connect.timeout&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;That&apos;s the timeout explained in my previous comment.&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;what&apos;s the timeout value that spark-submit uses in case of no available containers?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I don&apos;t believe there is one.&lt;/p&gt;</comment>
                            <comment id="15129392" author="xuefuz" created="Wed, 3 Feb 2016 00:01:54 +0000"  >&lt;p&gt;Thanks, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vanzin&quot; class=&quot;user-hover&quot; rel=&quot;vanzin&quot;&gt;Marcelo Vanzin&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If there is no timeout in spark-submit (wait indefinitely), I&apos;m wondering what happens if the cluster is busy. Here is my speculation. Hive will time out first (also corresponding to Rui&apos;s observation), but spark-submit will continue to run. If a container becomes available, Spark AM will start and connect to Hive. Hive of course refuses. Then, AM will error out.&lt;/p&gt;

&lt;p&gt;I&apos;m not sure if this what the user experienced. It would be good if we can cancel the submit. However, it doesn&apos;t look too bad even if we decide to live with it.&lt;/p&gt;

&lt;p&gt;Unless &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=JoyoungZhang%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;JoyoungZhang@gmail.com&quot;&gt;JoneZhang&lt;/a&gt; can provide more info, it doesn&apos;t seem we can do much here.&lt;/p&gt;</comment>
                            <comment id="15129595" author="lirui" created="Wed, 3 Feb 2016 01:56:19 +0000"  >&lt;blockquote&gt;&lt;p&gt;Regarding your last question, I tried submitting application when no container is available. Spark-submit will wait until timeout (90s).&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Sorry this comment is misleading. Actually I mean hive will timeout after 90s. But after this, we&apos;ll interrupt the driver thread:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
      &lt;span class=&quot;code-comment&quot;&gt;// The RPC server will take care of timeouts here.
&lt;/span&gt;      &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.driverRpc = rpcServer.registerClient(clientId, secret, protocol).get();
    } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (Throwable e) {
      LOG.warn(&lt;span class=&quot;code-quote&quot;&gt;&quot;Error &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; waiting &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; client to connect.&quot;&lt;/span&gt;, e);
      driverThread.interrupt();
      &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
        driverThread.join();
      } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (InterruptedException ie) {
        &lt;span class=&quot;code-comment&quot;&gt;// Give up.
&lt;/span&gt;        LOG.debug(&lt;span class=&quot;code-quote&quot;&gt;&quot;Interrupted before driver thread was finished.&quot;&lt;/span&gt;);
      }
      &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; Throwables.propagate(e);
    }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;which in turn will destroy the SparkSubmit process:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
        &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void run() {
          &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
            &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; exitCode = child.waitFor();
            &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (exitCode != 0) {
              rpcServer.cancelClient(clientId, &lt;span class=&quot;code-quote&quot;&gt;&quot;Child process exited before connecting back&quot;&lt;/span&gt;);
              LOG.warn(&lt;span class=&quot;code-quote&quot;&gt;&quot;Child process exited with code {}.&quot;&lt;/span&gt;, exitCode);
            }
          } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (InterruptedException ie) {
            LOG.warn(&lt;span class=&quot;code-quote&quot;&gt;&quot;Waiting thread interrupted, killing child process.&quot;&lt;/span&gt;);
            &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.interrupted();
            child.destroy();
          } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (Exception e) {
            LOG.warn(&lt;span class=&quot;code-quote&quot;&gt;&quot;Exception &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; waiting &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; child process.&quot;&lt;/span&gt;, e);
          }
        }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;So on my machine, after the timeout, SparkSubmit is terminated.&lt;br/&gt;
I think the &lt;tt&gt;Client closed before SASL negotiation finished.&lt;/tt&gt; exception is worth investigating and should be root cause here.&lt;/p&gt;</comment>
                            <comment id="15129655" author="xuefuz" created="Wed, 3 Feb 2016 02:33:30 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;Rui Li&lt;/a&gt;, thanks for the info. It&apos;s good that spark-submit is killed when Hive times out. Now the user&apos;s problem seems more interesting, though we cannot do much unless we have more information.&lt;/p&gt;

&lt;p&gt;&quot;Client closed before SASL negotiation finished&quot; could be caused by the fact that AM tries to connect back to Hive, but Hive has already timed out. While Spark-submit is killed, is possible that YARN RM still has the request which will be eventually served?&lt;/p&gt;</comment>
                            <comment id="15129682" author="lirui" created="Wed, 3 Feb 2016 03:00:18 +0000"  >&lt;p&gt;Thanks Xuefu. Yeah I tried again and found the application is served (AM launched) and failed eventually, even after SparkSubmit is killed. Although I didn&apos;t get the AM log due to some env issue.&lt;/p&gt;</comment>
                            <comment id="15129761" author="xuefuz" created="Wed, 3 Feb 2016 04:21:02 +0000"  >&lt;p&gt;I see. I think that&apos;s what the &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=JoyoungZhang%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;JoyoungZhang@gmail.com&quot;&gt;JoneZhang&lt;/a&gt; experienced as well. Killing spark-submit doesn&apos;t cancel AM request. When AM is finally launched, it tries to connect back to Hive and gets refused. As a result, it quickly errors out. (However, on spark side, the message, saying &quot;spark context initialization times out in xxx seconds&quot;, is very confusing.) I&apos;m not sure if we can do anything here.&lt;/p&gt;

&lt;p&gt;Nevertheless, it seems spark.yarn.am.waitTime isn&apos;t relevant after all.&lt;/p&gt;</comment>
                            <comment id="15148177" author="joyoungzhang@gmail.com" created="Tue, 16 Feb 2016 07:04:32 +0000"  >&lt;p&gt;Hi all,&lt;br/&gt;
I&apos;m sorrry reply you so late.&lt;/p&gt;

&lt;p&gt;Yes&lt;br/&gt;
hive.spark.client.server.connect.timeout and spark.yarn.am.waitTime does not have any relations.&lt;br/&gt;
hive.spark.client.server.connect.timeout is the timeout between RPC server and client handshake.When no container is available, hive cient  will exit after hive.spark.client.server.connect.timeout.&lt;br/&gt;
spark.yarn.am.waitTime is the time the Spark AM waits for the SparkContext to be created after the AM has been launched.&lt;/p&gt;

&lt;p&gt;There are two types of error log&lt;br/&gt;
1.Client closed before SASL negotiation finished was happened on resubmitted. See &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-12649&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HIVE-12649&lt;/a&gt;.&lt;br/&gt;
2.Connection refused: /hiveclientip:port was happend when am tries to connect back to Hive.&lt;/p&gt;

&lt;p&gt;Container: container_1448873753366_113453_01_000001 on 10.247.169.134_8041&lt;br/&gt;
============================================================================&lt;br/&gt;
LogType: stderr&lt;br/&gt;
LogLength: 3302&lt;br/&gt;
Log Contents:&lt;br/&gt;
Please use CMSClassUnloadingEnabled in place of CMSPermGenSweepingEnabled in the future&lt;br/&gt;
Please use CMSClassUnloadingEnabled in place of CMSPermGenSweepingEnabled in the future&lt;br/&gt;
15/12/09 02:11:48 INFO yarn.ApplicationMaster: Registered signal handlers for &lt;span class=&quot;error&quot;&gt;&amp;#91;TERM, HUP, INT&amp;#93;&lt;/span&gt;&lt;br/&gt;
15/12/09 02:11:48 INFO yarn.ApplicationMaster: ApplicationAttemptId: appattempt_1448873753366_113453_000001&lt;br/&gt;
15/12/09 02:11:49 INFO spark.SecurityManager: Changing view acls to: mqq&lt;br/&gt;
15/12/09 02:11:49 INFO spark.SecurityManager: Changing modify acls to: mqq&lt;br/&gt;
15/12/09 02:11:49 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(mqq); users with modify permissions: Set(mqq)&lt;br/&gt;
15/12/09 02:11:49 INFO yarn.ApplicationMaster: Starting the user application in a separate Thread&lt;br/&gt;
15/12/09 02:11:49 INFO yarn.ApplicationMaster: Waiting for spark context initialization&lt;br/&gt;
15/12/09 02:11:49 INFO yarn.ApplicationMaster: Waiting for spark context initialization ... &lt;br/&gt;
15/12/09 02:11:49 INFO client.RemoteDriver: Connecting to: 10.179.12.140:58013&lt;br/&gt;
15/12/09 02:11:49 ERROR yarn.ApplicationMaster: User class threw exception: java.util.concurrent.ExecutionException: java.net.ConnectException: Connection refused: /10.179.12.140:58013&lt;br/&gt;
java.util.concurrent.ExecutionException: java.net.ConnectException: Connection refused: /10.179.12.140:58013&lt;br/&gt;
        at io.netty.util.concurrent.AbstractFuture.get(AbstractFuture.java:37)&lt;br/&gt;
        at org.apache.hive.spark.client.RemoteDriver.&amp;lt;init&amp;gt;(RemoteDriver.java:156)&lt;br/&gt;
        at org.apache.hive.spark.client.RemoteDriver.main(RemoteDriver.java:556)&lt;br/&gt;
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)&lt;br/&gt;
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
        at java.lang.reflect.Method.invoke(Method.java:606)&lt;br/&gt;
        at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:483)&lt;br/&gt;
Caused by: java.net.ConnectException: Connection refused: /10.179.12.140:58013&lt;br/&gt;
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)&lt;br/&gt;
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)&lt;br/&gt;
        at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:208)&lt;br/&gt;
        at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:287)&lt;br/&gt;
        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)&lt;br/&gt;
        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)&lt;br/&gt;
        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)&lt;br/&gt;
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)&lt;br/&gt;
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:745)&lt;br/&gt;
15/12/09 02:11:49 INFO yarn.ApplicationMaster: Final app status: FAILED, exitCode: 15, (reason: User class threw exception: java.util.concurrent.ExecutionException: java.net.ConnectException: Connection refused: /10.179.12.140:58013)&lt;br/&gt;
15/12/09 02:11:59 ERROR yarn.ApplicationMaster: SparkContext did not initialize after waiting for 150000 ms. Please check earlier log output for errors. Failing the application.&lt;br/&gt;
15/12/09 02:11:59 INFO util.Utils: Shutdown hook called&lt;/p&gt;</comment>
                            <comment id="15192912" author="xhao1" created="Mon, 14 Mar 2016 08:32:50 +0000"  >&lt;p&gt;Encountered the issue based on Apache Hive 2.0.0.&lt;/p&gt;

&lt;p&gt;BTW, suggest to change the issue title to a new one to avoid confusion. E.g. &quot;Spark-submit is killed when Hive times out.  Killing spark-submit doesn&apos;t cancel AM request. When AM is finally launched, it tries to connect back to Hive and gets refused.&quot;. Thanks.&lt;/p&gt;</comment>
                            <comment id="15193174" author="lirui" created="Mon, 14 Mar 2016 12:25:18 +0000"  >&lt;p&gt;The timeout is necessary in case the RSC crashes due to some errors. But the issue here shows that it could also because the RSC is just waiting for resources from a busy cluster. I think we need a way to distinguish these two scenarios and don&apos;t timeout on the latter.&lt;/p&gt;</comment>
                            <comment id="15201029" author="lirui" created="Fri, 18 Mar 2016 05:14:17 +0000"  >&lt;p&gt;Here&apos;re my findings so far (for yarn-client mode).&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;If the cluster has no resources available, the &lt;tt&gt;RemoteDriver&lt;/tt&gt; is blocked at creating the SparkContext. Rpc has been set up at this point. So &lt;tt&gt;SparkClientImpl&lt;/tt&gt; believes that driver is up.&lt;/li&gt;
	&lt;li&gt;There&apos;re 2 points where hive can get timeout. If we enable container pre-warm, we&apos;ll timeout at &lt;tt&gt;RemoteHiveSparkClient#createRemoteClient#getExecutorCount&lt;/tt&gt;. If pre-warm is off, we&apos;ll timeout at &lt;tt&gt;RemoteSparkJobMonitor#startMonitor&lt;/tt&gt;. Both are because SparkContext is not created and RemoteDriver can&apos;t respond to requests. For the latter, the job status remains &lt;tt&gt;SENT&lt;/tt&gt; until timeout. Ideally it should be &lt;tt&gt;QUEUE&lt;/tt&gt; instead. My understanding is that the Rpc handler is blocked at RemoteDriver for &lt;tt&gt;ADD JAR/FILE&lt;/tt&gt; calls, which we submitted before the real job.&lt;/li&gt;
	&lt;li&gt;Currently YARN doesn&apos;t timeout a starving application, which means the app will eventually get served. Refer to &lt;a href=&quot;https://issues.apache.org/jira/browse/YARN-3813&quot; title=&quot;Support Application timeout feature in YARN. &quot; class=&quot;issue-link&quot; data-issue-key=&quot;YARN-3813&quot;&gt;YARN-3813&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/YARN-2266&quot; title=&quot;Add an application timeout service in RM to kill applications which are not getting resources&quot; class=&quot;issue-link&quot; data-issue-key=&quot;YARN-2266&quot;&gt;YARN-2266&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Based on these findings, I think we have to decide whether hive should timeout in this situation. Waiting is reasonable for busy clusters. But on the other hand, it seems difficult to tell whether we&apos;re blocked for lack of resources. I&apos;m not sure if spark has such facilities for it. For yarn-cluster mode, this may be even more difficult because RemoteDriver is not running in that case and we&apos;ll have less information.&lt;br/&gt;
What do you guys think?&lt;/p&gt;</comment>
                            <comment id="15205200" author="xuefuz" created="Mon, 21 Mar 2016 21:30:53 +0000"  >&lt;p&gt;Since Hive cannot really differentiate the scenarios, I&apos;m not sure if there is anything we can do better except for better error message and documentation. RPC timeout is necessary due to network.&lt;/p&gt;

&lt;p&gt;On a side note, Yarn queues are more appropriate for solving the starvation problem. The problem here seems more like an uncommon scenario.&lt;/p&gt;</comment>
                            <comment id="15205603" author="lirui" created="Tue, 22 Mar 2016 01:46:55 +0000"  >&lt;p&gt;Regarding better error message, do you think we can throw a timeout exception if SparkContext is not up after certain amount of time? Otherwise user only gets a timeout on the future and doesn&apos;t know the cause. On the other hand, this means adding another property and I think it only works for yarn-client.&lt;/p&gt;</comment>
                            <comment id="15208398" author="xuefuz" created="Wed, 23 Mar 2016 13:33:54 +0000"  >&lt;p&gt;I was thinking of just improving the current message, maybe by naming different possibilities when a timeout occurs. The new timeout you mentioned doesn&apos;t seem very helpful as yarn-cluster is what we recommend.&lt;/p&gt;</comment>
                            <comment id="15211335" author="lirui" created="Fri, 25 Mar 2016 02:54:45 +0000"  >&lt;p&gt;I think the difficult part is that we really don&apos;t know the possible reasons. Anyway all we get is a timeout, it could be due to network issue, exceptions, or the RSC is just busy.&lt;/p&gt;

&lt;p&gt;Another possible refinement is that we can make the behavior more consistent. Like I said, there&apos;re now 2 paths that can lead to timeout/failure and user will see different error messages. How about remove the timeout at &lt;tt&gt;RemoteHiveSparkClient#createRemoteClient#getExecutorCount&lt;/tt&gt;? I mean after certain amount of time, we can give up the pre-warm and eventually fail the job at job monitor.&lt;/p&gt;</comment>
                            <comment id="15211391" author="xuefuz" created="Fri, 25 Mar 2016 04:13:03 +0000"  >&lt;p&gt;Thanks, Rui. I think it&apos;s fine to list all possible causes in an error message when we don&apos;t actually know the exact one. We can also suggest user where to look further (such as yarn logs).&lt;/p&gt;

&lt;p&gt;I understand that prewarming containers complicates the things a bit, but I&apos;m not sure of your proposal. Could you provide a patch showing the changes you have in mind?&lt;/p&gt;</comment>
                            <comment id="15213875" author="lirui" created="Mon, 28 Mar 2016 06:38:24 +0000"  >&lt;p&gt;Assigned this to me and upload a patch.&lt;br/&gt;
The main change in the patch is that we don&apos;t error out when timeout pre-warming. I think it makes sense because we already allow timeout in pre-warm so it shouldn&apos;t be a fatal error.&lt;br/&gt;
The patch also adds some explanations in error messages so it should be more user friendly.&lt;/p&gt;

&lt;p&gt;One thing I noticed when I tested the patch is that, with yarn-client mode, we may end up with hanging spark AM trying to connect to the driver that has already timed out. But I think we have to live with that and the AM will eventually give up and exit.&lt;/p&gt;</comment>
                            <comment id="15213935" author="lirui" created="Mon, 28 Mar 2016 07:55:52 +0000"  >&lt;p&gt;Update patch to also improve messages in yarn-cluster mode. Here&apos;s the summary of behaviors under these two modes.&lt;/p&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;&amp;nbsp;&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Error users will see&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Will spark-submit be killed after timeout&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;yarn-cluster&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;Failed to create spark client&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;Y&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;yarn-client&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;Job hasn&apos;t been submitted&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;N&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;I think the bottom line here is that when the starving app gets served, the aborted query won&apos;t be executed so that resources won&apos;t be wasted.&lt;/p&gt;</comment>
                            <comment id="15216086" author="hiveqa" created="Tue, 29 Mar 2016 14:27:10 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12795584/HIVE-12650.2.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12795584/HIVE-12650.2.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to no test(s) being added or modified.&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to 4 failed/errored test(s), 9888 tests executed&lt;br/&gt;
&lt;b&gt;Failed tests:&lt;/b&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;TestSparkCliDriver-groupby3_map.q-sample2.q-auto_join14.q-and-12-more - did not produce a TEST-*.xml file
TestSparkCliDriver-groupby_map_ppr_multi_distinct.q-table_access_keys_stats.q-groupby4_noskew.q-and-12-more - did not produce a TEST-*.xml file
TestSparkCliDriver-join_rc.q-insert1.q-vectorized_rcfile_columnar.q-and-12-more - did not produce a TEST-*.xml file
TestSparkCliDriver-ppd_join4.q-join9.q-ppd_join3.q-and-12-more - did not produce a TEST-*.xml file
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Test results: &lt;a href=&quot;http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/7402/testReport&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/7402/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/7402/console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/7402/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-7402/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-7402/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 4 tests failed
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12795584 - PreCommit-HIVE-TRUNK-Build&lt;/p&gt;</comment>
                            <comment id="15221049" author="lirui" created="Fri, 1 Apr 2016 02:49:54 +0000"  >&lt;p&gt;I tried several failed tests locally and they were not reproduced.&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xuefuz&quot; class=&quot;user-hover&quot; rel=&quot;xuefuz&quot;&gt;Xuefu Zhang&lt;/a&gt; would you mind take a look at the patch when you have time? Thanks.&lt;/p&gt;</comment>
                            <comment id="15221149" author="xuefuz" created="Fri, 1 Apr 2016 04:45:40 +0000"  >&lt;p&gt;+1. Patch looks good to me. Thanks for working on this, Rui!&lt;/p&gt;</comment>
                            <comment id="15221257" author="lirui" created="Fri, 1 Apr 2016 06:42:41 +0000"  >&lt;p&gt;Committed to master. Thanks Xuefu for the review and guys for the discussions.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12795582" name="HIVE-12650.1.patch" size="5398" author="lirui" created="Mon, 28 Mar 2016 06:38:24 +0000"/>
                            <attachment id="12795584" name="HIVE-12650.2.patch" size="6785" author="lirui" created="Mon, 28 Mar 2016 07:55:52 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Mon, 1 Feb 2016 13:08:53 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            34 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2pr1z:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>