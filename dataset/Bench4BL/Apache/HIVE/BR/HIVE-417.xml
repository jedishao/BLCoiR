<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sun Dec 04 00:01:01 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HIVE-417/HIVE-417.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HIVE-417] Implement Indexing in Hive</title>
                <link>https://issues.apache.org/jira/browse/HIVE-417</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;Implement indexing on Hive so that lookup and range queries are efficient.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12422895">HIVE-417</key>
            <summary>Implement Indexing in Hive</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="he yongqiang">He Yongqiang</assignee>
                                    <reporter username="prasadc">Prasad Chakka</reporter>
                        <labels>
                    </labels>
                <created>Wed, 15 Apr 2009 15:51:02 +0000</created>
                <updated>Sat, 5 Apr 2014 21:56:04 +0000</updated>
                            <resolved>Fri, 30 Jul 2010 06:42:14 +0000</resolved>
                                    <version>0.3.0</version>
                    <version>0.4.0</version>
                    <version>0.6.0</version>
                                    <fixVersion>0.7.0</fixVersion>
                                    <component>Indexing</component>
                    <component>Metastore</component>
                    <component>Query Processor</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>31</watches>
                                                                                                            <comments>
                            <comment id="12699255" author="dhruba" created="Wed, 15 Apr 2009 16:12:19 +0000"  >&lt;p&gt;This sounds really awesome! Make hadoop-hive suitable for things other than brute force table-scans!&lt;/p&gt;</comment>
                            <comment id="12699258" author="prasadc" created="Wed, 15 Apr 2009 16:20:14 +0000"  >&lt;p&gt;We already have code (based on early version of hive) that does the important work. But we need to integrate this with Hive QL and determine the candidate indexes for a query. Also a way to create buckets (both hash and range) would really speed up the index processing. I didn&apos;t realize but predicate pushdown makes things simpler.&lt;/p&gt;</comment>
                            <comment id="12699297" author="athusoo" created="Wed, 15 Apr 2009 18:07:34 +0000"  >&lt;p&gt;+1 to this...&lt;/p&gt;

&lt;p&gt;We should also look at Katta&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://katta.wiki.sourceforge.net/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://katta.wiki.sourceforge.net/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;to see if we can leverage it to be a really fast index. I think that integration would really rock..&lt;/p&gt;

&lt;p&gt;Thoughts?&lt;/p&gt;</comment>
                            <comment id="12699307" author="prasadc" created="Wed, 15 Apr 2009 18:25:57 +0000"  >&lt;p&gt;Another way of doing it is to create a file format that contains index along with data... but i think that would take lot more time.&lt;/p&gt;</comment>
                            <comment id="12699459" author="he yongqiang" created="Thu, 16 Apr 2009 00:26:53 +0000"  >&lt;p&gt;Great feature! Looking forward to see it.&lt;/p&gt;</comment>
                            <comment id="12702769" author="prasadc" created="Sat, 25 Apr 2009 19:59:44 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-1230&quot; title=&quot;HBase handler should validate and reject irrelevant DDL specification such as bucketing&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-1230&quot;&gt;HIVE-1230&lt;/a&gt; has changed the interface for RecordReader and it no longer has getPos() method. The older interfaces are deprecated. I used this method in the prototype get the current position while creating the index and also while reading the actual data file. Even the SequenceFileRecordReader does not have this method. &lt;/p&gt;

&lt;p&gt;Without getPos() and seek() methods to RecordReader it becomes tough to implement any kind of generic indexing.&lt;/p&gt;</comment>
                            <comment id="12709899" author="prasadc" created="Fri, 15 May 2009 16:37:14 +0000"  >&lt;p&gt;Here is a very rough outline as how this can be done (prototype code has creation and execution parts but not he HiveQL related stuff)&lt;/p&gt;

&lt;p&gt;hive indexing:&lt;br/&gt;
goal of hive indexing is to speed up lookup queries on certain columns of the table. currently queries with predicates like &apos;WHERE tab1.col1 = 10&apos; has to load the complete table/partition and process all the rows. if there exists an index on col1 then only a small portion of the file can be loaded.&lt;/p&gt;

&lt;p&gt;command to create index:&lt;/p&gt;

&lt;p&gt;create index tab_idx1 on tab1 (col1, ...);&lt;/p&gt;

&lt;p&gt;if the base table is partitioned then the index is also partitioned. indexes can be created on base tables whose file format supports (getPos() and possible seek() or equivalent methods.)&lt;/p&gt;

&lt;p&gt;format of index:&lt;br/&gt;
index is also a hive table with the following columns&lt;br/&gt;
col_1...col_k &amp;#8211; key cols. base table columns on which this index is defined&lt;br/&gt;
list&amp;lt;offset&amp;gt;  &amp;#8211; positions of rows which contain these keys&lt;/p&gt;

&lt;p&gt;offset is a combination of following&lt;br/&gt;
file_name   &amp;#8211; relative path of the file in which this row is contained. (relative to the partition/table location)&lt;br/&gt;
byte_offset &amp;#8211; byte offset of the row in the file. row can be found at this byte offset or in the block starting at this byte offset for Block Compressed Sequence Files.&lt;/p&gt;

&lt;p&gt;when to create index:&lt;br/&gt;
traditionally databases try to update index when the table is loaded. hive doesn&apos;t process rows while loading tables using &apos;LOAD DATA INPATH&apos; command. also it may slow down the actual loading for &apos;INSERT ... SELECT ... FROM ...&apos; type of statements. so users should have an option whether the index is initialized during &apos;INSERT ... SELECT ...&apos; or initialized separately. Another command like &apos;update index tab_idx partition ..&apos; can be provided.&lt;/p&gt;

&lt;p&gt;how to create index:&lt;br/&gt;
index can be created using the following hive command augmented with &apos;offset&apos;&lt;br/&gt;
&apos;select col_1...col_k, offset from tab1&apos;&lt;/p&gt;

&lt;p&gt;offset can be provided as built in function which can be derived in HiveInputRecordReader which will in turn use the specific FileFormat&apos;s Reader getPos() method and the &apos;map.input.file&apos; for the file name (or from the tableDesc or partiionDesc).&lt;/p&gt;

&lt;p&gt;Algorithm For using index:&lt;br/&gt;
1) Hive QL needs to determine whether a particular query can use any existing indexes. This can be determined by examining the predicate tree. After predicate pushdown, all those predicates which can use index are in the child operator of a TableScanOperator. This predicate tree needs to be examined. If this contains any subset of columns of an index then that index can be used. Until stats are available, it is not possible to guess whether using index is beneficial. This needs to be fleshed out more to check both &apos;AND&apos; and &apos;OR&apos; predicates.&lt;/p&gt;

&lt;p&gt;2) For each of the qualified indexes, a map/reduce job can be created using the predicates determined in step 1. The output of this job should have the following information&lt;br/&gt;
file_name   &amp;#8211; fully qualified file name that contains the data&lt;br/&gt;
byte_offset &amp;#8211; position of row&lt;/p&gt;

&lt;p&gt;3) If there is more than one qualified index then the outputs of step2 needs to be combined depending on whether the predicates on these indexes have &apos;AND&apos; or &apos;OR&apos; between them.&lt;/p&gt;

&lt;p&gt;4) Modify the original plan to use only those FileSplits that appear in the output of step3. This reduces the number of mappers spawned by JobTracker.&lt;/p&gt;

&lt;p&gt;5) Modify the original plan to use HiveIndexRecordReader instead of regular record reader. Output of step3 (which is sorted) is available to the HiveIndexRecordReader. It can skip to these locations instead of reading every record in the input of the Mapper.&lt;/p&gt;
</comment>
                            <comment id="12709900" author="prasadc" created="Fri, 15 May 2009 16:39:32 +0000"  >&lt;p&gt;Thanks Yongqiang!&lt;/p&gt;</comment>
                            <comment id="12710198" author="he yongqiang" created="Sun, 17 May 2009 13:59:19 +0000"  >&lt;p&gt;Thanks Prasad for detailed description of the index design. &lt;br/&gt;
Several questions:&lt;br/&gt;
(1) &lt;br/&gt;
Is the index based on sort? Single-column index based on sort can be very useful for query involving this column, both point query and range query. But for sort-based multi-column index, it can not be utilized for queries not containing the column used as primary sort order in the index. For example, we create an sort based index on table1(col1,col2,col3). The index uses col1 as primary sort order, col2 as secondary sort order, and col3...&lt;br/&gt;
We can use this index to accelerate queries like:&lt;br/&gt;
1) select * from table1 where col1&amp;gt;2 and col2&amp;lt;34  &lt;br/&gt;
2) select * from table1 where col1&amp;lt;34 and col3 &amp;gt;45&lt;br/&gt;
3) selcet * from table1 where col1&amp;gt;23&lt;br/&gt;
but, we can not use it for queries like:&lt;br/&gt;
4) select * from table1 where col2&amp;gt;34 and col3&amp;lt;3&lt;br/&gt;
5) select * from table1 where col2 =34&lt;br/&gt;
6) select * from table1 where col3 &amp;lt;45&lt;/p&gt;

&lt;p&gt;(2) &lt;br/&gt;
Should we consider using index to accelerate query involving join several tables. For example, we have two tables:&lt;br/&gt;
user(userid,name,address, age,title,company);&lt;br/&gt;
click(userid,url,datetime);&lt;br/&gt;
And now we have a query like:&lt;br/&gt;
select url  from user, click where user.userid=click.userid and user.name=&quot;user_name&quot; and datetime between last month;  to select the url list the specified user visits in last month. &lt;br/&gt;
If we have an index: create index user_url on table user(name), click(datetime) where user.userid=click.userid, then the above query can be accelerated.&lt;/p&gt;

&lt;p&gt;(3) &lt;br/&gt;
Index can also be used in Group-by aggregation queries. Should we also consider them?&lt;br/&gt;
(4)&lt;br/&gt;
Another feature is to integrate Lucene index with Hive. Ashish suggested to integrate katta. I took a look at katta, and i think it maybe not necessary to include katta in. If we include it, the hive user will have to deploy katta and zookeeper in their cluster. I think we can integrate lucene internally without touch katta.&lt;/p&gt;</comment>
                            <comment id="12710204" author="prasadc" created="Sun, 17 May 2009 15:39:47 +0000"  >&lt;p&gt;1)&lt;br/&gt;
The question you raised applies only to B+Tree indexes. The index that I defined above is not really a traditional database index but a kind of summary table (or view) and any lookup/range-query on table requires reading of the whole index. So you can apply all predicates as long as columns referenced in the predicates exist in the index. So we should be able use index on (col1, col2, col3) for all the queries above. Sorting order has no impact here since the whole index is read into memory anyways.&lt;/p&gt;

&lt;p&gt;Since this index can be created in sorted order, we can create sparse index (similar to non-leaf nodes of a B+-Tree) if the index itself is too big (ie, index sizes are order of magnitude larger than HDFS block size). But this can be done as a later optimization.  &lt;/p&gt;

&lt;p&gt;2)&lt;br/&gt;
With the design above, indexes on joins will come free since predicate pushdown will push the &apos;user.name=&quot;user_name&quot;&apos; to above the join and only index filtered rows participate in join.&lt;/p&gt;

&lt;p&gt;But creating indexes on the joined output may increase the index size so as to decrease the overall effectiveness. But with sparse indexes this problem might be mitigated so we can support this kind of join indexes along with support for sparse indexes.&lt;/p&gt;

&lt;p&gt;3)&lt;br/&gt;
Yes, for some aggregation queries it may make sense to read the index (since it is a summary table as well). Aggregations or any queries that involve only columns from the index can operate only on the index and not the main table.&lt;/p&gt;

&lt;p&gt;4) &lt;br/&gt;
I also looked at it and not sure how it fits into Hive. Katta is more like an distributed index server.&lt;/p&gt;</comment>
                            <comment id="12710248" author="he yongqiang" created="Mon, 18 May 2009 02:36:19 +0000"  >&lt;p&gt;Prasad,the index you designed is a kind of hash index? And it requires to scan the whole table for either lookup/range-query. &lt;br/&gt;
Sort based query will only need to scan part of the index table, but it also has problems as i posted in the previous comment.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Since this index can be created in sorted order, we can create sparse index&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;how? I think sort based index can not be sparse index, and only can be dense index.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;I also looked at it and not sure how it fits into Hive. Katta is more like an distributed index server.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;yes, we do not need to fit katta into Hive. But we can integrate lucene index into Hive. But right now i think we can put this on the last part of our schedule.&lt;/p&gt;</comment>
                            <comment id="12710249" author="he yongqiang" created="Mon, 18 May 2009 02:46:59 +0000"  >&lt;p&gt;Prasad, I think the hash based index is more like a projection, because number of rows with same hash value of indexed columns will be very low. &lt;br/&gt;
And in most cases, the block/pos list&apos;s size will only be 1.&lt;/p&gt;</comment>
                            <comment id="12710494" author="prasadc" created="Mon, 18 May 2009 21:12:49 +0000"  >&lt;p&gt;The above index is not a hash index since you can&apos;t do range queries on hash index and lookups are constant time. not sure what to call this except that it is a view (simple projection) of the base table with offsets into the base table.&lt;/p&gt;

&lt;p&gt;on sparse index, i meant you can create a sparse index on top of the index i described above. but this can be done later.&lt;/p&gt;

&lt;p&gt;&amp;gt; And in most cases, the block/pos list&apos;s size will only be 1&lt;/p&gt;

&lt;p&gt;that is not the case if the index is on a non-primary key column. and i think, mostly this is the case where indexes will be used in data warehouses.&lt;/p&gt;</comment>
                            <comment id="12710591" author="he yongqiang" created="Tue, 19 May 2009 02:44:04 +0000"  >&lt;p&gt;&amp;gt;that is not the case if the index is on a non-primary key column. and i think, mostly this is the case where indexes will be used in data warehouses.&lt;br/&gt;
Yes. If the index is built on one column, the block/pos list&apos;s size will be large. But if it is built on many columns, i think the block/pos list&apos;s size will be small.&lt;br/&gt;
Anyway, we can build this index as the first step.&lt;br/&gt;
And after this finished, we can try other kinds of index, like:&lt;br/&gt;
1) sort based index&lt;br/&gt;
2) lucene index&lt;br/&gt;
3) block-scope B+Tree or R-tree or other advantage index data structures.&lt;/p&gt;

&lt;p&gt;Prasad, you said you already wrote some code, would you please attach it?&lt;/p&gt;</comment>
                            <comment id="12710601" author="he yongqiang" created="Tue, 19 May 2009 03:39:45 +0000"  >&lt;p&gt;Forgot one, Bitmap. I did some tests on Bitmap, it&apos;s performance is excellent.&lt;/p&gt;</comment>
                            <comment id="12710741" author="prasadc" created="Tue, 19 May 2009 14:25:38 +0000"  >&lt;p&gt;Yes, mostly the block/pos size will be small but I don&apos;t think we can assume that since there will be enough cases where it will not be true. &lt;/p&gt;

&lt;p&gt;We explore the other approaches later on. different indexes will be useful in different scenarios. &lt;/p&gt;

&lt;p&gt;i will try to post some code this week. &lt;/p&gt;</comment>
                            <comment id="12712342" author="prasadc" created="Sat, 23 May 2009 02:23:12 +0000"  >&lt;p&gt;patch built from hive prototype code. it compiles with hive code but needs lot of work before it can fit nicely with hive. &lt;/p&gt;

&lt;p&gt;Yongqing, the code is hard to understand in the context of open source hive code but makes sense from prototype code. let me know if you have any question. you might want to use this as a guide and start from scratch hive2 indexing code.&lt;/p&gt;</comment>
                            <comment id="12712352" author="he yongqiang" created="Sat, 23 May 2009 04:46:30 +0000"  >&lt;p&gt;Thanks a lot, Prasad. I will put questions on the jira. and will start working on it after we set the design. Looking forward to working on it. &lt;/p&gt;</comment>
                            <comment id="12713003" author="he yongqiang" created="Tue, 26 May 2009 13:19:03 +0000"  >&lt;p&gt;Checked how Mysql does with index and found mysql either can not use index to handle situations in my earlier post:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;but, we can not use it for queries like:&lt;br/&gt;
4) select * from table1 where col2&amp;gt;34 and col3&amp;lt;3&lt;br/&gt;
5) select * from table1 where col2 =34&lt;br/&gt;
6) select * from table1 where col3 &amp;lt;45 &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;And now a basic idea for our index design, just like Prasad commented in previous post:&lt;br/&gt;
1) index structure&lt;br/&gt;
use a mr job to create index, input is a file with all columns, and mapper output kv pairs, where key is &amp;lt;indexed col1, indexed col2,...&amp;gt; offset.&lt;br/&gt;
And we define a comparator for &amp;lt;indexed col1, indexed col2,...&amp;gt; to letting the shuffle phase sort all mappers&apos; output. And in reducer, we combine kv-pairs to &lt;br/&gt;
&amp;lt;indexed col1, indexed col2,...&amp;gt; list_of_offsets&lt;br/&gt;
This is a dense sorted index, then we create a sparse index on the dense index. And we also collect column data distribution informations (histogram) while doing this.&lt;br/&gt;
2)&lt;br/&gt;
we consider using index for a query only when the query involves the columns of leftmost part of the index. &lt;br/&gt;
And also need to consider index merge when involves two indexes, and a cost estimation to consider whether using index will decrease query time (this is the work need to do in the optimizer).&lt;/p&gt;

&lt;p&gt;But as first step, we can first finish part 1 and hive ql part. Then consider part two(optimizer part). After part1 finished, i will examine part2 in more detail.&lt;/p&gt;</comment>
                            <comment id="12714306" author="prasadc" created="Fri, 29 May 2009 05:40:16 +0000"  >&lt;p&gt;the plan looks good. i am not sure we need to create sparse index on the dense index in phase 1. In most cases the size of dense index will be small enough so that additional mr job for processing the sparse index will become unnecessary. if sparse index is not necessary then there is not need for the dense index be sorted.&lt;/p&gt;

&lt;p&gt;since the dense index is scanned completely while processing the query, we can use the index if any predicate column exists in index definition.&lt;/p&gt;</comment>
                            <comment id="12714492" author="seymourz" created="Fri, 29 May 2009 15:52:44 +0000"  >&lt;p&gt;Hello Prasad and Yongqiang, Thank you very much for this great effort. &lt;/p&gt;

&lt;p&gt;One of my suggestions would be that, since we&apos;ve done indexing with Mapreduce, and for some queries based on the generated indexes, can we just omit the time-consuming Mapreduce phase during the querying period, as we&apos;ve already got all of the files/offsets and we can go to these specific file offsets directly to get relevant rows of the table? This would greatly expedite the query process.&lt;/p&gt;

&lt;p&gt;This would be helpful for the following case in one of my usages with Hive. With Hive, I&apos;ve already sharded (by date), and bucketed (by cols hashing) of my log data into a hierachical files. Also I&apos;ve sorted each file with the hashing cols. As I may have many rows with same column values but different timestamps, to minimize index size, I&apos;d like to treat these rows of same col values as a block and only use a single index entry for this block. This will grealy reduce the index size of my data, but still very useful in my query request with those cols.&lt;/p&gt;</comment>
                            <comment id="12714730" author="he yongqiang" created="Sat, 30 May 2009 15:22:10 +0000"  >&lt;p&gt;Thanks for the suggestions, Seymour.&lt;br/&gt;
I have also thought what your said, directly fetch the data instead of initilize a new mr job. I will try include this, but it may be done in the second phase(the optimize phase).&lt;/p&gt;

&lt;p&gt;&amp;gt;&amp;gt;I&apos;d like to treat these rows of same col values as a block and only use a single index entry for this block&lt;br/&gt;
in the design, we indeed only use one index entry. And not only for contineous values, we use the same index entry for all rows with the same col value.&lt;/p&gt;</comment>
                            <comment id="12715153" author="jsensarma" created="Mon, 1 Jun 2009 17:16:46 +0000"  >&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;are we going to have one index file per hdfs file? (or one per partition?)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;related question is how this is going to interact with sampling? (i think currently the sampling predicate is optimized out for bucketed tables - although not terribly sure).&lt;/p&gt;

&lt;p&gt;i would love to see the api to invoke the index. &lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;ideally we would like to plug in different indexing schemes - as well with map-side joins - the hashmap storing the smaller table can be seen as an index on this table. It would seem that one should be able to replace a map-side join based on tables loaded into jdbm with tables with indices proposed here (and thereby do joins based on indices almost trivially).&lt;/li&gt;
	&lt;li&gt;we should enable people to be able to plug in their own indices (since it&apos;s quite likely that over time there will be multiple indexing efforts on hadoop files).&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12715345" author="he yongqiang" created="Tue, 2 Jun 2009 02:41:38 +0000"  >&lt;p&gt;Joydeep, Thanks for the concern.&lt;br/&gt;
&amp;gt;&amp;gt;are we going to have one index file per hdfs file?&lt;br/&gt;
yeah.&lt;br/&gt;
&amp;gt;&amp;gt;i would love to see the api to invoke the index&lt;br/&gt;
currently it is not settle down. I will try to give it in the next week.&lt;br/&gt;
&amp;gt;&amp;gt;enable people to be able to plug in their own indices&lt;br/&gt;
I think if we have a well-designed adaptable api, then this can be addressed.&lt;br/&gt;
&amp;gt;&amp;gt;we would like to plug in different indexing schemes&lt;br/&gt;
yes. i have proposed several schemes in previous posts. Can you give me some schemes, so i can compare and make a better design.&lt;/p&gt;

&lt;p&gt;BTW, I will try to write a proposal in next week. I have an important english exam this weekend. Sorry for the delay.&lt;/p&gt;</comment>
                            <comment id="12715539" author="seymourz" created="Tue, 2 Jun 2009 14:26:03 +0000"  >
&lt;p&gt;&amp;gt;&amp;gt;are we going to have one index file per hdfs file?&lt;/p&gt;

&lt;p&gt;Can we also support exporting these index files as a table to some other storage system like HBase or Tokyou Cabinet,  i.e. these seperate index files for each HDFS file, can be expressed as a single table in Hive?&lt;/p&gt;</comment>
                            <comment id="12722722" author="schubertzhang" created="Mon, 22 Jun 2009 18:01:18 +0000"  >&lt;p&gt;Prasad,&lt;/p&gt;

&lt;p&gt;About yout comments at 17/May/09 08:39 AM.&lt;br/&gt;
&amp;gt;&amp;gt; ... So we should be able use index on (col1, col2, col3) for all the queries above. Sorting order has no impact here since the whole index is read into memory anyways. &lt;/p&gt;

&lt;p&gt;If the size of index is big out of memory size, how to read whole index into memory?&lt;/p&gt;</comment>
                            <comment id="12722773" author="prasadc" created="Mon, 22 Jun 2009 19:27:59 +0000"  >&lt;p&gt;Schubert,&lt;/p&gt;

&lt;p&gt;We can run another map-reduce job that scans the index and builds out the results file sorted by the index key. This file can be read sequentially and determine which input table HDFS blocks to be fed to the actual job for the query.&lt;/p&gt;

&lt;p&gt;Another way is to build a sparse index on the index. But if the table itself is sorted, we can build the sparse index (ala MapFile) directly and use it. @Facebook, the usecase we have doesn&apos;t have this sorting property but I can envision this being useful for primary indexes where the index sort order and the table sort order are same.&lt;/p&gt;

&lt;p&gt;Can you think of any other ways? Ofcourse, we can process index files using HBase or TokyoCabinet but that requires another system to be setup and administered and both systems need to be available for index processing. But in some cases these solutions also work. The indexing scheme described above should play well with Hbase and TokyoCabinet since index is a file with rows containg a key and position parameters. In Hadoop we can stored that in SequenceFile or may be TFile but if they have to be stored in external systems, we can plug-in a custom SerDe and change the default location of these two a location where the external systems can access these files.&lt;/p&gt;</comment>
                            <comment id="12725103" author="schubertzhang" created="Mon, 29 Jun 2009 09:55:32 +0000"  >&lt;p&gt;Prasad,&lt;/p&gt;

&lt;p&gt;Thanks for your comments. Now, I understand your comments.&lt;/p&gt;

&lt;p&gt;Yes, in one of our projects, we sorted the data table and build sparse index which record the block keys and file offsets. Then, we load the index files into HBase to service for query. I works fine.&lt;/p&gt;
</comment>
                            <comment id="12728605" author="he yongqiang" created="Wed, 8 Jul 2009 10:29:13 +0000"  >&lt;p&gt;Had a talk with Prasad and Zheng about the index design. &lt;br/&gt;
And i write a summary of the meeting: &lt;a href=&quot;http://docs.google.com/View?id=dc9jpfdr_6dkrt82c7&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://docs.google.com/View?id=dc9jpfdr_6dkrt82c7&lt;/a&gt;&lt;br/&gt;
Any suggestion is appreciated!&lt;br/&gt;
BTW, In this meeting and the doc, we did not include any other index types. And i think we first implement one index type, and after that, others will be more clear.&lt;/p&gt;</comment>
                            <comment id="12728900" author="prasadc" created="Wed, 8 Jul 2009 20:56:51 +0000"  >&lt;p&gt;One thing that isn&apos;t mentioned is that in List&amp;lt;bucketname, List&amp;lt;offset&amp;gt;&amp;gt; column, offsets are sorted. &lt;/p&gt;

&lt;p&gt;Another thing missing is, when an index on a partition is built then a new partition will be created for that index table (similar to that of creating a partition for a regular table).&lt;/p&gt;

&lt;p&gt;We can distinguish index tables and regular tables by having a table parameter.&lt;/p&gt;

&lt;p&gt;We can skip partition specific indexes in the first phase if it reduces amount of work and assume indexes defined on a table can be created on all partitions.&lt;/p&gt;</comment>
                            <comment id="12732896" author="he yongqiang" created="Sat, 18 Jul 2009 13:33:13 +0000"  >&lt;p&gt;A Draft version of buiding(create/update) index.&lt;br/&gt;
Still needs: &lt;br/&gt;
1) reducer sorts a set positions of one key in memory, which may cause out of memory when a index key occures too many times &lt;br/&gt;
2) the index output format can only use IgoreKeyTextOutputFormat right now.&lt;/p&gt;</comment>
                            <comment id="12734203" author="prasadc" created="Wed, 22 Jul 2009 17:25:02 +0000"  >&lt;p&gt;1) Are you worried about the sort phase of the reducer or the IndexBuilder&apos;s reducer code? I don&apos;t think former issue will be a problem. The later issue can be avoided by writing multiple rows for a key if the number of offsets exceed a certain limit. So reducer can flush the offsets periodically to disk thus avoiding OutOfMemory exceptions in reducer.&lt;/p&gt;

&lt;p&gt;2) What are the other options for the index output format?&lt;/p&gt;</comment>
                            <comment id="12734405" author="he yongqiang" created="Thu, 23 Jul 2009 00:23:44 +0000"  >&lt;p&gt;1) For a given key, we are using a sorted set for each bucket to store positions at the reduer. I am worried that &quot;one sorted set for each bucket&quot; may cause out of memory problem.&lt;br/&gt;
as you commentted earlier: &quot;List&amp;lt;bucketname, List&amp;lt;offset&amp;gt;&amp;gt; column, offsets are sorted&quot;. &lt;br/&gt;
Think about one extreme situation: one file contains a single value million times. So at the reducer we are storing million positions in a sorted set. &lt;/p&gt;

&lt;p&gt;&amp;gt;&amp;gt;So reducer can flush the offsets periodically to disk thus avoiding OutOfMemory exceptions in reducer. &lt;br/&gt;
If we do this, how we can guarantee they are sorted. I mean offsets after this flush are greater than offsets in previous flush.&lt;/p&gt;

&lt;p&gt;2)What are the other options for the index output format?&lt;br/&gt;
I think there is no other options. We need to discard the key part. And i think in hive only IgnoreKeyTextOutputFormat does that. And Of course all hive&apos;s custom HiveOutputFormat can discard key part, but they can not be specified in the map-reduce jobconf, since they do not extend OutputFormat.&lt;/p&gt;</comment>
                            <comment id="12734408" author="prasadc" created="Thu, 23 Jul 2009 00:30:45 +0000"  >&lt;p&gt;well the number of offsets can&apos;t exceed number of SequenceFile blocks since we can only index the SequenceFile block offsets. So the problem is not as dire as it can be. And also if there are that many (i.e. more than 10% of rows in traditional RDBMS but may more in Hadoop case) have same key then index may not be efficient after all since it is better to read the whole table anyways.&lt;/p&gt;</comment>
                            <comment id="12734419" author="prasadc" created="Thu, 23 Jul 2009 01:44:02 +0000"  >&lt;p&gt;what i am trying to say is for such frequent keys indexing may not be of much help so may be we can relax &apos;sort&apos; property? i don&apos;t think there is another easy way out other than do a disk based sort. check you can reuse any of the hadoop sorting code. Or can we piggyback this sorting on top of hadoop reduce sort phase some how?&lt;/p&gt;</comment>
                            <comment id="12734437" author="prasadc" created="Thu, 23 Jul 2009 03:05:26 +0000"  >&lt;p&gt;i am not able to apply the patch to latest trunk.&lt;/p&gt;</comment>
                            <comment id="12734440" author="he yongqiang" created="Thu, 23 Jul 2009 03:15:41 +0000"  >&lt;p&gt;I am about to update it. And making some modifications according to our offline talk.&lt;br/&gt;
And i need to add some testcases and do more tests. &lt;br/&gt;
So i will submit a new patch by this weekend, and will notify you any updates.&lt;/p&gt;</comment>
                            <comment id="12734473" author="he yongqiang" created="Thu, 23 Jul 2009 05:59:53 +0000"  >&lt;p&gt;This jira page is a bit too long to follow.&lt;br/&gt;
With discussions with Prasad and Zheng, will open several sub-tasks for index-building, generalize index interface to allow plugin different index strategies, and optimize queries with index  etc.&lt;/p&gt;

&lt;p&gt;Let&apos;s keep discussions involved overall indexing here in this jira page, and move other discussions to separate jira pages.&lt;/p&gt;</comment>
                            <comment id="12735271" author="he yongqiang" created="Sat, 25 Jul 2009 13:19:30 +0000"  >&lt;p&gt;Created &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-678&quot; title=&quot;Add support for building index table&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-678&quot;&gt;&lt;del&gt;HIVE-678&lt;/del&gt;&lt;/a&gt; for add support for building index.&lt;br/&gt;
see &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-678&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HIVE-678&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="12758067" author="hammer" created="Tue, 22 Sep 2009 00:02:40 +0000"  >&lt;p&gt;Another type of index worth knowing about: the &quot;negative index&quot;/&quot;storage index&quot; from Exadata, described at &lt;a href=&quot;http://blogs.oracle.com/datawarehousing/2009/09/500gbsec_and_database_machine.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://blogs.oracle.com/datawarehousing/2009/09/500gbsec_and_database_machine.html&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We get some &quot;negative indexing&quot; for free with partitions, but this may be useful for more &quot;distinctive&quot; scans over columns for which we have not partitioned.&lt;/p&gt;</comment>
                            <comment id="12758084" author="prasadc" created="Tue, 22 Sep 2009 00:27:57 +0000"  >&lt;p&gt;@jeff, i think this is more suitable for storing it along with data where blocks of data can skipped while scanning rows. i think columnar storage might already be doing this. &lt;/p&gt;</comment>
                            <comment id="12758094" author="hammer" created="Tue, 22 Sep 2009 01:35:52 +0000"  >&lt;p&gt;Yeah, I think so as well. Did my comment make it seem like I thought otherwise?&lt;/p&gt;</comment>
                            <comment id="12758097" author="prasadc" created="Tue, 22 Sep 2009 01:43:55 +0000"  >&lt;p&gt;there can be a summary index here as well (every SequenceFile block will have min &amp;amp; max column values in the index). thought you are hinting at that.&lt;/p&gt;</comment>
                            <comment id="12758116" author="jsensarma" created="Tue, 22 Sep 2009 04:18:01 +0000"  >&lt;p&gt;are there any references on this technique?&lt;/p&gt;

&lt;p&gt;someone had earlier suggested this (apparently from reading Netezza documentation) - but i don&apos;t understand when it would work. why would a (fairly large) sequencefile block only limited range of values (assuming the metadata stores a min-max range). most cases i can imagine in our dataset would either have low cardinality columns (so most values would be present) or for large cardinality ones - the distribution would be random (relative to the primary sort key) - and the range would seem ineffective.&lt;/p&gt;

&lt;p&gt;unless there are columns that are closely related to the how data is sorted/partitioned (perhaps some product ids are limited to specific range of time - but the partitioning is on time and not product id - and even that sounds dubious).&lt;/p&gt;

&lt;p&gt;a bloom filter would seem much more plausible at allowing good filtering. even then don&apos;t understand why this sort of metadata should be kept along with the block and not separately (much more flexible - can be added on demand) as this jira is headed towards.&lt;/p&gt;</comment>
                            <comment id="12758127" author="prasadc" created="Tue, 22 Sep 2009 05:14:44 +0000"  >&lt;p&gt;i don&apos;t think it makes much sense unless there is some clustering or sorting property. if there is clustering and sorting and the selectivity of a query is much higher than 10% then storing this metadata along with data makes sense instead of a separate block. the 10% threshold may be larger for Hive but the point still stands. in OLAP case data is change seldom and the size of this kind of metadata is much smaller than the data itself so the overhead of storing this data is negligible.&lt;/p&gt;

&lt;p&gt;something similar to this is done in DB2 Multi-Dimensional Clustering where whole blocks (disk blocks) are skipped if the key value doesn&apos;t fit the query.&lt;/p&gt;</comment>
                            <comment id="12758135" author="jsensarma" created="Tue, 22 Sep 2009 06:06:07 +0000"  >&lt;p&gt;MDC also maintains metadata separately - at least based on their paper (&lt;a href=&quot;http://www.research.ibm.com/compsci/project_spotlight/datamgmt/SIGMOD2003.pdf&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.research.ibm.com/compsci/project_spotlight/datamgmt/SIGMOD2003.pdf&lt;/a&gt;)&lt;/p&gt;</comment>
                            <comment id="12758139" author="prasadc" created="Tue, 22 Sep 2009 06:12:07 +0000"  >&lt;p&gt;yes they do but they don&apos;t use for table scans which are done if the query selectivity is greater than 10% (or some such). they use the index for index scans and in joins. I wrote the table scan code &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="12758556" author="schubertzhang" created="Wed, 23 Sep 2009 03:01:09 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Prasad Chakka added a comment - 15/Apr/09 11:25 AM&lt;br/&gt;
Another way of doing it is to create a file format that contains index along with data... but i think that would take lot more time. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We are trying to store data in sorted and block-indexed files (such as HFile or TFile). Then I think we can know the startKey and lastKey of each file and each block. This block index(block summary) is just for primary key. &lt;/p&gt;</comment>
                            <comment id="12794263" author="appodictic" created="Wed, 23 Dec 2009 23:14:16 +0000"  >&lt;p&gt; I currently am benching an 11 node hive cluster against a 16 TB MySQL system 4x quad core 32 GB RAM 5.1 with partitioning.&lt;/p&gt;

&lt;p&gt;Hive destroys mysql with any query like:&lt;/p&gt;

&lt;p&gt;(date_id is my partition column.)&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;set mapred.map.tasks=34;
set mapred.reduce.tasks=11;
FROM pageviews
insert overwrite directory &apos;/user/ecapriolo/hivetest4&apos;
select sitename_id, user_id, count(user_id)   WHERE date_id=20091250 group by sitename_id,user_id
12098855 Rows loaded to /user/ecapriolo/hivetest4
OK
Time taken: 185.528 seconds
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The same query can take over 3000 seconds on MySQL because these large summary queries are always written to a temp table and then writes bottleneck your read queries.&lt;/p&gt;

&lt;p&gt;However, if mysql has an index (and if the index is in memory, which is hard in a warehouse) on some other value in the where clause like:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;select sitename_id, user_id, count(user_id)   WHERE date_id=20091250 and sitename_id=400 group by sitename_id,user_id 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;MySQL gets a relative performance speed-up, while hive ends up scanning the entire table.&lt;/p&gt;

&lt;p&gt;I agree with dhruba,&lt;br/&gt;
&amp;gt;&amp;gt;This sounds really awesome! Make hadoop-hive suitable for things other than brute force table-scans! &lt;/p&gt;

&lt;p&gt;If we had indexes helping stop some brute force scans, that would just open up other doors to what hive could do.&lt;/p&gt;</comment>
                            <comment id="12835569" author="he yongqiang" created="Fri, 19 Feb 2010 03:48:41 +0000"  >&lt;p&gt;Got talked with Prasad about this issue today.&lt;br/&gt;
I may not able to finish this in the coming one or two months. I am now spending most of my time working on some other issues. I am sorry about that.&lt;br/&gt;
If anyone want this feature in, please feel free to take over from me. And i will provide all help that i can.  If no one picked up, i can finish it after finishing issues at hand.&lt;br/&gt;
Thanks.&lt;/p&gt;</comment>
                            <comment id="12876676" author="prafulla" created="Tue, 8 Jun 2010 13:29:52 +0000"  >&lt;p&gt;He Yongqiang ,&lt;br/&gt;
Have you started working on this one ?&lt;br/&gt;
If not, I was interested in taking a look at it.&lt;br/&gt;
Patch link hive- 417&#65293;2009-07-18.patch is not working, can you share latest patch here ?&lt;/p&gt;</comment>
                            <comment id="12876717" author="he yongqiang" created="Tue, 8 Jun 2010 16:43:52 +0000"  >&lt;p&gt;Cool. Yes. i do have a latest patch for this jira. I will cleanup it and post. &lt;/p&gt;</comment>
                            <comment id="12876822" author="he yongqiang" created="Tue, 8 Jun 2010 21:48:26 +0000"  >&lt;p&gt;With this patch, the index can work. but it is not so intelligent. &lt;/p&gt;

&lt;p&gt;This is how this patch works:&lt;/p&gt;

&lt;p&gt;=== how to create the index table and generate index data ===&lt;br/&gt;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;&lt;/p&gt;

&lt;p&gt;drop table src_rc_index;&lt;/p&gt;

&lt;p&gt;//create an index table on table src_rc, and the index col is key. &lt;br/&gt;
//And the index table&apos;s data is stored using textfile (also work with seq, rcfile)&lt;br/&gt;
create index src_rc_index type compact on table src_rc(key) stored as textfile; &lt;/p&gt;

&lt;p&gt;hive&amp;gt; show table extended like src_rc_index;&lt;br/&gt;
tableName:src_rc_index&lt;br/&gt;
owner:heyongqiang&lt;br/&gt;
location:&lt;a href=&quot;file:/user/hive/warehouse/src_rc_index&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;file:/user/hive/warehouse/src_rc_index&lt;/a&gt;&lt;br/&gt;
inputformat:org.apache.hadoop.mapred.TextInputFormat&lt;br/&gt;
outputformat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&lt;br/&gt;
columns:struct columns &lt;/p&gt;
{ i32 key, string _bucketname, list&amp;lt;string&amp;gt; _offsets}

&lt;p&gt;About the index table&apos;s schema. besides the index columns from the base table, the index table has two more columns (_bucketname string, array(string) offsets )&lt;/p&gt;


&lt;p&gt;//generate the actuall index table&apos;s data (here also support partition)&lt;br/&gt;
update index src_rc_index;&lt;/p&gt;

&lt;p&gt;====How to use the index table====&lt;/p&gt;

&lt;p&gt;//find the offset for &apos;key=0&apos; in the index table, and put the bucketname and offset list in a temp directory&lt;br/&gt;
insert overwrite directory &quot;/tmp/index_result&quot; select `_bucketname` ,  `_offsets` from src_rc_index where key=0;&lt;/p&gt;

&lt;p&gt;set hive.exec.index_file=/tmp/index_result; &lt;/p&gt;

&lt;p&gt;//use a new index file format to prune inputsplit based on the offset list &lt;br/&gt;
//stored in &quot;hive.exec.index_file&quot; which is populated in previous command&lt;br/&gt;
set hive.input.format=org.apache.hadoop.hive.ql.index.io.HiveIndexInputFormat;&lt;/p&gt;

&lt;p&gt;//this query will not scan the whole base data&lt;br/&gt;
select key, value from src_rc where key=0;&lt;/p&gt;


&lt;p&gt;Things done in the patch:&lt;br/&gt;
1) hql command for creating index table&lt;br/&gt;
2) hql command and map-reduce job for updating index (generating the index table&apos;s data). &lt;br/&gt;
3) a HiveIndexInputFormat to leverage the offsets got from index table to reduce number of blocks/map-tasks&lt;/p&gt;

&lt;p&gt;Things need to be done:&lt;br/&gt;
1) right now the index table is manually specified in queries. we need this to be more intelligent by automatically generating the plan using index .&lt;br/&gt;
2) The HiveIndexInputFormat needs a new RecordReader to seek to a given offset instead of scanning the whole block. &lt;br/&gt;
3) right now we use a map-reduce job to scan the whole index table to find hits offsets. But since the index table is sorted, we can leverage the sort property to avoid the map-reduce job in many cases. (easiest way is to do a binary search in client.)&lt;/p&gt;

&lt;p&gt;The first todo is the most important part.  I think the third may need much more work (maybe not true).&lt;/p&gt;

&lt;p&gt;(Note: although this patch has been tested in production cluster, it could still have bugs. We will be really appreciate if you can report bugs you find here.)&lt;/p&gt;</comment>
                            <comment id="12876827" author="he yongqiang" created="Tue, 8 Jun 2010 22:00:40 +0000"  >&lt;p&gt;I forgot to add this line &quot;set hive.exec.compress.output=false;&quot; in the above snippet before selecting from the index table.&lt;/p&gt;</comment>
                            <comment id="12877049" author="prafulla" created="Wed, 9 Jun 2010 11:39:45 +0000"  >&lt;p&gt;I was thinking of adding something called query rewrite module.&lt;br/&gt;
It would be rule-based query rewrite system and it would &lt;br/&gt;
rewrite the query into semantically equivalent query which is &lt;br/&gt;
more optimized and/or uses indexes (not just for scans, but&lt;br/&gt;
for other query operators, e.g. GroupBy etc.)&lt;/p&gt;

&lt;p&gt;Eg.&lt;/p&gt;

&lt;p&gt;select distinct c1&lt;br/&gt;
from t1;&lt;/p&gt;

&lt;p&gt;This query, if we have densed index (&apos;compact summary index&apos; in this&lt;br/&gt;
hive indexing patch) on c1 can be replaced with query on index table &lt;br/&gt;
itself.&lt;/p&gt;

&lt;p&gt;select idx_key&lt;br/&gt;
from t1_cmpct_sum_idx;&lt;/p&gt;

&lt;p&gt;Similar query transformation can happen for other queries.&lt;/p&gt;

&lt;p&gt;Module will be placed just before optimizer and will help optimizer.&lt;br/&gt;
Module structure looks like below.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;Query parser&amp;#93;&lt;/span&gt;&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;Query rewrites&amp;#93;&lt;/span&gt; --&amp;gt; new phase&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;Query optimization&amp;#93;&lt;/span&gt;&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;Query execution planner&amp;#93;&lt;/span&gt;&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;Query execution engine&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The rewrite module is &apos;generic&apos;, not just for above indexing case,&lt;br/&gt;
but for other cases too, e.g. OR predicates to union (for efficiency?), outer join&lt;br/&gt;
to union of anti &amp;amp; semi joins, moving out &apos;order by&apos; out of union&lt;br/&gt;
subquery etc etc.&lt;/p&gt;

&lt;p&gt;The aim is to implement a very simple, light-weight rewrite support,&lt;br/&gt;
implement the indexing related rewrites (above rewrite does not&lt;br/&gt;
even need a new run-time map-red operator) and integrate indexing&lt;br/&gt;
support quickly and cleanly. As noted above, this rewrite phase&lt;br/&gt;
is rule-based (and not cost-based), sort of early optimization.&lt;/p&gt;

&lt;p&gt;Let me know what u think. I&apos;ll start with reading ur patch.&lt;br/&gt;
This would do most part from TODO 1, &lt;br/&gt;
TODO 2 and 3 will have to be looked into. &lt;/p&gt;</comment>
                            <comment id="12877144" author="he yongqiang" created="Wed, 9 Jun 2010 17:20:11 +0000"  >&lt;p&gt;Plan sounds perfectly good to me!&lt;/p&gt;</comment>
                            <comment id="12877236" author="athusoo" created="Wed, 9 Jun 2010 22:21:31 +0000"  >&lt;p&gt;A couple of comments on this:&lt;/p&gt;

&lt;p&gt;A complication that happens by doing a rewrite just after parse is that you loose the ability to report back errors that correspond to the original query. Also the &lt;br/&gt;
metadata that you need to do the rewrite is only available after phase 1 of semantic analysis. So in my opinion the rewrite should be done after semantic analysis but before plan generation. Is that what you had in mind...&lt;/p&gt;

&lt;p&gt;so something like...&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;Query parser&amp;#93;&lt;/span&gt;&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;Query semantic analysis&amp;#93;&lt;/span&gt;&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;Query optimization&amp;#93;&lt;/span&gt;&lt;br/&gt;
...&lt;/p&gt;</comment>
                            <comment id="12877295" author="prafulla" created="Thu, 10 Jun 2010 02:45:17 +0000"  >&lt;p&gt;Yes Ashish,&lt;br/&gt;
Thats what I had in mind.&lt;/p&gt;

&lt;p&gt;Rewrite system would need metadata, and hence it should be invoked &lt;br/&gt;
after semantic analysis phase which would make metadata available.&lt;/p&gt;</comment>
                            <comment id="12880611" author="prafulla" created="Sun, 20 Jun 2010 09:37:50 +0000"  >&lt;p&gt;Hi All,&lt;br/&gt;
Here are first set of changes about query rewrite module in Hive and&lt;br/&gt;
intial rewrite rule to transform groupby and distinct queries so that&lt;br/&gt;
they make use of indexes.&lt;br/&gt;
Note that patch contains createIndex* related changes from Yongqiang&lt;br/&gt;
Here are the files that I have touched for rewrite related changes&lt;br/&gt;
ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java&lt;br/&gt;
&amp;#8211; hasIndex and getIndex related APIs&lt;br/&gt;
ql/src/java/org/apache/hadoop/hive/ql/parse/QB.java&lt;br/&gt;
ql/src/java/org/apache/hadoop/hive/ql/parse/QBParseInfo.java&lt;br/&gt;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java&lt;br/&gt;
ql/src/java/org/apache/hadoop/hive/ql/rewrite/HiveRewriteEngine.java&lt;br/&gt;
&amp;#8211; Rewrite engine infrastructure&lt;br/&gt;
ql/src/java/org/apache/hadoop/hive/ql/rewrite/rules/GbToCompactSumIdxRewrite.java&lt;br/&gt;
&amp;#8211; Actual rewrite rule which does above mentioned optimization&lt;br/&gt;
ql/src/java/org/apache/hadoop/hive/ql/rewrite/rules/HiveRwRule.java&lt;br/&gt;
ql/src/java/org/apache/hadoop/hive/ql/rewrite/rules/HiveRwRuleContext.java&lt;br/&gt;
ql/src/test/queries/clientpositive/ql_rewrite_gbtoidx.q&lt;br/&gt;
&amp;#8211; Unit test including various queries.&lt;br/&gt;
ql/src/test/results/clientpositive/ql_rewrite_gbtoidx.q.out&lt;/p&gt;

&lt;p&gt;I&apos;ve tested some queries end-to-end with this GbToIdx rewrite rule on.&lt;br/&gt;
It is working well.&lt;/p&gt;

&lt;p&gt;Rewrite changes can be disabled with hive.ql.rewrite boolean flag &lt;br/&gt;
and this perticular rewrite can be disabled/enabled based on &lt;br/&gt;
hive.ql.rewrite.gbtoidx boolean flag.&lt;br/&gt;
Default value for hive.ql.rewrite is true&lt;br/&gt;
and that for hive.ql.rewrite.gbtoidx is false.&lt;/p&gt;

&lt;p&gt;There are currently following limitations (which are being worked upon)&lt;br/&gt;
1. Rewrite engine does not invoke rewrite in recursive manner.&lt;br/&gt;
   It currently just rewrites topLevel QB.&lt;br/&gt;
2. GbToIdx rewrite is disabled for all queries having &quot;where clause&quot; , we&lt;br/&gt;
   can support certain &quot;where clauses&quot; in which all colrefs are index key &lt;br/&gt;
   columns.&lt;br/&gt;
3. We need some API to know if indexes are up-to-date or not. We need to&lt;br/&gt;
   do this rewrite only when index has up-to-date data.&lt;/p&gt;

&lt;p&gt;Let me know your review comments.&lt;br/&gt;
I am using fork of apache-git repository for development on github.&lt;br/&gt;
All these changes are also available on &lt;a href=&quot;http://github.com/prafullat/hive&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://github.com/prafullat/hive&lt;/a&gt;   &lt;/p&gt;</comment>
                            <comment id="12884067" author="namit" created="Wed, 30 Jun 2010 21:27:57 +0000"  >&lt;p&gt;Looking at the patch (not yet in detail) seems to suggest the following:&lt;/p&gt;

&lt;p&gt;1. The index file can only be a text file.&lt;br/&gt;
2. PROJECTION index is not used - I mean, to start, can we just get the basic COMPACT+SUMMARY and only support that.&lt;/p&gt;</comment>
                            <comment id="12884110" author="namit" created="Thu, 1 Jul 2010 00:51:45 +0000"  >&lt;p&gt;DDLSemanticAnalyzer.java&lt;/p&gt;

&lt;p&gt;    if (outputFormat == null) &lt;/p&gt;
{
      outputFormat = RCFileOutputFormat.class;
    }


&lt;p&gt;use the default - dont hardcode.&lt;/p&gt;</comment>
                            <comment id="12884404" author="namit" created="Thu, 1 Jul 2010 20:24:52 +0000"  >&lt;p&gt;Few higher level comments:&lt;/p&gt;

&lt;p&gt;1. Populate the index at create index.&lt;br/&gt;
2. Instead of proposing a new syntax, why dont we use &apos;alter index &amp;lt;INDEX_NAME&amp;gt; ON &amp;lt;TABLE_NAME&amp;gt; REBUILD;&lt;br/&gt;
3. Since the code is in a prototype stage, can we move the index code to contrib ?&lt;/p&gt;</comment>
                            <comment id="12884418" author="he yongqiang" created="Thu, 1 Jul 2010 20:56:07 +0000"  >&lt;p&gt;1. Populate the index at create index.&lt;br/&gt;
Let&apos;s do this in a followup jira.&lt;/p&gt;

&lt;p&gt;3. Since the code is in a prototype stage, can we move the index code to contrib ?&lt;br/&gt;
Move to contrib is not good. In future we need to plug index automatically to the query.&lt;/p&gt;</comment>
                            <comment id="12884431" author="hammer" created="Thu, 1 Jul 2010 21:31:35 +0000"  >&lt;blockquote&gt;&lt;p&gt;3. Since the code is in a prototype stage, can we move the index code to contrib ?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It&apos;s been the experience of other Hadoop-related projects that contrib gets messy. It has proven effective to either keep experimental features in mainline trunk or to put them up on github.&lt;/p&gt;</comment>
                            <comment id="12884434" author="namit" created="Thu, 1 Jul 2010 21:37:17 +0000"  >&lt;p&gt;        if (work.getReducer() != null) &lt;/p&gt;
{
          work.getReducer().jobClose(job, success, feedBack);
        }

&lt;p&gt;        if (IndexBuilderBaseReducer.class.isAssignableFrom(this&lt;br/&gt;
            .getReducerClass())) &lt;/p&gt;
{
          this.closeIndexBuilder(job, success);
        }
&lt;p&gt;      }&lt;/p&gt;


&lt;p&gt;Instead of the above code in ExecDriver,  IndexBuilderBaseReducer/CompactSumReducer should have a jobClose - no code&lt;br/&gt;
change needed in ExecDriver.&lt;/p&gt;

&lt;p&gt;I would still vote for the index code to be in contrib, it will take some time to clean it up - then it should be moved to the mainline.&lt;br/&gt;
Till then, it is usable, but in a prototype state.&lt;/p&gt;

&lt;p&gt;What we should aim for is minimum changes in ql/. and put all changes in contrib for now. As they become stable, we can pull them&lt;br/&gt;
in - even the DDLSemanticAnalyzer should be factored in contrib&lt;/p&gt;</comment>
                            <comment id="12884685" author="athusoo" created="Fri, 2 Jul 2010 14:20:14 +0000"  >&lt;p&gt;Looked at the code and have some questions...&lt;/p&gt;

&lt;p&gt;Can you explain how the metastore object model is laid out. It seems that the table names of the index are stored in key value properties of the table that the index is created on. Is that correct? Would it be better to put a key reference from the index table to the base table instead (similar to what is done for partitions)?&lt;/p&gt;

&lt;p&gt;Also, how would this be used to query the table? Can you give an example?&lt;/p&gt;

&lt;p&gt;Is the idea here to select from the index an then pass the offsets to another query to look up the table? An example or a test which shows the query on the base table would be useful.&lt;/p&gt;</comment>
                            <comment id="12884869" author="jvs" created="Sat, 3 Jul 2010 00:24:38 +0000"  >&lt;p&gt;Had a chat with Ashish and Yongqiang offline, and came up with three alternatives.&lt;/p&gt;

&lt;p&gt;1)  &quot;Shortest path to checkin&quot;:  Treat current code as prototype and move it into contrib, providing a utility for creating/updating the index, and keeping changes to core classes to a minimum.  As Yongqiang pointed out, this makes it harder to follow up with automatic use of the index due to the lack of metadata.  If we do this, we should create a new JIRA issue for its limited scope.&lt;/p&gt;

&lt;p&gt;2) &quot;Full-fledged index support&quot;:  change the JDO metamodel to add support for indexes as first class objects, and come up with a pluggable index creation+access design framework which can encompass a variety of index types likely to be needed in the future.  Code from this patch would become the first such index implementation provided.  If we do this, we should continue on in this truly epic JIRA issue.&lt;/p&gt;

&lt;p&gt;3) &quot;Rework as materialized view&quot;:  keep the JDO metamodel as is (adding a new table type for MATERIALIZED_VIEW) but change the DDL to CREATE MATERIALIZED VIEW AS SELECT ... and then come up with the system functions needed (e.g. for accessing file offsets) in order to be able to express the index construction as SQL.  We would then execute view materialization in a fashion similar to CREATE TABLE AS SELECT.  This approach best reflects the way the current code models an index as an ordinary table, but requires some other changes (e.g. CTAS + dynamic partitioning, something we want anyway).  If we do this, we should create a new JIRA issue since it&apos;s a different feature from the user POV.&lt;/p&gt;

&lt;p&gt;We&apos;re aiming to reach a decision next week; input is welcome on whether these alternatives make sense (and on others we should consider).&lt;/p&gt;

&lt;p&gt;Since this JIRA issue is already so overloaded, we would also like to treat the following two items as separate followup JIRA issues rather than trying to address it all at once:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;rewrite framework&lt;/li&gt;
	&lt;li&gt;automatic usage of index or materialized view by optimizer&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12886308" author="prafulla" created="Thu, 8 Jul 2010 11:54:05 +0000"  >&lt;p&gt;Hi Yongqiang,&lt;br/&gt;
I am facing some problem for creating SUMMARY indexes.&lt;br/&gt;
This index is not built with update index command.&lt;br/&gt;
COMPACT SUMMARY index works fine. Is there any problem with&lt;br/&gt;
creation of SUMMARY index table ?&lt;/p&gt;</comment>
                            <comment id="12886380" author="he yongqiang" created="Thu, 8 Jul 2010 17:25:31 +0000"  >&lt;p&gt;I think SUMMARY index&apos;s mapper code is comment out in the uploaded patch.&lt;/p&gt;</comment>
                            <comment id="12886587" author="jvs" created="Fri, 9 Jul 2010 02:12:47 +0000"  >&lt;p&gt;Based on discussion with Yongqiang, we&apos;ve decided to go for &quot;Full-fledged index support&quot;.&lt;/p&gt;</comment>
                            <comment id="12886634" author="hammer" created="Fri, 9 Jul 2010 06:31:27 +0000"  >&lt;p&gt;Hey,&lt;/p&gt;

&lt;p&gt;Any chance you guys could post a more detailed design document for &quot;full-fledged index support&quot;? I&apos;m quite curious to read up on it.&lt;/p&gt;

&lt;p&gt;Thanks,&lt;br/&gt;
Jeff&lt;/p&gt;</comment>
                            <comment id="12886764" author="jvs" created="Fri, 9 Jul 2010 16:36:52 +0000"  >&lt;p&gt;@Jeff:  Yes, we&apos;ll put it up on the wiki, similar to how we did for storage handler + HBase.&lt;/p&gt;</comment>
                            <comment id="12888089" author="he yongqiang" created="Tue, 13 Jul 2010 23:09:50 +0000"  >&lt;p&gt;Attache a new patch for review. Added metastore remodeling based on offline discussion with John and Paul.&lt;/p&gt;

&lt;p&gt;Also addressed namit&apos;s suggestions: &quot;Instead of proposing a new syntax, why dont we use &apos;alter index &amp;lt;INDEX_NAME&amp;gt; ON &amp;lt;TABLE_NAME&amp;gt; REBUILD;&quot;&lt;/p&gt;

&lt;p&gt;@Ashish,&lt;br/&gt;
&amp;gt;&amp;gt;Also, how would this be used to query the table? Can you give an example?&lt;/p&gt;

&lt;p&gt;Added four test files in clientpositive. You can now refer thoee files for examples.&lt;/p&gt;

&lt;p&gt;An example from one qfile:&lt;br/&gt;
CREATE INDEX srcpart_index_proj TYPE COMPACT ON TABLE srcpart(key);&lt;br/&gt;
ALTER INDEX srcpart_index_proj ON srcpart REBUILD;&lt;/p&gt;

&lt;p&gt;SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;&lt;br/&gt;
INSERT OVERWRITE DIRECTORY &quot;/tmp/index_test_index_result&quot; SELECT `&lt;em&gt;bucketname` ,  `_offsets` FROM default&lt;/em&gt;&lt;em&gt;srcpart_srcpart_index_proj&lt;/em&gt;_ x WHERE x.key=100 AND x.ds = &apos;2008-04-08&apos;;&lt;br/&gt;
SET hive.exec.index_file=/tmp/index_test_index_result;&lt;br/&gt;
SET hive.input.format=org.apache.hadoop.hive.ql.index.io.HiveIndexInputFormat;&lt;br/&gt;
SELECT key, value FROM srcpart WHERE key=100 AND ds = &apos;2008-04-08&apos;;&lt;/p&gt;

&lt;p&gt;Will update the patch and open follow up jiras after get more comments. &lt;/p&gt;

&lt;p&gt;Thanks&lt;br/&gt;
Yongqiang&lt;/p&gt;</comment>
                            <comment id="12888945" author="jvs" created="Thu, 15 Jul 2010 22:47:59 +0000"  >&lt;p&gt;Attaching metastore data model diagram reverse-engineered from Yongqiang&apos;s latest patch (using Power Architect); can be used in a design doc.  Cosmetic:  I renamed INDEXS to IDXS for consistency with TBLS, likewise for IDX_ID and other attributes.&lt;/p&gt;

&lt;p&gt;I marked one of the relationships between index and table as optional since we want to allow for indexes which don&apos;t store their data structure in tabular form.  I&apos;m not sure whether there&apos;s something we need to do in package.jdo to make sure the corresponding key is nullable.&lt;/p&gt;</comment>
                            <comment id="12888946" author="jvs" created="Thu, 15 Jul 2010 22:51:12 +0000"  >&lt;p&gt;Whoops, relationships connecting TBLS/SDS and IDXS/SDS got lost; will attach another diagram which fixes that.&lt;/p&gt;</comment>
                            <comment id="12888948" author="jvs" created="Thu, 15 Jul 2010 23:05:48 +0000"  >&lt;p&gt;idx2.png&lt;/p&gt;</comment>
                            <comment id="12889345" author="jvs" created="Fri, 16 Jul 2010 21:45:42 +0000"  >&lt;p&gt;Here are some preliminary comments on the metastore work.  We can move on to the plugin design next week and start getting all of this into a doc.&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;We should support a property on the index which controls the name of the index table, and only generate an index table name automatically in the case where the user doesn&apos;t supply the property.  For this, we&apos;ll need to add property key/values to the grammar (IDXPROPERTIES like TBLPROPERTIES and SERDEPROPERTIES?).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;The grammar supports control over the tableFileFormat for the index table; what about other attributes such as row format, location, and TBLPROPERTIES?  Some of these may be dictated by the index implementation, but it may be useful to override in some cases (same as tableFileFormat).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Is the partitioning for the index independent of the partitioning for the table?  Don&apos;t we need to allow control over this in the grammar?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;I think we should track the status of the index (when was the last time it was rebuilt, if ever) so that we know whether it is fresh with respect to the base table data.  How should we model this in such a way that it takes per-partition indexing into account?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Some metastore followups to be logged separately:  COMMENT clause on index definition; DESCRIBE INDEX; SHOW INDEXES; dealing with base table columns being dropped/renamed out from under the index&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;For generating the index table structure, we&apos;ll need to move that to plugin (rather than in Hive.java), since each index will need a different table structure (or no table structure at all).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Test queries:  remember to add ORDER BY for determinism.  Also, I&apos;m not sure whether it is safe to use /tmp in the local file system (it may not exist, e.g. on Windows).  I used it in hbase_bulk.m, but that uses a mini HDFS cluster (not the local file system).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Dropping a table with an index on it currently gives the exception below (in Derby; I didn&apos;t test MySQL yet).  Same for attempting to drop an index table directly (instead of dropping the index).  The second case should either fail with a meaningful exception, or implicitly drop the index definition as a trigger from dropping the table.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;hive&amp;gt; create table t1(i int);&lt;br/&gt;
OK&lt;br/&gt;
hive&amp;gt; create index q type compact on table t1&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;;&lt;br/&gt;
OK&lt;br/&gt;
hive&amp;gt; drop table t1;&lt;br/&gt;
FAILED: Error in metadata: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore&lt;br/&gt;
NestedThrowables:&lt;br/&gt;
java.sql.BatchUpdateException: DELETE on table &apos;TBLS&apos; caused a violation of foreign key constraint &apos;INDEXS_FK3&apos; for key (12).  The statement has been rolled back.&lt;br/&gt;
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask&lt;/p&gt;

&lt;p&gt;hive&amp;gt; create table t5(i int);&lt;br/&gt;
OK&lt;br/&gt;
hive&amp;gt; create index r type compact on table t5&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;;&lt;br/&gt;
OK&lt;br/&gt;
hive&amp;gt; drop table default_&lt;em&gt;t5_r&lt;/em&gt;_;&lt;br/&gt;
FAILED: Error in metadata: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore&lt;br/&gt;
NestedThrowables:&lt;br/&gt;
java.sql.BatchUpdateException: DELETE on table &apos;TBLS&apos; caused a violation of foreign key constraint &apos;INDEXS_FK2&apos; for key (17).  The statement has been rolled back.&lt;br/&gt;
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask&lt;/p&gt;</comment>
                            <comment id="12889376" author="he yongqiang" created="Fri, 16 Jul 2010 22:44:15 +0000"  >&lt;p&gt;THANKS FOR THE DETAILED COMMENTS.&lt;/p&gt;

&lt;p&gt;&amp;gt;&amp;gt;We should support a property on the index which controls the name of the index table, and only generate an index table name automatically in the case where the user doesn&apos;t supply the property. &lt;br/&gt;
will add this in the following patch.&lt;/p&gt;

&lt;p&gt;&amp;gt;&amp;gt;For this, we&apos;ll need to add property key/values to the grammar (IDXPROPERTIES like TBLPROPERTIES and SERDEPROPERTIES?).&lt;br/&gt;
Let&apos;s do it in a followup jira.&lt;/p&gt;

&lt;p&gt;&amp;gt;&amp;gt;The grammar supports control over the tableFileFormat for the index table; what about other attributes such as row format, location, and TBLPROPERTIES? Some of these may be dictated by the index implementation, but it may be useful to override in some cases (same as tableFileFormat).&lt;br/&gt;
We can add this when we see the requirement. For now we can leave this out.&lt;/p&gt;

&lt;p&gt;&amp;gt;&amp;gt;I think we should track the status of the index (when was the last time it was rebuilt, if ever) so that we know whether it is fresh with respect to the base table data. How should we model this in such a way that it takes per-partition indexing into account?&lt;br/&gt;
I think it&apos;s the same as the one of key/value property. no?&lt;/p&gt;

&lt;p&gt;&amp;gt;&amp;gt;Test queries: remember to add ORDER BY for determinism. &lt;br/&gt;
will add this in the following patch.&lt;/p&gt;

&lt;p&gt;&amp;gt;&amp;gt;Also, I&apos;m not sure whether it is safe to use /tmp in the local file system (it may not exist, e.g. on Windows). I used it in hbase_bulk.m, but that uses a mini HDFS cluster (not the local file system).&lt;br/&gt;
I think it&apos;s should be ok because it&apos;s not local tmp. it&apos;s mini HDFS /tmp&lt;/p&gt;

&lt;p&gt;&amp;gt;&amp;gt;Dropping a table with an index on it currently gives the exception below (in Derby; I didn&apos;t test MySQL yet). Same for attempting to drop an index table directly (instead of dropping the index). The second case should either fail with a meaningful exception, or implicitly drop the index definition as a trigger from dropping the table.&lt;br/&gt;
Actually this is reported by Prafulla offline. Will add this in the following patch. For the second case, i am planning to report error.&lt;/p&gt;</comment>
                            <comment id="12890137" author="jvs" created="Tue, 20 Jul 2010 02:34:03 +0000"  >&lt;p&gt;Preliminary draft of design doc is here&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://wiki.apache.org/hadoop/Hive/IndexDev&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://wiki.apache.org/hadoop/Hive/IndexDev&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Yongqiang and I are still working out some of the details.&lt;/p&gt;</comment>
                            <comment id="12891274" author="nzhang" created="Thu, 22 Jul 2010 18:34:32 +0000"  >&lt;p&gt;Based on some internal discussions below are some comments about the design doc:&lt;/p&gt;

&lt;p&gt;1) the staleness (inconsistency) between the index and the base table should be addressed more precisely. &lt;br/&gt;
   Since the current implementation allows the user to query the index table directly, we should guarantee that the index is consistent with the base table at the query time. This means at the query START time, the index was built completely based on the data stored in the base table. The current design does not satisfy this criteria in that it only record the last_modification_time (LMT) of the base table and the index table, and check if the latter is larger than the former. This leaves the following example break:&lt;/p&gt;

&lt;p&gt;timestamp0: last update of partition P1&lt;br/&gt;
timestamp1: start create index on partition P1&lt;br/&gt;
timestamp2: start insert overwrite P1&lt;br/&gt;
timestamp3: finish insert overwrite P1&lt;br/&gt;
timestamp4: finish index creation on P1&lt;br/&gt;
timestamp 5: query on P1&lt;/p&gt;

&lt;p&gt;The LMTs of the index and the base table are timestamp4 and timestamp3 respectively so the optimizer will conclude the index is consistent with base table. However, the index was built based on stale data at the timestamp5. So the index should not be used. &lt;/p&gt;

&lt;p&gt;Instead of recording the LMT of the index table, we probably should record the LMT of the base table in the index metadata at the beginning of the index creation.  In the above example, the timestamp recorded in the index metadata should be timestamp0. This means the index was created based on the base table at timestamp0. At the query time, we should check timestamp0 against timestamp 3, which correctly conclude the index is stale. &lt;/p&gt;

&lt;p&gt;BTW, all the timestamp should be coming from some centralized clock such as the DFS directory update time (from the namenode).&lt;/p&gt;

&lt;p&gt;2) The above consistency problem does not only present in the case of &quot;DEFERRED REBUILD&quot;. Even if the index rebuild starts right away after INSERT OVERWRITE, there is still a time window that the index is stale (before the index creation is complete). So we need the same mechanism to figure out stale indexes. &lt;/p&gt;

&lt;p&gt;3) I think a lock-based concurrency may not be the best choice as well. If the index creation takes a long time, it defers the availability of the base table. If we have the optimizer, we should always query against the base tables, and let the optimizer to figure out whether an index is available and fresh. So if an index creation is not finished, we can just use the base table, otherwise we can use the index if the cost is less expensive. &lt;/p&gt;

&lt;p&gt;4) Another case is that if the index creation finished and the query is using the index, and then an DML happened on the base table and finished before the query finish. Here we only guarantee snapshot consistency (results consisting with the data at the beginning of the query, not after the query). &lt;/p&gt;

&lt;p&gt;5) If we have the mechanism to check consistency of the index, then the &quot;index rebuild&quot; command could just return if the index is consistent. We can also allow a &quot;force&quot; option in case we need to compensate for bad metadata. &lt;/p&gt;</comment>
                            <comment id="12892183" author="he yongqiang" created="Mon, 26 Jul 2010 05:42:34 +0000"  >&lt;p&gt;Attach a new diff for review.&lt;/p&gt;

&lt;p&gt;Update:&lt;br/&gt;
Based on some offline discussions, this patch takes a virtual column approach to build index.&lt;br/&gt;
Added 2 virtual columns in this patch, one is input filename, and the other is file-wise block_offset.&lt;br/&gt;
An example of querying virtual columns:&lt;br/&gt;
select INPUT_&lt;em&gt;FILE&lt;/em&gt;&lt;em&gt;NAME, key, BLOCK&lt;/em&gt;&lt;em&gt;OFFSET&lt;/em&gt;&lt;em&gt;INSIDE&lt;/em&gt;_FILE from src;&lt;/p&gt;
</comment>
                            <comment id="12892413" author="jvs" created="Mon, 26 Jul 2010 19:00:04 +0000"  >&lt;p&gt;Yongqiang, I looked at hive.indexing.10.patch, but I don&apos;t see the virtual columns in there?&lt;/p&gt;</comment>
                            <comment id="12892430" author="jvs" created="Mon, 26 Jul 2010 19:40:09 +0000"  >&lt;p&gt;Metastore upgrade scripts will  need to be added in &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-1427&quot; title=&quot;Provide metastore schema migration scripts (0.5 -&amp;gt; 0.6)&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-1427&quot;&gt;&lt;del&gt;HIVE-1427&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="12892492" author="he yongqiang" created="Mon, 26 Jul 2010 21:55:35 +0000"  >&lt;p&gt;Sorry, hive.indexing.10.patch was created from a wrong workspace.&lt;br/&gt;
Attached a new diff. &lt;/p&gt;</comment>
                            <comment id="12892594" author="jvs" created="Tue, 27 Jul 2010 01:18:17 +0000"  >&lt;p&gt;First pass of review comments on latest patch (I&apos;ll probably have more tomorrow).&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;INDEX_NAME precision in the metastore should be 128 characters (not 767), following convention for other identifiers&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;I don&apos;t think we need INDEX_TABLE_NAME at all in the metastore; it should only be used during CREATE INDEX and then forgotten&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Move HiveIndexInputFormat and HiveIndexResult to package org.apache.hadoop.hive.ql.index.compact, and add Compact in their names (I&apos;d still prefer to move this entire package out to a new subproj, but I guess we can skip that part now since most of the code went away with the virtual column approach); rename property hive.exec.index_file to hive.index.compact.file&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Support WITH DEFERRED REBUILD, and require this to be specified for now to avoid confusion (per discussion in design meeting)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;when generating reentrant INSERT, need to quote identifiers such as table/column names (use HiveUtils.unparseIdentifier), and may need extra escaping for special characters in getPartKVPairStringArray (I&apos;m not sure--check with Paul)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;thread_local should be private (and named threadLocal); go through public IOContext.get() instead; likewise use public getter/setter methods on IOContext instead of accessing its data members directly&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;need ORDER BY in virtual_column.q&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;remove extra semicolon in other ORDER BY&apos;s, and make sure they cover a unique key in all cases&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;don&apos;t need TYPE and UPDATE as keywords in grammar&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12892613" author="jvs" created="Tue, 27 Jul 2010 02:24:38 +0000"  >&lt;p&gt;Whoops, forgot two leftover from a private diff review:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;metastore/if/hive_metastore.thrift:102 instead of including the full indexTable structure inside the Index structure, can we omit it but then pass it as an additional parameter to add_index?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;ql/src/java/org/apache/hadoop/hive/ql/index/compact/CompactIndexHandler.java:86 Move generic partition analysis out into Hive, since it will be the same for all plugins.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;We can talk more about these tomorrow if it&apos;s not clear.&lt;/p&gt;</comment>
                            <comment id="12892865" author="jvs" created="Tue, 27 Jul 2010 17:37:44 +0000"  >&lt;p&gt;Regarding the mkset function:  can we rename this to collect_array to hint that it is a UDAF?  The @Description should also make this clear.&lt;/p&gt;

&lt;p&gt;Collect is the standard SQL name for this aggregate function, but the standard version returns a multiset rather than an array, so let&apos;s call it collect_array to be specific.&lt;/p&gt;

&lt;p&gt;Also, it will need its own independent unit tests (open a followup JIRA issue for this).&lt;/p&gt;</comment>
                            <comment id="12892932" author="athusoo" created="Tue, 27 Jul 2010 21:22:05 +0000"  >&lt;p&gt;Started looking at this. One initial question I had - why is virtualcolumn class in the serde2 package?&lt;/p&gt;</comment>
                            <comment id="12892937" author="jvs" created="Tue, 27 Jul 2010 21:29:49 +0000"  >&lt;p&gt;Another followup needed:  REBUILD should be propagating lineage and read/write info from the reentrant INSERT statement up to the top-level statement so that hooks get called with the right information.&lt;/p&gt;</comment>
                            <comment id="12892939" author="athusoo" created="Tue, 27 Jul 2010 21:30:45 +0000"  >&lt;p&gt;Also, how is the file name populated? That is not done through the IOContext?&lt;/p&gt;</comment>
                            <comment id="12892946" author="he yongqiang" created="Tue, 27 Jul 2010 21:46:19 +0000"  >&lt;p&gt;@Ashish&lt;br/&gt;
&amp;gt;&amp;gt;why is virtualcolumn class in the serde2 package?&lt;br/&gt;
will put it to ql.io package. I put it to serde2 package just because i thought it maybe needed by the serde layer. Since all codes are almost done and it is not accessed by serde, &lt;br/&gt;
it makes sense to move it to ql.&lt;br/&gt;
&amp;gt;&amp;gt;how is the file name populated&lt;br/&gt;
filename and block offset are all populated by record reader. filename is populated by looking at the split path when we construct the record reader. Offset is generated at runtime by record reader.&lt;/p&gt;</comment>
                            <comment id="12892947" author="he yongqiang" created="Tue, 27 Jul 2010 21:47:53 +0000"  >&lt;p&gt;IOContext is just a container, HiveContextAwareRecordReader is responsible for filling it with actual values.&lt;/p&gt;</comment>
                            <comment id="12893026" author="he yongqiang" created="Wed, 28 Jul 2010 01:24:12 +0000"  >&lt;p&gt;A new patch integrates comments from John and Ashish. Thanks!&lt;/p&gt;</comment>
                            <comment id="12893333" author="jvs" created="Wed, 28 Jul 2010 19:49:43 +0000"  >&lt;p&gt;Thanks Yongqiang.  Looking at it now.&lt;/p&gt;</comment>
                            <comment id="12893402" author="jvs" created="Wed, 28 Jul 2010 22:35:22 +0000"  >&lt;p&gt;+1.  Will commit when tests pass.  I noticed a number of trivial issues (like Javadoc mismatches) which I&apos;ll put in a followup.&lt;/p&gt;</comment>
                            <comment id="12893455" author="jsensarma" created="Thu, 29 Jul 2010 00:52:40 +0000"  >&lt;p&gt;i am waiting for a commit on hive-1408. that&apos;s probably gonna collide.&lt;/p&gt;</comment>
                            <comment id="12893461" author="jvs" created="Thu, 29 Jul 2010 01:02:43 +0000"  >&lt;p&gt;Thanks Joydeep.  Yeah, this one has tons of plan diffs due to the virtual columns.&lt;/p&gt;</comment>
                            <comment id="12893488" author="jvs" created="Thu, 29 Jul 2010 03:45:57 +0000"  >&lt;p&gt;Yongqiang, I passed tests on Hadoop 0.20, but Ning has committed &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-1408&quot; title=&quot;add option to let hive automatically run in local mode based on tunable heuristics&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-1408&quot;&gt;&lt;del&gt;HIVE-1408&lt;/del&gt;&lt;/a&gt;, which conflicts, so you&apos;ll need to rebase against that and then I&apos;ll try again.&lt;/p&gt;</comment>
                            <comment id="12893522" author="jvs" created="Thu, 29 Jul 2010 06:06:49 +0000"  >&lt;p&gt;Since another patch is needed, here are the review comments I mentioned above.&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Javadoc for Hive.createIndex needs parameters fixed&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Javadoc for HiveIndexHandler.analyzeIndexDefinition:  remove storageDesc]&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;In HiveUtils.getIndexHandler:  the message should be &quot;Error in loading index handler&quot; rather than &quot;Error in loading storage handler&quot;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;GenericUDAFCollectSet @Description :  &quot;with no duplication elements&quot; should be &quot;with duplicate elements eliminated&quot;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;DDLSemanticAnalyzer.analyzeCreateIndex:  hanlder is misspelled&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Property AbstractIndexHandler.INDEX_COLS_KEY is never used; get rid of it?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;For HiveIndex.INDEX_TABLE_CREATETIME property name, spell out &quot;lastModifiedTime&quot; instead of &quot;lmt&quot;&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12893886" author="he yongqiang" created="Fri, 30 Jul 2010 00:51:08 +0000"  >&lt;p&gt;a new patch against trunk&lt;/p&gt;</comment>
                            <comment id="12893890" author="jvs" created="Fri, 30 Jul 2010 01:05:17 +0000"  >&lt;p&gt;OK, testing lucky patch 13...&lt;/p&gt;</comment>
                            <comment id="12893947" author="jvs" created="Fri, 30 Jul 2010 06:42:14 +0000"  >&lt;p&gt;Committed.  Thanks Yongqiang!&lt;/p&gt;</comment>
                            <comment id="12894221" author="he yongqiang" created="Sat, 31 Jul 2010 00:31:51 +0000"  >&lt;p&gt;For mysql metastore upgrade, please refer to&lt;br/&gt;
&lt;a href=&quot;http://wiki.apache.org/hadoop/Hive/IndexDev#Metastore_Upgrades&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://wiki.apache.org/hadoop/Hive/IndexDev#Metastore_Upgrades&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="12435865">HIVE-837</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12480471">HIVE-1803</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12495102">HIVE-1904</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12470567">HIVE-1501</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12412420">CHUKWA-22</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12476655">HIVE-1694</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12494780">HIVE-1889</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12470561">HIVE-1496</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12470565">HIVE-1499</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12470566">HIVE-1500</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12470568">HIVE-1502</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12470569">HIVE-1503</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12470560">HIVE-1495</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12470562">HIVE-1497</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12470563">HIVE-1498</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12408864" name="hive-417.proto.patch" size="39673" author="prasadc" created="Sat, 23 May 2009 02:23:12 +0000"/>
                            <attachment id="12413928" name="hive-417&#65293;2009-07-18.patch" size="164104" author="he yongqiang" created="Sat, 18 Jul 2009 13:33:13 +0000"/>
                            <attachment id="12449399" name="hive-indexing-8-thrift-metastore-remodel.patch" size="1340853" author="he yongqiang" created="Tue, 13 Jul 2010 23:09:49 +0000"/>
                            <attachment id="12446629" name="hive-indexing.3.patch" size="153046" author="he yongqiang" created="Tue, 8 Jun 2010 21:48:26 +0000"/>
                            <attachment id="12448438" name="hive-indexing.5.thrift.patch" size="199587" author="he yongqiang" created="Wed, 30 Jun 2010 18:57:09 +0000"/>
                            <attachment id="12450528" name="hive.indexing.11.patch" size="1404551" author="he yongqiang" created="Mon, 26 Jul 2010 21:55:34 +0000"/>
                            <attachment id="12450657" name="hive.indexing.12.patch" size="2911488" author="he yongqiang" created="Wed, 28 Jul 2010 01:24:11 +0000"/>
                            <attachment id="12450877" name="hive.indexing.13.patch" size="2910138" author="he yongqiang" created="Fri, 30 Jul 2010 00:51:07 +0000"/>
                            <attachment id="12449601" name="idx2.png" size="172212" author="jvs" created="Thu, 15 Jul 2010 23:05:48 +0000"/>
                            <attachment id="12447554" name="indexing_with_ql_rewrites_trunk_953221.patch" size="195487" author="prafulla" created="Sun, 20 Jun 2010 09:37:50 +0000"/>
                    </attachments>
                <subtasks>
                            <subtask id="12431219">HIVE-678</subtask>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>10.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Wed, 15 Apr 2009 16:12:19 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>71329</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            6 years, 19 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i010h3:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3842</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310192" key="com.atlassian.jira.plugin.system.customfieldtypes:textarea">
                        <customfieldname>Release Note</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Index support requires metastore schema upgrade (TBD).</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>