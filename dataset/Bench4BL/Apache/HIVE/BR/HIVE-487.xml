<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 02 23:21:43 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HIVE-487/HIVE-487.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HIVE-487] Hive does not compile with Hadoop 0.20.0</title>
                <link>https://issues.apache.org/jira/browse/HIVE-487</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;Attempting to compile Hive with Hadoop 0.20.0 fails:&lt;/p&gt;

&lt;p&gt;aaron@jargon:~/src/ext/svn/hive-0.3.0$ ant -Dhadoop.version=0.20.0 package&lt;br/&gt;
(several lines elided)&lt;br/&gt;
compile:&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;echo&amp;#93;&lt;/span&gt; Compiling: hive&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;javac&amp;#93;&lt;/span&gt; Compiling 261 source files to /home/aaron/src/ext/svn/hive-0.3.0/build/ql/classes&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;javac&amp;#93;&lt;/span&gt; /home/aaron/src/ext/svn/hive-0.3.0/build/ql/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java:94: cannot find symbol&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;javac&amp;#93;&lt;/span&gt; symbol  : method getCommandLineConfig()&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;javac&amp;#93;&lt;/span&gt; location: class org.apache.hadoop.mapred.JobClient&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;javac&amp;#93;&lt;/span&gt;       Configuration commandConf = JobClient.getCommandLineConfig();&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;javac&amp;#93;&lt;/span&gt;                                            ^&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;javac&amp;#93;&lt;/span&gt; /home/aaron/src/ext/svn/hive-0.3.0/build/ql/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java:241: cannot find symbol&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;javac&amp;#93;&lt;/span&gt; symbol  : method validateInput(org.apache.hadoop.mapred.JobConf)&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;javac&amp;#93;&lt;/span&gt; location: interface org.apache.hadoop.mapred.InputFormat&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;javac&amp;#93;&lt;/span&gt;       inputFormat.validateInput(newjob);&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;javac&amp;#93;&lt;/span&gt;                  ^&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;javac&amp;#93;&lt;/span&gt; Note: Some input files use or override a deprecated API.&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;javac&amp;#93;&lt;/span&gt; Note: Recompile with -Xlint:deprecation for details.&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;javac&amp;#93;&lt;/span&gt; Note: Some input files use unchecked or unsafe operations.&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;javac&amp;#93;&lt;/span&gt; Note: Recompile with -Xlint:unchecked for details.&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;javac&amp;#93;&lt;/span&gt; 2 errors&lt;/p&gt;

&lt;p&gt;BUILD FAILED&lt;br/&gt;
/home/aaron/src/ext/svn/hive-0.3.0/build.xml:145: The following error occurred while executing this line:&lt;br/&gt;
/home/aaron/src/ext/svn/hive-0.3.0/ql/build.xml:135: Compile failed; see the compiler error output for details.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12425378">HIVE-487</key>
            <summary>Hive does not compile with Hadoop 0.20.0</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.png">Blocker</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="justinlynn">Justin Lynn</assignee>
                                    <reporter username="kimballa">Aaron Kimball</reporter>
                        <labels>
                    </labels>
                <created>Wed, 13 May 2009 20:54:34 +0000</created>
                <updated>Sat, 17 Dec 2011 00:07:14 +0000</updated>
                            <resolved>Tue, 4 Aug 2009 23:11:12 +0000</resolved>
                                    <version>0.3.0</version>
                                    <fixVersion>0.4.0</fixVersion>
                                        <due></due>
                            <votes>3</votes>
                                    <watches>13</watches>
                                                                <comments>
                            <comment id="12711771" author="justinlynn" created="Thu, 21 May 2009 19:47:13 +0000"  >&lt;p&gt;This patch (based on trunk) will allow a compile and passes all but two unit tests (see attached junit test report). This is my first time contributing so I&apos;m going to need a bit of help with the SQL result differences.&lt;/p&gt;</comment>
                            <comment id="12711838" author="justinlynn" created="Thu, 21 May 2009 21:59:16 +0000"  >&lt;p&gt;Actually it appears that those test failures are /not/ related to my changes (I checked out vanilla trunk and built and tested it, and those two tests still failed).&lt;/p&gt;</comment>
                            <comment id="12711930" author="athusoo" created="Fri, 22 May 2009 02:30:49 +0000"  >&lt;p&gt;Assigned.&lt;/p&gt;</comment>
                            <comment id="12711931" author="athusoo" created="Fri, 22 May 2009 02:31:37 +0000"  >&lt;p&gt;Thanks for your contribution.&lt;br/&gt;
Please do a submit patch when you attach a patch so that we get it in the patch submitted queue.&lt;/p&gt;</comment>
                            <comment id="12712332" author="athusoo" created="Sat, 23 May 2009 01:24:56 +0000"  >&lt;p&gt;Hi Justin, Why have all the @Overrides have been taken out from the code.&lt;/p&gt;</comment>
                            <comment id="12712621" author="justinlynn" created="Mon, 25 May 2009 01:20:06 +0000"  >&lt;p&gt;The @Override annotations were removed because they were causing runtime exceptions to be generated in the latest sun Java VM. This is because those annotations were used on methods that were not correctly overriding anything in the respective superclass.&lt;/p&gt;</comment>
                            <comment id="12712627" author="prasadc" created="Mon, 25 May 2009 03:14:18 +0000"  >&lt;p&gt;which version of JAVA VM are you using?&lt;/p&gt;

&lt;p&gt;The below doesn&apos;t throw such errors but some configurations of Eclipse does throw such errors.&lt;/p&gt;

&lt;p&gt;pchakka@dev111 ~ &amp;gt; java -version&lt;br/&gt;
java version &quot;1.6.0_07&quot;&lt;br/&gt;
Java(TM) SE Runtime Environment (build 1.6.0_07-b06)&lt;br/&gt;
Java HotSpot(TM) 64-Bit Server VM (build 10.0-b23, mixed mode)&lt;/p&gt;</comment>
                            <comment id="12712835" author="justinlynn" created="Tue, 26 May 2009 03:07:51 +0000"  >&lt;p&gt;Output of my java -version reads:&lt;br/&gt;
java version &quot;1.6.0_13&quot;&lt;br/&gt;
Java(TM) SE Runtime Environment (build 1.6.0_13-b03)&lt;br/&gt;
Java HotSpot(TM) Server VM (build 11.3-b02, mixed mode)&lt;/p&gt;

&lt;p&gt;Looking for more differences or reasons why this might happen. Double checking some things.&lt;/p&gt;</comment>
                            <comment id="12712854" author="justinlynn" created="Tue, 26 May 2009 04:53:21 +0000"  >&lt;p&gt;This patch removes all extraneous @override removals (potentially not neccessary, should&apos;ve been done in another bug). Passes all unit tests on trunk (rev. 778559).&lt;/p&gt;</comment>
                            <comment id="12712855" author="justinlynn" created="Tue, 26 May 2009 04:57:49 +0000"  >&lt;p&gt;This patch does not attempt to re-implement the hadoop version specific functionality that was removed. This could potentially cause a bug in running trunk hive on hadoop 0.17.x as indicated by the comment on the removed code. Is this acceptable or should an attempt be made at reimplementation of intention using non-depreciated/removed interfaces?&lt;/p&gt;</comment>
                            <comment id="12713348" author="athusoo" created="Wed, 27 May 2009 00:54:45 +0000"  >&lt;p&gt;Actually this has to compile with hadoop 0.17.x otherwise we will not be able to deploy this internally at FB. We are still on hadoop 0.17 and we have already lauched trunk into production.&lt;/p&gt;</comment>
                            <comment id="12713349" author="athusoo" created="Wed, 27 May 2009 00:54:46 +0000"  >&lt;p&gt;Actually this has to compile with hadoop 0.17.x otherwise we will not be able to deploy this internally at FB. We are still on hadoop 0.17 and we have already lauched trunk into production.&lt;/p&gt;</comment>
                            <comment id="12713357" author="justinlynn" created="Wed, 27 May 2009 01:14:07 +0000"  >&lt;p&gt;Understood, I&apos;ll see what I can do. However, it appears that the API is starting to pull away with what would easily be reverse compatible. A 0.20.0+ branch might be warranted.&lt;/p&gt;</comment>
                            <comment id="12713448" author="kimballa" created="Wed, 27 May 2009 06:55:29 +0000"  >&lt;p&gt;I&apos;m +1 on an 0.20 branch. Cloudera&apos;s Distribution for Hadoop will be moving toward an 0.20 base and we would like to offer a compatible edition of Hive.&lt;/p&gt;</comment>
                            <comment id="12713450" author="ashk" created="Wed, 27 May 2009 06:59:28 +0000"  >&lt;p&gt;Another +1 for the 0.20 branch.&lt;/p&gt;</comment>
                            <comment id="12713780" author="jsensarma" created="Wed, 27 May 2009 22:30:48 +0000"  >&lt;p&gt;I agree that at some point we would drop support for older branches (&amp;lt;=17). But it&apos;s still worth looking at whether we can avoid doing this right now. For example for the ExecDriver change:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// workaround for hadoop-17 - jobclient only looks at commandlineconfig&lt;/li&gt;
	&lt;li&gt;Configuration commandConf = JobClient.getCommandLineConfig();&lt;/li&gt;
	&lt;li&gt;if (commandConf != null) {&lt;/li&gt;
	&lt;li&gt;commandConf.set(&quot;tmpfiles&quot;, realFiles);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;we can do this using reflection - (grep -i declaredmethod HiveInputFormat.java).&lt;/p&gt;

&lt;p&gt;are the mapredtask.java changes necessary?&lt;/p&gt;</comment>
                            <comment id="12715946" author="jsensarma" created="Wed, 3 Jun 2009 15:07:49 +0000"  >&lt;p&gt;this version uses reflection in a couple of places so that ql continues to compile with older versions.&lt;/p&gt;

&lt;p&gt;i haven&apos;t had a chance to look at the hwi code and make that portable (it doesn&apos;t compile with 19 currently). If Edward could take a look - that would be awesome.&lt;/p&gt;</comment>
                            <comment id="12723588" author="appodictic" created="Wed, 24 Jun 2009 14:54:53 +0000"  >&lt;p&gt;I gave a quick try and was getting hunk errors. Are these patches cumulative? Should I apply them in order &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-487&quot; title=&quot;Hive does not compile with Hadoop 0.20.0&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-487&quot;&gt;&lt;del&gt;HIVE-487&lt;/del&gt;&lt;/a&gt;.patch, &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-487&quot; title=&quot;Hive does not compile with Hadoop 0.20.0&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-487&quot;&gt;&lt;del&gt;HIVE-487&lt;/del&gt;&lt;/a&gt;-2.patch, hive-487.3.patch?&lt;/p&gt;

&lt;p&gt;I looked at jetty the HWI server in the patch. The changes are cosmetic.  We might be able to make that portable with reflection as well if that is what we want. &lt;/p&gt;</comment>
                            <comment id="12725662" author="jsensarma" created="Tue, 30 Jun 2009 15:08:14 +0000"  >&lt;p&gt;regenerated. &lt;/p&gt;

&lt;p&gt;the HWI stuff does not compile against 0.19 and prior because of the change to using new jetty apis. one option is to bundle the new jetty jar with hive. hadoop seems to have moved to using ivy and i am wondering if we should do the same.&lt;/p&gt;</comment>
                            <comment id="12725667" author="jsensarma" created="Tue, 30 Jun 2009 15:29:22 +0000"  >&lt;p&gt;have to use reflection. putting new jetty jars in hive does not matter since hadoop&apos;s jars take precedence at runtime (since we launch everything via hadoop)&lt;/p&gt;

&lt;p&gt;can take a shot - looks simple ..&lt;/p&gt;</comment>
                            <comment id="12725669" author="prasadc" created="Tue, 30 Jun 2009 15:32:27 +0000"  >&lt;p&gt;i don&apos;t think we can keep single version of Hive for all active versions of Hadoop. Why don&apos;t we release a branch for 20 and periodically merge from trunk to 20?&lt;/p&gt;</comment>
                            <comment id="12725715" author="jsensarma" created="Tue, 30 Jun 2009 17:30:41 +0000"  >&lt;p&gt;branching will get fairly complicated. where will 0.4.0 be branched off? sure there will be a time to deprecate support for older hadoop ersions - just not convinced this is it.&lt;/p&gt;

&lt;p&gt;Note that the dependency in this case is particularly frivolous. We could ship Hive with a single version of jetty that Hive components require - but instead we are depending on Hadoop to provide it. This seems more like a setup problem on our side.&lt;/p&gt;

&lt;p&gt;One simple option (to not use reflection) is to add the right version of jetty into the runtime classpath (if it&apos;s not there already). the compile time works fine already. (since we control the classpath from build xmls)&lt;/p&gt;</comment>
                            <comment id="12726820" author="jsensarma" created="Fri, 3 Jul 2009 08:06:12 +0000"  >&lt;p&gt;so i tried a custom classloader to force the new jetty jars to be used preferentially for loading classes.&lt;/p&gt;

&lt;p&gt;alas - it does not work. it seems that some classes from the hadoop jetty jars are already loaded by the time control is transferred to hive/hwi. trying to load remaining classes from the new jetty jar causes a &apos;sealing violation exception&apos;. (this is with hadoop-19)&lt;/p&gt;

&lt;p&gt;only reasonable alternative i can think of now is to run the hwiserver by spawning a new jvm (with a modified classpath that omits hadoop&apos;s jetty jars)&lt;/p&gt;</comment>
                            <comment id="12727039" author="appodictic" created="Fri, 3 Jul 2009 16:36:11 +0000"  >&lt;p&gt;Joydeep,&lt;br/&gt;
We do depend on Hadoop to provide Jetty. The rational was not to require extra or external libraries for the user. At the time Hive had just become its own project from being a Hadoop contrib project so it made sense to depend on Hadoop Jetty. We have a few other options. We can use a completely different Web Server. &lt;a href=&quot;http://tjws.sourceforge.net/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://tjws.sourceforge.net/&lt;/a&gt;. Now we have no conflicts. Or we can just build a war with no embedded type options. Even if we switch to tjws we still might end up using reflection since the API could change over time although we would chose when to upgrade the servlet engine, not hadoop. For now I will make a version that uses reflection to start up the server, since these changes are mostly cosmetic.&lt;/p&gt;</comment>
                            <comment id="12732929" author="appodictic" created="Sat, 18 Jul 2009 19:40:21 +0000"  >&lt;p&gt;Ok half way there. I added an abstraction to use reflection  to completely kick up the HWI Jetty Server. Right now I only added the code for jetty5 0.19.0. 20 soon. Is this what everyone had in mind?&lt;/p&gt;</comment>
                            <comment id="12733948" author="athusoo" created="Wed, 22 Jul 2009 01:49:08 +0000"  >&lt;p&gt;This sounds reasonable to me. Will go over the patch in more detail. Are you planning to upload another one soon or should I just review this one?&lt;/p&gt;</comment>
                            <comment id="12733973" author="appodictic" created="Wed, 22 Jul 2009 04:01:15 +0000"  >&lt;p&gt;This is going a little slow. The reflection aspect is pretty painful coding. I  think I am 98% percent complete. New test cases added. Hopefully I can have a final take in a day or two, sometimes its is hard to decide what exceptions to throw etc since there are very few design patters based around reflecting entire applications &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;/p&gt;</comment>
                            <comment id="12734609" author="appodictic" created="Thu, 23 Jul 2009 15:04:35 +0000"  >&lt;p&gt;This patch passes all unit tests. Also deployed and tested to a 0.19.0 cluster and a 0.20.0 cluster. &lt;/p&gt;

&lt;p&gt;Changes to hive-default.conf are to correct &lt;br/&gt;
hive-hwi.war&lt;br/&gt;
to&lt;br/&gt;
hive_hwi.war&lt;/p&gt;

&lt;p&gt;My changes to HWIServer.java clobbered other changes in the previous 0.20 patch since HWIServer.java delegates most duty to ServerWrapper.java&lt;/p&gt;</comment>
                            <comment id="12734674" author="namit" created="Thu, 23 Jul 2009 18:08:07 +0000"  >&lt;p&gt;Overall, it looks good - but can you do a simple cosmetic changes.&lt;/p&gt;

&lt;p&gt;1. Have 1 patch instead of different patches for jetty and 487&lt;br/&gt;
2. Add more comments:&lt;/p&gt;

&lt;p&gt;/**&lt;br/&gt;
				29 		 Hadoop 17-19 used Jetty5. Hadoop 20 uses jetty6. Hive still should compile and &lt;br/&gt;
...run with all versions.&lt;br/&gt;
				30 		 Java is strongly typed Class based language. The Reflection API is required to &lt;br/&gt;
...circumvent the strong &lt;br/&gt;
				31 		 typing. We have used the reflection API to deal with the known versions of Jetty &lt;br/&gt;
...we must work with. &lt;br/&gt;
				32 		 CS students: If you are ever in a debate about classless VS classful programming &lt;br/&gt;
...be sure to &lt;br/&gt;
				33 		 reference this code.&lt;br/&gt;
				34 		 */&lt;/p&gt;


&lt;p&gt;is very good:&lt;/p&gt;

&lt;p&gt;Can you repeat a subset of this in HadoopVersion also ?&lt;br/&gt;
Hive still should compile and &lt;br/&gt;
...run with all versions. (17-20)&lt;/p&gt;


&lt;p&gt;3. usesJobShell: can you add more comments here &amp;#8211; it is true for version 20 but not for 20 etc.&lt;br/&gt;
4. This may be outside the scope of this - but should some unit tests run for hadoop17, and some for hadoop 20, as part of ant test.&lt;br/&gt;
    Currently, all of them use the default 19. As I mentioned before, this can be done in a follow-up also.&lt;/p&gt;</comment>
                            <comment id="12734696" author="namit" created="Thu, 23 Jul 2009 18:45:45 +0000"  >&lt;p&gt;4. is not needed - we can enable 20 from hudson&lt;/p&gt;

&lt;p&gt;+1&lt;/p&gt;

&lt;p&gt;Can you add some more comments, and then I can commit it&lt;/p&gt;</comment>
                            <comment id="12734749" author="tlipcon" created="Thu, 23 Jul 2009 19:37:09 +0000"  >&lt;p&gt;A couple thoughts:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Does the same compiled jar truly work in all versions of Hadoop between 0.17 and 0.19? That is to say, can we consider an option in which we use some build.xml rules to, depending on the value of a hadoop.version variable, swap between two implementations of the same .java file (one compatible with Jetty 5, one with Jetty 6)? Then in the build product we could simply include two jars and have the wrapper scripts swap between them based on version. If size is a concern, the variant classes could be put in their own jar that would only be a few KB.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;The reflection code in this patch is pretty messy. I mocked up an idea for a slightly cleaner way to do it, and will attach it as a tarball momentarily. The idea is to define our own interfaces which have the same methods as we need to use in Jetty, and use a dynamic proxy to forward those invocations through to the actual implementation class. Dynamically choosing between the two interfaces is simple at runtime by simply checking that the method signatures correspond. This is still dirty (and a bad role model for CS students &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; ) but it should reduce the number of Class.forName and .getMethod calls in the wrapper class&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12734750" author="tlipcon" created="Thu, 23 Jul 2009 19:41:23 +0000"  >&lt;p&gt;Here&apos;s a tarball showing the technique mentioned in the comment above. The script &quot;run.sh&quot; will compile and run the example once with &quot;v1&quot; on the classpath, and a second time with &quot;v2&quot; on the classpath. I&apos;m not certain that this will cover all the cases that are needed for Jetty, but I figured I would throw it out there.&lt;/p&gt;</comment>
                            <comment id="12734781" author="appodictic" created="Thu, 23 Jul 2009 20:56:54 +0000"  >&lt;p&gt;@Todd - Where were you a few weeks ago? &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Then in the build product we could simply include two jars and have the wrapper scripts swap between them based on version
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The jars are upstream in Hadoop core. I did not look into this closely but the talk about &apos;Sealing exceptions&apos; above led me to believe I should not try this.&lt;/p&gt;

&lt;p&gt;I have wrapped my head around most of your Dynamic Proxy idea. My only concern is will the ant process cooperate? Will eclipse think the HWI classes are broken? Can we translate your run.sh into something ant/eclipse can deal with?&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;public class WebServer {
  public void someMethod(String arg) {
    System.out.println(&quot;Webserver v1: &quot; + arg);
  }
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I really don&apos;t want to have one &apos;someMethod&apos; per each Jetty method. Just start(), stop(), init(). I like your implementation, but this is such a &apos;hacky&apos; thing, I wonder is it worth thinking that hard? Hopefully the Jetty crew will be happy with their API for the next few years. Hopefully, we will not be supporting Hadoop 0.17.0 indefinitely. Honestly all that reflection has me &apos;burnt out&apos;.&lt;/p&gt;

&lt;p&gt;If you/we can tackle the ant/eclipse issues I would be happy to use the &apos;Dynamic Proxy&apos;, but maybe we tackle it in a different Jira because this is a pretty big blocker and I am sure many people want to see this in the trunk. &lt;/p&gt;</comment>
                            <comment id="12734795" author="tlipcon" created="Thu, 23 Jul 2009 21:25:06 +0000"  >&lt;blockquote&gt;&lt;p&gt;@Todd - Where were you a few weeks ago? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Chillin&apos; over on the HADOOP jira &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; We&apos;re gearing up for release of our distribution that includes Hadoop 0.20.0, so just started watching this one more carefully.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The jars are upstream in Hadoop core. I did not look into this closely but the talk about &apos;Sealing exceptions&apos; above led me to believe I should not try this.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Sorry, what I meant here is that the hive tarball would include lib/hive-0.4.0.jar, lib/jetty-shims/hive-jetty-shim-v6.jar and lib/jetty-shims/hive-jetty-shim-v5.jar&lt;/p&gt;

&lt;p&gt;In those jars we&apos;d have two different implementations of the shim. The hive wrapper script would then do something like:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
HADOOP_JAR=$HADOOP_HOME/hadoop*core*jar
&lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; [[ $HADOOP_JAR =~ 0.1[789] ]]; then
  JETTY_SHIM=lib/jetty-shims/jetty-shim-v5.jar
&lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt;
  JETTY_SHIM=lib/jetty-shims/jetty-shim-v6.jar
fi
CLASSPATH=$CLASSPATH:$JETTY_SHIM
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To generate the shim jars at compile time, we&apos;d compile two different JettyShim.java files - one against the v5 API, and one against the v6 API.&lt;/p&gt;

&lt;p&gt;As for eclipse properly completing/warning for the right versions for the right files, I haven&apos;t the foggiest idea. But I am pretty sure it&apos;s not going to warn if your reflective calls are broken either &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;My only concern is will the ant process cooperate?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don&apos;t see why not - my example build here is just to show how it works in a self contained way. The stuff inside v1-classes and v2-classes in the example are the equivalent of the two jetty jar versions - we don&apos;t have to compile them. The only code that has to compile is DynamicProxy.java which is completely normal code.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;If you/we can tackle the ant/eclipse issues I would be happy to use the &apos;Dynamic Proxy&apos;, but maybe we tackle it in a different Jira because this is a pretty big blocker and I am sure many people want to see this in the trunk. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;As for committing now and not worrying, that sounds pretty reasonable, as long as there&apos;s some kind of deprecation timeline set out. (e.g &quot;in Hive 0.5.0 we will drop support for versions of Hadoop that use Jetty v5&quot; or whatever). As someone who isn&apos;t a major Hive contributor, I&apos;ll defer to you guys completely &amp;#8211; I just wanted to throw the idea up on the JIRA.&lt;/p&gt;</comment>
                            <comment id="12734830" author="jsensarma" created="Thu, 23 Jul 2009 23:03:13 +0000"  >&lt;p&gt;need to add the shell script hack to switch the -libjars option as well based on the jar version.&lt;/p&gt;</comment>
                            <comment id="12734972" author="tlipcon" created="Fri, 24 Jul 2009 08:38:30 +0000"  >&lt;p&gt;Here&apos;s a patch which adds a project called &quot;shims&quot; with separate source directories for 0.17, 0.18, 0.19, and 0.20. Inside each there is an implementation of JettyShims and HadoopShims which encapsulate all of the version-dependent code. The build.xml is set in such a way that $&lt;/p&gt;
{hadoop.version}
&lt;p&gt; determines which one gets compiled.&lt;/p&gt;

&lt;p&gt;This probably needs a bit more javadoc before it&apos;s commitable, but I think it&apos;s worth considering this approach over reflection.&lt;/p&gt;

&lt;p&gt;Also, it seems like hadoop.version may be 0.18.0, 0.18.1, 0.18.2, etc. As long as it&apos;s kosher by Apache SVN standards, we should put a symlink for each of those versions in the shims/src/ directory pointing to 0.18, and same for the other minor releases. If symlinks aren&apos;t kosher, we need some way of parsing out the major version from within ant.&lt;/p&gt;

&lt;p&gt;Not being a regular contributor, I don&apos;t have a good test environment set up, but I&apos;ve verified that this at least builds in all of the above versions.&lt;/p&gt;</comment>
                            <comment id="12735772" author="athusoo" created="Mon, 27 Jul 2009 21:00:09 +0000"  >&lt;p&gt;I had added a GetVersionPref.java some time back to ant extensions in hive. It was later not used because we decided not to use preprocessing for 0.19 changes to validateInput and instead decided to rely on reflection. That can easily be resurrected.&lt;/p&gt;

&lt;p&gt;Let me look at this version as well. Also I am going to change this to a blocker as many people are waiting for this.&lt;/p&gt;</comment>
                            <comment id="12735773" author="athusoo" created="Mon, 27 Jul 2009 21:00:29 +0000"  >&lt;p&gt;changing to a blocker.&lt;/p&gt;</comment>
                            <comment id="12735783" author="athusoo" created="Mon, 27 Jul 2009 21:22:12 +0000"  >&lt;p&gt;Looks like a clean implementation. However, I do think that this will need some changes to the eclipse-templates to make it work with eclipse. We would want to conditionally add the src directory in shims corresponding to the proper version of hadoop to the eclipse launch templates in hive/eclipse-templates. Will try this out.&lt;/p&gt;</comment>
                            <comment id="12735787" author="athusoo" created="Mon, 27 Jul 2009 21:33:43 +0000"  >&lt;p&gt;Seems to not compile with 0.17.0 &lt;/p&gt;

&lt;p&gt;ant -Dhadoop.version=0.17.0 clean package ....&lt;/p&gt;


&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;ivy:retrieve&amp;#93;&lt;/span&gt;  1 artifacts copied, 0 already retrieved (14101kB/79ms)&lt;/p&gt;

&lt;p&gt;install-hadoopcore-internal:&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;untar&amp;#93;&lt;/span&gt; Expanding: /data/users/athusoo/commits/hive_trunk_ws9/.ptest_0/build/hadoopcore/hadoop-0.17.0.tar.gz into /data/users/athusoo/commits/hive_trunk_ws9/.ptest_0/build/hadoopcore&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;touch&amp;#93;&lt;/span&gt; Creating /data/users/athusoo/commits/hive_trunk_ws9/.ptest_0/build/hadoopcore/hadoop-0.17.0.installed&lt;/p&gt;

&lt;p&gt;compile:&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;echo&amp;#93;&lt;/span&gt; Compiling: shims&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;javac&amp;#93;&lt;/span&gt; Compiling 2 source files to /data/users/athusoo/commits/hive_trunk_ws9/.ptest_0/build/shims/classes&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;javac&amp;#93;&lt;/span&gt; /data/users/athusoo/commits/hive_trunk_ws9/.ptest_0/shims/src/0.17.0/java/org/apache/hadoop/hive/shims/HadoopShims.java:48: cannot find symbol&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;javac&amp;#93;&lt;/span&gt; symbol  : variable JobClient&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;javac&amp;#93;&lt;/span&gt; location: class org.apache.hadoop.hive.shims.HadoopShims&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;javac&amp;#93;&lt;/span&gt;     Configuration conf = JobClient.getCommandLineConfig();&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;javac&amp;#93;&lt;/span&gt;                          ^&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;javac&amp;#93;&lt;/span&gt; 1 error&lt;/p&gt;
</comment>
                            <comment id="12735789" author="tlipcon" created="Mon, 27 Jul 2009 21:39:05 +0000"  >&lt;p&gt;Woops, sorry about that. Simply add an import for o.a.h.mapred.JobClient and it compiles. New patch in a second&lt;/p&gt;</comment>
                            <comment id="12735791" author="tlipcon" created="Mon, 27 Jul 2009 21:39:56 +0000"  >&lt;p&gt;Fixes the missing import. Now compiles with hadoop.version=0.17.0&lt;/p&gt;</comment>
                            <comment id="12737126" author="athusoo" created="Thu, 30 Jul 2009 15:02:01 +0000"  >&lt;p&gt;Modified Todd&apos;s patch so that it compiles cleanly with 0.20 and 0.17 as well. I have also added support in this patch to generate the proper eclipse files and have verified that this works with eclipse. Additionally the directory names within in shims have been renamed to 0.17, 0.18, ... 0.20 instead of 0.17.0, .. 0.20.0&lt;/p&gt;

&lt;p&gt;Please take a look at this. Want to get this in as soon as possible so that we can move ahead with the branching.&lt;/p&gt;

&lt;p&gt;Joy was mentioning that an additional change to the cli shell script needs to be made for -libjars support. Joy, can you elaborate on that?&lt;/p&gt;</comment>
                            <comment id="12737191" author="tlipcon" created="Thu, 30 Jul 2009 17:34:10 +0000"  >&lt;p&gt;Patch looks good for me (just inspected it visually over here)&lt;/p&gt;

&lt;p&gt;One question: once we use these shims, is it possible that we could have just a single hive distribution which works for all versions of Hadoop? I think we may be able to accomplish this by making the shim jar output be libs/shims/hive_shims-{$hadoop.version.prefix}.jar. Then either through ClassLoader magic or shell wrapper magic, we put the right one on the classpath at runtime based on which hadoop version is on the classpath.&lt;/p&gt;

&lt;p&gt;Is this possible? Having different tarballs of hive for different versions of hadoop makes our lives slightly difficult for packaging.&lt;/p&gt;</comment>
                            <comment id="12737230" author="athusoo" created="Thu, 30 Jul 2009 18:57:24 +0000"  >&lt;p&gt;It would be ideal if we can make a single jar work with different hadoop versions through classloader magic. There are also some things that are needed in the hive cli script which would have to be abstracted away through a configuration/envrionment variable. Lets try to do that for the long term, but get this in for 0.4.0. Does that sound reasonable?&lt;/p&gt;
</comment>
                            <comment id="12737236" author="tlipcon" created="Thu, 30 Jul 2009 19:06:03 +0000"  >&lt;p&gt;Hi Ashish,&lt;/p&gt;

&lt;p&gt;That does sound reasonable, though I will likely take it on in the short term, as we will be distributing packages for hadoop-0.18 and hadoop-0.20 until the majority of the community and our customers have transitioned over. During that time period we&apos;d like to have a single &quot;hive&quot; package which will function with either. We can apply my work on top of the 0.4.0 release for our distribution, so it shouldn&apos;t block it, but I do think it would be nice if this feature were &quot;upstream&quot; in the Apache release.&lt;/p&gt;

&lt;p&gt;I&apos;ve got some time blocked off to work on this - if I get something working this week do you think it might be able to go into 0.4.0?&lt;/p&gt;

&lt;p&gt;-Todd&lt;/p&gt;</comment>
                            <comment id="12737239" author="athusoo" created="Thu, 30 Jul 2009 19:20:31 +0000"  >&lt;p&gt;sure. We can get it into 0.4.0. We can wait for your checkin before freezing on 0.4.0 but we would like to at least branch 0.4.0 this week (will have a vote out for it soon). All that means is that if we branch before your checkin, you will have to provide patches for trunk and 0.4.0. Is that ok?&lt;/p&gt;</comment>
                            <comment id="12737258" author="tlipcon" created="Thu, 30 Jul 2009 19:59:08 +0000"  >&lt;p&gt;Yep, that&apos;s fine. I&apos;m a git user, branches don&apos;t faze me &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="12737263" author="jsensarma" created="Thu, 30 Jul 2009 20:22:40 +0000"  >&lt;p&gt;attached the previous version with modifications for cli.sh. these modifications are required (even though they don&apos;t fix the problem entirely - see below).&lt;/p&gt;

&lt;p&gt;the reason i am not able to fix the problem entirely is because -libjars is no longer processed automatically by RunJar. We have to convert CliDriver to implement &apos;Tool&apos; interface for this to happen. this is easy - but i would rather not hold up things for that.&lt;/p&gt;

&lt;p&gt;I would suggest incorporating the patch as such - open a new jira for auxlibs/auxpath not working in 0.4/trunk and fix it there.&lt;/p&gt;</comment>
                            <comment id="12737268" author="jsensarma" created="Thu, 30 Jul 2009 20:37:09 +0000"  >&lt;p&gt;on second thoughts - there&apos;s some code in execdriver that i need to call from CliDriver and things should work. will upload another one soon.&lt;/p&gt;</comment>
                            <comment id="12737345" author="jsensarma" created="Thu, 30 Jul 2009 22:59:18 +0000"  >&lt;p&gt;one more:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;--auxpath works in both 19 and 20 now. auxlib should also work - i haven&apos;t tested it separately&lt;/li&gt;
	&lt;li&gt;removed -libjars for hadoop versions 20 and above from cli shell script. changes to CliDriver to add aux jars to classpath at runtime&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;note that hive server and hwi don&apos;t work with auxpath/lib in 20 and above (since that would also require non trivial changes to HWIServer and HiveServer). we can fix this as a followon (in case someone is using the two in combination - which seems doubtful).&lt;/p&gt;

&lt;p&gt;please review changes to bin/ext/cli.sh and CliDriver &lt;/p&gt;</comment>
                            <comment id="12737774" author="namit" created="Fri, 31 Jul 2009 23:40:34 +0000"  >&lt;p&gt;   &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; Begin query: alter2.q&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; diff -a -I &amp;#40;&lt;a href=&quot;file:)&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;file:&amp;amp;#41;\&lt;/a&gt;|&amp;#40;/tmp/.*&amp;#41; /data/users/njain/hive_commit1/hive_commit1/build/ql/test/logs/clientpositive/alter2.q.out /data/users/njain/hive_commit1/hive_commit1/ql/src/test/results/clientpositive/alter2.q.out&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; Done query: alter2.q&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; Begin query: alter3.q&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; plan = /tmp/plan60193.xml&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; java.lang.NoClassDefFoundError: org/apache/hadoop/hive/shims/HadoopShims&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; 	at org.apache.hadoop.hive.ql.exec.ExecDriver.initializeFiles(ExecDriver.java:95)&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; 	at org.apache.hadoop.hive.ql.exec.ExecDriver.execute(ExecDriver.java:358)&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; 	at org.apache.hadoop.hive.ql.exec.ExecDriver.main(ExecDriver.java:571)&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; 	at java.lang.reflect.Method.invoke(Method.java:597)&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; 	at org.apache.hadoop.util.RunJar.main(RunJar.java:165)&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; 	at org.apache.hadoop.mapred.JobShell.run(JobShell.java:54)&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; 	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; 	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; 	at org.apache.hadoop.mapred.JobShell.main(JobShell.java:68)&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hive.shims.HadoopShims&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; 	at java.net.URLClassLoader$1.run(URLClassLoader.java:200)&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; 	at java.security.AccessController.doPrivileged(Native Method)&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:188)&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; 	at java.lang.ClassLoader.loadClass(ClassLoader.java:306)&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; 	at java.lang.ClassLoader.loadClass(ClassLoader.java:251)&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; 	at java.lang.ClassLoader.loadClassInternal(ClassLoader.java:319)&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; 	... 12 more&lt;/p&gt;




&lt;p&gt;Most of the tests are failing&lt;/p&gt;</comment>
                            <comment id="12738571" author="tlipcon" created="Mon, 3 Aug 2009 20:38:44 +0000"  >&lt;p&gt;Taking a look at these failing tests now... any chance someone could hop on IRC in ##hive on freenode? I&apos;m happy to do the work, but would appreciate having someone to hit with quick questions since I&apos;m not too familiar with the code base.&lt;/p&gt;</comment>
                            <comment id="12738596" author="tlipcon" created="Mon, 3 Aug 2009 21:15:46 +0000"  >&lt;p&gt;The issue turned out to be that the shim classes weren&apos;t getting built into hive_exec.jar, which seems to include the built classes of many other of the components. I&apos;m not entirely sure why this is designed like this (why not just have hive_exec.jar add the other jars to its own classloader at startup?) but including build/shims/classes in there fixed the tests. Attaching new patch momentarily&lt;/p&gt;</comment>
                            <comment id="12738613" author="jsensarma" created="Mon, 3 Aug 2009 21:30:59 +0000"  >&lt;p&gt;hive_exec is the one that&apos;s submitted to hadoop to execute the map-reduce jobs. so we bundle all the required classes in it up front.&lt;/p&gt;

&lt;p&gt;it could be done differently (using libjars) - but was the path of least resistance at the start.&lt;/p&gt;</comment>
                            <comment id="12738844" author="tlipcon" created="Tue, 4 Aug 2009 05:49:30 +0000"  >&lt;p&gt;Attaching a new patch which makes the shim behavior happen at runtime. Here&apos;s the general idea:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;the shims/build.xml now uses Ivy to download tarballs for Hadoop 17, 18, 19, and 20. It builds each of the shim sources (from src/0.XX/), which have now been renamed so that each classname is unique (eg Hadoop20Shims.class).&lt;/li&gt;
	&lt;li&gt;The results of all of these builds end up in a single hive_shims.jar&lt;/li&gt;
	&lt;li&gt;Instead of being classes with all static methods, the shim classes are now non-static and are instantiated using ShimLoader.class, in a new shims/src/common/ directory&lt;/li&gt;
	&lt;li&gt;ShimLoader simply uses o.a.h.util.VersionInfo to determine the current version info, and reflection to instantiate the proper shims for the current version.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I&apos;ve tested this against pseudodistributed 18 and 20 clusters and it seemed to work. Unit tests also appear to work, though I haven&apos;t had a chance to let them run all the way through. I have not tested HWI at all as of yet.&lt;/p&gt;

&lt;p&gt;Still TODO:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;I may have broken eclipse integration somewhat. I&apos;m hoping someone who uses Eclipse can twiddle the necessary stuff there.&lt;/li&gt;
	&lt;li&gt;I would appreciate a review of the javadocs for the HadoopShims interface. I don&apos;t know the specifics of some of the 17 behavior, so my docs are lame and vague.&lt;/li&gt;
	&lt;li&gt;I think build.xml needs to be modified just a bit more so that the output directory/tarball no longer includes $
{hadoop.version}
&lt;p&gt; in it. Additionally there are one or two ant conditionals based on hadoop version - I haven&apos;t had a chance to investigate them, but they should probably be removed&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;I think we should have a policy that hadoop.version defaults to the most recently released apache trunk - right now it defaults to 0.19.&lt;/li&gt;
	&lt;li&gt;To compile the shims we&apos;re downloading the entire release tarballs off the apache mirror. Would be nicer if we could just download the specific jars we need to compile against, but that might be a pipe dream.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12738851" author="jsensarma" created="Tue, 4 Aug 2009 06:22:26 +0000"  >&lt;p&gt;hey - how do i apply this git diff using patch?&lt;/p&gt;</comment>
                            <comment id="12738855" author="tlipcon" created="Tue, 4 Aug 2009 06:30:31 +0000"  >&lt;p&gt;Normal patch -p0 ought to work:&lt;/p&gt;

&lt;p&gt;todd@todd-laptop:~/cloudera/cdh/repos/hive$ patch -p0 &amp;lt; /tmp/hive-487-runtime.patch &lt;br/&gt;
patching file ant/build.xml&lt;br/&gt;
patching file bin/ext/cli.sh&lt;br/&gt;
patching file build-common.xml&lt;br/&gt;
patching file build.xml&lt;br/&gt;
etc...&lt;/p&gt;

&lt;p&gt;(from a clean trunk checkout)&lt;/p&gt;</comment>
                            <comment id="12738864" author="jsensarma" created="Tue, 4 Aug 2009 06:47:29 +0000"  >&lt;p&gt;my bad .. code looks pretty clean.&lt;/p&gt;

&lt;p&gt;one concern is the stuff that u mentioned already - that all hadoop versions need to be downloaded. in particular - sometime back i had made some fixes to allow hive to compile against a specific hadoop tree (see &lt;a href=&quot;http://bit.ly/2d4Ch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://bit.ly/2d4Ch&lt;/a&gt;). but this would be reverting that i imagine.&lt;/p&gt;</comment>
                            <comment id="12739233" author="namit" created="Tue, 4 Aug 2009 23:11:12 +0000"  >&lt;p&gt;Committed. Thanks Todd&lt;/p&gt;</comment>
                            <comment id="12739356" author="zshao" created="Wed, 5 Aug 2009 07:09:53 +0000"  >&lt;p&gt;When I try to start Hive with hadoop built by myself, I saw an exception:&lt;/p&gt;

&lt;p&gt;java.lang.RuntimeException: Illegal Hadoop Version: Unknown (expected A.B.* format)&lt;br/&gt;
        at org.apache.hadoop.hive.shims.ShimLoader.getMajorVersion(ShimLoader.java:101)&lt;br/&gt;
        at org.apache.hadoop.hive.shims.ShimLoader.loadShims(ShimLoader.java:80)&lt;br/&gt;
        at org.apache.hadoop.hive.shims.ShimLoader.getHadoopShims(ShimLoader.java:62)&lt;br/&gt;
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:226)&lt;br/&gt;
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)&lt;br/&gt;
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)&lt;br/&gt;
        at java.lang.reflect.Method.invoke(Method.java:597)&lt;br/&gt;
        at org.apache.hadoop.util.RunJar.main(RunJar.java:166)&lt;br/&gt;
        at org.apache.hadoop.mapred.JobShell.run(JobShell.java:194)&lt;br/&gt;
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)&lt;br/&gt;
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)&lt;br/&gt;
        at org.apache.hadoop.mapred.JobShell.main(JobShell.java:220)&lt;/p&gt;

&lt;p&gt;I guess I need to specify some Hadoop version information when compiling hadoop?&lt;/p&gt;</comment>
                            <comment id="12739556" author="tlipcon" created="Wed, 5 Aug 2009 15:11:38 +0000"  >&lt;p&gt;Hi Zheng,&lt;/p&gt;

&lt;p&gt;There is some kind of bug I&apos;ve seen before in Hadoop&apos;s build process where the version info doesn&apos;t get generated on your first compile. It&apos;s silly, but try running &apos;ant package&apos; a second time in your Hadoop build tree? Running &quot;hadoop version&quot; should let you know whether the version info got compiled in.&lt;/p&gt;

&lt;p&gt;-Todd&lt;/p&gt;</comment>
                            <comment id="12739636" author="zshao" created="Wed, 5 Aug 2009 17:29:51 +0000"  >&lt;p&gt;By doing &quot;ant ... package package&quot; I am able to generate the hadoop distribution with version information, and Hive runs fine with it now! Thanks, Todd!&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12432308">HIVE-726</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12408992" name="HIVE-487-2.patch" size="5248" author="justinlynn" created="Tue, 26 May 2009 04:53:21 +0000"/>
                            <attachment id="12408731" name="HIVE-487.patch" size="40623" author="justinlynn" created="Thu, 21 May 2009 19:47:13 +0000"/>
                            <attachment id="12414370" name="dynamic-proxy.tar.gz" size="2104" author="tlipcon" created="Thu, 23 Jul 2009 19:41:23 +0000"/>
                            <attachment id="12414180" name="hive-487-jetty-2.diff" size="17723" author="appodictic" created="Wed, 22 Jul 2009 04:01:15 +0000"/>
                            <attachment id="12414345" name="hive-487-jetty.patch" size="17445" author="appodictic" created="Thu, 23 Jul 2009 15:04:35 +0000"/>
                            <attachment id="12415460" name="hive-487-runtime.patch" size="57211" author="tlipcon" created="Tue, 4 Aug 2009 05:49:30 +0000"/>
                            <attachment id="12415069" name="hive-487-with-cli-changes.2.patch" size="40828" author="jsensarma" created="Thu, 30 Jul 2009 22:59:18 +0000"/>
                            <attachment id="12415410" name="hive-487-with-cli-changes.3.patch" size="42478" author="tlipcon" created="Mon, 3 Aug 2009 21:17:00 +0000"/>
                            <attachment id="12415056" name="hive-487-with-cli-changes.patch" size="38852" author="jsensarma" created="Thu, 30 Jul 2009 20:22:40 +0000"/>
                            <attachment id="12409779" name="hive-487.3.patch" size="9135" author="jsensarma" created="Wed, 3 Jun 2009 15:07:49 +0000"/>
                            <attachment id="12412175" name="hive-487.4.patch" size="9107" author="jsensarma" created="Tue, 30 Jun 2009 15:08:14 +0000"/>
                            <attachment id="12414667" name="hive-487.txt" size="34899" author="tlipcon" created="Mon, 27 Jul 2009 21:39:56 +0000"/>
                            <attachment id="12414418" name="hive-487.txt" size="34883" author="tlipcon" created="Fri, 24 Jul 2009 08:38:30 +0000"/>
                            <attachment id="12413932" name="jetty-patch.patch" size="8110" author="appodictic" created="Sat, 18 Jul 2009 19:40:21 +0000"/>
                            <attachment id="12408732" name="junit-patch1.html" size="380622" author="justinlynn" created="Thu, 21 May 2009 19:47:13 +0000"/>
                            <attachment id="12415026" name="patch-487.txt" size="38040" author="athusoo" created="Thu, 30 Jul 2009 15:02:01 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>16.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 21 May 2009 19:47:13 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>73501</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 18 weeks, 2 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0la1r:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>122278</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>