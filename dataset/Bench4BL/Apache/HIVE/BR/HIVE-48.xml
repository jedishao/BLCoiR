<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sat Dec 03 00:27:41 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HIVE-48/HIVE-48.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HIVE-48] Support JDBC connections for interoperability between Hive and RDBMS</title>
                <link>https://issues.apache.org/jira/browse/HIVE-48</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;In many DW and BI systems, the data are stored in RDBMS for now such as oracle, mysql, postgresql ... for reporting, charting and etc.&lt;br/&gt;
It would be useful to be able to import data from RDBMS and export data to RDBMS using JDBC connections.&lt;br/&gt;
If Hive support JDBC connections, It wll be much easier to use 3rd party DW/BI tools.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12403853">HIVE-48</key>
            <summary>Support JDBC connections for interoperability between Hive and RDBMS</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="rsm">Raghotham Murthy</assignee>
                                    <reporter username="warwithin">YoungWoo Kim</reporter>
                        <labels>
                    </labels>
                <created>Sun, 7 Sep 2008 15:40:50 +0000</created>
                <updated>Sat, 17 Dec 2011 00:09:04 +0000</updated>
                            <resolved>Tue, 6 Jan 2009 02:07:16 +0000</resolved>
                                                    <fixVersion>0.3.0</fixVersion>
                                    <component>JDBC</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>9</watches>
                                                                <comments>
                            <comment id="12629267" author="athusoo" created="Mon, 8 Sep 2008 19:33:44 +0000"  >&lt;p&gt;completely agree on this. With a jdbc driver the front end integration would be much easier.&lt;/p&gt;</comment>
                            <comment id="12629269" author="athusoo" created="Mon, 8 Sep 2008 19:44:10 +0000"  >&lt;p&gt;Also I wanted to add that we have tried to structure the Driver code in such a way that we follow the execute/fetch paradgm that is followed by JDBC drivers - though admittedly the metadata part of jdbc is harder than the data part.&lt;/p&gt;

&lt;p&gt;Also Raghu was looking into creating a simple jdbc driver for hive. We should add that to the hive roadmap wiki.&lt;/p&gt;</comment>
                            <comment id="12629280" author="rsm" created="Mon, 8 Sep 2008 20:28:12 +0000"  >&lt;p&gt;I had already added it to the roadmap. Regarding the simple jdbc driver, I will submit a patch next week.&lt;/p&gt;</comment>
                            <comment id="12642581" author="hammer" created="Sat, 25 Oct 2008 00:57:39 +0000"  >&lt;p&gt;Raghu, any progress on the JDBC driver?&lt;/p&gt;</comment>
                            <comment id="12642860" author="rsm" created="Mon, 27 Oct 2008 06:34:39 +0000"  >&lt;p&gt;I had a preliminary set of classes. I didnt get a chance to finish working on them though. Michi has now taken those classes and I believe he has something working now. I&apos;ll let him post a patch.&lt;/p&gt;</comment>
                            <comment id="12642863" author="michim" created="Mon, 27 Oct 2008 06:47:48 +0000"  >&lt;p&gt;Added a JDBC driver for hive. Look at src/contrib/hive/ql/src/test/org/apache/hadoop/hive/ql/jdbc/TestHiveDriver.java for example.&lt;/p&gt;

&lt;p&gt;Next steps:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;provide a hive standalone server&lt;/li&gt;
	&lt;li&gt;integrate with hive metastore (e.g. support different types)&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12642869" author="hammer" created="Mon, 27 Oct 2008 07:17:17 +0000"  >&lt;p&gt;Nice, Michi! Will poke at this tomorrow.&lt;/p&gt;</comment>
                            <comment id="12642983" author="namit" created="Mon, 27 Oct 2008 17:59:43 +0000"  >&lt;p&gt;Michi, did you consider having a client-server approach for the JDBC server ? There is nothing wrong with this approach - infact, this way, the server does not become a single point of failure. &lt;br/&gt;
The client does become thicker, which may be acceptable. I just wanted to know did you consider the pros-cons of that approach.&lt;/p&gt;</comment>
                            <comment id="12642994" author="namit" created="Mon, 27 Oct 2008 18:14:30 +0000"  >&lt;p&gt;The Driver API has changed - it is now integrated with the serde and returns a vector&amp;lt;string&amp;gt; instead of vector&amp;lt;vector&amp;lt;string&amp;gt;&amp;gt; wrongly.&lt;br/&gt;
That needs to be changed also.&lt;/p&gt;</comment>
                            <comment id="12643000" author="jsensarma" created="Mon, 27 Oct 2008 18:30:12 +0000"  >&lt;p&gt;how are we planning on picking up the hadoop and hive configuration file? (the cli picks them up through the classpath). the same concern applies to jar files (there&apos;s configuration in the cli shell script to set it up to include jars in auxlib).&lt;/p&gt;

&lt;p&gt;We will need a client-server model. the cli does not, for example, run on cygwin/windows and there are all manner of pathing issues that we would need to fix to make that work. within facebook - we won&apos;t even be able to access hdfs directly from windows agents that are outside the secure zone (only http ports are available i believe). i verified from Dhruba that this is the case in yahoo as well. so - we just can&apos;t run queries directly from windows machines without a server side that is within the secure zone.&lt;/p&gt;</comment>
                            <comment id="12643014" author="jsensarma" created="Mon, 27 Oct 2008 18:53:42 +0000"  >&lt;p&gt;ok - Ashish just walked us through a couple of scenarios:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;BI tool has server side. in this case the approach in this patch might work - but the concern about setting up classpaths and the suitability of running hadoop code setting classloaders and stuff on the same JVM as the BI server is suspect. At the minimum this has significant integration issues for each BI server.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;BI tool does not have a server side - only a client. I think this is a very common scenario and something which we should try to cover (since the whole premise of hadoop/hive is to avoid spending a lot of money - which is what BI tools with server side will require). In this case - the approach in this patch will be hard to make work because of firewalling issues that i had mentioned in the previous post (even if all the technical issues like hive treatment of windows paths are resolved).&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;hopefully this captures the issues more accurately.&lt;/p&gt;</comment>
                            <comment id="12643036" author="prasadc" created="Mon, 27 Oct 2008 20:04:33 +0000"  >&lt;p&gt;There is already a MetaStore server (HiveMetaStore.java). It is a thrift service so I am not sure it would fit requirements for JDBC server. If it does, we should add JDBC functionality to this server. &lt;/p&gt;</comment>
                            <comment id="12643400" author="michim" created="Wed, 29 Oct 2008 03:09:22 +0000"  >&lt;p&gt;I talked about this with Ragho.&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;The next step is to separate client from the server.&lt;/li&gt;
	&lt;li&gt;I&apos;ll check if we can use thrift to implement JDBC server/client.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;--Michi&lt;/p&gt;</comment>
                            <comment id="12644812" author="namit" created="Mon, 3 Nov 2008 21:29:49 +0000"  >&lt;p&gt;Hi Michi, Any updates on this. If you want to meet to discuss in more detail, we can also meet&lt;/p&gt;</comment>
                            <comment id="12644829" author="rsm" created="Mon, 3 Nov 2008 22:20:23 +0000"  >&lt;p&gt;Michi and I were discussing this over the weekend. Here&apos;s our current thinking about the design. Michi, pls confirm.&lt;/p&gt;

&lt;p&gt;1. implement a thrift client/server for hive. for now, the interface consists only of execute and fetch_row. we were able to setup the framework with a thrift server and a java client which talks to the server. next step is to get the server to run the queries. &lt;br/&gt;
notes: we looked at the metastore code and thought it might be simpler to first implement a separate thrift client/server before merging it with the metastore. some installations might want to have separate instances of metastore and hive server. and, its easier to test a smaller interface where we understand the code. also, metastore code seems to have classes which arent being used at all and the scripts to start/stop metastore dont really work in non-facebook installations (need to file separate jiras for those).&lt;/p&gt;

&lt;p&gt;2. build a jdbc interface which makes calls to the generated java thrift client. we could also have python and perl dbi interfaces which can be make calls to the generated thrift client code in those languages. so, the thrift interface is a generic interface which is not specific to any particular standard (jdbc/dbi etc).&lt;/p&gt;

&lt;p&gt;3. the directory structure in the code would be as follows in src/contrib/hive. it follows a similar model to metastore.&lt;/p&gt;

&lt;p&gt;service/if/hive_service.thrift&lt;br/&gt;
service/include/&amp;lt;headers from thrift&amp;gt;&lt;br/&gt;
service/fb303/&amp;lt;scripts for service_ctrl to manage server&amp;gt;&lt;br/&gt;
service/src/gen-javabean/&amp;lt;generated java code&amp;gt;&lt;br/&gt;
service/src/gen-php/&amp;lt;generated php&amp;gt;&lt;br/&gt;
service/src/gen-py/&amp;lt;generated python&amp;gt;&lt;br/&gt;
service/src/gen-perl/&amp;lt;generated perl&amp;gt;&lt;br/&gt;
service/src/scripts/&amp;lt;ctrl scripts for server&amp;gt;&lt;br/&gt;
service/src/java/org/apache/hadoop/hive/service/HiveServer.java&lt;br/&gt;
service/src/java/org/apache/hadoop/hive/service/HiveClient.java&lt;br/&gt;
jdbc/src/java/org/apache/hadoop/hive/jdbc/&amp;lt;whatever is in current jdbc patch&amp;gt;&lt;br/&gt;
dbi/&amp;lt;perl dbi interface calling service/src/gen-perl&amp;gt;&lt;br/&gt;
cli/&amp;lt;changed to use HiveClient or HiveJdbc&amp;gt;&lt;/p&gt;

&lt;p&gt;4. next steps&lt;br/&gt;
a. get server to run queries and return results to client.&lt;br/&gt;
b. move ql/Driver.java to service since the actual running of the query is not really part of the query language.&lt;br/&gt;
c. change cli to use the service&lt;br/&gt;
d. verify which parts of the metastore interface are needed by jdbc and move/copy over parts to hive_service - i dont think it makes sense to do it the other way around i.e. put the hive service into metastore since metastore is not the right abstraction to actually run queries.&lt;br/&gt;
e. there is common thrift code in metastore and service. we should either move it to a seprate thrift directory or make metastore use stuff from service.&lt;/p&gt;

&lt;p&gt;It will be good to meet up to discuss them in more detail. I&apos;ll let Michi provide a patch for the hive server/client and jdbc wrappers for the hive client.&lt;/p&gt;</comment>
                            <comment id="12644835" author="prasadc" created="Mon, 3 Nov 2008 22:31:28 +0000"  >&lt;p&gt;Regarding unused files in metastore, these are the files that got carried over hive prototype which used file based metastore. We left them there in case some one wants to use file based metastore. So in a sense they are useful and there are tests.&lt;/p&gt;

&lt;p&gt;I think we should combine the servers now. It will be difficult and time consuming to merge them later. Advanced users can still have two installations of the same server but direct metadata calls to one server and data calls to another server. But the default case, there will be only one server and easier for maintenance.&lt;/p&gt;

&lt;p&gt;Only issue I see is that metastore code is independent of ql/cli code. So it might be better to build JDBC server on top of metastore server (ie extend metastore server) and import metastore thrift IDL into service thrift IDL. So the JDBC service would be a superset of metastore functionality.&lt;/p&gt;

&lt;p&gt;What do you guys think?&lt;/p&gt;</comment>
                            <comment id="12644837" author="michim" created="Mon, 3 Nov 2008 22:39:17 +0000"  >&lt;p&gt;Ragho: I confirm.&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;I should be able to finish implementing HiveServer.java/HiveClient.java by the end of this week (maybe by Sunday). As Ragho said, right now we have only 2 methods: void execute(String query) and list&amp;lt;String&amp;gt; fetch_row().&lt;/li&gt;
	&lt;li&gt;After that, I will modify the JDBC driver to use HiveClient.&lt;/li&gt;
	&lt;li&gt;Command line interface can use either HiveClient or JDBC driver.&lt;/li&gt;
	&lt;li&gt;I&apos;m usually available after 7 on tue-fri.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;--Michi&lt;/p&gt;</comment>
                            <comment id="12645110" author="athusoo" created="Tue, 4 Nov 2008 23:38:25 +0000"  >&lt;p&gt;How are you planning to implement the metadata calls. There is a lot of inheritance in the JDBC metadata calls and from what I understand, thrift does not support inheritance.&lt;/p&gt;

&lt;p&gt;Also, if you do go the thrift route, it may be better to share the server container code between the metastore and the JDBC driver, the apis I think should be independent and should be kept separate. While reorganizing the code, it may be worthwhile to put the server portion of it in common and then share it between the metastore and service..&lt;/p&gt;</comment>
                            <comment id="12645292" author="michim" created="Wed, 5 Nov 2008 18:55:03 +0000"  >&lt;p&gt;I was thinking the JDBC driver will be of type 4:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/JDBC_driver#Type_4_Driver_-_Native-Protocol_Driver&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/JDBC_driver#Type_4_Driver_-_Native-Protocol_Driver&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;which means there is a server &amp;lt;--&amp;gt; client api that is independent of JDBC, and JDBC driver uses the client api. &lt;/p&gt;

&lt;p&gt;We should meet up to make sure we are all on the same page. Ragho, can you set up a meeting?&lt;/p&gt;

&lt;p&gt;--Michi&lt;/p&gt;</comment>
                            <comment id="12646202" author="rsm" created="Mon, 10 Nov 2008 08:34:56 +0000"  >&lt;p&gt;I am not sure I understand what Ashish meant by &apos;inheritance in JDBC metadata calls&apos;. The plan is to include metastore.thrift in hive_service.thrift and then hive_service will just forward metadata calls to the metastore code. I guess with inheritance we wouldnt have to implement the forwarding functions. Is this what you mean Ashish?&lt;/p&gt;

&lt;p&gt;And yes, we should have a single implementation of the thrift server container for HiveServer and Metastore. JDBC would then be a wrapper on top of the thrift hive client.&lt;/p&gt;

&lt;p&gt;update: step 4a above has been completed - can now issue queries via HiveClient and retrieve results. HiveServer - a thrift server - actually runs the queries via ql/Driver.  I am attaching the patch with the code for the thrift server/client.&lt;/p&gt;

&lt;p&gt;We should meet up to figure out what the plan is for the JDBC client.&lt;/p&gt;</comment>
                            <comment id="12646323" author="athusoo" created="Mon, 10 Nov 2008 18:45:24 +0000"  >&lt;p&gt;Type 4 should work I guess.&lt;/p&gt;

&lt;p&gt;I guess if you use that then you can sidestep the inheritance stuff that I was alluding to. Basically my concern was that if the server APIs mimicked the javax.sql APIs then inheritance would be a problem.&lt;/p&gt;

&lt;p&gt;Can you guys come over tomorrow sometime in the afternoon?&lt;/p&gt;</comment>
                            <comment id="12646326" author="michim" created="Mon, 10 Nov 2008 18:59:40 +0000"  >&lt;p&gt;7pm?&lt;/p&gt;

&lt;p&gt;--Michi&lt;/p&gt;</comment>
                            <comment id="12646348" author="namit" created="Mon, 10 Nov 2008 20:13:47 +0000"  >&lt;p&gt;7pm is fine with me &lt;/p&gt;</comment>
                            <comment id="12646623" author="athusoo" created="Tue, 11 Nov 2008 18:47:39 +0000"  >&lt;p&gt;yes just come over at 7pm.&lt;/p&gt;

&lt;p&gt;Also hive mailing list have changed to&lt;/p&gt;

&lt;p&gt;hive-user@hadoop.apache.org&lt;br/&gt;
hive-dev@hadoop.apache.org&lt;br/&gt;
hive-commits@hadoop.apache.org&lt;/p&gt;

&lt;p&gt;So you may want to subscribe to those (this is because hive is in the process of becoming a subproject under hadoop).&lt;/p&gt;</comment>
                            <comment id="12652734" author="michim" created="Wed, 3 Dec 2008 10:40:22 +0000"  >&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;Attached hadoop-4101.3.patch&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;A temporary patch for jdbc support. You need to apply the patch from &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-73&quot; title=&quot;Thrift Server and Client for Hive&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-73&quot;&gt;&lt;del&gt;HIVE-73&lt;/del&gt;&lt;/a&gt; before using this patch. See jdbc/src/test/org/apache/hadoop/hive/jdbc/TestHiveDriver.java for supported methods. &lt;/p&gt;</comment>
                            <comment id="12652937" author="rsm" created="Wed, 3 Dec 2008 20:00:31 +0000"  >&lt;p&gt;This patch doesnt seem to work. guess you need to upload a patch which adds the jdbc directory. also, can you generate a patch by running &apos;git diff --no-prefix&apos;. That will allow us to apply the patch with patch -p0.&lt;/p&gt;</comment>
                            <comment id="12652995" author="michim" created="Wed, 3 Dec 2008 22:04:51 +0000"  >&lt;p&gt;Previous patch was not working. Giving another try.&lt;/p&gt;</comment>
                            <comment id="12653387" author="appodictic" created="Thu, 4 Dec 2008 18:00:53 +0000"  >&lt;p&gt;A few people developing servers Thrift/JDBC need to modify the bin/hive script. &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-107&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;HIVE-107&lt;/a&gt; what you think of this idea and if it works for you.&lt;/p&gt;</comment>
                            <comment id="12658006" author="rsm" created="Fri, 19 Dec 2008 04:45:53 +0000"  >&lt;p&gt;Fixed url parsing. Also added standalone server option for testing.&lt;/p&gt;</comment>
                            <comment id="12660282" author="rsm" created="Fri, 2 Jan 2009 09:24:40 +0000"  >&lt;p&gt;I have added functionality to getByte, getBoolean, getDouble, getFloat, getInt, getLong, getObject, getShort, and changed getString to do a toString.&lt;/p&gt;

&lt;p&gt;Also added a single test to test getInt. Right now, anything other than select * uses MetaDataTypedColumnSetSerDe - so, the result schema has all columns as strings. Will add more tests to jdbc once we start using DynamicSerDe for all queries.&lt;/p&gt;</comment>
                            <comment id="12660774" author="athusoo" created="Mon, 5 Jan 2009 14:49:08 +0000"  >&lt;p&gt;A few questions:&lt;br/&gt;
1. What is the motivation for Driver.java:87 change?&lt;br/&gt;
2. In JdbcSessionState.java what are execString and fileName variables used for?&lt;br/&gt;
3. Shouldn&apos;t HiveResultSetMetadata.java be doing something?&lt;br/&gt;
4. In HiveResultSet.java:436 the getDouble function is missing TODO comment? Same is true for getFloat and getInt.&lt;br/&gt;
5. For the non implemented functions should we through a non implemented run time exception instead of just returning 0.&lt;/p&gt;

</comment>
                            <comment id="12660854" author="rsm" created="Mon, 5 Jan 2009 18:55:22 +0000"  >&lt;p&gt;&amp;gt; 1. What is the motivation for Driver.java:87 change?&lt;/p&gt;

&lt;p&gt;The reason was that there is currently no way to retrieve the result table name in the server. Also, the driver is always returning the result of queries, so, &apos;result&apos; seems to be a reasonable name for the table. Ideally, we should have a function which converts a schema DDL to a Schema object and we should be able to query the schema object. Right now we just pass the schema string as is to the DynamicSerDe.&lt;/p&gt;

&lt;p&gt;&amp;gt; 2. In JdbcSessionState.java what are execString and fileName variables used for?&lt;/p&gt;

&lt;p&gt;Right now JdbcSessionState is a dummy class. Its not being used for anything. The plan is to use it later on. I just copied over the class from Cli. I can make it an empty class. &lt;/p&gt;

&lt;p&gt;&amp;gt; 3. Shouldn&apos;t HiveResultSetMetadata.java be doing something?&lt;/p&gt;

&lt;p&gt;The plan was to stage the JDBC implementation. There are a bunch of auto-generated classes which will be used later on.&lt;/p&gt;

&lt;p&gt;&amp;gt; 4. In HiveResultSet.java:436 the getDouble function is missing TODO comment? Same is true for getFloat and getInt.&lt;/p&gt;

&lt;p&gt;The TODO comment is inside the function body. Isnt that enough?&lt;/p&gt;

&lt;p&gt;&amp;gt; 5. For the non implemented functions should we throw a non implemented run time exception instead of just returning 0. &lt;/p&gt;

&lt;p&gt;return 0 was auto-generated. I&apos;ll change them to throw SQLException instead.&lt;/p&gt;</comment>
                            <comment id="12660941" author="rsm" created="Mon, 5 Jan 2009 21:45:45 +0000"  >&lt;p&gt;Two changes:&lt;br/&gt;
1. made JdbcSessionState a dummy class&lt;br/&gt;
2. now throwing SQLException for unimplemented functions.&lt;/p&gt;</comment>
                            <comment id="12660983" author="athusoo" created="Mon, 5 Jan 2009 23:15:52 +0000"  >&lt;p&gt;+1.&lt;/p&gt;

&lt;p&gt;Looks good to me.&lt;/p&gt;</comment>
                            <comment id="12661019" author="dhruba" created="Tue, 6 Jan 2009 01:30:05 +0000"  >&lt;p&gt;I get compilation problems:&lt;/p&gt;

&lt;p&gt;core-compile:&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;javac&amp;#93;&lt;/span&gt; Compiling 10 source files to /mnt/vol/devrs004.snc1/dhruba/commithive/build/jdbc/classes&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;javac&amp;#93;&lt;/span&gt; /mnt/vol/devrs004.snc1/dhruba/commithive/jdbc/src/java/org/apache/hadoop/hive/jdbc/HiveConnection.java:452: unreported exception java.sql.SQLException; must be caught or declared to be thrown&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;javac&amp;#93;&lt;/span&gt;     throw new SQLException(&quot;Method not supported&quot;);&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;javac&amp;#93;&lt;/span&gt;     ^&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;javac&amp;#93;&lt;/span&gt; /mnt/vol/devrs004.snc1/dhruba/commithive/jdbc/src/java/org/apache/hadoop/hive/jdbc/HiveConnection.java:462: unreported exception java.sql.SQLException; must be caught or declared to be thrown&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;javac&amp;#93;&lt;/span&gt;     throw new SQLException(&quot;Method not supported&quot;);&lt;/p&gt;</comment>
                            <comment id="12661028" author="rsm" created="Tue, 6 Jan 2009 01:48:13 +0000"  >&lt;p&gt;Oops. My method of generating diffs seems to be broken with git - this happened twice today &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; switching back to svn. Uploaded fixed patch.&lt;/p&gt;</comment>
                            <comment id="12661032" author="dhruba" created="Tue, 6 Jan 2009 02:07:16 +0000"  >&lt;p&gt;I just committed this. Thanks Raghu and Michi.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10032">
                    <name>Blocker</name>
                                                                <inwardlinks description="is blocked by">
                                        <issuelink>
            <issuekey id="12408847">HIVE-73</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12471478">HIVE-1536</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12392852" name="hadoop-4101.1.patch" size="201766" author="michim" created="Mon, 27 Oct 2008 06:47:48 +0000"/>
                            <attachment id="12393621" name="hadoop-4101.2.patch" size="519780" author="rsm" created="Mon, 10 Nov 2008 08:34:56 +0000"/>
                            <attachment id="12395175" name="hadoop-4101.3.patch" size="9235" author="michim" created="Wed, 3 Dec 2008 10:40:21 +0000"/>
                            <attachment id="12395218" name="hadoop-4101.4.patch" size="204997" author="michim" created="Wed, 3 Dec 2008 22:04:51 +0000"/>
                            <attachment id="12396464" name="hive-48.5.patch" size="206879" author="rsm" created="Fri, 19 Dec 2008 04:45:53 +0000"/>
                            <attachment id="12397014" name="hive-48.6.patch" size="201279" author="rsm" created="Fri, 2 Jan 2009 09:24:40 +0000"/>
                            <attachment id="12397149" name="hive-48.7.patch" size="235595" author="rsm" created="Mon, 5 Jan 2009 21:45:45 +0000"/>
                            <attachment id="12397168" name="hive-48.8.patch" size="235589" author="rsm" created="Tue, 6 Jan 2009 01:48:13 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>8.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Mon, 8 Sep 2008 19:33:44 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>73815</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 48 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0iukv:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>108048</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310192" key="com.atlassian.jira.plugin.system.customfieldtypes:textarea">
                        <customfieldname>Release Note</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>JDBC Driver</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>