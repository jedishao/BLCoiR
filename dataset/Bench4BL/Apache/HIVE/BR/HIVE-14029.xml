<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 02 06:00:17 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HIVE-14029/HIVE-14029.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HIVE-14029] Update Spark version to 2.0.0</title>
                <link>https://issues.apache.org/jira/browse/HIVE-14029</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;There are quite some new optimizations in Spark 2.0.0. We need to bump up Spark to 2.0.0 to benefit those performance improvements.&lt;br/&gt;
To update Spark version to 2.0.0, the following changes are required:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Spark API updates:
	&lt;ul&gt;
		&lt;li&gt;SparkShuffler#call return Iterator instead of Iterable&lt;/li&gt;
		&lt;li&gt;SparkListener -&amp;gt; JavaSparkListener&lt;/li&gt;
		&lt;li&gt;InputMetrics constructor doesn&#8217;t accept readMethod&lt;/li&gt;
		&lt;li&gt;Method remoteBlocksFetched and localBlocksFetched in ShuffleReadMetrics return long type instead of integer&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Dependency upgrade:
	&lt;ul&gt;
		&lt;li&gt;Jackson: 2.4.2 -&amp;gt; 2.6.5&lt;/li&gt;
		&lt;li&gt;Netty version: 4.0.23.Final -&amp;gt; 4.0.29.Final&lt;/li&gt;
		&lt;li&gt;Scala binary version: 2.10 -&amp;gt; 2.11&lt;/li&gt;
		&lt;li&gt;Scala version: 2.10.4 -&amp;gt; 2.11.8&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;

</description>
                <environment></environment>
        <key id="12979634">HIVE-14029</key>
            <summary>Update Spark version to 2.0.0</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="Ferd">Ferdinand Xu</assignee>
                                    <reporter username="Ferd">Ferdinand Xu</reporter>
                        <labels>
                            <label>Incompatible</label>
                            <label>TODOC2.2</label>
                    </labels>
                <created>Thu, 16 Jun 2016 02:11:30 +0000</created>
                <updated>Tue, 18 Oct 2016 03:42:00 +0000</updated>
                            <resolved>Wed, 28 Sep 2016 01:22:01 +0000</resolved>
                                                    <fixVersion>2.2.0</fixVersion>
                                        <due></due>
                            <votes>1</votes>
                                    <watches>18</watches>
                                                                <comments>
                            <comment id="15332903" author="ferd" created="Thu, 16 Jun 2016 02:11:49 +0000"  >&lt;p&gt;cc &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xuefuz&quot; class=&quot;user-hover&quot; rel=&quot;xuefuz&quot;&gt;Xuefu Zhang&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15334568" author="sershe" created="Thu, 16 Jun 2016 19:59:23 +0000"  >&lt;p&gt;Maybe we&apos;ll also get newer Hadoop version in the damn tgz file so we can actually upgrade that too!&lt;/p&gt;</comment>
                            <comment id="15335510" author="ferd" created="Fri, 17 Jun 2016 06:23:54 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=spena&quot; class=&quot;user-hover&quot; rel=&quot;spena&quot;&gt;Sergio Pe&#241;a&lt;/a&gt; do you know how to update the tgz file &lt;a href=&quot;http://d3jw87u4immizc.cloudfront.net/spark-tarball/spark-$&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://d3jw87u4immizc.cloudfront.net/spark-tarball/spark-$&lt;/a&gt;&lt;/p&gt;
{spark.version}
&lt;p&gt;-bin-hadoop2-without-hive.tgz with a newly built version? &lt;/p&gt;</comment>
                            <comment id="15336230" author="spena" created="Fri, 17 Jun 2016 14:42:41 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Ferd&quot; class=&quot;user-hover&quot; rel=&quot;Ferd&quot;&gt;Ferdinand Xu&lt;/a&gt; I will investigate that. However, I see that spark 2.0 is a preview release, and it is not stable yet. Should we want to upgrade to this now? I read on the website that preview releases may contain critical bugs or documentation errors, so I think we should wait until it is officially released as GA.&lt;/p&gt;</comment>
                            <comment id="15338890" author="ferd" created="Mon, 20 Jun 2016 02:01:24 +0000"  >&lt;p&gt;OK, let us wait for GA release.&lt;/p&gt;</comment>
                            <comment id="15407497" author="lirui" created="Thu, 4 Aug 2016 09:49:05 +0000"  >&lt;p&gt;I think we can resume the work here since Spark 2.0 has released.&lt;/p&gt;</comment>
                            <comment id="15407930" author="spena" created="Thu, 4 Aug 2016 15:22:36 +0000"  >&lt;p&gt;Let&apos;s wait until &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-14240&quot; title=&quot;HoS itests shouldn&amp;#39;t depend on a Spark distribution&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-14240&quot;&gt;HIVE-14240&lt;/a&gt; is resolved. The current spark assembly used for all itests uses spark 1.6 so this patch won&apos;t work. Also, I&apos;ve heard that spark 2.0 won&apos;t use spark assembly anymore, so we need to depend on spark maven dependencies to run the tests.&lt;/p&gt;</comment>
                            <comment id="15408774" author="lirui" created="Fri, 5 Aug 2016 02:17:32 +0000"  >&lt;p&gt;Yeah it&apos;d be great if we can get rid of that tar, or at least make it smaller - we currently package the example jar into it which shouldn&apos;t be necessary.&lt;/p&gt;</comment>
                            <comment id="15500015" author="ferd" created="Sun, 18 Sep 2016 01:37:45 +0000"  >&lt;p&gt;This patch is used to test whether qtest is passed.&lt;/p&gt;</comment>
                            <comment id="15500035" author="hiveqa" created="Sun, 18 Sep 2016 01:52:04 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12829075/HIVE-14029.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12829075/HIVE-14029.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to build exiting with an error&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1225/testReport&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1225/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1225/console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1225/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/jenkins-PreCommit-HIVE-Build-1225/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/jenkins-PreCommit-HIVE-Build-1225/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command &apos;bash /data/hive-ptest/working/scratch/source-prep.sh&apos; failed with exit status 1 and output &apos;+ date &apos;+%Y-%m-%d %T.%3N&apos;
2016-09-18 01:52:20.390
+ [[ -n /usr/java/jdk1.8.0_25 ]]
+ export JAVA_HOME=/usr/java/jdk1.8.0_25
+ JAVA_HOME=/usr/java/jdk1.8.0_25
+ export PATH=/usr/java/jdk1.8.0_25/bin/:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+ PATH=/usr/java/jdk1.8.0_25/bin/:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+ export &apos;ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m &apos;
+ ANT_OPTS=&apos;-Xmx1g -XX:MaxPermSize=256m &apos;
+ export &apos;M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128&apos;
+ M2_OPTS=&apos;-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128&apos;
+ cd /data/hive-ptest/working/
+ tee /data/hive-ptest/logs/jenkins-PreCommit-HIVE-Build-1225/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ git = \s\v\n ]]
+ [[ git = \g\i\t ]]
+ [[ -z master ]]
+ [[ -d apache-github-source-source ]]
+ [[ ! -d apache-github-source-source/.git ]]
+ [[ ! -d apache-github-source-source ]]
+ date &apos;+%Y-%m-%d %T.%3N&apos;
2016-09-18 01:52:20.392
+ cd apache-github-source-source
+ git fetch origin
From https://github.com/apache/hive
   05e2510..c90eed2  master     -&amp;gt; origin/master
+ git reset --hard HEAD
warning: unable to access &apos;/home/sseth/.config/git/attributes&apos;: Permission denied
HEAD is now at 05e2510 HIVE-14767: Migrate slow MiniMr tests to faster options (Prasanth Jayachandran reviewed by Siddharth Seth)
+ git clean -f -d
warning: unable to access &apos;/home/sseth/.config/git/ignore&apos;: Permission denied
+ git checkout master
warning: unable to access &apos;/home/sseth/.config/git/attributes&apos;: Permission denied
warning: unable to access &apos;/home/sseth/.config/git/ignore&apos;: Permission denied
Already on &apos;master&apos;
Your branch is behind &apos;origin/master&apos; by 1 commit, and can be fast-forwarded.
  (use &quot;git pull&quot; to update your local branch)
+ git reset --hard origin/master
warning: unable to access &apos;/home/sseth/.config/git/attributes&apos;: Permission denied
HEAD is now at c90eed2 HIVE-14734: Detect ptest profile and submit to ptest-server from jenkins-execute-build.sh (Sergio Pena, reviewed by Siddarth Seth)
+ git merge --ff-only origin/master
Already up-to-date.
+ date &apos;+%Y-%m-%d %T.%3N&apos;
2016-09-18 01:52:22.343
+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hive-ptest/working/scratch/build.patch
+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]
+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh
+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch
warning: unable to access &apos;/home/sseth/.config/git/attributes&apos;: Permission denied
error: a/hcatalog/webhcat/svr/src/test/java/org/apache/hive/hcatalog/templeton/mock/MockUriInfo.java: No such file or directory
error: a/itests/pom.xml: No such file or directory
error: a/pom.xml: No such file or directory
error: a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveBaseFunctionResultList.java: No such file or directory
error: a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveMapFunction.java: No such file or directory
error: a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveReduceFunction.java: No such file or directory
error: a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SortByShuffler.java: No such file or directory
error: a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/status/impl/JobMetricsListener.java: No such file or directory
error: a/ql/src/java/org/apache/hadoop/hive/ql/parse/TaskCompiler.java: No such file or directory
error: a/ql/src/test/org/apache/hadoop/hive/ql/exec/spark/TestHiveKVResultCache.java: No such file or directory
error: a/spark-client/src/main/java/org/apache/hive/spark/client/MetricsCollection.java: No such file or directory
error: a/spark-client/src/main/java/org/apache/hive/spark/client/RemoteDriver.java: No such file or directory
error: a/spark-client/src/main/java/org/apache/hive/spark/client/metrics/InputMetrics.java: No such file or directory
error: a/spark-client/src/main/java/org/apache/hive/spark/client/metrics/Metrics.java: No such file or directory
error: a/spark-client/src/main/java/org/apache/hive/spark/client/metrics/ShuffleReadMetrics.java: No such file or directory
error: a/spark-client/src/main/java/org/apache/hive/spark/client/metrics/ShuffleWriteMetrics.java: No such file or directory
error: a/spark-client/src/test/java/org/apache/hive/spark/client/TestMetricsCollection.java: No such file or directory
The patch does not appear to apply with p0, p1, or p2
+ exit 1
&apos;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12829075 - jenkins-PreCommit-HIVE-Build&lt;/p&gt;</comment>
                            <comment id="15500085" author="hiveqa" created="Sun, 18 Sep 2016 02:31:25 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12829079/HIVE-14029.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12829079/HIVE-14029.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to build exiting with an error&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1226/testReport&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1226/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1226/console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1226/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/jenkins-PreCommit-HIVE-Build-1226/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/jenkins-PreCommit-HIVE-Build-1226/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command &apos;bash /data/hive-ptest/working/scratch/source-prep.sh&apos; failed with exit status 1 and output &apos;+ date &apos;+%Y-%m-%d %T.%3N&apos;
2016-09-18 02:31:43.131
+ [[ -n /usr/java/jdk1.8.0_25 ]]
+ export JAVA_HOME=/usr/java/jdk1.8.0_25
+ JAVA_HOME=/usr/java/jdk1.8.0_25
+ export PATH=/usr/java/jdk1.8.0_25/bin/:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+ PATH=/usr/java/jdk1.8.0_25/bin/:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+ export &apos;ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m &apos;
+ ANT_OPTS=&apos;-Xmx1g -XX:MaxPermSize=256m &apos;
+ export &apos;M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128&apos;
+ M2_OPTS=&apos;-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128&apos;
+ cd /data/hive-ptest/working/
+ tee /data/hive-ptest/logs/jenkins-PreCommit-HIVE-Build-1226/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ git = \s\v\n ]]
+ [[ git = \g\i\t ]]
+ [[ -z master ]]
+ [[ -d apache-github-source-source ]]
+ [[ ! -d apache-github-source-source/.git ]]
+ [[ ! -d apache-github-source-source ]]
+ date &apos;+%Y-%m-%d %T.%3N&apos;
2016-09-18 02:31:43.133
+ cd apache-github-source-source
+ git fetch origin
+ git reset --hard HEAD
warning: unable to access &apos;/home/sseth/.config/git/attributes&apos;: Permission denied
warning: unable to access &apos;/home/sseth/.config/git/attributes&apos;: Permission denied
HEAD is now at c90eed2 HIVE-14734: Detect ptest profile and submit to ptest-server from jenkins-execute-build.sh (Sergio Pena, reviewed by Siddarth Seth)
+ git clean -f -d
warning: unable to access &apos;/home/sseth/.config/git/ignore&apos;: Permission denied
+ git checkout master
warning: unable to access &apos;/home/sseth/.config/git/ignore&apos;: Permission denied
Already on &apos;master&apos;
Your branch is up-to-date with &apos;origin/master&apos;.
+ git reset --hard origin/master
HEAD is now at c90eed2 HIVE-14734: Detect ptest profile and submit to ptest-server from jenkins-execute-build.sh (Sergio Pena, reviewed by Siddarth Seth)
+ git merge --ff-only origin/master
Already up-to-date.
+ date &apos;+%Y-%m-%d %T.%3N&apos;
2016-09-18 02:31:44.018
+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hive-ptest/working/scratch/build.patch
+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]
+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh
+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch
warning: unable to access &apos;/home/sseth/.config/git/attributes&apos;: Permission denied
error: patch failed: ql/src/java/org/apache/hadoop/hive/ql/parse/TaskCompiler.java:76
error: ql/src/java/org/apache/hadoop/hive/ql/parse/TaskCompiler.java: patch does not apply
The patch does not appear to apply with p0, p1, or p2
+ exit 1
&apos;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12829079 - jenkins-PreCommit-HIVE-Build&lt;/p&gt;</comment>
                            <comment id="15500137" author="ferd" created="Sun, 18 Sep 2016 03:14:02 +0000"  >&lt;p&gt;Rebase patch&lt;/p&gt;</comment>
                            <comment id="15500150" author="hiveqa" created="Sun, 18 Sep 2016 03:24:12 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12829082/HIVE-14029.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12829082/HIVE-14029.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to build exiting with an error&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1227/testReport&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1227/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1227/console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1227/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/jenkins-PreCommit-HIVE-Build-1227/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/jenkins-PreCommit-HIVE-Build-1227/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;**** This message was trimmed, see log for full details ****
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/util/concurrent/ConcurrentHashMap.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/io/FileNotFoundException.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/net/URISyntaxException.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/lang/Integer.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/apache-github-source-source/metastore/target/hive-metastore-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/metastore/IMetaStoreClient.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/apache-github-source-source/metastore/target/hive-metastore-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/metastore/api/MetaException.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/io/Text.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/security/Credentials.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/security/token/Token.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/apache-github-source-source/hcatalog/core/target/hive-hcatalog-core-2.2.0-SNAPSHOT.jar(org/apache/hive/hcatalog/common/HCatUtil.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/thrift/libthrift/0.9.3/libthrift-0.9.3.jar(org/apache/thrift/TException.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/security/Groups.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/util/HashSet.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/util/Set.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/util/Date.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/apache-github-source-source/common/target/hive-common-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/common/classification/InterfaceAudience$Private.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/io/BufferedReader.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/io/InputStream.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/io/InputStreamReader.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/io/PrintWriter.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/util/Map$Entry.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/util/concurrent/Semaphore.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/commons/commons-exec/1.1/commons-exec-1.1.jar(org/apache/commons/exec/CommandLine.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/commons/commons-exec/1.1/commons-exec-1.1.jar(org/apache/commons/exec/DefaultExecutor.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/commons/commons-exec/1.1/commons-exec-1.1.jar(org/apache/commons/exec/ExecuteWatchdog.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/commons/commons-exec/1.1/commons-exec-1.1.jar(org/apache/commons/exec/PumpStreamHandler.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/util/Shell.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/lang/Thread.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/lang/Runnable.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/io/DataInput.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/io/DataOutput.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/InputSplit.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/io/OutputStreamWriter.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar(org/apache/curator/framework/CuratorFramework.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/io/NullWritable.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/RecordReader.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/TaskAttemptContext.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar(org/apache/zookeeper/CreateMode.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar(org/apache/zookeeper/KeeperException.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar(org/apache/zookeeper/ZooDefs.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar(org/apache/zookeeper/ZooDefs$Ids.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/InputFormat.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/JobContext.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/apache-github-source-source/common/target/hive-common-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/common/classification/InterfaceStability$Evolving.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/net/URLConnection.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/net/URLDecoder.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/util/Enumeration.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/util/Properties.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/util/StringTokenizer.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/core/UriBuilder.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/apache-github-source-source/common/target/hive-common-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/common/LogUtils.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/lang/Class.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/lang/StringBuilder.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/JobID.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/Mapper.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/util/Iterator.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/util/LinkedList.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/util/concurrent/ExecutorService.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/util/concurrent/Executors.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/util/concurrent/TimeUnit.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/lang/Process.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/Mapper$Context.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar(org/apache/curator/framework/CuratorFrameworkFactory.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar(org/apache/curator/retry/ExponentialBackoffRetry.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/conf/Configured.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapred/JobClient.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapred/JobConf.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/Job.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/lib/output/NullOutputFormat.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/security/token/delegation/DelegationTokenIdentifier.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/util/Tool.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/conf/Configurable.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/lang/ClassNotFoundException.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapred/RunningJob.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-annotations/2.7.2/hadoop-annotations-2.7.2.jar(org/apache/hadoop/classification/InterfaceAudience.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-annotations/2.7.2/hadoop-annotations-2.7.2.jar(org/apache/hadoop/classification/InterfaceAudience$LimitedPrivate.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/lang/annotation/Annotation.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/lang/SuppressWarnings.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/lang/annotation/Retention.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/lang/annotation/RetentionPolicy.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/lang/annotation/Target.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/lang/annotation/ElementType.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/HttpMethod.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/lang/Override.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(sun/misc/Contended.class)]]
[loading RegularFileObject[/data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/SecureProxySupport$3.class]]
[loading RegularFileObject[/data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/HcatDelegator$1.class]]
[loading RegularFileObject[/data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/LauncherDelegator$1.class]]
[loading RegularFileObject[/data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/Server$1.class]]
[loading RegularFileObject[/data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/HcatException$1.class]]
[loading RegularFileObject[/data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/SecureProxySupport$2.class]]
[loading RegularFileObject[/data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/SecureProxySupport$1.class]]
[loading RegularFileObject[/data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/tool/LogRetriever$1.class]]
[loading RegularFileObject[/data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/tool/HDFSStorage$1.class]]
[loading RegularFileObject[/data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/tool/ZooKeeperStorage$1.class]]
[loading RegularFileObject[/data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/tool/TempletonUtils$1.class]]
[loading RegularFileObject[/data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob$1.class]]
[loading RegularFileObject[/data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob$1$1.class]]
[done in 2216 ms]
[WARNING] Javadoc Warnings
[WARNING] Sep 18, 2016 3:24:28 AM com.sun.jersey.wadl.resourcedoc.ResourceDoclet start
[WARNING] INFO: Wrote /data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/resourcedoc.xml
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-webhcat ---
[INFO] Using &apos;UTF-8&apos; encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-webhcat ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/tmp/conf
     [copy] Copying 15 files to /data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-webhcat ---
[INFO] Compiling 9 source files to /data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/test-classes
[INFO] -------------------------------------------------------------
[WARNING] COMPILATION WARNING : 
[INFO] -------------------------------------------------------------
[WARNING] /data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/src/test/java/org/apache/hive/hcatalog/templeton/TestWebHCatE2e.java: Some input files use or override a deprecated API.
[WARNING] /data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/src/test/java/org/apache/hive/hcatalog/templeton/TestWebHCatE2e.java: Recompile with -Xlint:deprecation for details.
[WARNING] /data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/src/test/java/org/apache/hive/hcatalog/templeton/TestWebHCatE2e.java: Some input files use unchecked or unsafe operations.
[WARNING] /data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/src/test/java/org/apache/hive/hcatalog/templeton/TestWebHCatE2e.java: Recompile with -Xlint:unchecked for details.
[INFO] 4 warnings 
[INFO] -------------------------------------------------------------
[INFO] -------------------------------------------------------------
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/src/test/java/org/apache/hive/hcatalog/templeton/mock/MockUriInfo.java:[67,3] method does not override or implement a method from a supertype
[ERROR] /data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/src/test/java/org/apache/hive/hcatalog/templeton/mock/MockUriInfo.java:[72,3] method does not override or implement a method from a supertype
[INFO] 2 errors 
[INFO] -------------------------------------------------------------
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive .............................................. SUCCESS [1.987s]
[INFO] Hive Shims Common ................................. SUCCESS [3.928s]
[INFO] Hive Shims 0.23 ................................... SUCCESS [2.028s]
[INFO] Hive Shims Scheduler .............................. SUCCESS [0.789s]
[INFO] Hive Shims ........................................ SUCCESS [0.493s]
[INFO] Hive Storage API .................................. SUCCESS [1.208s]
[INFO] Hive ORC .......................................... SUCCESS [4.733s]
[INFO] Hive Common ....................................... SUCCESS [4.729s]
[INFO] Hive Service RPC .................................. SUCCESS [3.410s]
[INFO] Hive Serde ........................................ SUCCESS [3.448s]
[INFO] Hive Metastore .................................... SUCCESS [19.085s]
[INFO] Hive Ant Utilities ................................ SUCCESS [0.267s]
[INFO] Hive Llap Common .................................. SUCCESS [2.802s]
[INFO] Hive Llap Client .................................. SUCCESS [1.071s]
[INFO] Hive Llap Tez ..................................... SUCCESS [1.214s]
[INFO] Spark Remote Client ............................... SUCCESS [23.836s]
[INFO] Hive Query Language ............................... SUCCESS [53.022s]
[INFO] Hive Llap Server .................................. SUCCESS [3.353s]
[INFO] Hive Service ...................................... SUCCESS [3.861s]
[INFO] Hive Accumulo Handler ............................. SUCCESS [2.300s]
[INFO] Hive JDBC ......................................... SUCCESS [9.583s]
[INFO] Hive Beeline ...................................... SUCCESS [1.997s]
[INFO] Hive CLI .......................................... SUCCESS [1.501s]
[INFO] Hive Contrib ...................................... SUCCESS [0.808s]
[INFO] Hive Druid Handler ................................ SUCCESS [3.230s]
[INFO] Hive HBase Handler ................................ SUCCESS [2.421s]
[INFO] Hive HCatalog ..................................... SUCCESS [0.202s]
[INFO] Hive HCatalog Core ................................ SUCCESS [2.674s]
[INFO] Hive HCatalog Pig Adapter ......................... SUCCESS [2.686s]
[INFO] Hive HCatalog Server Extensions ................... SUCCESS [1.478s]
[INFO] Hive HCatalog Webhcat Java Client ................. SUCCESS [1.310s]
[INFO] Hive HCatalog Webhcat ............................. FAILURE [4.897s]
[INFO] Hive HCatalog Streaming ........................... SKIPPED
[INFO] Hive HPL/SQL ...................................... SKIPPED
[INFO] Hive HWI .......................................... SKIPPED
[INFO] Hive Llap External Client ......................... SKIPPED
[INFO] Hive Shims Aggregator ............................. SKIPPED
[INFO] Hive TestUtils .................................... SKIPPED
[INFO] Hive Packaging .................................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 2:51.263s
[INFO] Finished at: Sun Sep 18 03:24:28 UTC 2016
[INFO] Final Memory: 254M/935M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:testCompile (default-testCompile) on project hive-webhcat: Compilation failure: Compilation failure:
[ERROR] /data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/src/test/java/org/apache/hive/hcatalog/templeton/mock/MockUriInfo.java:[67,3] method does not override or implement a method from a supertype
[ERROR] /data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/src/test/java/org/apache/hive/hcatalog/templeton/mock/MockUriInfo.java:[72,3] method does not override or implement a method from a supertype
[ERROR] -&amp;gt; [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn &amp;lt;goals&amp;gt; -rf :hive-webhcat
+ exit 1
&apos;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12829082 - jenkins-PreCommit-HIVE-Build&lt;/p&gt;</comment>
                            <comment id="15500530" author="hiveqa" created="Sun, 18 Sep 2016 08:04:31 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12829095/HIVE-14029.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12829095/HIVE-14029.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to build exiting with an error&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1228/testReport&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1228/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1228/console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1228/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/jenkins-PreCommit-HIVE-Build-1228/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/jenkins-PreCommit-HIVE-Build-1228/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;**** This message was trimmed, see log for full details ****
[INFO] --- maven-antrun-plugin:1.7:run (download-spark) @ hive-hcatalog-it-unit ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-hcatalog-it-unit ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-hcatalog-it-unit ---
[INFO] Using &apos;UTF-8&apos; encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-hcatalog-it-unit ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-hcatalog-it-unit ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-hcatalog-it-unit ---
[INFO] Using &apos;UTF-8&apos; encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-hcatalog-it-unit ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/tmp/conf
     [copy] Copying 15 files to /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-hcatalog-it-unit ---
[INFO] Compiling 8 source files to /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/test-classes
[WARNING] /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatHiveThriftCompatibility.java: Some input files use or override a deprecated API.
[WARNING] /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatHiveThriftCompatibility.java: Recompile with -Xlint:deprecation for details.
[INFO] 
[INFO] --- maven-surefire-plugin:2.19.1:test (default-test) @ hive-hcatalog-it-unit ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ hive-hcatalog-it-unit ---
[INFO] Building jar: /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-2.2.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-hcatalog-it-unit ---
[INFO] 
[INFO] --- maven-jar-plugin:2.4:test-jar (default) @ hive-hcatalog-it-unit ---
[INFO] Building jar: /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-2.2.0-SNAPSHOT-tests.jar
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-hcatalog-it-unit ---
[INFO] Installing /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-2.2.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hcatalog-it-unit/2.2.0-SNAPSHOT/hive-hcatalog-it-unit-2.2.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-hcatalog-it-unit/2.2.0-SNAPSHOT/hive-hcatalog-it-unit-2.2.0-SNAPSHOT.pom
[INFO] Installing /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-2.2.0-SNAPSHOT-tests.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hcatalog-it-unit/2.2.0-SNAPSHOT/hive-hcatalog-it-unit-2.2.0-SNAPSHOT-tests.jar
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Integration - Testing Utilities 2.2.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-it-util ---
[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/itests/util/target
[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/itests/util (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-no-snapshots) @ hive-it-util ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (download-spark) @ hive-it-util ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-it-util ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-it-util ---
[INFO] Using &apos;UTF-8&apos; encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-github-source-source/itests/util/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-it-util ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-it-util ---
[INFO] Compiling 66 source files to /data/hive-ptest/working/apache-github-source-source/itests/util/target/classes
[WARNING] /data/hive-ptest/working/apache-github-source-source/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsSubdirectoryOfTableHook.java: Some input files use or override a deprecated API.
[WARNING] /data/hive-ptest/working/apache-github-source-source/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsSubdirectoryOfTableHook.java: Recompile with -Xlint:deprecation for details.
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-it-util ---
[INFO] Using &apos;UTF-8&apos; encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-github-source-source/itests/util/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-it-util ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/itests/util/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/itests/util/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/itests/util/target/tmp/conf
     [copy] Copying 15 files to /data/hive-ptest/working/apache-github-source-source/itests/util/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-it-util ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.19.1:test (default-test) @ hive-it-util ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ hive-it-util ---
[INFO] Building jar: /data/hive-ptest/working/apache-github-source-source/itests/util/target/hive-it-util-2.2.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-it-util ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-it-util ---
[INFO] Installing /data/hive-ptest/working/apache-github-source-source/itests/util/target/hive-it-util-2.2.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-it-util/2.2.0-SNAPSHOT/hive-it-util-2.2.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-github-source-source/itests/util/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-it-util/2.2.0-SNAPSHOT/hive-it-util-2.2.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Integration - Unit Tests 2.2.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-it-unit ---
[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/itests/hive-unit/target
[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/itests/hive-unit (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-no-snapshots) @ hive-it-unit ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (download-spark) @ hive-it-unit ---
[INFO] Executing tasks

main:
     [exec] + /bin/pwd
     [exec] /data/hive-ptest/working/apache-github-source-source/itests/hive-unit
     [exec] + BASE_DIR=./target
     [exec] + HIVE_ROOT=./target/../../../
     [exec] + DOWNLOAD_DIR=./../thirdparty
     [exec] + mkdir -p ./../thirdparty
     [exec] + download &apos;https://www.dropbox.com/s/cv47dca5f2hqefn/spark-2.0.0-bin-hadoop2-without-hive.tgz?dl=0&apos; spark
     [exec] + url=&apos;https://www.dropbox.com/s/cv47dca5f2hqefn/spark-2.0.0-bin-hadoop2-without-hive.tgz?dl=0&apos;
     [exec] + finalName=spark
     [exec] ++ basename &apos;https://www.dropbox.com/s/cv47dca5f2hqefn/spark-2.0.0-bin-hadoop2-without-hive.tgz?dl=0&apos;
     [exec] + tarName=&apos;spark-2.0.0-bin-hadoop2-without-hive.tgz?dl=0&apos;
     [exec] + rm -rf ./target/spark
     [exec] + [[ ! -f ./../thirdparty/spark-2.0.0-bin-hadoop2-without-hive.tgz?dl=0 ]]
     [exec] + curl -Sso &apos;./../thirdparty/spark-2.0.0-bin-hadoop2-without-hive.tgz?dl=0&apos; &apos;https://www.dropbox.com/s/cv47dca5f2hqefn/spark-2.0.0-bin-hadoop2-without-hive.tgz?dl=0&apos;
     [exec] + tar -zxf &apos;./../thirdparty/spark-2.0.0-bin-hadoop2-without-hive.tgz?dl=0&apos; -C ./target
     [exec] tar (child): ./../thirdparty/spark-2.0.0-bin-hadoop2-without-hive.tgz?dl=0: Cannot open: No such file or directory
     [exec] tar (child): Error is not recoverable: exiting now
     [exec] tar: Child returned status 2
     [exec] tar: Error is not recoverable: exiting now
     [exec] + mv ./target/spark-2.0.0-preview-bin-hadoop2-without-hive ./target/spark
     [exec] mv: cannot stat ?./target/spark-2.0.0-preview-bin-hadoop2-without-hive?: No such file or directory
     [exec] + cp -f ./target/../../..//data/conf/spark/log4j2.properties ./target/spark/conf/
     [exec] cp: cannot create regular file ?./target/spark/conf/?: No such file or directory
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive Integration - Parent ......................... SUCCESS [1.708s]
[INFO] Hive Integration - Custom Serde ................... SUCCESS [4.071s]
[INFO] Hive Integration - Custom udfs .................... SUCCESS [0.714s]
[INFO] Hive Integration - Custom UDFs - udf-classloader-util  SUCCESS [0.696s]
[INFO] Hive Integration - Custom UDFs - udf-classloader-udf1  SUCCESS [0.711s]
[INFO] Hive Integration - Custom UDFs - udf-classloader-udf2  SUCCESS [0.623s]
[INFO] Hive Integration - Custom UDFs - udf-vectorized-badexample  SUCCESS [0.894s]
[INFO] Hive Integration - HCatalog Unit Tests ............ SUCCESS [4.463s]
[INFO] Hive Integration - Testing Utilities .............. SUCCESS [4.189s]
[INFO] Hive Integration - Unit Tests ..................... FAILURE [1.870s]
[INFO] Hive Integration - Test Serde ..................... SKIPPED
[INFO] Hive Integration - QFile Tests .................... SKIPPED
[INFO] Hive Integration - QFile Accumulo Tests ........... SKIPPED
[INFO] JMH benchmark: Hive ............................... SKIPPED
[INFO] Hive Integration - Unit Tests - Hadoop 2 .......... SKIPPED
[INFO] Hive Integration - Unit Tests with miniKdc ........ SKIPPED
[INFO] Hive Integration - QFile Spark Tests .............. SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 20.754s
[INFO] Finished at: Sun Sep 18 08:04:50 UTC 2016
[INFO] Final Memory: 92M/697M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.7:run (download-spark) on project hive-it-unit: An Ant BuildException has occured: exec returned: 1
[ERROR] around Ant part ...&amp;lt;exec failonerror=&quot;true&quot; dir=&quot;/data/hive-ptest/working/apache-github-source-source/itests/hive-unit&quot; executable=&quot;bash&quot;&amp;gt;... @ 4:122 in /data/hive-ptest/working/apache-github-source-source/itests/hive-unit/target/antrun/build-main.xml
[ERROR] -&amp;gt; [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn &amp;lt;goals&amp;gt; -rf :hive-it-unit
+ exit 1
&apos;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12829095 - jenkins-PreCommit-HIVE-Build&lt;/p&gt;</comment>
                            <comment id="15500571" author="hiveqa" created="Sun, 18 Sep 2016 08:33:50 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12829099/HIVE-14029.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12829099/HIVE-14029.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to build exiting with an error&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1229/testReport&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1229/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1229/console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1229/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/jenkins-PreCommit-HIVE-Build-1229/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/jenkins-PreCommit-HIVE-Build-1229/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;**** This message was trimmed, see log for full details ****
[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-no-snapshots) @ hive-hcatalog-it-unit ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (download-spark) @ hive-hcatalog-it-unit ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-hcatalog-it-unit ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-hcatalog-it-unit ---
[INFO] Using &apos;UTF-8&apos; encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-hcatalog-it-unit ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-hcatalog-it-unit ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-hcatalog-it-unit ---
[INFO] Using &apos;UTF-8&apos; encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-hcatalog-it-unit ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/tmp/conf
     [copy] Copying 15 files to /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-hcatalog-it-unit ---
[INFO] Compiling 8 source files to /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/test-classes
[WARNING] /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatHiveThriftCompatibility.java: Some input files use or override a deprecated API.
[WARNING] /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatHiveThriftCompatibility.java: Recompile with -Xlint:deprecation for details.
[INFO] 
[INFO] --- maven-surefire-plugin:2.19.1:test (default-test) @ hive-hcatalog-it-unit ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ hive-hcatalog-it-unit ---
[INFO] Building jar: /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-2.2.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-hcatalog-it-unit ---
[INFO] 
[INFO] --- maven-jar-plugin:2.4:test-jar (default) @ hive-hcatalog-it-unit ---
[INFO] Building jar: /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-2.2.0-SNAPSHOT-tests.jar
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-hcatalog-it-unit ---
[INFO] Installing /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-2.2.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hcatalog-it-unit/2.2.0-SNAPSHOT/hive-hcatalog-it-unit-2.2.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-hcatalog-it-unit/2.2.0-SNAPSHOT/hive-hcatalog-it-unit-2.2.0-SNAPSHOT.pom
[INFO] Installing /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-2.2.0-SNAPSHOT-tests.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hcatalog-it-unit/2.2.0-SNAPSHOT/hive-hcatalog-it-unit-2.2.0-SNAPSHOT-tests.jar
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Integration - Testing Utilities 2.2.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-it-util ---
[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/itests/util/target
[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/itests/util (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-no-snapshots) @ hive-it-util ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (download-spark) @ hive-it-util ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-it-util ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-it-util ---
[INFO] Using &apos;UTF-8&apos; encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-github-source-source/itests/util/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-it-util ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-it-util ---
[INFO] Compiling 66 source files to /data/hive-ptest/working/apache-github-source-source/itests/util/target/classes
[WARNING] /data/hive-ptest/working/apache-github-source-source/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsSubdirectoryOfTableHook.java: Some input files use or override a deprecated API.
[WARNING] /data/hive-ptest/working/apache-github-source-source/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsSubdirectoryOfTableHook.java: Recompile with -Xlint:deprecation for details.
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-it-util ---
[INFO] Using &apos;UTF-8&apos; encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-github-source-source/itests/util/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-it-util ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/itests/util/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/itests/util/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/itests/util/target/tmp/conf
     [copy] Copying 15 files to /data/hive-ptest/working/apache-github-source-source/itests/util/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-it-util ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.19.1:test (default-test) @ hive-it-util ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ hive-it-util ---
[INFO] Building jar: /data/hive-ptest/working/apache-github-source-source/itests/util/target/hive-it-util-2.2.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-it-util ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-it-util ---
[INFO] Installing /data/hive-ptest/working/apache-github-source-source/itests/util/target/hive-it-util-2.2.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-it-util/2.2.0-SNAPSHOT/hive-it-util-2.2.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-github-source-source/itests/util/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-it-util/2.2.0-SNAPSHOT/hive-it-util-2.2.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Integration - Unit Tests 2.2.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-it-unit ---
[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/itests/hive-unit/target
[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/itests/hive-unit (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-no-snapshots) @ hive-it-unit ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (download-spark) @ hive-it-unit ---
[INFO] Executing tasks

main:
     [exec] + /bin/pwd
     [exec] + BASE_DIR=./target
     [exec] + HIVE_ROOT=./target/../../../
     [exec] + DOWNLOAD_DIR=./../thirdparty
     [exec] + mkdir -p ./../thirdparty
     [exec] /data/hive-ptest/working/apache-github-source-source/itests/hive-unit
     [exec] + download http://blog.sundp.me/spark/spark-2.0.0-bin-hadoop2-without-hive.tgz spark
     [exec] + url=http://blog.sundp.me/spark/spark-2.0.0-bin-hadoop2-without-hive.tgz
     [exec] + finalName=spark
     [exec] ++ basename http://blog.sundp.me/spark/spark-2.0.0-bin-hadoop2-without-hive.tgz
     [exec] + tarName=spark-2.0.0-bin-hadoop2-without-hive.tgz
     [exec] + rm -rf ./target/spark
     [exec] + [[ ! -f ./../thirdparty/spark-2.0.0-bin-hadoop2-without-hive.tgz ]]
     [exec] + curl -Sso ./../thirdparty/spark-2.0.0-bin-hadoop2-without-hive.tgz http://blog.sundp.me/spark/spark-2.0.0-bin-hadoop2-without-hive.tgz
     [exec] + tar -zxf ./../thirdparty/spark-2.0.0-bin-hadoop2-without-hive.tgz -C ./target
     [exec] + mv ./target/spark-2.0.0-preview-bin-hadoop2-without-hive ./target/spark
     [exec] mv: cannot stat ?./target/spark-2.0.0-preview-bin-hadoop2-without-hive?: No such file or directory
     [exec] + cp -f ./target/../../..//data/conf/spark/log4j2.properties ./target/spark/conf/
     [exec] cp: cannot create regular file ?./target/spark/conf/?: No such file or directory
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive Integration - Parent ......................... SUCCESS [1.573s]
[INFO] Hive Integration - Custom Serde ................... SUCCESS [3.578s]
[INFO] Hive Integration - Custom udfs .................... SUCCESS [0.591s]
[INFO] Hive Integration - Custom UDFs - udf-classloader-util  SUCCESS [1.068s]
[INFO] Hive Integration - Custom UDFs - udf-classloader-udf1  SUCCESS [0.834s]
[INFO] Hive Integration - Custom UDFs - udf-classloader-udf2  SUCCESS [0.644s]
[INFO] Hive Integration - Custom UDFs - udf-vectorized-badexample  SUCCESS [0.615s]
[INFO] Hive Integration - HCatalog Unit Tests ............ SUCCESS [4.321s]
[INFO] Hive Integration - Testing Utilities .............. SUCCESS [4.851s]
[INFO] Hive Integration - Unit Tests ..................... FAILURE [9.328s]
[INFO] Hive Integration - Test Serde ..................... SKIPPED
[INFO] Hive Integration - QFile Tests .................... SKIPPED
[INFO] Hive Integration - QFile Accumulo Tests ........... SKIPPED
[INFO] JMH benchmark: Hive ............................... SKIPPED
[INFO] Hive Integration - Unit Tests - Hadoop 2 .......... SKIPPED
[INFO] Hive Integration - Unit Tests with miniKdc ........ SKIPPED
[INFO] Hive Integration - QFile Spark Tests .............. SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 28.170s
[INFO] Finished at: Sun Sep 18 08:34:08 UTC 2016
[INFO] Final Memory: 90M/757M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.7:run (download-spark) on project hive-it-unit: An Ant BuildException has occured: exec returned: 1
[ERROR] around Ant part ...&amp;lt;exec failonerror=&quot;true&quot; dir=&quot;/data/hive-ptest/working/apache-github-source-source/itests/hive-unit&quot; executable=&quot;bash&quot;&amp;gt;... @ 4:122 in /data/hive-ptest/working/apache-github-source-source/itests/hive-unit/target/antrun/build-main.xml
[ERROR] -&amp;gt; [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn &amp;lt;goals&amp;gt; -rf :hive-it-unit
+ exit 1
&apos;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12829099 - jenkins-PreCommit-HIVE-Build&lt;/p&gt;</comment>
                            <comment id="15500712" author="hiveqa" created="Sun, 18 Sep 2016 10:09:42 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12829100/HIVE-14029.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12829100/HIVE-14029.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;green&quot;&gt;SUCCESS:&lt;/font&gt; +1 due to 2 test(s) being added or modified.&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to 9 failed/errored test(s), 10497 tests executed&lt;br/&gt;
&lt;b&gt;Failed tests:&lt;/b&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]
org.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching
org.apache.hive.spark.client.TestSparkClient.testAddJarsAndFiles
org.apache.hive.spark.client.TestSparkClient.testCounters
org.apache.hive.spark.client.TestSparkClient.testMetricsCollection
org.apache.hive.spark.client.TestSparkClient.testRemoteClient
org.apache.hive.spark.client.TestSparkClient.testSimpleSparkJob
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1230/testReport&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1230/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1230/console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1230/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/jenkins-PreCommit-HIVE-Build-1230/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/jenkins-PreCommit-HIVE-Build-1230/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 9 tests failed
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12829100 - jenkins-PreCommit-HIVE-Build&lt;/p&gt;</comment>
                            <comment id="15505709" author="ferd" created="Tue, 20 Sep 2016 06:04:38 +0000"  >&lt;p&gt;Fix some dependencies issues&lt;/p&gt;</comment>
                            <comment id="15506147" author="hiveqa" created="Tue, 20 Sep 2016 09:25:19 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12829333/HIVE-14029.1.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12829333/HIVE-14029.1.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;green&quot;&gt;SUCCESS:&lt;/font&gt; +1 due to 2 test(s) being added or modified.&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to 7 failed/errored test(s), 10498 tests executed&lt;br/&gt;
&lt;b&gt;Failed tests:&lt;/b&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_mapjoin]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ctas]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]
org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.testMetaDataCounts
org.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/1237/testReport&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/1237/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/1237/console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/1237/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1237/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1237/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 7 tests failed
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12829333 - PreCommit-HIVE-Build&lt;/p&gt;</comment>
                            <comment id="15506455" author="aihuaxu" created="Tue, 20 Sep 2016 13:05:33 +0000"  >&lt;p&gt;Do we need to finish &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-14240&quot; title=&quot;HoS itests shouldn&amp;#39;t depend on a Spark distribution&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-14240&quot;&gt;HIVE-14240&lt;/a&gt; to get this unblocked? &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=spena&quot; class=&quot;user-hover&quot; rel=&quot;spena&quot;&gt;Sergio Pe&#241;a&lt;/a&gt;  It doesn&apos;t look like to me.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Ferd&quot; class=&quot;user-hover&quot; rel=&quot;Ferd&quot;&gt;Ferdinand Xu&lt;/a&gt;  There are several baselines which got updated by moving &lt;tt&gt;&quot;BASIC_STATS&quot;:&quot;true&quot;&lt;/tt&gt;. Do you know what causes it? &lt;/p&gt;</comment>
                            <comment id="15506647" author="lirui" created="Tue, 20 Sep 2016 14:08:56 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Ferd&quot; class=&quot;user-hover&quot; rel=&quot;Ferd&quot;&gt;Ferdinand Xu&lt;/a&gt;, could you also explain why we need to upgrade the dependencies, and why DataReadMethod is removed from InputMetrics?&lt;/p&gt;</comment>
                            <comment id="15506658" author="ferd" created="Tue, 20 Sep 2016 14:12:11 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aihuaxu&quot; class=&quot;user-hover&quot; rel=&quot;aihuaxu&quot;&gt;Aihua Xu&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-14240&quot; title=&quot;HoS itests shouldn&amp;#39;t depend on a Spark distribution&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-14240&quot;&gt;HIVE-14240&lt;/a&gt; is trying to replacing current tgz file. This patch can bypass it by using a tmp file. It doesn&apos;t block &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-14240&quot; title=&quot;HoS itests shouldn&amp;#39;t depend on a Spark distribution&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-14240&quot;&gt;HIVE-14240&lt;/a&gt;. &lt;br/&gt;
I am not quite sure why the order changed. It looks strange that the context is the same expect the displaying order.&lt;/p&gt;</comment>
                            <comment id="15506676" author="ferd" created="Tue, 20 Sep 2016 14:14:58 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;Rui Li&lt;/a&gt;, some APIs are changed in Spark side and the updates for dependencies are required since Spark use newer version which will lead inconsistent errors for HoS.&lt;/p&gt;</comment>
                            <comment id="15506697" author="ferd" created="Tue, 20 Sep 2016 14:21:45 +0000"  >&lt;p&gt;&lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/1239&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/1239&lt;/a&gt; was failed. Retest the patch.&lt;/p&gt;</comment>
                            <comment id="15506716" author="spena" created="Tue, 20 Sep 2016 14:25:18 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Ferd&quot; class=&quot;user-hover&quot; rel=&quot;Ferd&quot;&gt;Ferdinand Xu&lt;/a&gt; I think we should try to fix &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-14240&quot; title=&quot;HoS itests shouldn&amp;#39;t depend on a Spark distribution&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-14240&quot;&gt;HIVE-14240&lt;/a&gt; first to avoid dependencies issues when Spark and Hive are running in the same machine. I talked with the Spark team a few times, and they think this assembly tar.gz will cause issues due to other Hive libraries Spark depends, such as Hive 1.2 metastore and Hive 1.2 serde.&lt;/p&gt;

&lt;p&gt;Would you like to start working on &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-14240&quot; title=&quot;HoS itests shouldn&amp;#39;t depend on a Spark distribution&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-14240&quot;&gt;HIVE-14240&lt;/a&gt;? You can ask &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stakiar&quot; class=&quot;user-hover&quot; rel=&quot;stakiar&quot;&gt;Sahil Takiar&lt;/a&gt; if you&apos;re interested.&lt;/p&gt;</comment>
                            <comment id="15506738" author="ferd" created="Tue, 20 Sep 2016 14:32:27 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=spena&quot; class=&quot;user-hover&quot; rel=&quot;spena&quot;&gt;Sergio Pe&#241;a&lt;/a&gt;, I am not quite sure why assembly tar.gz  will cause issues for Hive since it&apos;s included in Hive itest only. Could you explain a little bit more?  BTW, I will take a look at how to remove it from itest.&lt;/p&gt;</comment>
                            <comment id="15506772" author="spena" created="Tue, 20 Sep 2016 14:43:27 +0000"  >&lt;p&gt;Sure. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stakiar&quot; class=&quot;user-hover&quot; rel=&quot;stakiar&quot;&gt;Sahil Takiar&lt;/a&gt; Let me know if the below statements are correct, and feel free to correct me.&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Spark2 uses a fork of Hive 1.2 due to issues with Apache Hive. They called this project &lt;tt&gt;spark-hive&lt;/tt&gt;. Spark only uses Hive 1.2 metastore/serde/udf jars form this forked project.&lt;br/&gt;
  They download this from &lt;a href=&quot;https://mvnrepository.com/artifact/org.apache.spark/spark-hive_2.10&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://mvnrepository.com/artifact/org.apache.spark/spark-hive_2.10&lt;/a&gt; &lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Spark2 assembly without hive will be built without any of the above dependencies.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Hive2 itests will use Spark2 assembly to run Hive2 tests. This means Hive2 might not test Spark2 correctly due to the lack of Hive 1.2 libraries in it.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="15506833" author="lirui" created="Tue, 20 Sep 2016 15:09:20 +0000"  >&lt;blockquote&gt;&lt;p&gt;some APIs are changed in Spark side&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Is there any other way we can track the read method? If not, guess we can just remove the class from Hive side.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Hive2 itests will use Spark2 assembly to run Hive2 tests. This means Hive2 might not test Spark2 correctly due to the lack of Hive 1.2 libraries in it.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I&apos;m not sure what problem spark has without hive libraries. We have been requiring that spark is built without hive. Otherwise we&apos;ll have different hive libraries in our classpath which causes conflicts.&lt;br/&gt;
I don&apos;t think &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-14240&quot; title=&quot;HoS itests shouldn&amp;#39;t depend on a Spark distribution&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-14240&quot;&gt;HIVE-14240&lt;/a&gt; blocks this one. Actually &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-14240&quot; title=&quot;HoS itests shouldn&amp;#39;t depend on a Spark distribution&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-14240&quot;&gt;HIVE-14240&lt;/a&gt; should be implemented for Spark 2.0 right?&lt;/p&gt;</comment>
                            <comment id="15506839" author="lirui" created="Tue, 20 Sep 2016 15:12:29 +0000"  >&lt;p&gt;My understanding is spark needs hive libraries only for SparkSQL, which is not needed for HoS.&lt;/p&gt;</comment>
                            <comment id="15506942" author="hiveqa" created="Tue, 20 Sep 2016 15:53:50 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12829398/HIVE-14029.1.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12829398/HIVE-14029.1.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;green&quot;&gt;SUCCESS:&lt;/font&gt; +1 due to 2 test(s) being added or modified.&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to 23 failed/errored test(s), 10556 tests executed&lt;br/&gt;
&lt;b&gt;Failed tests:&lt;/b&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_mapjoin]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ctas]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[bucket4]
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[bucket5]
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[disable_merge_for_bucketing]
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[list_bucket_dml_10]
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[reduce_deduplicate]
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[bucket4]
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[disable_merge_for_bucketing]
org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.testMetaDataCounts
org.apache.hadoop.hive.ql.exec.spark.session.TestSparkSessionManagerImpl.testMultiSessionMultipleUse
org.apache.hadoop.hive.ql.exec.spark.session.TestSparkSessionManagerImpl.testSingleSessionMultipleUse
org.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching
org.apache.hive.spark.client.TestSparkClient.testAddJarsAndFiles
org.apache.hive.spark.client.TestSparkClient.testCounters
org.apache.hive.spark.client.TestSparkClient.testErrorJob
org.apache.hive.spark.client.TestSparkClient.testJobSubmission
org.apache.hive.spark.client.TestSparkClient.testMetricsCollection
org.apache.hive.spark.client.TestSparkClient.testRemoteClient
org.apache.hive.spark.client.TestSparkClient.testSimpleSparkJob
org.apache.hive.spark.client.TestSparkClient.testSyncRpc
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/1242/testReport&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/1242/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/1242/console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/1242/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1242/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1242/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 23 tests failed
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12829398 - PreCommit-HIVE-Build&lt;/p&gt;</comment>
                            <comment id="15507641" author="stakiar" created="Tue, 20 Sep 2016 20:03:24 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Ferd&quot; class=&quot;user-hover&quot; rel=&quot;Ferd&quot;&gt;Ferdinand Xu&lt;/a&gt; how was the &lt;a href=&quot;http://blog.sundp.me/spark/spark-2.0.0-bin-hadoop2-without-hive.tgz&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://blog.sundp.me/spark/spark-2.0.0-bin-hadoop2-without-hive.tgz&lt;/a&gt; built?&lt;/p&gt;

&lt;p&gt;I don&apos;t think &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-14240&quot; title=&quot;HoS itests shouldn&amp;#39;t depend on a Spark distribution&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-14240&quot;&gt;HIVE-14240&lt;/a&gt; is a blocker for this assuming the tar-ball was built in a supported way, but I&apos;m trying to contact some Spark committers to see if they have any input.&lt;/p&gt;</comment>
                            <comment id="15508316" author="ferd" created="Wed, 21 Sep 2016 01:03:41 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stakiar&quot; class=&quot;user-hover&quot; rel=&quot;stakiar&quot;&gt;Sahil Takiar&lt;/a&gt;, the tgz was built via the following commands:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
sh ./dev/make-distribution.sh&#160; --name hadoop2-without-hive --tgz -Phadoop-2.7 -Pyarn -Pparquet-provided -Dhadoop.version=2.7.3
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dapengsun&quot; class=&quot;user-hover&quot; rel=&quot;dapengsun&quot;&gt;Dapeng Sun&lt;/a&gt;, can you confirm it please?&lt;/p&gt;</comment>
                            <comment id="15508350" author="ferd" created="Wed, 21 Sep 2016 01:22:15 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=spena&quot; class=&quot;user-hover&quot; rel=&quot;spena&quot;&gt;Sergio Pe&#241;a&lt;/a&gt;, I think we should move it forwards since &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-14240&quot; title=&quot;HoS itests shouldn&amp;#39;t depend on a Spark distribution&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-14240&quot;&gt;HIVE-14240&lt;/a&gt; needs further discussions and it doesn&apos;t block this ticket. We can upload the tgz into a stable location to upgrade the Spark version and once we fixed &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-14240&quot; title=&quot;HoS itests shouldn&amp;#39;t depend on a Spark distribution&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-14240&quot;&gt;HIVE-14240&lt;/a&gt;, we can easily remove this tgz. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;Rui Li&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stakiar&quot; class=&quot;user-hover&quot; rel=&quot;stakiar&quot;&gt;Sahil Takiar&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aihuaxu&quot; class=&quot;user-hover&quot; rel=&quot;aihuaxu&quot;&gt;Aihua Xu&lt;/a&gt; any thoughts?&lt;/p&gt;</comment>
                            <comment id="15508353" author="dapengsun" created="Wed, 21 Sep 2016 01:23:45 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Ferd&quot; class=&quot;user-hover&quot; rel=&quot;Ferd&quot;&gt;Ferdinand Xu&lt;/a&gt;&lt;br/&gt;
Yes, I used this command&lt;/p&gt;</comment>
                            <comment id="15508437" author="lirui" created="Wed, 21 Sep 2016 02:10:10 +0000"  >&lt;p&gt;I agree to move this forward. &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-14240&quot; title=&quot;HoS itests shouldn&amp;#39;t depend on a Spark distribution&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-14240&quot;&gt;HIVE-14240&lt;/a&gt; can be done in parallel, if it doesn&apos;t depend on this one &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15508636" author="ferd" created="Wed, 21 Sep 2016 03:48:37 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;Rui Li&lt;/a&gt; &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Is there any other way we can track the read method? If not, guess we can just remove the class from Hive side.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I will investigate this in a separate JIRA. Thank you for pointing this out.&lt;/p&gt;</comment>
                            <comment id="15508990" author="hiveqa" created="Wed, 21 Sep 2016 07:01:06 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12829489/HIVE-14029.2.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12829489/HIVE-14029.2.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;green&quot;&gt;SUCCESS:&lt;/font&gt; +1 due to 2 test(s) being added or modified.&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to 6 failed/errored test(s), 10556 tests executed&lt;br/&gt;
&lt;b&gt;Failed tests:&lt;/b&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_mapjoin]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ctas]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]
org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.testMetaDataCounts
org.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/1249/testReport&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/1249/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/1249/console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/1249/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1249/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1249/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 6 tests failed
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12829489 - PreCommit-HIVE-Build&lt;/p&gt;</comment>
                            <comment id="15509101" author="ferd" created="Wed, 21 Sep 2016 07:46:47 +0000"  >&lt;p&gt;Hi folks, the failed cases are not related. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=spena&quot; class=&quot;user-hover&quot; rel=&quot;spena&quot;&gt;Sergio Pe&#241;a&lt;/a&gt;, do you have comments for the patch? I am going to commit it if no further comments.&lt;/p&gt;</comment>
                            <comment id="15509231" author="lirui" created="Wed, 21 Sep 2016 08:40:45 +0000"  >&lt;p&gt;+1 to the latest patch.&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Ferd&quot; class=&quot;user-hover&quot; rel=&quot;Ferd&quot;&gt;Ferdinand Xu&lt;/a&gt;, is there anything needs to be updated in our getting started &lt;a href=&quot;https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;wiki&lt;/a&gt;? I think at least we need to update how to integrate spark since we no longer have the assembly jar.&lt;/p&gt;</comment>
                            <comment id="15509847" author="ferd" created="Wed, 21 Sep 2016 13:02:06 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;Rui Li&lt;/a&gt;, WIKI needs to be updated because for Spark 2.0.0 or above, there is no assembly jar built. Before updating the wiki, I need to verify it locally.&lt;/p&gt;</comment>
                            <comment id="15509966" author="lirui" created="Wed, 21 Sep 2016 13:50:03 +0000"  >&lt;p&gt;OK. AFAIK, HoS only needs spark-core. So we can try adding spark-core and all its dependencies to hive&apos;s classpath.&lt;/p&gt;</comment>
                            <comment id="15510135" author="spena" created="Wed, 21 Sep 2016 14:37:14 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Ferd&quot; class=&quot;user-hover&quot; rel=&quot;Ferd&quot;&gt;Ferdinand Xu&lt;/a&gt;. I uploaded the spark 2.0 assembly jar to the stable location. Could you upload a new patch without the URL spark assembly change? To test that it will work.&lt;/p&gt;

&lt;p&gt;I will review the patch today.&lt;/p&gt;</comment>
                            <comment id="15510245" author="ferd" created="Wed, 21 Sep 2016 15:14:08 +0000"  >&lt;p&gt;Do you mean the tgz file? What&apos;s the new address for it or the same name as before? &lt;/p&gt;</comment>
                            <comment id="15510257" author="spena" created="Wed, 21 Sep 2016 15:18:18 +0000"  >&lt;p&gt;It is the same as before.&lt;/p&gt;</comment>
                            <comment id="15510337" author="ferd" created="Wed, 21 Sep 2016 15:45:41 +0000"  >&lt;p&gt;Attached as &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-14029&quot; title=&quot;Update Spark version to 2.0.0&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-14029&quot;&gt;&lt;del&gt;HIVE-14029&lt;/del&gt;&lt;/a&gt;.3.patch.&lt;/p&gt;</comment>
                            <comment id="15510503" author="hiveqa" created="Wed, 21 Sep 2016 16:54:34 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12829601/HIVE-14029.3.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12829601/HIVE-14029.3.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;green&quot;&gt;SUCCESS:&lt;/font&gt; +1 due to 2 test(s) being added or modified.&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to 6 failed/errored test(s), 10556 tests executed&lt;br/&gt;
&lt;b&gt;Failed tests:&lt;/b&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_mapjoin]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ctas]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]
org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.testMetaDataCounts
org.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/1254/testReport&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/1254/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/1254/console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/1254/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1254/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1254/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 6 tests failed
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12829601 - PreCommit-HIVE-Build&lt;/p&gt;</comment>
                            <comment id="15510656" author="stakiar" created="Wed, 21 Sep 2016 17:51:38 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Ferd&quot; class=&quot;user-hover&quot; rel=&quot;Ferd&quot;&gt;Ferdinand Xu&lt;/a&gt; overall this looks good to me. It would be nice if you could update the description to list out the high-level changes that needed to be made to Hive to add support for Spark 2.0.0. For example, dependency updates, which APIs changed (change from Iterable to Iterator, InputMetrics constructor change).&lt;/p&gt;</comment>
                            <comment id="15511038" author="spena" created="Wed, 21 Sep 2016 20:16:15 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Ferd&quot; class=&quot;user-hover&quot; rel=&quot;Ferd&quot;&gt;Ferdinand Xu&lt;/a&gt; The patch looks good. +1&lt;/p&gt;

&lt;p&gt;I just found a variable that is not used anymore.&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;MetricsCollection.java
 - Should we remove &apos;DataReadMethod readMethod = null&apos;? is not used anymore.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xuefuz&quot; class=&quot;user-hover&quot; rel=&quot;xuefuz&quot;&gt;Xuefu Zhang&lt;/a&gt; Do you think this patch is ready to go to start supporting spark 2.0?&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Ferd&quot; class=&quot;user-hover&quot; rel=&quot;Ferd&quot;&gt;Ferdinand Xu&lt;/a&gt; Have we run any other tests in an environment with spark 2.0 and hive 2.1? I think we should do that if you haven&apos;t yet before commit the patch. Just to confirm we don&apos;t have issues with the classpath.&lt;/p&gt;</comment>
                            <comment id="15511533" author="xuefuz" created="Wed, 21 Sep 2016 23:36:36 +0000"  >&lt;p&gt;Hi guys, thanks for working/reviewing this. The patch looks good. I understand that there is a pending discussion about removing spark tarball from the test. However, in this long thread there seems a confusion of this with the spark&apos;s assembly jar which is part of spark build as of 1.6. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Ferd&quot; class=&quot;user-hover&quot; rel=&quot;Ferd&quot;&gt;Ferdinand Xu&lt;/a&gt;, do we have a clear picture of that for 2.0? If there is any change, we do want to update the doc. For instance, I used to get the assembly.jar from spark build and copy it to hive&apos;s /lib directory and I&apos;m ready to run Hive on Spark.&lt;/p&gt;

&lt;p&gt;Sorry I&apos;m a little behind Spark 2.0. I will try to figure it out on my end as well.&lt;/p&gt;</comment>
                            <comment id="15511577" author="xuefuz" created="Thu, 22 Sep 2016 00:00:03 +0000"  >&lt;p&gt;I made a build of spark 2.0 and indeed spark-assembly.jar is missing.&lt;/p&gt;</comment>
                            <comment id="15511610" author="ferd" created="Thu, 22 Sep 2016 00:16:48 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xuefuz&quot; class=&quot;user-hover&quot; rel=&quot;xuefuz&quot;&gt;Xuefu Zhang&lt;/a&gt;, Spark assembly was removed since Spark 2.0.0. They don&apos;t provide an assembly jar considering some dependency conflicts. I find some comments in the root pom file for Spark. To support 2.0.0, we have to copy all Spark related jars under the hive/lib AFAIK.&lt;/p&gt;</comment>
                            <comment id="15511614" author="ferd" created="Thu, 22 Sep 2016 00:19:25 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=spena&quot; class=&quot;user-hover&quot; rel=&quot;spena&quot;&gt;Sergio Pe&#241;a&lt;/a&gt;, do we need to support in Hive 2.1? I do some smoke tests in current upstream and Spark 2.0 and it passed if you set SPARK_HOME correctly and copy all lib jars of Spark into hive/lib folder. This needed to be updated in Hive On Spark WIKI.&lt;/p&gt;</comment>
                            <comment id="15511682" author="lirui" created="Thu, 22 Sep 2016 00:54:16 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Ferd&quot; class=&quot;user-hover&quot; rel=&quot;Ferd&quot;&gt;Ferdinand Xu&lt;/a&gt;, the classpath is just for HS2/CLI, so I don&apos;t think we need all the spark jars. Please find a minimum set of required jars. You can start with spark-core.&lt;/p&gt;</comment>
                            <comment id="15512191" author="ferd" created="Thu, 22 Sep 2016 05:15:29 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stakiar&quot; class=&quot;user-hover&quot; rel=&quot;stakiar&quot;&gt;Sahil Takiar&lt;/a&gt; for your review. Description is updated.&lt;/p&gt;</comment>
                            <comment id="15512315" author="ferd" created="Thu, 22 Sep 2016 06:13:59 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;Rui Li&lt;/a&gt;, &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;I don&apos;t think we need all the spark jars.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Agree. It&apos;s not required for all Spark jars. From Hive pom.xml file, we see that it depends only on Spark_core. You can find the dependency for Spark_core. All of them should be included into HIVE_CLASSPATH. Do we really need to filter those jars? It isn&apos;t very user friendly because users have to find them one by one in Spark jars and add it to HIVE classpath. I think we can simple add the whole folder to HIVE classpath when running Hive on Spark. Any thoughts?&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
core]# mvn dependency:tree
[INFO] Scanning &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; projects...
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Spark Project Core 2.0.0
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-dependency-plugin:2.10:tree (&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;-cli) @ spark-core_2.11 ---
[INFO] org.apache.spark:spark-core_2.11:jar:2.0.0
[INFO] +- org.apache.avro:avro-mapred:jar:hadoop2:1.7.7:compile
[INFO] |  +- org.apache.avro:avro-ipc:jar:1.7.7:compile
[INFO] |  |  \- org.apache.avro:avro:jar:1.7.7:compile
[INFO] |  +- org.apache.avro:avro-ipc:jar:tests:1.7.7:test
[INFO] |  +- org.codehaus.jackson:jackson-core-asl:jar:1.9.13:compile
[INFO] |  \- org.codehaus.jackson:jackson-mapper-asl:jar:1.9.13:compile
[INFO] +- com.google.guava:guava:jar:14.0.1:provided
[INFO] +- com.twitter:chill_2.11:jar:0.8.0:compile
[INFO] |  \- com.esotericsoftware:kryo-shaded:jar:3.0.3:compile
[INFO] |     \- com.esotericsoftware:minlog:jar:1.3.0:compile
[INFO] +- com.twitter:chill-java:jar:0.8.0:compile
[INFO] +- org.apache.xbean:xbean-asm5-shaded:jar:4.4:compile
[INFO] +- org.apache.hadoop:hadoop-client:jar:2.2.0:compile
[INFO] |  +- org.apache.hadoop:hadoop-common:jar:2.2.0:compile
[INFO] |  |  +- commons-cli:commons-cli:jar:1.2:compile
[INFO] |  |  +- xmlenc:xmlenc:jar:0.52:compile
[INFO] |  |  +- commons-io:commons-io:jar:2.4:compile
[INFO] |  |  +- commons-lang:commons-lang:jar:2.6:compile
[INFO] |  |  +- commons-configuration:commons-configuration:jar:1.6:compile
[INFO] |  |  |  +- commons-digester:commons-digester:jar:1.8:compile
[INFO] |  |  |  |  \- commons-beanutils:commons-beanutils:jar:1.7.0:compile
[INFO] |  |  |  \- commons-beanutils:commons-beanutils-core:jar:1.8.0:compile
[INFO] |  |  +- com.google.protobuf:protobuf-java:jar:2.5.0:compile
[INFO] |  |  +- org.apache.hadoop:hadoop-auth:jar:2.2.0:compile
[INFO] |  |  \- org.apache.commons:commons-compress:jar:1.4.1:compile
[INFO] |  |     \- org.tukaani:xz:jar:1.0:compile
[INFO] |  +- org.apache.hadoop:hadoop-hdfs:jar:2.2.0:compile
[INFO] |  |  \- org.mortbay.jetty:jetty-util:jar:6.1.26:compile
[INFO] |  +- org.apache.hadoop:hadoop-mapreduce-client-app:jar:2.2.0:compile
[INFO] |  |  +- org.apache.hadoop:hadoop-mapreduce-client-common:jar:2.2.0:compile
[INFO] |  |  |  +- org.apache.hadoop:hadoop-yarn-client:jar:2.2.0:compile
[INFO] |  |  |  |  \- com.google.inject:guice:jar:3.0:compile
[INFO] |  |  |  |     +- javax.inject:javax.inject:jar:1:compile
[INFO] |  |  |  |     \- aopalliance:aopalliance:jar:1.0:compile
[INFO] |  |  |  \- org.apache.hadoop:hadoop-yarn-server-common:jar:2.2.0:compile
[INFO] |  |  \- org.apache.hadoop:hadoop-mapreduce-client-shuffle:jar:2.2.0:compile
[INFO] |  +- org.apache.hadoop:hadoop-yarn-api:jar:2.2.0:compile
[INFO] |  +- org.apache.hadoop:hadoop-mapreduce-client-core:jar:2.2.0:compile
[INFO] |  |  \- org.apache.hadoop:hadoop-yarn-common:jar:2.2.0:compile
[INFO] |  +- org.apache.hadoop:hadoop-mapreduce-client-jobclient:jar:2.2.0:compile
[INFO] |  \- org.apache.hadoop:hadoop-annotations:jar:2.2.0:compile
[INFO] +- org.apache.spark:spark-launcher_2.11:jar:2.0.0:compile
[INFO] +- org.apache.spark:spark-network-common_2.11:jar:2.0.0:compile
[INFO] +- org.apache.spark:spark-network-shuffle_2.11:jar:2.0.0:compile
[INFO] |  +- org.fusesource.leveldbjni:leveldbjni-all:jar:1.8:compile
[INFO] |  \- com.fasterxml.jackson.core:jackson-annotations:jar:2.6.5:compile
[INFO] +- org.apache.spark:spark-unsafe_2.11:jar:2.0.0:compile
[INFO] +- net.java.dev.jets3t:jets3t:jar:0.7.1:compile
[INFO] |  +- commons-codec:commons-codec:jar:1.10:compile
[INFO] |  \- commons-httpclient:commons-httpclient:jar:3.1:compile
[INFO] +- org.apache.curator:curator-recipes:jar:2.4.0:compile
[INFO] |  +- org.apache.curator:curator-framework:jar:2.4.0:compile
[INFO] |  |  \- org.apache.curator:curator-client:jar:2.4.0:compile
[INFO] |  \- org.apache.zookeeper:zookeeper:jar:3.4.5:compile
[INFO] +- org.eclipse.jetty:jetty-plus:jar:9.2.16.v20160414:compile
[INFO] |  +- org.eclipse.jetty:jetty-webapp:jar:9.2.16.v20160414:compile
[INFO] |  |  \- org.eclipse.jetty:jetty-xml:jar:9.2.16.v20160414:compile
[INFO] |  \- org.eclipse.jetty:jetty-jndi:jar:9.2.16.v20160414:compile
[INFO] +- org.eclipse.jetty:jetty-security:jar:9.2.16.v20160414:compile
[INFO] +- org.eclipse.jetty:jetty-util:jar:9.2.16.v20160414:compile
[INFO] +- org.eclipse.jetty:jetty-server:jar:9.2.16.v20160414:compile
[INFO] |  \- org.eclipse.jetty:jetty-io:jar:9.2.16.v20160414:compile
[INFO] +- org.eclipse.jetty:jetty-http:jar:9.2.16.v20160414:compile
[INFO] +- org.eclipse.jetty:jetty-continuation:jar:9.2.16.v20160414:compile
[INFO] +- org.eclipse.jetty:jetty-servlet:jar:9.2.16.v20160414:compile
[INFO] +- org.eclipse.jetty:jetty-servlets:jar:9.2.16.v20160414:compile
[INFO] +- javax.servlet:javax.servlet-api:jar:3.1.0:compile
[INFO] +- org.apache.commons:commons-lang3:jar:3.3.2:compile
[INFO] +- org.apache.commons:commons-math3:jar:3.4.1:compile
[INFO] +- com.google.code.findbugs:jsr305:jar:1.3.9:compile
[INFO] +- org.slf4j:slf4j-api:jar:1.7.16:compile
[INFO] +- org.slf4j:jul-to-slf4j:jar:1.7.16:compile
[INFO] +- org.slf4j:jcl-over-slf4j:jar:1.7.16:compile
[INFO] +- log4j:log4j:jar:1.2.17:compile
[INFO] +- org.slf4j:slf4j-log4j12:jar:1.7.16:compile
[INFO] +- com.ning:compress-lzf:jar:1.0.3:compile
[INFO] +- org.xerial.snappy:snappy-java:jar:1.1.2.4:compile
[INFO] +- net.jpountz.lz4:lz4:jar:1.3.0:compile
[INFO] +- org.roaringbitmap:RoaringBitmap:jar:0.5.11:compile
[INFO] +- commons-net:commons-net:jar:2.2:compile
[INFO] +- org.scala-lang:scala-library:jar:2.11.8:compile
[INFO] +- org.json4s:json4s-jackson_2.11:jar:3.2.11:compile
[INFO] |  \- org.json4s:json4s-core_2.11:jar:3.2.11:compile
[INFO] |     +- org.json4s:json4s-ast_2.11:jar:3.2.11:compile
[INFO] |     +- com.thoughtworks.paranamer:paranamer:jar:2.6:compile
[INFO] |     \- org.scala-lang:scalap:jar:2.11.8:compile
[INFO] |        \- org.scala-lang:scala-compiler:jar:2.11.8:compile
[INFO] |           \- org.scala-lang.modules:scala-parser-combinators_2.11:jar:1.0.4:compile
[INFO] +- org.glassfish.jersey.core:jersey-client:jar:2.22.2:compile
[INFO] |  +- javax.ws.rs:javax.ws.rs-api:jar:2.0.1:compile
[INFO] |  +- org.glassfish.hk2:hk2-api:jar:2.4.0-b34:compile
[INFO] |  |  +- org.glassfish.hk2:hk2-utils:jar:2.4.0-b34:compile
[INFO] |  |  \- org.glassfish.hk2.external:aopalliance-repackaged:jar:2.4.0-b34:compile
[INFO] |  +- org.glassfish.hk2.external:javax.inject:jar:2.4.0-b34:compile
[INFO] |  \- org.glassfish.hk2:hk2-locator:jar:2.4.0-b34:compile
[INFO] +- org.glassfish.jersey.core:jersey-common:jar:2.22.2:compile
[INFO] |  +- javax.annotation:javax.annotation-api:jar:1.2:compile
[INFO] |  +- org.glassfish.jersey.bundles.repackaged:jersey-guava:jar:2.22.2:compile
[INFO] |  \- org.glassfish.hk2:osgi-resource-locator:jar:1.0.1:compile
[INFO] +- org.glassfish.jersey.core:jersey-server:jar:2.22.2:compile
[INFO] |  +- org.glassfish.jersey.media:jersey-media-jaxb:jar:2.22.2:compile
[INFO] |  \- javax.validation:validation-api:jar:1.1.0.Final:compile
[INFO] +- org.glassfish.jersey.containers:jersey-container-servlet:jar:2.22.2:compile
[INFO] +- org.glassfish.jersey.containers:jersey-container-servlet-core:jar:2.22.2:compile
[INFO] +- org.apache.mesos:mesos:jar:shaded-protobuf:0.21.1:compile
[INFO] +- io.netty:netty-all:jar:4.0.29.Final:compile
[INFO] +- io.netty:netty:jar:3.8.0.Final:compile
[INFO] +- com.clearspring.analytics:stream:jar:2.7.0:compile
[INFO] +- io.dropwizard.metrics:metrics-core:jar:3.1.2:compile
[INFO] +- io.dropwizard.metrics:metrics-jvm:jar:3.1.2:compile
[INFO] +- io.dropwizard.metrics:metrics-json:jar:3.1.2:compile
[INFO] +- io.dropwizard.metrics:metrics-graphite:jar:3.1.2:compile
[INFO] +- com.fasterxml.jackson.core:jackson-databind:jar:2.6.5:compile
[INFO] |  \- com.fasterxml.jackson.core:jackson-core:jar:2.6.5:compile
[INFO] +- com.fasterxml.jackson.module:jackson-module-scala_2.11:jar:2.6.5:compile
[INFO] |  +- org.scala-lang:scala-reflect:jar:2.11.8:compile
[INFO] |  \- com.fasterxml.jackson.module:jackson-module-paranamer:jar:2.6.5:compile
[INFO] +- org.apache.derby:derby:jar:10.11.1.1:test
[INFO] +- org.apache.ivy:ivy:jar:2.4.0:compile
[INFO] +- oro:oro:jar:2.0.8:compile
[INFO] +- org.seleniumhq.selenium:selenium-java:jar:2.52.0:test
[INFO] |  +- org.seleniumhq.selenium:selenium-chrome-driver:jar:2.52.0:test
[INFO] |  |  \- org.seleniumhq.selenium:selenium-remote-driver:jar:2.52.0:test
[INFO] |  |     +- cglib:cglib-nodep:jar:2.1_3:test
[INFO] |  |     +- com.google.code.gson:gson:jar:2.3.1:test
[INFO] |  |     \- org.seleniumhq.selenium:selenium-api:jar:2.52.0:test
[INFO] |  +- org.seleniumhq.selenium:selenium-edge-driver:jar:2.52.0:test
[INFO] |  |  \- org.apache.commons:commons-exec:jar:1.3:test
[INFO] |  +- org.seleniumhq.selenium:selenium-firefox-driver:jar:2.52.0:test
[INFO] |  +- org.seleniumhq.selenium:selenium-ie-driver:jar:2.52.0:test
[INFO] |  |  +- net.java.dev.jna:jna:jar:4.1.0:test
[INFO] |  |  \- net.java.dev.jna:jna-platform:jar:4.1.0:test
[INFO] |  +- org.seleniumhq.selenium:selenium-safari-driver:jar:2.52.0:test
[INFO] |  +- org.seleniumhq.selenium:selenium-support:jar:2.52.0:test
[INFO] |  +- org.webbitserver:webbit:jar:0.4.14:test
[INFO] |  \- org.seleniumhq.selenium:selenium-leg-rc:jar:2.52.0:test
[INFO] +- org.seleniumhq.selenium:selenium-htmlunit-driver:jar:2.52.0:test
[INFO] |  +- net.sourceforge.htmlunit:htmlunit:jar:2.18:test
[INFO] |  |  +- xalan:xalan:jar:2.7.2:test
[INFO] |  |  |  \- xalan:serializer:jar:2.7.2:test
[INFO] |  |  +- org.apache.httpcomponents:httpmime:jar:4.5.2:test
[INFO] |  |  +- net.sourceforge.htmlunit:htmlunit-core-js:jar:2.17:test
[INFO] |  |  +- xerces:xercesImpl:jar:2.11.0:test
[INFO] |  |  +- net.sourceforge.nekohtml:nekohtml:jar:1.9.22:test
[INFO] |  |  +- net.sourceforge.cssparser:cssparser:jar:0.9.16:test
[INFO] |  |  |  \- org.w3c.css:sac:jar:1.3:test
[INFO] |  |  +- commons-logging:commons-logging:jar:1.2:test
[INFO] |  |  \- org.eclipse.jetty.websocket:websocket-client:jar:9.2.12.v20150709:test
[INFO] |  |     \- org.eclipse.jetty.websocket:websocket-common:jar:9.2.12.v20150709:test
[INFO] |  |        \- org.eclipse.jetty.websocket:websocket-api:jar:9.2.12.v20150709:test
[INFO] |  +- commons-collections:commons-collections:jar:3.2.2:compile
[INFO] |  \- org.apache.httpcomponents:httpclient:jar:4.5.2:test
[INFO] |     \- org.apache.httpcomponents:httpcore:jar:4.4.4:test
[INFO] +- xml-apis:xml-apis:jar:1.4.01:test
[INFO] +- org.hamcrest:hamcrest-core:jar:1.3:test
[INFO] +- org.hamcrest:hamcrest-library:jar:1.3:test
[INFO] +- org.mockito:mockito-core:jar:1.10.19:test
[INFO] |  \- org.objenesis:objenesis:jar:2.1:compile
[INFO] +- org.scalacheck:scalacheck_2.11:jar:1.12.5:test
[INFO] |  \- org.scala-sbt:test-&lt;span class=&quot;code-keyword&quot;&gt;interface&lt;/span&gt;:jar:1.0:test
[INFO] +- org.apache.curator:curator-test:jar:2.4.0:test
[INFO] |  +- org.javassist:javassist:jar:3.15.0-GA:compile
[INFO] |  \- org.apache.commons:commons-math:jar:2.2:compile
[INFO] +- net.razorvine:pyrolite:jar:4.9:compile
[INFO] +- net.sf.py4j:py4j:jar:0.10.1:compile
[INFO] +- org.apache.spark:spark-tags_2.11:jar:2.0.0:compile
[INFO] +- org.apache.commons:commons-crypto:jar:1.0.0:compile
[INFO] +- org.spark-project.spark:unused:jar:1.0.0:compile
[INFO] +- org.scalatest:scalatest_2.11:jar:2.2.6:test
[INFO] |  \- org.scala-lang.modules:scala-xml_2.11:jar:1.0.2:compile
[INFO] +- junit:junit:jar:4.12:test
[INFO] \- com.novocode:junit-&lt;span class=&quot;code-keyword&quot;&gt;interface&lt;/span&gt;:jar:0.11:test
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 2.475 s
[INFO] Finished at: 2016-09-22T06:34:12+08:00
[INFO] Final Memory: 23M/963M
[INFO] ------------------------------------------------------------------------
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15512366" author="githubbot" created="Thu, 22 Sep 2016 06:39:56 +0000"  >&lt;p&gt;GitHub user winningsix opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/hive/pull/103&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/hive/pull/103&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-14029&quot; title=&quot;Update Spark version to 2.0.0&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-14029&quot;&gt;&lt;del&gt;HIVE-14029&lt;/del&gt;&lt;/a&gt;: Update Spark version to 2.0.0&lt;/p&gt;

&lt;p&gt;    Changes include:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Spark API updates:&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    1. SparkShuffler#call return Iterator instead of Iterable&lt;br/&gt;
    2. SparkListener -&amp;gt; JavaSparkListener&lt;br/&gt;
    3. InputMetrics constructor doesn&#8217;t accept readMethod&lt;br/&gt;
    4. Method remoteBlocksFetched and localBlocksFetched in ShuffleReadMetrics return long type instead of integer&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Dependency upgrade:&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    1. Jackson: 2.4.2 -&amp;gt; 2.6.5&lt;br/&gt;
    2. Netty version: 4.0.23.Final -&amp;gt; 4.0.29.Final&lt;br/&gt;
    3. Scala binary version: 2.10 -&amp;gt; 2.11&lt;br/&gt;
    4. Scala version: 2.10.4 -&amp;gt; 2.11.8&lt;/p&gt;

&lt;p&gt;    Test done by smoke tests in a cluster and integration test in Jenkins&lt;/p&gt;

&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/winningsix/hive&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/winningsix/hive&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-14029&quot; title=&quot;Update Spark version to 2.0.0&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-14029&quot;&gt;&lt;del&gt;HIVE-14029&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/hive/pull/103.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/hive/pull/103.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #103&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit 965e57295a83b06db61b22f3fda0bb19e47c248a&lt;br/&gt;
Author: Ferdinand Xu &amp;lt;cheng.a.xu@intel.com&amp;gt;&lt;br/&gt;
Date:   2016-09-17T19:10:04Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-14029&quot; title=&quot;Update Spark version to 2.0.0&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-14029&quot;&gt;&lt;del&gt;HIVE-14029&lt;/del&gt;&lt;/a&gt;: Update Spark version to 2.0.0&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="15512372" author="ferd" created="Thu, 22 Sep 2016 06:44:30 +0000"  >&lt;p&gt;Update patch addressing &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=spena&quot; class=&quot;user-hover&quot; rel=&quot;spena&quot;&gt;Sergio Pe&#241;a&lt;/a&gt;&apos;s comments.&lt;/p&gt;</comment>
                            <comment id="15512499" author="lirui" created="Thu, 22 Sep 2016 07:47:40 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Ferd&quot; class=&quot;user-hover&quot; rel=&quot;Ferd&quot;&gt;Ferdinand Xu&lt;/a&gt;, My thought was we should be able to tell the user what are actually needed (i.e. the minimum set of required jars) for HoS to work. Users can decide whether they want to add just the required jars, or all the jars under spark&apos;s dir for convenience. This is just something good to have and doesn&apos;t block this ticket - we used to add the whole assembly anyway.&lt;br/&gt;
Besides, I think not all the dependencies of spark-core are needed because some of them should be already in Hive&apos;s classpath, e.g. hadoop, commons, etc.&lt;/p&gt;</comment>
                            <comment id="15512678" author="lirui" created="Thu, 22 Sep 2016 09:00:46 +0000"  >&lt;p&gt;When I tried the patch locally, I got a compile error:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;org.apache.hive.hcatalog.templeton.mock.MockUriInfo is not abstract and does not override abstract method relativize(java.net.URI) in javax.ws.rs.core.UriInfo
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Does anybody have the same issue?&lt;/p&gt;</comment>
                            <comment id="15512929" author="hiveqa" created="Thu, 22 Sep 2016 10:46:40 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12829777/HIVE-14029.4.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12829777/HIVE-14029.4.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;green&quot;&gt;SUCCESS:&lt;/font&gt; +1 due to 2 test(s) being added or modified.&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to 7 failed/errored test(s), 10555 tests executed&lt;br/&gt;
&lt;b&gt;Failed tests:&lt;/b&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_mapjoin]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ctas]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]
org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.testMetaDataCounts
org.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching
org.apache.hive.spark.client.TestSparkClient.testJobSubmission
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/1269/testReport&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/1269/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/1269/console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/1269/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1269/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1269/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 7 tests failed
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12829777 - PreCommit-HIVE-Build&lt;/p&gt;</comment>
                            <comment id="15513291" author="ferd" created="Thu, 22 Sep 2016 13:22:01 +0000"  >&lt;p&gt;I have the same issue. You need to update it with following changes:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
--- a/hcatalog/webhcat/svr/src/test/java/org/apache/hive/hcatalog/templeton/mock/MockUriInfo.java
+++ b/hcatalog/webhcat/svr/src/test/java/org/apache/hive/hcatalog/templeton/mock/MockUriInfo.java
@@ -64,6 +64,14 @@ &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; UriBuilder getBaseUriBuilder() {
     &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;;
   }
 
+  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; URI resolve(URI uri) {
+    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;;
+  }
+
+  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; URI relativize(URI uri) {
+    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;;
+  }
+
   @Override
   &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; List&amp;lt;&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;&amp;gt; getMatchedURIs() {
     &lt;span class=&quot;code-comment&quot;&gt;// TODO Auto-generated method stub
&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I am not quite sure why this happens which can be reproduced in Jenkins. Possibly related to JDK version.&lt;/p&gt;</comment>
                            <comment id="15513430" author="spena" created="Thu, 22 Sep 2016 14:26:00 +0000"  >&lt;p&gt;Sorry Fer, I meant 2.2 &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/tongue.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;. I got confused with numbers.&lt;/p&gt;</comment>
                            <comment id="15513473" author="lirui" created="Thu, 22 Sep 2016 14:41:01 +0000"  >&lt;p&gt;Seems we have two &lt;tt&gt;javax.ws.rs.core.UriInfo&lt;/tt&gt; interfaces from two jars: javax.ws.rs-api and jersey-core. Before the patch, we only have one from jersey-core. Maybe there&apos;s some conflicts in the dependency upgrade. We need to fix it because it breaks build.&lt;/p&gt;</comment>
                            <comment id="15513528" author="xuefuz" created="Thu, 22 Sep 2016 15:08:45 +0000"  >&lt;p&gt;+1 on identifying the minimum set.&lt;/p&gt;</comment>
                            <comment id="15513534" author="spena" created="Thu, 22 Sep 2016 15:12:51 +0000"  >&lt;p&gt;Which JDK you&apos;re using? Jenkins is using JDK8&lt;/p&gt;</comment>
                            <comment id="15513539" author="lirui" created="Thu, 22 Sep 2016 15:14:59 +0000"  >&lt;p&gt;I&apos;m using:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;java version &quot;1.8.0_91&quot;
Java(TM) SE Runtime Environment (build 1.8.0_91-b14)
Java HotSpot(TM) 64-Bit Server VM (build 25.91-b14, mixed mode)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15515104" author="ferd" created="Fri, 23 Sep 2016 02:01:57 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=spena&quot; class=&quot;user-hover&quot; rel=&quot;spena&quot;&gt;Sergio Pe&#241;a&lt;/a&gt;, it&apos;s weird why Jenkins can build it successfully. Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;Rui Li&lt;/a&gt; I exclude the &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;javax.ws.rs&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt; imported by spark-core in 5th patch.&lt;/p&gt;</comment>
                            <comment id="15515131" author="ferd" created="Fri, 23 Sep 2016 02:17:59 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;Rui Li&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xuefuz&quot; class=&quot;user-hover&quot; rel=&quot;xuefuz&quot;&gt;Xuefu Zhang&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-14825&quot; title=&quot;Figure out the minimum set of required jars for Hive on Spark after bumping up to Spark 2.0.0&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-14825&quot;&gt;&lt;del&gt;HIVE-14825&lt;/del&gt;&lt;/a&gt; was created addressing this.&lt;/p&gt;</comment>
                            <comment id="15515964" author="lirui" created="Fri, 23 Sep 2016 09:39:43 +0000"  >&lt;p&gt;The offending jar comes as a dependency of jersey-client:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;[INFO] |  +- org.glassfish.jersey.core:jersey-client:jar:2.22.2:compile
[INFO] |  |  +- javax.ws.rs:javax.ws.rs-api:jar:2.0.1:compile
[INFO] |  |  +- org.glassfish.hk2:hk2-api:jar:2.4.0-b34:compile
[INFO] |  |  |  +- org.glassfish.hk2:hk2-utils:jar:2.4.0-b34:compile
[INFO] |  |  |  \- org.glassfish.hk2.external:aopalliance-repackaged:jar:2.4.0-b34:compile
[INFO] |  |  +- org.glassfish.hk2.external:javax.inject:jar:2.4.0-b34:compile
[INFO] |  |  \- org.glassfish.hk2:hk2-locator:jar:2.4.0-b34:compile
[INFO] |  |     \- org.javassist:javassist:jar:3.18.1-GA:compile
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;I think it&apos;s related to &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-12154&quot; title=&quot;Upgrade to Jersey 2&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-12154&quot;&gt;&lt;del&gt;SPARK-12154&lt;/del&gt;&lt;/a&gt;. Spark updated to Jersey 2 and replaced com.sun.jersey with org.glassfish.jersey. Good news is seems we don&apos;t pack the jersey stuff in hive-exec. But not sure if this only affects the compile.&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xuefuz&quot; class=&quot;user-hover&quot; rel=&quot;xuefuz&quot;&gt;Xuefu Zhang&lt;/a&gt; what do you think about this?&lt;/p&gt;</comment>
                            <comment id="15516797" author="hiveqa" created="Fri, 23 Sep 2016 15:45:06 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12829975/HIVE-14029.5.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12829975/HIVE-14029.5.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;green&quot;&gt;SUCCESS:&lt;/font&gt; +1 due to 2 test(s) being added or modified.&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to 7 failed/errored test(s), 10559 tests executed&lt;br/&gt;
&lt;b&gt;Failed tests:&lt;/b&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_mapjoin]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ctas]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]
org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.testMetaDataCounts
org.apache.hadoop.hive.thrift.TestHadoopAuthBridge23.testDelegationTokenSharedStore
org.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/1287/testReport&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/1287/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/1287/console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/1287/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1287/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1287/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 7 tests failed
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12829975 - PreCommit-HIVE-Build&lt;/p&gt;</comment>
                            <comment id="15516900" author="xuefuz" created="Fri, 23 Sep 2016 16:27:52 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;Rui Li&lt;/a&gt;, Thanks for the investigation. I&apos;m wondering if that dependency can be excluded in Hive&apos;s build? While the latest patch builds, it changes Hive&apos;s existing dependency, which might cause some problem.&lt;/p&gt;

&lt;p&gt;Also, we are upgrading the following libraries. I&apos;m not sure If it&apos;s absolutely necessary. From my build alone, it seems not. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Ferd&quot; class=&quot;user-hover&quot; rel=&quot;Ferd&quot;&gt;Ferdinand Xu&lt;/a&gt;, any thoughts?&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
	&lt;li&gt;
	&lt;ul&gt;
		&lt;li&gt;Jackson: 2.4.2 -&amp;gt; 2.6.5&lt;/li&gt;
		&lt;li&gt;Netty version: 4.0.23.Final -&amp;gt; 4.0.29.Final&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;</comment>
                            <comment id="15518237" author="ferd" created="Sat, 24 Sep 2016 03:04:31 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xuefuz&quot; class=&quot;user-hover&quot; rel=&quot;xuefuz&quot;&gt;Xuefu Zhang&lt;/a&gt; These two dependencies (Jackson and Netty) are not required in build. It&apos;s required for the runtime. If you try to run some HoS job, it will fail to create Spark client since API changes in these two library. You can see failed queries above for the reference.&lt;/p&gt;</comment>
                            <comment id="15518326" author="ferd" created="Sat, 24 Sep 2016 04:09:22 +0000"  >&lt;p&gt;Let&apos;s see whether it breaks qtest after removing org.glassfish.jersey related dependencies from Spark_core&lt;/p&gt;</comment>
                            <comment id="15518578" author="hiveqa" created="Sat, 24 Sep 2016 07:08:13 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12830171/HIVE-14029.6.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12830171/HIVE-14029.6.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;green&quot;&gt;SUCCESS:&lt;/font&gt; +1 due to 2 test(s) being added or modified.&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to 16 failed/errored test(s), 10629 tests executed&lt;br/&gt;
&lt;b&gt;Failed tests:&lt;/b&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_mapjoin]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ctas]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]
org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.testMetaDataCounts
org.apache.hadoop.hive.ql.exec.spark.session.TestSparkSessionManagerImpl.testMultiSessionMultipleUse
org.apache.hadoop.hive.ql.exec.spark.session.TestSparkSessionManagerImpl.testSingleSessionMultipleUse
org.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching
org.apache.hive.spark.client.TestSparkClient.testAddJarsAndFiles
org.apache.hive.spark.client.TestSparkClient.testCounters
org.apache.hive.spark.client.TestSparkClient.testErrorJob
org.apache.hive.spark.client.TestSparkClient.testJobSubmission
org.apache.hive.spark.client.TestSparkClient.testMetricsCollection
org.apache.hive.spark.client.TestSparkClient.testRemoteClient
org.apache.hive.spark.client.TestSparkClient.testSimpleSparkJob
org.apache.hive.spark.client.TestSparkClient.testSyncRpc
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/1299/testReport&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/1299/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/1299/console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/1299/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1299/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1299/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 16 tests failed
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12830171 - PreCommit-HIVE-Build&lt;/p&gt;</comment>
                            <comment id="15518591" author="ferd" created="Sat, 24 Sep 2016 07:18:11 +0000"  >&lt;p&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xuefuz&quot; class=&quot;user-hover&quot; rel=&quot;xuefuz&quot;&gt;Xuefu Zhang&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;Rui Li&lt;/a&gt;, Removing the dependency about org.glassfish.jersey will fail QTest with following error. I think we should use and commit the 5th patch instead.  Any thoughts about it? &lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2016-09-23T23:48:02,527  INFO [Driver] util.log: Logging initialized @3685ms
Exception in thread &lt;span class=&quot;code-quote&quot;&gt;&quot;Driver&quot;&lt;/span&gt; java.lang.NoClassDefFoundError: org/glassfish/jersey/servlet/ServletContainer
	at org.apache.spark.status.api.v1.ApiRootResource$.getServletHandler(ApiRootResource.scala:193)
	at org.apache.spark.ui.SparkUI.initialize(SparkUI.scala:75)
	at org.apache.spark.ui.SparkUI.&amp;lt;init&amp;gt;(SparkUI.scala:81)
	at org.apache.spark.ui.SparkUI$.create(SparkUI.scala:215)
	at org.apache.spark.ui.SparkUI$.createLiveUI(SparkUI.scala:157)
	at org.apache.spark.SparkContext.&amp;lt;init&amp;gt;(SparkContext.scala:443)
	at org.apache.spark.api.java.JavaSparkContext.&amp;lt;init&amp;gt;(JavaSparkContext.scala:58)
	at org.apache.hive.spark.client.RemoteDriver.&amp;lt;init&amp;gt;(RemoteDriver.java:157)
	at org.apache.hive.spark.client.RemoteDriver.main(RemoteDriver.java:516)
	at org.apache.hive.spark.client.SparkClientImpl$2.run(SparkClientImpl.java:228)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
Caused by: java.lang.ClassNotFoundException: org.glassfish.jersey.servlet.ServletContainer
	at java.net.URLClassLoader$1.run(URLClassLoader.java:372)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:361)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:360)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;ClassLoader&lt;/span&gt;.loadClass(&lt;span class=&quot;code-object&quot;&gt;ClassLoader&lt;/span&gt;.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;ClassLoader&lt;/span&gt;.loadClass(&lt;span class=&quot;code-object&quot;&gt;ClassLoader&lt;/span&gt;.java:357)
	... 11 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
</comment>
                            <comment id="15519134" author="lirui" created="Sat, 24 Sep 2016 14:40:25 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Ferd&quot; class=&quot;user-hover&quot; rel=&quot;Ferd&quot;&gt;Ferdinand Xu&lt;/a&gt;, you mean unit test will fail right? I don&apos;t see failed spark qtest in last QA report. For qtest/runtime, we use spark-submit to submit the app, so spark should add all its dependencies to classpath. One problem I can think of is if we don&apos;t exclude the glassfish jersey, will hive pull two versions of jersey into its classpath, i.e. the lib dir? If so, that can cause problem for hive&apos;s functionalities that depend on jersey.&lt;/p&gt;</comment>
                            <comment id="15521852" author="ferd" created="Mon, 26 Sep 2016 02:41:17 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;Rui Li&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;you mean unit test will fail right?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;It&apos;s my fault, it&apos;s failing some unit tests. HMM, the failed cases are caused by lack of those jars. To fix them, include Glassfish related jars in &lt;b&gt;test only&lt;/b&gt;. Attached is the new version addressing above.&lt;/p&gt;</comment>
                            <comment id="15521956" author="hiveqa" created="Mon, 26 Sep 2016 03:54:22 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12830239/HIVE-14029.7.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12830239/HIVE-14029.7.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;green&quot;&gt;SUCCESS:&lt;/font&gt; +1 due to 2 test(s) being added or modified.&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to 8 failed/errored test(s), 10629 tests executed&lt;br/&gt;
&lt;b&gt;Failed tests:&lt;/b&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_mapjoin]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ctas]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]
org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.testMetaDataCounts
org.apache.hadoop.hive.ql.exec.spark.session.TestSparkSessionManagerImpl.testMultiSessionMultipleUse
org.apache.hadoop.hive.ql.exec.spark.session.TestSparkSessionManagerImpl.testSingleSessionMultipleUse
org.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/1300/testReport&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/1300/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/1300/console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/1300/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1300/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1300/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 8 tests failed
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12830239 - PreCommit-HIVE-Build&lt;/p&gt;</comment>
                            <comment id="15522284" author="hiveqa" created="Mon, 26 Sep 2016 07:04:26 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12830248/HIVE-14029.7.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12830248/HIVE-14029.7.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;green&quot;&gt;SUCCESS:&lt;/font&gt; +1 due to 2 test(s) being added or modified.&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to 7 failed/errored test(s), 10629 tests executed&lt;br/&gt;
&lt;b&gt;Failed tests:&lt;/b&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_mapjoin]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ctas]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]
org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.testMetaDataCounts
org.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching
org.apache.hive.spark.client.rpc.TestRpc.testClientTimeout
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/1301/testReport&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/1301/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/1301/console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/1301/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1301/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1301/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 7 tests failed
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12830248 - PreCommit-HIVE-Build&lt;/p&gt;</comment>
                            <comment id="15522527" author="hiveqa" created="Mon, 26 Sep 2016 09:09:09 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12830260/HIVE-14029.7.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12830260/HIVE-14029.7.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;green&quot;&gt;SUCCESS:&lt;/font&gt; +1 due to 2 test(s) being added or modified.&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to 8 failed/errored test(s), 10629 tests executed&lt;br/&gt;
&lt;b&gt;Failed tests:&lt;/b&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_mapjoin]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ctas]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]
org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.testMetaDataCounts
org.apache.hadoop.hive.ql.exec.spark.session.TestSparkSessionManagerImpl.testMultiSessionMultipleUse
org.apache.hadoop.hive.ql.exec.spark.session.TestSparkSessionManagerImpl.testSingleSessionMultipleUse
org.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/1303/testReport&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/1303/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/1303/console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/1303/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1303/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1303/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 8 tests failed
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12830260 - PreCommit-HIVE-Build&lt;/p&gt;</comment>
                            <comment id="15525265" author="hiveqa" created="Tue, 27 Sep 2016 06:45:49 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12830442/HIVE-14029.8.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12830442/HIVE-14029.8.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;green&quot;&gt;SUCCESS:&lt;/font&gt; +1 due to 2 test(s) being added or modified.&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to 7 failed/errored test(s), 10640 tests executed&lt;br/&gt;
&lt;b&gt;Failed tests:&lt;/b&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_mapjoin]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ctas]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]
org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.testMetaDataCounts
org.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching
org.apache.hive.spark.client.TestSparkClient.testJobSubmission
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/1309/testReport&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/1309/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/1309/console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/1309/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1309/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1309/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 7 tests failed
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12830442 - PreCommit-HIVE-Build&lt;/p&gt;</comment>
                            <comment id="15525416" author="lirui" created="Tue, 27 Sep 2016 07:59:25 +0000"  >&lt;p&gt;I tried the 5th patch locally. The jersey 2 stuff won&apos;t be pulled to lib, or the hive-exec jar. I also tried to identify the minimum set. I managed to run some simple queries (spark on yarn) with only &lt;tt&gt;scala-library, spark-core, spark-network-common, spark-network-shuffle&lt;/tt&gt;. However, if we want to support local mode, we need more jars added to hive&apos;s lib, including the jersey 2. Then we may have conflict problem. Other than that, I think the 5th patch is enough for us (although I think we should exclude jersey 2 instead of just javax.ws.rs).&lt;/p&gt;

&lt;p&gt;If we still want to go the way as the 6th, 7th patches, maybe we can look at how we handle the guava conflict in the pom of spark-client and do something similar.&lt;/p&gt;</comment>
                            <comment id="15525460" author="ferd" created="Tue, 27 Sep 2016 08:19:46 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;Rui Li&lt;/a&gt;, thank you for your investigation. Can you please update &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-14825&quot; title=&quot;Figure out the minimum set of required jars for Hive on Spark after bumping up to Spark 2.0.0&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-14825&quot;&gt;&lt;del&gt;HIVE-14825&lt;/del&gt;&lt;/a&gt; about the minimum jar set?&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;If we still want to go the way as the 6th, 7th patches, maybe we can look at how we handle the guava conflict in the pom of spark-client and do something similar.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We can do it in a separate ticket about Guava conflict.  For the 7th patch, there&apos;s still one failed test case &quot;org.apache.hive.spark.client.TestSparkClient.testJobSubmission&quot; which I can&apos;t reproduce locally. Let&apos;s wait for another HIVE QA report to see whether it&apos;s reproducible.&lt;/p&gt;</comment>
                            <comment id="15525573" author="lirui" created="Tue, 27 Sep 2016 09:16:05 +0000"  >&lt;p&gt;Yeah I&apos;ll update &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-14825&quot; title=&quot;Figure out the minimum set of required jars for Hive on Spark after bumping up to Spark 2.0.0&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-14825&quot;&gt;&lt;del&gt;HIVE-14825&lt;/del&gt;&lt;/a&gt; once I have identified the minimum set for different modes.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;We can do it in a separate ticket about Guava conflict.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I mean we used to have guava conflict (&lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-7387&quot; title=&quot;Guava version conflict between hadoop and spark [Spark-Branch]&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-7387&quot;&gt;&lt;del&gt;HIVE-7387&lt;/del&gt;&lt;/a&gt;), which is similar to this one: spark uses a newer version while hive/hadoop stick to the old one. At the end, spark shaded guava in the assembly to solve the issue (&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-2848&quot; title=&quot;Shade Guava in Spark deliverables&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-2848&quot;&gt;&lt;del&gt;SPARK-2848&lt;/del&gt;&lt;/a&gt;). You can refer to the pom of spark-client about how to explicitly add the guava jars to run the unit tests.&lt;/p&gt;</comment>
                            <comment id="15525706" author="hiveqa" created="Tue, 27 Sep 2016 10:26:46 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12830453/HIVE-14029.8.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12830453/HIVE-14029.8.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;green&quot;&gt;SUCCESS:&lt;/font&gt; +1 due to 2 test(s) being added or modified.&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to 6 failed/errored test(s), 10640 tests executed&lt;br/&gt;
&lt;b&gt;Failed tests:&lt;/b&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_mapjoin]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ctas]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]
org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.testMetaDataCounts
org.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/1311/testReport&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/1311/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/1311/console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/1311/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1311/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1311/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 6 tests failed
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12830453 - PreCommit-HIVE-Build&lt;/p&gt;</comment>
                            <comment id="15526164" author="ferd" created="Tue, 27 Sep 2016 13:44:13 +0000"  >&lt;p&gt;The latest patch passed all tests. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xuefuz&quot; class=&quot;user-hover&quot; rel=&quot;xuefuz&quot;&gt;Xuefu Zhang&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;Rui Li&lt;/a&gt;, do you have any further comments? I&apos;d like to commit it if you have no further comments about the latest patch.&lt;/p&gt;</comment>
                            <comment id="15526180" author="ferd" created="Tue, 27 Sep 2016 13:47:52 +0000"  >&lt;p&gt;Thank you for providing this information. I will try to investigate it in Spark side. Considering Spark 2.0.0 is already released, if there is some work to do in Spark side, we may have to wait for next release.&lt;/p&gt;</comment>
                            <comment id="15528012" author="ferd" created="Wed, 28 Sep 2016 01:22:01 +0000"  >&lt;p&gt;Committed to the master. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aihuaxu&quot; class=&quot;user-hover&quot; rel=&quot;aihuaxu&quot;&gt;Aihua Xu&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=szehon&quot; class=&quot;user-hover&quot; rel=&quot;szehon&quot;&gt;Szehon Ho&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xuefuz&quot; class=&quot;user-hover&quot; rel=&quot;xuefuz&quot;&gt;Xuefu Zhang&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;Rui Li&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=spena&quot; class=&quot;user-hover&quot; rel=&quot;spena&quot;&gt;Sergio Pe&#241;a&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stakiar&quot; class=&quot;user-hover&quot; rel=&quot;stakiar&quot;&gt;Sahil Takiar&lt;/a&gt; Thank you for the reviews.&lt;/p&gt;</comment>
                            <comment id="15528017" author="githubbot" created="Wed, 28 Sep 2016 01:24:14 +0000"  >&lt;p&gt;Github user winningsix closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/hive/pull/103&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/hive/pull/103&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15528484" author="lefty@hortonworks.com" created="Wed, 28 Sep 2016 05:39:15 +0000"  >&lt;p&gt;Should this be documented in the wiki?&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;Hive on Spark: Getting Started &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="15528490" author="ferd" created="Wed, 28 Sep 2016 05:44:24 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=leftylev&quot; class=&quot;user-hover&quot; rel=&quot;leftylev&quot;&gt;Lefty Leverenz&lt;/a&gt; for the remind. It should be updated in WIKI. Shall we add a new section for Hive on Spark 2.0?&lt;/p&gt;</comment>
                            <comment id="15528537" author="lirui" created="Wed, 28 Sep 2016 06:01:36 +0000"  >&lt;p&gt;Since different versions of Spark are not binary compatible, it&apos;d be good if we can document the min/max supported Spark version for each release of Hive, e.g. the minimum supported Spark version is 2.0.0 for Hive 2.2.0. Should have done this in previous upgrades &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15528563" author="lefty@hortonworks.com" created="Wed, 28 Sep 2016 06:14:41 +0000"  >&lt;p&gt;Okay, I added a TODOC2.2 label.&lt;/p&gt;</comment>
                            <comment id="15529838" author="spena" created="Wed, 28 Sep 2016 14:35:07 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;Rui Li&lt;/a&gt; Oh, Sounds like a big compatibility change for Hive 2.x series. &lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xuefuz&quot; class=&quot;user-hover&quot; rel=&quot;xuefuz&quot;&gt;Xuefu Zhang&lt;/a&gt; Do you know how we handle these breaking changes on Hive versions? &lt;/p&gt;</comment>
                            <comment id="15531017" author="stakiar" created="Wed, 28 Sep 2016 22:04:49 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Ferd&quot; class=&quot;user-hover&quot; rel=&quot;Ferd&quot;&gt;Ferdinand Xu&lt;/a&gt; can you also add an &quot;Incompatible Change&quot; Flag to this JIRA.&lt;/p&gt;

&lt;p&gt;I&apos;m guessing this should go into the Hive 2.2.0 release since its an incompatible change, and I agree we should document this all on the wiki. I don&apos;t know much about Spark 2, but will Hive-on-Spark2 be able to run against a Spark1 cluster, or vice versa?&lt;/p&gt;</comment>
                            <comment id="15531389" author="ferd" created="Thu, 29 Sep 2016 00:41:46 +0000"  >&lt;blockquote&gt;&lt;p&gt;I&apos;m guessing this should go into the Hive 2.2.0 release since its an incompatible change&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Agree.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;will Hive-on-Spark2 be able to run against a Spark1 cluster, or vice versa?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;AFAIK, it will not able to run against Spark1 cluster for the dependency conflicts. If we want to support different Spark cluster, we may need a shim loader for Spark in Spark client. Do we have a strong requirement for that?&lt;/p&gt;</comment>
                            <comment id="15542999" author="stakiar" created="Mon, 3 Oct 2016 18:01:03 +0000"  >&lt;p&gt;We probably don&apos;t need a shim loader right now. I don&apos;t know of any requirements to have one, so we should be good for now. If users starting hitting upgrade issues then it may be something to consider in the future.&lt;/p&gt;</comment>
                            <comment id="15544407" author="lefty@hortonworks.com" created="Tue, 4 Oct 2016 05:33:13 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Ferd&quot; class=&quot;user-hover&quot; rel=&quot;Ferd&quot;&gt;Ferdinand Xu&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;Rui Li&lt;/a&gt;, yes we should add a section on Spark versions that are compatible with different Hive releases, and include as much information as possible.&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;Hive on Spark:  Getting Started &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I was about to add such a section at the beginning of the doc (before Spark Installation) but hesitated because I don&apos;t know what version(s) can be used with the installation instructions.&lt;/p&gt;</comment>
                            <comment id="15583248" author="spena" created="Mon, 17 Oct 2016 19:51:23 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xuefuz&quot; class=&quot;user-hover&quot; rel=&quot;xuefuz&quot;&gt;Xuefu Zhang&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Ferd&quot; class=&quot;user-hover&quot; rel=&quot;Ferd&quot;&gt;Ferdinand Xu&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;Rui Li&lt;/a&gt; Back to compatibility discussion, I think we should continue keeping Spark 1.x compatibility on Hive 2.x series (as we did on Hive 1.x with Hadoop 1.x/2.x). If there are users using Spark 1.x, then they won&apos;t be able to upgrade to Hive 2.2, and they do not necessary need to upgrade to Spark 2.0 as it is still a new release, and not many people upgrade to a 2.0 version immediately.&lt;/p&gt;

&lt;p&gt;What do you thing about this guys? is it important to keep compatibility on Hive 2.x until we release Hive 3.0 in the future?&lt;/p&gt;</comment>
                            <comment id="15583356" author="xuefuz" created="Mon, 17 Oct 2016 20:31:45 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=spena&quot; class=&quot;user-hover&quot; rel=&quot;spena&quot;&gt;Sergio Pe&#241;a&lt;/a&gt;, Keeping b/c is a good thing in general. Before we take the effort (which seems a lot) to do it, I think we should clearly understand and define what b/c is in this case. Spark is rapidly releasing w/o much b/c in mind. So far, Hive on Spark has once depended on Spark 1.2, 1.3, 1.4, 1.5, and 1.6. I&apos;m not sure what versions of Spark Hive has been released with, but one thing is clear, Spark isn&apos;t b/c between these releases. Before Spark community has a good sense of keeping b/c in their APIs, it&apos;s going to be very hard and burdensome for Hive to maintain support for different Spark releases, not to mention the library dependency issues we have had.&lt;/p&gt;

&lt;p&gt;I&apos;m okay to start thinking of a shim layer to support multiple versions of Spark, but it sounds daunting to me due to the dynamics of Spark project.&lt;/p&gt;</comment>
                            <comment id="15583393" author="spena" created="Mon, 17 Oct 2016 20:41:26 +0000"  >&lt;p&gt;Interesting, so even between Spark 1.x versions, Hive wasn&apos;t compatible at all with them? This is going to be a lot of work as you said. If Spark 2.1 isn&apos;t compatible with Spark 2.0 for instance, then we will have a shim layer with minor changes per Spark version to keep compatibility.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xuefuz&quot; class=&quot;user-hover&quot; rel=&quot;xuefuz&quot;&gt;Xuefu Zhang&lt;/a&gt; Were there users in the community complaining about Spark 1.x incompatibilities with Hive in the past? &lt;/p&gt;</comment>
                            <comment id="15583487" author="xuefuz" created="Mon, 17 Oct 2016 21:12:07 +0000"  >&lt;p&gt;Spark claims API compatibility within a major release, but it doesn&apos;t seem so based on our experience. &lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-9726&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HIVE-9726&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-10999&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HIVE-10999&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-11473&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HIVE-11473&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-12828&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HIVE-12828&lt;/a&gt;&lt;br/&gt;
In two of the four upgrades, there are incompatibility API changes.&lt;/p&gt;

&lt;p&gt;Spark is still a young project, so people may have lower expectation on this.&lt;/p&gt;</comment>
                            <comment id="15584325" author="lirui" created="Tue, 18 Oct 2016 03:42:00 +0000"  >&lt;p&gt;Hmm even with a shim layer, it&apos;s difficult to support different Spark versions if b/c is not maintained between minor releases of Spark.&lt;br/&gt;
I&apos;m wondering if the Spark used by Hive can be considered as some kind of embedded binaries that exclusively used for HoS. On Hive side, we just need to set spark.home pointing to this Spark. User&apos;s other Spark applications, e.g. SparkSQL, streaming, can still run against the current Spark they have in the cluster. Will this make it easier for the upgrade?&lt;br/&gt;
I think we also need to be more careful to upgrade Spark in the future, if the upgrade is breaking compatibility. For such upgrade, we need to firstly make sure there&apos;s no obvious regression in functionality and performance.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="13005467">HIVE-14777</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="13010907">HIVE-14919</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12919145">SPARK-12154</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="13005420">SPARK-17563</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="13007015">HIVE-14825</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12829398" name="HIVE-14029.1.patch" size="33133" author="Ferd" created="Tue, 20 Sep 2016 14:21:45 +0000"/>
                            <attachment id="12829489" name="HIVE-14029.2.patch" size="24839" author="Ferd" created="Wed, 21 Sep 2016 02:57:39 +0000"/>
                            <attachment id="12829601" name="HIVE-14029.3.patch" size="24245" author="Ferd" created="Wed, 21 Sep 2016 15:44:29 +0000"/>
                            <attachment id="12829777" name="HIVE-14029.4.patch" size="24491" author="Ferd" created="Thu, 22 Sep 2016 06:44:30 +0000"/>
                            <attachment id="12829975" name="HIVE-14029.5.patch" size="24925" author="Ferd" created="Fri, 23 Sep 2016 02:01:57 +0000"/>
                            <attachment id="12830171" name="HIVE-14029.6.patch" size="26458" author="Ferd" created="Sat, 24 Sep 2016 04:09:22 +0000"/>
                            <attachment id="12830260" name="HIVE-14029.7.patch" size="28300" author="Ferd" created="Mon, 26 Sep 2016 07:58:19 +0000"/>
                            <attachment id="12830453" name="HIVE-14029.8.patch" size="27927" author="Ferd" created="Tue, 27 Sep 2016 07:20:30 +0000"/>
                            <attachment id="12829100" name="HIVE-14029.patch" size="23418" author="Ferd" created="Sun, 18 Sep 2016 08:47:41 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 16 Jun 2016 19:59:23 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            6 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2zj7b:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>