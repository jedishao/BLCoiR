<?xml version = "1.0" encoding = "UTF-8" ?>
<bugrepository name="HIVE">
	<bug id="84" opendate="2008-11-27 11:03:48" fixdate="2009-01-06 22:03:49" resolution="Fixed">
		<buginformation>
			<summary>MetaStore Client is not thread safe</summary>
			<description>when running DDL Tasks in concurrent threads - the following exception trace is observed:
java.sql.SQLIntegrityConstraintViolationException: The statement was aborted because it would have caused a duplicate ke\ y value in a unique or primary key constraint or unique index identified by &amp;amp;apos;UNIQUETABLE&amp;amp;apos; defined on &amp;amp;apos;TBLS&amp;amp;apos;.
  at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:207)
  at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:209)
  at org.apache.hadoop.hive.ql.Driver.run(Driver.java:174)
  at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:185)
  at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:210)
  at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:390)
  at org.apache.hadoop.hive.ql.QTestUtil$QTRunner.run(QTestUtil.java:681)
  at java.lang.Thread.run(Thread.java:619)
Caused by: javax.jdo.JDODataStoreException: Insert of object "org.apache.hadoop.hive.metastore.model.MTable@3bc8d400" us\ ing statement "INSERT INTO TBLS (TBL_ID,CREATE_TIME,DB_ID,RETENTION,TBL_NAME,SD_ID,OWNER,LAST_ACCESS_TIME) VALUES (?,?,?\ ,?,?,?,?,?)" failed : The statement was aborted because it would have caused a duplicate key value in a unique or primar\ y key constraint or unique index identified by &amp;amp;apos;UNIQUETABLE&amp;amp;apos; defined on &amp;amp;apos;TBLS&amp;amp;apos;.
NestedThrowables:
java.sql.SQLIntegrityConstraintViolationException: The statement was aborted because it would have caused a duplicate ke\ y value in a unique or primary key constraint or unique index identified by &amp;amp;apos;UNIQUETABLE&amp;amp;apos; defined on &amp;amp;apos;TBLS&amp;amp;apos;.
  at org.jpox.jdo.JPOXJDOHelper.getJDOExceptionForJPOXException(JPOXJDOHelper.java:291)
  at org.jpox.jdo.AbstractPersistenceManager.jdoMakePersistent(AbstractPersistenceManager.java:671)
  at org.jpox.jdo.AbstractPersistenceManager.makePersistent(AbstractPersistenceManager.java:691)
  at org.apache.hadoop.hive.metastore.ObjectStore.createTable(ObjectStore.java:479)
  at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table(HiveMetaStore.java:292)
  at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:252)
  at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:205)
  ... 7 more
Caused by: java.sql.SQLIntegrityConstraintViolationException: The statement was aborted because it would have caused a d\ uplicate key value in a unique or primary key constraint or unique index identified by &amp;amp;apos;UNIQUETABLE&amp;amp;apos; defined on &amp;amp;apos;TBLS&amp;amp;apos;.
  at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)
  at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
  at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
  at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
  at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
  at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
  at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
  at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeStatement(Unknown Source)
  at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeUpdate(Unknown Source)
  at org.jpox.store.rdbms.SQLController.executeStatementUpdate(SQLController.java:396)
  at org.jpox.store.rdbms.request.InsertRequest.execute(InsertRequest.java:370)
  at org.jpox.store.rdbms.RDBMSPersistenceHandler.insertTable(RDBMSPersistenceHandler.java:157)
  at org.jpox.store.rdbms.RDBMSPersistenceHandler.insertObject(RDBMSPersistenceHandler.java:136)
  at org.jpox.state.JDOStateManagerImpl.internalMakePersistent(JDOStateManagerImpl.java:3082)
  at org.jpox.state.JDOStateManagerImpl.makePersistent(JDOStateManagerImpl.java:3062)
  at org.jpox.ObjectManagerImpl.persistObjectInternal(ObjectManagerImpl.java:1231)
  at org.jpox.ObjectManagerImpl.persistObject(ObjectManagerImpl.java:1077)
  at org.jpox.jdo.AbstractPersistenceManager.jdoMakePersistent(AbstractPersistenceManager.java:666)
  ... 12 more
Caused by: java.sql.SQLException: The statement was aborted because it would have caused a duplicate key value in a uniq\ ue or primary key constraint or unique index identified by &amp;amp;apos;UNIQUETABLE&amp;amp;apos; defined on &amp;amp;apos;TBLS&amp;amp;apos;.
  at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
  at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)
  ... 30 more
Caused by: ERROR 23505: The statement was aborted because it would have caused a duplicate key value in a unique or prim\ ary key constraint or unique index identified by &amp;amp;apos;UNIQUETABLE&amp;amp;apos; defined on &amp;amp;apos;TBLS&amp;amp;apos;.
  at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
  at org.apache.derby.impl.sql.execute.IndexChanger.insertAndCheckDups(Unknown Source)
  at org.apache.derby.impl.sql.execute.IndexChanger.doInsert(Unknown Source)
  at org.apache.derby.impl.sql.execute.IndexChanger.insert(Unknown Source)
  at org.apache.derby.impl.sql.execute.IndexSetChanger.insert(Unknown Source)
  at org.apache.derby.impl.sql.execute.RowChangerImpl.insertRow(Unknown Source)
  at org.apache.derby.impl.sql.execute.InsertResultSet.normalInsertCore(Unknown Source)
  at org.apache.derby.impl.sql.execute.InsertResultSet.open(Unknown Source)
when running normal select queries as well - one hits exception, stack trace:
2008-11-27 01:54:00,216 ERROR metadata.Hive (Hive.java:getTable(275)) - NoSuchObjectException(message:default.dummySrc t\
able not found)
  at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_table(HiveMetaStore.java:347)
  at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getTable(HiveMetaStoreClient.java:433)
  at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getTable(HiveMetaStoreClient.java:472)
  at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:272)
  at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:254)
  at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(SemanticAnalyzer.java:544)
  at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:3192)
  at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:73)
  at org.apache.hadoop.hive.ql.QTestUtil.analyzeAST(QTestUtil.java:672)
  at org.apache.hadoop.hive.ql.parse.TestParseNegative.testParseNegative_unknown_table1(TestParseNegative.java:231)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
  at java.lang.reflect.Method.invoke(Method.java:597)
  at junit.framework.TestCase.runTest(TestCase.java:154)
  at junit.framework.TestCase.runBare(TestCase.java:127)
  at junit.framework.TestResult$1.protect(TestResult.java:106)
  at junit.framework.TestResult.runProtected(TestResult.java:124)
  at junit.framework.TestResult.run(TestResult.java:109)
  at junit.framework.TestCase.run(TestCase.java:118)
  at junit.framework.TestSuite.runTest(TestSuite.java:208)
  at junit.framework.TestSuite.run(TestSuite.java:203)
  at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:297)
  at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:672)
  at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:567)
</description>
			<version>0.6.0</version>
			<fixedVersion>0.3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.session.SessionState.java</file>
			<file type="M">org.apache.hadoop.hive.ql.TestMTQueries.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
			<file type="M">org.apache.hadoop.hive.ql.QTestUtil.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
	</bug>
	<bug id="264" opendate="2009-02-02 03:29:27" fixdate="2009-02-05 23:40:13" resolution="Fixed">
		<buginformation>
			<summary>TBinarySortable Protocol should support null characters</summary>
			<description>Currently TBinarySortable Protocol does not support serializing null "\0" characters which confused a lot of users.
We should support that.</description>
			<version>0.6.0</version>
			<fixedVersion>0.3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.java</file>
			<file type="M">org.apache.hadoop.hive.ql.QTestUtil.java</file>
		</fixedFiles>
	</bug>
	<bug id="269" opendate="2009-02-04 05:03:09" fixdate="2009-02-06 01:37:33" resolution="Fixed">
		<buginformation>
			<summary>Add log/exp UDF functions to Hive</summary>
			<description>See http://dev.mysql.com/doc/refman/5.0/en/mathematical-functions.html


EXP() 	Raise to the power of
LN() 	Return the natural logarithm of the argument
LOG10() 	Return the base-10 logarithm of the argument
LOG2() 	Return the base-2 logarithm of the argument
LOG() 	Return the natural logarithm of the first argument 
POW() 	Return the argument raised to the specified power
POWER() 	Return the argument raised to the specified power

</description>
			<version>0.3.0</version>
			<fixedVersion>0.3.0</fixedVersion>
			<type>New Feature</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">245</link>
		</links>
	</bug>
	<bug id="323" opendate="2009-03-04 05:59:49" fixdate="2009-03-08 03:59:01" resolution="Duplicate">
		<buginformation>
			<summary>row counts for one query are being in printed subsequent queries</summary>
			<description>when executing multiple queries from the cli - i am seeing the row count state being maintained/printed across queries:
&amp;gt;q1
N1 rows
&amp;gt;q2
N1 rows inserted
N2 rows inserted</description>
			<version>0.3.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.history.HiveHistory.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">327</link>
		</links>
	</bug>
	<bug id="308" opendate="2009-02-26 06:23:24" fixdate="2009-03-09 09:41:17" resolution="Fixed">
		<buginformation>
			<summary>UNION ALL should create different destination directories for different operands</summary>
			<description>The following query hangs:

 
select * from (select 1 from zshao_lazy union all select 2 from zshao_lazy) a;


The following query produce wrong results: (one map-reduce job overwrite/cannot overwrite the result of the other)

 
select * from (select 1 as id from zshao_lazy cluster by id union all select 2 as id from zshao_meta) a;


The reason of both is that the destination directory of the file sink operator conflicts with each other.</description>
			<version>0.3.0</version>
			<fixedVersion>0.3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
		</fixedFiles>
	</bug>
	<bug id="245" opendate="2009-01-23 09:17:54" fixdate="2009-03-09 09:59:19" resolution="Duplicate">
		<buginformation>
			<summary>Add POW(X, Y) UDF</summary>
			<description>See http://dev.mysql.com/doc/refman/5.0/en/mathematical-functions.html#function_pow</description>
			<version>0.3.0</version>
			<fixedVersion></fixedVersion>
			<type>New Feature</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">269</link>
		</links>
	</bug>
	<bug id="326" opendate="2009-03-05 06:36:16" fixdate="2009-03-11 17:45:09" resolution="Duplicate">
		<buginformation>
			<summary>[hive] groupby count distinct with nulls has some problems</summary>
			<description>select a, count(distinct b) from T group by a;
had some problems if b is null.
I will construct the exact testcase and get back</description>
			<version>0.3.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">320</link>
		</links>
	</bug>
	<bug id="325" opendate="2009-03-05 06:35:05" fixdate="2009-03-11 17:47:59" resolution="Duplicate">
		<buginformation>
			<summary>[Hive] rand() should be ignored by input pruning</summary>
			<description>select * from T where rand() &amp;lt; 0.5
may return 0 rows because all partitions may simply be eliminated by partition pruning if rand() &amp;lt; 0.5 happens to be false</description>
			<version>0.6.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.PartitionPruner.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFRand.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.UDF.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">253</link>
		</links>
	</bug>
	<bug id="327" opendate="2009-03-05 18:35:24" fixdate="2009-03-12 17:33:36" resolution="Fixed">
		<buginformation>
			<summary>row count getting printed wrongly</summary>
			<description>When multiple queries are executed in same session, row count of the first query is getting printed for subsequent queries. </description>
			<version>0.3.0</version>
			<fixedVersion>0.3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.history.HiveHistory.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">323</link>
		</links>
	</bug>
	<bug id="322" opendate="2009-03-04 05:55:15" fixdate="2009-03-16 08:03:42" resolution="Duplicate">
		<buginformation>
			<summary>cannot create temporary udf dynamically, with a ClassNotFoundException </summary>
			<description>I found the ClassLoader cannot load my UDF when doing FunctionTask, because the ClassLoader hasnot append its classpaths on-the-fly yet.
The ExecDriver&amp;amp;apos; s addToClassPath(String[] newPaths) method is the only entry for ClassLoader dynamically append its classhpaths (besides hadoop&amp;amp;apos;s GenericOptionsParser).
But that function wasnot called before FunctionTask getting my UDF class by class name. I think this is the reason why I came across that failure.
scenario description:
I set a peroperty in hive-site.xml to configure the classpath of my udf. 
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hive.aux.jars.path&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;/home/hadoop/hdpsoft/hive-auxs/zhoumin.jar&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
but failed to register it with a ClassNotFoundException when creating udf through the sql command.
CREATE TEMPORARY FUNCTION strlen AS &amp;amp;apos;hadoop.hive.udf.UdfStringLength&amp;amp;apos;
I&amp;amp;apos;ll make a patch soon.
</description>
			<version>0.3.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapRedTask.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.service.HiveServer.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.SerDeUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.session.SessionState.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.hwi.HWISessionItem.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FunctionTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Table.java</file>
			<file type="D">org.apache.hadoop.hive.ql.CommandProcessor.java</file>
			<file type="D">org.apache.hadoop.hive.cli.SetProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.cli.CliDriver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">338</link>
		</links>
	</bug>
	<bug id="320" opendate="2009-03-04 02:50:47" fixdate="2009-03-17 01:14:44" resolution="Fixed">
		<buginformation>
			<summary>Issuing queries with COUNT(DISTINCT) on a column that may contain null values hits a NPE</summary>
			<description>When issuing a query that may contain a null value, I get a NPE. 
E.g. if &amp;amp;apos;middle_name&amp;amp;apos; potentially holds null values,
select count(distinct middle_name) from people; will fail with the below exception.
Other queries that work with the same input set:
select distinct middle_name from people;
select count(1), middle_name from people group by middle_name;
java.io.IOException: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:169)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:318)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2198)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.process(GroupByOperator.java:424)
	at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:164)
	... 2 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.updateAggregations(GroupByOperator.java:376)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.processAggr(GroupByOperator.java:477)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.process(GroupByOperator.java:420)
	... 3 more
</description>
			<version>0.3.0</version>
			<fixedVersion>0.3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">326</link>
		</links>
	</bug>
	<bug id="251" opendate="2009-01-26 20:12:29" fixdate="2009-03-18 01:57:46" resolution="Fixed">
		<buginformation>
			<summary>Failures in Transform don&amp;apos;t stop the job</summary>
			<description>If the program executed via a SELECT TRANSFORM() USING &amp;amp;apos;foo&amp;amp;apos; exits with a non-zero exit status, Hive proceeds as if nothing bad happened.  The main way that the user knows something bad has happened is if the user checks the logs (probably because he got no output).  This is doubly bad if the program only fails part of the time (say, on certain inputs) since the job will still produce output and thus the problem will likely go undetected.</description>
			<version>0.6.0</version>
			<fixedVersion>0.3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Operator.java</file>
		</fixedFiles>
	</bug>
	<bug id="317" opendate="2009-03-03 01:08:59" fixdate="2009-03-18 17:58:32" resolution="Fixed">
		<buginformation>
			<summary>HiveServer can not define its port  correctly</summary>
			<description>HiveServer.java accept one argument stands for the port of  this server,  but I found that can not accept this argument.
By digging into the source code, I found it may caused by these lines of main function.
if (args.length &amp;gt; 1) 
{
        port = Integer.getInteger(args[0]);
      }

I think they should be: 
if (args.length &amp;gt;= 1) 
{
        port = Integer.parseInt(args[0]);
      }

The author may have some different intention,  I think.  </description>
			<version>0.3.0</version>
			<fixedVersion>0.3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.service.HiveServer.java</file>
		</fixedFiles>
	</bug>
	<bug id="285" opendate="2009-02-10 01:49:40" fixdate="2009-03-18 18:29:49" resolution="Fixed">
		<buginformation>
			<summary>UNION ALL does not allow different types in the same column</summary>
			<description>
explain INSERT OVERWRITE TABLE t
    SELECT s.r, s.c, sum(s.v) FROM
    (
      SELECT a.r AS r, a.c AS c, a.v AS v FROM t1 a
      UNION ALL
      SELECT b.r AS r, b.c AS c, 0 + b.v AS v FROM t2 b
    ) s
    GROUP BY s.r, s.c;


Both a and b have 3 string columns: r, c, and v.
It compiled successfully but failed during runtime.
"Explain" shows that the plan for the 2 union-all operands have different output types that are converged to STRING, but there is no UDFToString inserted for "0 + b.v AS v" and as a result, SerDe was failing because it expects a String but is passed a Double.
</description>
			<version>0.3.0</version>
			<fixedVersion>0.3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">297</link>
		</links>
	</bug>
	<bug id="253" opendate="2009-01-28 00:52:34" fixdate="2009-03-18 22:02:38" resolution="Fixed">
		<buginformation>
			<summary>rand() gets precomputated in compilation phase</summary>
			<description>SELECT * FROM t WHERE rand() &amp;lt; 0.01;
Hive will say: "No need to submit job", because the condition evaluates to false.
The rand() function is special in the sense that every time it evaluates to a different value. We should disallow computing the value in the compiling phase.
One way to do that is to add an annotation in the UDFRand and check that in the compiling phase.
</description>
			<version>0.6.0</version>
			<fixedVersion>0.3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.PartitionPruner.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFRand.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.UDF.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">325</link>
		</links>
	</bug>
	<bug id="337" opendate="2009-03-10 23:31:31" fixdate="2009-03-24 23:49:56" resolution="Fixed">
		<buginformation>
			<summary>LazySimpleSerDe should support multi-level nested array, map, struct types</summary>
			<description>Once we do that, we can completely deprecate DynamicSerDe/TCTLSeparatedProtocol, and close any bugs that DynamicSerDe/TCTLSeparatedProtocol has.</description>
			<version>0.6.0</version>
			<fixedVersion>0.3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TestExecDriver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.exprNodeColumnDesc.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.StandardMapObjectInspector.java</file>
			<file type="D">org.apache.hadoop.hive.ql.typeinfo.StructTypeInfo.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyByte.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.LazySimpleStructObjectInspector.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TestOperators.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.TestLazySimpleSerDe.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.InputSignature.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.exprNodeDesc.java</file>
			<file type="D">org.apache.hadoop.hive.ql.typeinfo.PrimitiveTypeInfo.java</file>
			<file type="D">org.apache.hadoop.hive.ql.typeinfo.TypeInfoUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.exprNodeFieldDesc.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyLong.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExprNodeFuncEvaluator.java</file>
			<file type="D">org.apache.hadoop.hive.ql.typeinfo.TypeInfo.java</file>
			<file type="D">org.apache.hadoop.hive.ql.typeinfo.TypeInfoFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.exprNodeConstantDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TestPlan.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.PartitionPruner.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyObject.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyInteger.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyString.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyPrimitive.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.exprNodeNullDesc.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyStruct.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="D">org.apache.hadoop.hive.ql.typeinfo.MapTypeInfo.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.MapObjectInspector.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.exprNodeIndexDesc.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.TestLazyPrimitive.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyDouble.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.exprNodeFuncDesc.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyShort.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ColumnInfo.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
			<file type="D">org.apache.hadoop.hive.ql.typeinfo.ListTypeInfo.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TestExpressionEvaluator.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">336</link>
			<link type="Blocker" description="blocks">136</link>
			<link type="Blocker" description="blocks">365</link>
			<link type="Blocker" description="blocks">352</link>
			<link type="Blocker" description="blocks">358</link>
			<link type="Blocker" description="blocks">266</link>
			<link type="Blocker" description="is blocked by">270</link>
		</links>
	</bug>
	<bug id="349" opendate="2009-03-14 00:54:21" fixdate="2009-03-25 04:55:21" resolution="Fixed">
		<buginformation>
			<summary>HiveHistory: TestCLiDriver fails if there are test cases with  no tasks</summary>
			<description>TestCLIDriver Fails for some test cases.</description>
			<version>0.3.0</version>
			<fixedVersion>0.3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="D">org.apache.hadoop.hive.utils.ByteStream.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">342</link>
		</links>
	</bug>
	<bug id="363" opendate="2009-03-24 17:55:22" fixdate="2009-03-25 05:56:12" resolution="Fixed">
		<buginformation>
			<summary>[hive] extra rows for count distinct</summary>
			<description>select count(distinct a) from T 
returns dummy rows from all reducers if number of reducers are more than 1</description>
			<version>0.3.0</version>
			<fixedVersion>0.3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
	</bug>
	<bug id="342" opendate="2009-03-11 17:37:25" fixdate="2009-03-25 21:14:48" resolution="Fixed">
		<buginformation>
			<summary>TestMTQueries is broken</summary>
			<description>It has been broken for quite sometime but the build is not failing.</description>
			<version>0.6.0</version>
			<fixedVersion>0.3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.QTestUtil.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="is blocked by">349</link>
		</links>
	</bug>
	<bug id="367" opendate="2009-03-25 22:15:57" fixdate="2009-03-26 20:26:32" resolution="Fixed">
		<buginformation>
			<summary>[hive] problem in group by in case of empty input files</summary>
			<description></description>
			<version>0.3.0</version>
			<fixedVersion>0.3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
		</fixedFiles>
	</bug>
	<bug id="373" opendate="2009-03-26 21:49:41" fixdate="2009-03-27 01:17:18" resolution="Fixed">
		<buginformation>
			<summary>[hive] 1 reducer should be used if no grouping key is present in all scenarios</summary>
			<description></description>
			<version>0.3.0</version>
			<fixedVersion>0.3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
	</bug>
	<bug id="391" opendate="2009-04-06 23:53:49" fixdate="2009-04-07 04:55:25" resolution="Fixed">
		<buginformation>
			<summary>udafcount merge does not handle nulls</summary>
			<description>udafcount merge does not handle nulls
If the mapper does not emit any row on null input, i.e both count and count distinct are present, and the aggregation function is count, 
it will get a null pointer
select count(1), count(distinct x.value) from src x where x.key = 9999;</description>
			<version>0.3.0</version>
			<fixedVersion>0.4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDAFCount.java</file>
		</fixedFiles>
	</bug>
	<bug id="381" opendate="2009-03-31 09:59:16" fixdate="2009-04-07 18:45:48" resolution="Fixed">
		<buginformation>
			<summary>[JDBC component] HiveResultSet next() always returned true due to bad string comparison</summary>
			<description>Method next() is comparing String using "!=" operator resulted in "true" being returned all the time.  Can be fix by using String equals() operation to check. </description>
			<version>0.3.0</version>
			<fixedVersion>0.4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveResultSet.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
		</fixedFiles>
	</bug>
	<bug id="324" opendate="2009-03-04 12:15:56" fixdate="2009-04-08 01:26:01" resolution="Fixed">
		<buginformation>
			<summary>AccessControlException when load data into table</summary>
			<description>when loading data in non-supergroup user of hadoop,  hadoop will throw a AccessControlException bacuase Hive try to do write operation at  /tmp directory.
This is obviously not allowed.
see line 752 in Hive.java
Path tmppath = new Path("/tmp/"+randGen.nextInt());
      try 
{
          fs.mkdirs(tmppath);
      ...
    }

those lines will cause that exception. </description>
			<version>0.3.0</version>
			<fixedVersion>0.3.0, 0.4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.history.TestHiveHistory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TestExecDriver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.QTestUtil.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.loadTableDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Partition.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Table.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
	</bug>
	<bug id="393" opendate="2009-04-07 20:49:48" fixdate="2009-04-09 06:06:30" resolution="Fixed">
		<buginformation>
			<summary>MoveTask will bail out if we cannot open the compressed SequenceFile because hadoop native lib is missing</summary>
			<description></description>
			<version>0.3.0</version>
			<fixedVersion>0.3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.moveWork.java</file>
		</fixedFiles>
	</bug>
	<bug id="435" opendate="2009-04-20 19:36:44" fixdate="2009-04-20 21:41:56" resolution="Fixed">
		<buginformation>
			<summary>Empty passwd param causing NPE in ExecDriver</summary>
			<description>HIVE-403 can cause NPE if the password param is empty. </description>
			<version>0.3.0</version>
			<fixedVersion>0.3.0, 0.4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
		</fixedFiles>
	</bug>
	<bug id="403" opendate="2009-04-10 03:24:10" fixdate="2009-04-27 23:40:50" resolution="Fixed">
		<buginformation>
			<summary>remove password password params from job config that is submitted to job tracker</summary>
			<description>Do not show metastore db password when it is sent to job tracker and do not print this option in logs.</description>
			<version>0.3.0</version>
			<fixedVersion>0.3.0, 0.4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
	</bug>
	<bug id="485" opendate="2009-05-13 20:45:12" fixdate="2009-05-19 00:52:48" resolution="Fixed">
		<buginformation>
			<summary>join assumes all columns are strings</summary>
			<description>join assumes all columns are string - pass the objectinspector from execreducer and use that</description>
			<version>0.4.0</version>
			<fixedVersion>0.4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.LimitOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.CollectOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ForwardOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecReducer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FilterOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.SelectOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Operator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecMapper.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExtractOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TestOperators.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">405</link>
		</links>
	</bug>
	<bug id="498" opendate="2009-05-20 18:24:20" fixdate="2009-05-20 21:55:00" resolution="Fixed">
		<buginformation>
			<summary>UDFRegExp NullPointerException on empty replacement string</summary>
			<description>UDFRegExp, UDFRegExpReplace, UDFRegExpExtract will throw out NullPointerException if the replacement String is empty or pattern is empty</description>
			<version>0.4.0</version>
			<fixedVersion>0.4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFRegExpExtract.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFRegExpReplace.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFRegExp.java</file>
		</fixedFiles>
	</bug>
	<bug id="499" opendate="2009-05-20 21:07:44" fixdate="2009-05-20 22:36:54" resolution="Fixed">
		<buginformation>
			<summary>CAST(intcolumn as INT) is failing</summary>
			<description>The bug can be reproduced by:


CREATE TABLE zshao_int(a int);
select cast(a as int) from zshao_int;

</description>
			<version>0.4.0</version>
			<fixedVersion>0.4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
		</fixedFiles>
	</bug>
	<bug id="501" opendate="2009-05-21 01:33:29" fixdate="2009-05-21 02:30:03" resolution="Fixed">
		<buginformation>
			<summary>UDFLower is doing uppercase instead of lowercase</summary>
			<description>The current code is:


public class UDFLower extends UDF {

  Text t = new Text();
  public UDFLower() {
  }

  public Text evaluate(Text s) {
    if (s == null) {
      return null;
    }
    t.set(s.toString().toUpperCase());
    return t;
  }

}

</description>
			<version>0.4.0</version>
			<fixedVersion>0.4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFLower.java</file>
		</fixedFiles>
	</bug>
	<bug id="504" opendate="2009-05-21 19:47:13" fixdate="2009-05-22 03:12:16" resolution="Fixed">
		<buginformation>
			<summary>script operator fails when there is an empty input file</summary>
			<description>Get the following error:
Exception in thread "Timer-2" 09/05/21 11:58:13 INFO exec.FilterOperator: Initialization Done
 java.lang.NullPointerException
   at org.apache.hadoop.hive.ql.exec.ScriptOperator$ReporterTask.run(ScriptOperator.java:485)
   at java.util.TimerThread.mainLoop(Timer.java:512)
   at java.util.TimerThread.run(Timer.java:462)</description>
			<version>0.4.0</version>
			<fixedVersion>0.4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
		</fixedFiles>
	</bug>
	<bug id="488" opendate="2009-05-13 20:55:15" fixdate="2009-05-26 17:04:59" resolution="Fixed">
		<buginformation>
			<summary>loading into a partition with more than one partition column fails if the partition is not created before.</summary>
			<description>Following test fails on HDFS cluster but not on local file system.
drop table hive_test_src;
drop table hive_test_dst;
create table hive_test_src ( col1 string ) stored as textfile ;
load data local inpath &amp;amp;apos;../data/files/test.dat&amp;amp;apos; overwrite into table hive_test_src ;
create table hive_test_dst ( col1 string ) partitioned by ( pcol1 string , pcol2 string) stored as sequencefile;
insert overwrite table hive_test_dst partition ( pcol1=&amp;amp;apos;test_part&amp;amp;apos;, pcol2=&amp;amp;apos;test_part&amp;amp;apos;) select col1 from hive_test_src ;
select * from hive_test_dst where pcol1=&amp;amp;apos;test_part&amp;amp;apos; and pcol2=&amp;amp;apos;test_part&amp;amp;apos;;  returns zero rows.
drop table hive_test_src;
drop table hive_test_dst;</description>
			<version>0.4.0</version>
			<fixedVersion>0.4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
			<file type="M">org.apache.hadoop.hive.ql.QTestUtil.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
		</fixedFiles>
	</bug>
	<bug id="517" opendate="2009-05-26 20:33:12" fixdate="2009-05-27 00:27:45" resolution="Duplicate">
		<buginformation>
			<summary>Silent flag does not work on local jobs.</summary>
			<description>The commands
  hive2 -S -e "from tmp_foo select count(1)" &amp;gt; my_stdout.txt
and
  hive2 -S -hiveconf mapred.job.tracker=local -hiveconf mapred.local.dir=/tmp/foo -e "from tmp_foo select count(1)" &amp;gt; my_stdout.txt
give different results.
The former looks like:
56
and the latter looks like:
plan = /tmp/plan61908.xml
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=&amp;lt;number&amp;gt;
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=&amp;lt;number&amp;gt;
In order to set a constant number of reducers:
  set mapred.reduce.tasks=&amp;lt;number&amp;gt;
Job running in-process (local Hadoop)
 map = 100%,  reduce =0%
 map = 100%,  reduce =100%
Ended Job = job_local_1
56</description>
			<version>0.5.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.common.FileUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapRedTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.session.SessionState.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Context.java</file>
			<file type="M">org.apache.hadoop.hive.cli.CliDriver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">88</link>
		</links>
	</bug>
	<bug id="442" opendate="2009-04-24 01:54:28" fixdate="2009-05-27 01:03:39" resolution="Fixed">
		<buginformation>
			<summary>Partition is created before data is moved thus creating a window where data is incomplete</summary>
			<description>During the said window, processes waiting for the partition to be created can run queries on partial data thus causing untold misery.</description>
			<version>0.3.0</version>
			<fixedVersion>0.4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Table.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Partition.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.Warehouse.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
	</bug>
	<bug id="511" opendate="2009-05-23 00:12:17" fixdate="2009-05-28 00:03:10" resolution="Fixed">
		<buginformation>
			<summary>Change the hashcode for DoubleWritable</summary>
			<description>The current DoubleWritable hashCode takes only the last 32 bits. This is a big problem because for small integer values like 1.0, 2.0, 15.0, the hashCode are all 0.</description>
			<version>0.4.0</version>
			<fixedVersion>0.4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFDefaultSampleHashFn.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
		</fixedFiles>
	</bug>
	<bug id="514" opendate="2009-05-25 05:02:43" fixdate="2009-05-29 00:27:21" resolution="Fixed">
		<buginformation>
			<summary>partition key names should be case insensitive in alter table add partition statement.</summary>
			<description>create table testpc(a int) partitioned by (ds string, hr string);
alter table testpc add partition (ds="1", hr="1"); --&amp;gt; works
alter table testpc add partition (ds="1", Hr="1"); --&amp;gt; doesn&amp;amp;apos;t work</description>
			<version>0.4.0</version>
			<fixedVersion>0.4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Partition.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.Warehouse.java</file>
		</fixedFiles>
	</bug>
	<bug id="523" opendate="2009-05-28 23:05:45" fixdate="2009-05-29 07:02:00" resolution="Fixed">
		<buginformation>
			<summary>FIx PartitionPruner not to fetch all partitions at once</summary>
			<description>All partitions are fetched at once causing metastore to go OutOfMemory. </description>
			<version>0.4.0</version>
			<fixedVersion>0.4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.PartitionPruner.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.Warehouse.java</file>
		</fixedFiles>
	</bug>
	<bug id="497" opendate="2009-05-19 23:53:59" fixdate="2009-05-30 19:20:55" resolution="Fixed">
		<buginformation>
			<summary>predicate pushdown fails if all columns are not selected</summary>
			<description>predicate pushdown seems to fail in some scenarios... it is ok if all the columns are selected.
create table ppda(a string, b string);
select a from ppda where ppda.a &amp;gt; 10; --&amp;gt; fails
select b from ppda where ppda.a &amp;gt; 10; --&amp;gt; ok
select * from ppda where ppda.a &amp;gt; 10; --&amp;gt; ok
select b from appd where appd.b &amp;gt; 10 and appd.a &amp;gt; 20; --&amp;gt; ok</description>
			<version>0.4.0</version>
			<fixedVersion>0.4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ColumnPruner.java</file>
		</fixedFiles>
	</bug>
	<bug id="520" opendate="2009-05-27 16:59:28" fixdate="2009-05-30 22:54:48" resolution="Fixed">
		<buginformation>
			<summary>TestTCTLSeparatedProtocol is broken</summary>
			<description>Some of the tests in TestTCTLSeparatedProtocol throws a NullPointedException that is catched, logged and not rethrown. This means the tests don&amp;amp;apos;t fail even though an unexpected exception is thrown.</description>
			<version>0.4.0</version>
			<fixedVersion>0.4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.serde2.TestTCTLSeparatedProtocol.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.java</file>
		</fixedFiles>
	</bug>
	<bug id="532" opendate="2009-06-02 18:42:59" fixdate="2009-06-03 05:11:42" resolution="Fixed">
		<buginformation>
			<summary>predicate clause with limit should not be pushed down.</summary>
			<description>in queries like below , &amp;amp;apos;v.c2 &amp;gt; 20&amp;amp;apos; shouldn&amp;amp;apos;t be pushed up.
select * from (select * from t where t.c &amp;gt; 20 limit 20) v where v.c2 &amp;gt; 20
</description>
			<version>0.4.0</version>
			<fixedVersion>0.4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.ppd.ExprWalkerInfo.java</file>
			<file type="M">org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.ppd.ExprWalkerProcFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.ppd.PredicatePushDown.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.LimitOperator.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">516</link>
		</links>
	</bug>
	<bug id="534" opendate="2009-06-02 19:13:53" fixdate="2009-06-03 20:24:03" resolution="Fixed">
		<buginformation>
			<summary>cli adds a new line at the beginning of every query</summary>
			<description>this results in error messages always specify a line which is one more than the actual error.
hive&amp;gt; select count* from abc; 
FAILED: Parse Error: line 2:14 cannot recognize input &amp;amp;apos;from&amp;amp;apos; in expression specification
</description>
			<version>0.3.0</version>
			<fixedVersion>0.4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.cli.CliDriver.java</file>
		</fixedFiles>
	</bug>
	<bug id="443" opendate="2009-04-24 21:47:14" fixdate="2009-06-04 19:16:04" resolution="Fixed">
		<buginformation>
			<summary>Remove deprecated functions from Hive.java</summary>
			<description>remove deprecated createTable and dropTable functions</description>
			<version>0.3.0</version>
			<fixedVersion>0.4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.TestExecDriver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.history.TestHiveHistory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.QTestUtil.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
		</fixedFiles>
	</bug>
	<bug id="544" opendate="2009-06-05 01:56:35" fixdate="2009-06-05 03:39:22" resolution="Fixed">
		<buginformation>
			<summary>predicate pushdown is not handling exprFieldNodeDesc correctly</summary>
			<description>complex column fields are not handled correctly resulting in ClassCastException.</description>
			<version>0.4.0</version>
			<fixedVersion>0.4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.LimitOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.ppd.ExprWalkerProcFactory.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">516</link>
		</links>
	</bug>
	<bug id="528" opendate="2009-06-02 05:21:13" fixdate="2009-06-07 07:42:56" resolution="Fixed">
		<buginformation>
			<summary>Map Join followup: split MapJoinObject into MapJoinObjectKey and MapJoinObjectValue</summary>
			<description>split MapJoinObject into MapJoinObjectKey and MapJoinObjectValue for code cleanup</description>
			<version>0.4.0</version>
			<fixedVersion>0.4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Operator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapJoinObject.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
	</bug>
	<bug id="547" opendate="2009-06-07 06:01:42" fixdate="2009-06-12 00:18:28" resolution="Fixed">
		<buginformation>
			<summary>NullPointerException in ExecDriver</summary>
			<description>We saw a job failed with the following message in hive.log.

2009-06-06 22:50:55,275 ERROR exec.ExecDriver (SessionState.java:printError(279)) - Ended Job = job_200905211352_145363 with exception &amp;amp;apos;java.lang.NullPointerException(null)&amp;amp;apos;
java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.exec.ExecDriver.jobProgress(ExecDriver.java:193)
        at org.apache.hadoop.hive.ql.exec.ExecDriver.execute(ExecDriver.java:395)
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:307)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:213)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:176)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:216)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:273)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:166)
        at org.apache.hadoop.mapred.JobShell.run(JobShell.java:194)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
        at org.apache.hadoop.mapred.JobShell.main(JobShell.java:220)


The corresponding code is:


  public RunningJob jobProgress(JobClient jc, RunningJob rj) throws IOException {
    String lastReport = "";
    while (!rj.isComplete()) {
      try {
        Thread.sleep(1000);
      } catch (InterruptedException e) {
      }
      rj = jc.getJob(rj.getJobID());
      String report = null;
193  report = " map = " + Math.round(rj.mapProgress() * 100) + "%,  reduce ="
          + Math.round(rj.reduceProgress() * 100) + "%";


</description>
			<version>0.4.0</version>
			<fixedVersion>0.4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
		</fixedFiles>
	</bug>
	<bug id="557" opendate="2009-06-11 18:08:23" fixdate="2009-06-16 00:50:12" resolution="Fixed">
		<buginformation>
			<summary>Exception in FileSinkOperator&amp;apos;s close should NOT be ignored</summary>
			<description>FileSinkOperator currently ignores all IOExceptions from close() and commit(). We should not ignore them, or the output file can be incomplete or missing.</description>
			<version>0.3.0</version>
			<fixedVersion>0.4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecMapper.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecReducer.java</file>
		</fixedFiles>
	</bug>
	<bug id="338" opendate="2009-03-11 01:44:47" fixdate="2009-06-24 04:08:33" resolution="Incomplete">
		<buginformation>
			<summary>Executing cli commands into thrift server</summary>
			<description>Let thrift server support set, add/delete file/jar and normal HSQL query.</description>
			<version>0.3.0</version>
			<fixedVersion>0.4.0</fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapRedTask.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.service.HiveServer.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.SerDeUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.session.SessionState.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.hwi.HWISessionItem.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FunctionTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Table.java</file>
			<file type="D">org.apache.hadoop.hive.ql.CommandProcessor.java</file>
			<file type="D">org.apache.hadoop.hive.cli.SetProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.cli.CliDriver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">574</link>
			<link type="Duplicate" description="duplicates">322</link>
		</links>
	</bug>
	<bug id="560" opendate="2009-06-15 20:43:15" fixdate="2009-06-25 05:51:42" resolution="Fixed">
		<buginformation>
			<summary>column pruning not working with map joins</summary>
			<description>drop table tst1;
drop table tst2;
create table tst1(a1 string, a2 string, a3 string, a4 string);
create table tst2(b1 string, b2 string, b3 string, b4 string);
explain select /*+ MAPJOIN(a) */ a.a1, a.a2 from tst1 a join tst2 b ON a.a2=b.b2;
the select is after the join - column pruning is not happening</description>
			<version>0.4.0</version>
			<fixedVersion>0.4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcCtx.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.joinDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ColumnPruner.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.mapJoinDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">460</link>
		</links>
	</bug>
	<bug id="587" opendate="2009-06-27 01:38:57" fixdate="2009-06-29 20:43:23" resolution="Fixed">
		<buginformation>
			<summary>Duplicate result from multiple TIPs of the same task</summary>
			<description>On our cluster we found a job committed with duplicate output from different TIPs of the same Task (from FileSinkOperator).
The reason is that FileSinkOperator.commit can be called at multiple TIPs of the same task.
FileSinkOperator.jobClose() (which is called at the Hive Client side) should do either:
A. Get all successful TIPs and only move the output files of those TIPs to the output directory
B. Ignore TIPs from the JobInProgress, but only move one file out of potentially several output files 
B is preferred because A might be slow (if the job finished and immediately got moved out of the JobTracker memory). Since we control the file name by ourselves, we know exactly what the file names are.</description>
			<version>0.3.0</version>
			<fixedVersion>0.4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
		</fixedFiles>
	</bug>
	<bug id="574" opendate="2009-06-24 04:15:31" fixdate="2009-06-30 18:09:28" resolution="Fixed">
		<buginformation>
			<summary>Hive should use ClassLoader from hadoop Configuration</summary>
			<description>See HIVE-338.
Hive should always use the getClassByName method from hadoop Configuration, so that we choose the correct ClassLoader. Examples include all plug-in interfaces, including UDF/GenericUDF/UDAF, SerDe, and FileFormats. Basically the following code snippet shows the idea:


package org.apache.hadoop.conf;
public class Configuration implements Iterable&amp;lt;Map.Entry&amp;lt;String,String&amp;gt;&amp;gt; {
   ...
  /**
   * Load a class by name.
   * 
   * @param name the class name.
   * @return the class object.
   * @throws ClassNotFoundException if the class is not found.
   */
  public Class&amp;lt;?&amp;gt; getClassByName(String name) throws ClassNotFoundException {
    return Class.forName(name, true, classLoader);
  }


</description>
			<version>0.3.0</version>
			<fixedVersion>0.4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecReducer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapRedTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.RCFile.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.ql.session.SessionState.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecMapper.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.ThriftDeserializer.java</file>
			<file type="M">org.apache.hadoop.hive.common.JavaUtils.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">584</link>
			<link type="Blocker" description="is blocked by">338</link>
		</links>
	</bug>
	<bug id="136" opendate="2008-12-08 21:36:23" fixdate="2009-07-02 02:03:46" resolution="Fixed">
		<buginformation>
			<summary>SerDe should escape some special characters</summary>
			<description>MetadataTypedColumnsetSerDe and DynamicSerDe should escape some special characters like &amp;amp;apos;\n&amp;amp;apos; or the column/item/key separator.
Otherwise the data will look corrupted.
We plan to deprecate MetadataTypedColumnsetSerDe and DynamicSerDe for the simple delimited format, and use LazySimpleSerDe instead.
So LazySimpleSerDe needs to have the capability of escaping and unescaping.</description>
			<version>0.3.0</version>
			<fixedVersion>0.4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.plan.createTableDesc.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.TestLazyArrayMapStruct.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyArray.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyByte.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.LazySimpleStructObjectInspector.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.LazyMapObjectInspector.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.TestRCFile.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.SerDeUtils.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyNonPrimitive.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.TestLazySimpleSerDe.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyString.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyMap.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyLong.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyFloat.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyObject.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyInteger.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyUtils.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyPrimitive.java</file>
			<file type="M">org.apache.hadoop.hive.serde.Constants.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyStruct.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.ColumnarStructObjectInspector.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveWritableObjectInspector.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.TestLazyPrimitive.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.columnar.ColumnarStruct.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyDouble.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyFactory.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyShort.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.LazyListObjectInspector.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExprNodeIndexEvaluator.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaStringObjectInspector.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveJavaObjectInspector.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="is blocked by">337</link>
			<link type="Reference" description="is related to">1898</link>
			<link type="Reference" description="is related to">270</link>
		</links>
	</bug>
	<bug id="612" opendate="2009-07-06 23:35:19" fixdate="2009-07-07 00:55:47" resolution="Fixed">
		<buginformation>
			<summary>Problem in removing temp files in FileSinkOperator.jobClose</summary>
			<description>We are doing double delete for files with _tmp prefix.</description>
			<version>0.3.0</version>
			<fixedVersion>0.4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
		</fixedFiles>
	</bug>
	<bug id="635" opendate="2009-07-14 19:01:58" fixdate="2009-07-16 01:54:27" resolution="Fixed">
		<buginformation>
			<summary>UnionOperator fails when different inputs have different ObjectInspector (but the same TypeInfo)</summary>
			<description>The current UnionOperator code assumes the ObjectInspectors from all parents are the same.
But in reality, they can be different, and UnionOperator needs to do conversion if necessary.</description>
			<version>0.3.0</version>
			<fixedVersion>0.4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.UnionOperator.java</file>
		</fixedFiles>
	</bug>
	<bug id="405" opendate="2009-04-10 08:07:18" fixdate="2009-07-17 03:42:38" resolution="Fixed">
		<buginformation>
			<summary>Cleanup operator initialization</summary>
			<description>We are always passing the same ObjectInspector, so there is no need to pass it again and again in forward.
Also there is a problem that can ONLY be fixed by passing ObjectInspector in init: Outer Joins - Outer Joins may not be able to get ObjectInspectors for all inputs, as a result, there is no way to construct an output ObjectInspector based on the inputs. Currently we have hard-coded code that assumes joins are always outputting Strings, which did break but was hidden by the old framework (because we do toString() when serializing the output, and toString() is defined for all Java Classes).</description>
			<version>0.4.0</version>
			<fixedVersion>0.4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.LimitOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.CollectOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ForwardOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecReducer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FilterOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.SelectOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.UnionOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Operator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecMapper.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExtractOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TestOperators.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="is blocked by">369</link>
			<link type="Reference" description="is related to">485</link>
			<link type="Reference" description="is related to">164</link>
		</links>
	</bug>
	<bug id="592" opendate="2009-06-29 21:22:55" fixdate="2009-07-19 00:19:07" resolution="Fixed">
		<buginformation>
			<summary>renaming internal table should rename HDFS and also change path of the table and partitions accordingly.</summary>
			<description>rename table changes just the name of the table in metastore but not hdfs. so if a table with old name is created, it uses the hdfs directory pointing to the renamed table.</description>
			<version>0.3.0</version>
			<fixedVersion>0.4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.Warehouse.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="is blocked by">445</link>
			<link type="Duplicate" description="is duplicated by">419</link>
			<link type="Reference" description="relates to">1116</link>
		</links>
	</bug>
	<bug id="487" opendate="2009-05-13 20:54:34" fixdate="2009-08-04 23:11:12" resolution="Fixed">
		<buginformation>
			<summary>Hive does not compile with Hadoop 0.20.0</summary>
			<description>Attempting to compile Hive with Hadoop 0.20.0 fails:
aaron@jargon:~/src/ext/svn/hive-0.3.0$ ant -Dhadoop.version=0.20.0 package
(several lines elided)
compile:
[echo] Compiling: hive
[javac] Compiling 261 source files to /home/aaron/src/ext/svn/hive-0.3.0/build/ql/classes
[javac] /home/aaron/src/ext/svn/hive-0.3.0/build/ql/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java:94: cannot find symbol
[javac] symbol  : method getCommandLineConfig()
[javac] location: class org.apache.hadoop.mapred.JobClient
[javac]       Configuration commandConf = JobClient.getCommandLineConfig();
[javac]                                            ^
[javac] /home/aaron/src/ext/svn/hive-0.3.0/build/ql/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java:241: cannot find symbol
[javac] symbol  : method validateInput(org.apache.hadoop.mapred.JobConf)
[javac] location: interface org.apache.hadoop.mapred.InputFormat
[javac]       inputFormat.validateInput(newjob);
[javac]                  ^
[javac] Note: Some input files use or override a deprecated API.
[javac] Note: Recompile with -Xlint:deprecation for details.
[javac] Note: Some input files use unchecked or unsafe operations.
[javac] Note: Recompile with -Xlint:unchecked for details.
[javac] 2 errors
BUILD FAILED
/home/aaron/src/ext/svn/hive-0.3.0/build.xml:145: The following error occurred while executing this line:
/home/aaron/src/ext/svn/hive-0.3.0/ql/build.xml:135: Compile failed; see the compiler error output for details.</description>
			<version>0.3.0</version>
			<fixedVersion>0.4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.QTestUtil.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapRedTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.contrib.fileformat.base64.Base64TextInputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.cli.CliDriver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
			<file type="M">org.apache.hadoop.hive.hwi.HWIServer.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">726</link>
		</links>
	</bug>
	<bug id="218" opendate="2009-01-07 20:03:20" fixdate="2009-08-06 22:58:35" resolution="Duplicate">
		<buginformation>
			<summary>predicate on partitioning column is ignored in many places</summary>
			<description>We tried two queries yesterday that bought up several problems:
1. predicate on partitioning column within a join clause was ignored:
FROM (FROM xxx a SELECT a.xx, a.yy, a.ds WHERE a.ds=2009-01-05 UNION ALL FROM yyy SELECT b.xx, b.yy, b.ds WHERE b.ds=2009-01-05 UNION ALL FROM zzz c SELECT c.xx, c.yy, c.ds WHERE c.ds=2009-01-05) d JOIN aaa e ON (d.xx=e.xx AND e.ds=2009-01-05) INSERT OVERWRITE TABLE ...
the plan tried to scan all partitions!
2. predicate on partitioning clause inside insert clause was ignored (we took the previous query and moved the partition filter to the insert statement)
FROM (FROM xxx a SELECT a.xx, a.yy, a.ds WHERE a.ds=2009-01-05 UNION ALL FROM yyy SELECT b.xx, b.yy, b.ds WHERE b.ds=2009-01-05 UNION ALL FROM zzz c SELECT c.xx, c.yy, c.ds WHERE c.ds=2009-01-05) d JOIN aaa e ON (d.xx=e.xx ) INSERT OVERWRITE TABLE ... WHERE e.ds=2009-01-05; 
the plan again tried to scan all partitions
the really bad thing is that we were able to detect this problem only because of metastore inconsistencies - otherwise - we would have merrily scanned all the data. This is really critical to get fixed - because this means that we may actually be scanning tons of unnecessary data in production.</description>
			<version>0.3.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TestOperators.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TestExecDriver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.exprNodeColumnDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ColumnInfo.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.tableScanDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.filterDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java</file>
			<file type="D">org.apache.hadoop.hive.ql.parse.PartitionPruner.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TestPlan.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TestExpressionEvaluator.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">578</link>
		</links>
	</bug>
	<bug id="454" opendate="2009-04-28 15:49:34" fixdate="2009-08-07 23:04:35" resolution="Fixed">
		<buginformation>
			<summary>Support escaping of ; in strings in cli</summary>
			<description>If ; appears in string literals in a query the hive cli is not able to escape them properly.</description>
			<version>0.3.0</version>
			<fixedVersion>0.4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.cli.CliDriver.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">645</link>
		</links>
	</bug>
	<bug id="578" opendate="2009-06-24 15:29:41" fixdate="2009-08-11 02:06:50" resolution="Fixed">
		<buginformation>
			<summary>Refactor partition pruning code as an optimizer transformation</summary>
			<description>Some bugs with partition pruning have been reported and the correct fix for many of them is to rewrite the partition pruning code as an optimizer transformation which gets kicked in after the predicate pushdown code. This refactor also uses the graph walker framework so that the partition pruning code gets consolidated well with the frameworks and does not work on the query block but rather works on the operator tree.</description>
			<version>0.3.0</version>
			<fixedVersion>0.4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TestOperators.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TestExecDriver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.exprNodeColumnDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ColumnInfo.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.tableScanDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.filterDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java</file>
			<file type="D">org.apache.hadoop.hive.ql.parse.PartitionPruner.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TestPlan.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TestExpressionEvaluator.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">218</link>
		</links>
	</bug>
	<bug id="773" opendate="2009-08-19 05:54:01" fixdate="2009-08-19 10:01:34" resolution="Fixed">
		<buginformation>
			<summary>remove lzocodec import from FileSinkOperator</summary>
			<description></description>
			<version>0.5.0</version>
			<fixedVersion>0.5.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
		</fixedFiles>
	</bug>
	<bug id="774" opendate="2009-08-20 01:03:04" fixdate="2009-08-20 03:37:16" resolution="Fixed">
		<buginformation>
			<summary>Fix the behavior of "/" and add "DIV"</summary>
			<description>In hive, "select 3/2" will return 1 while MySQL returns 1.5.
See http://dev.mysql.com/doc/refman/5.0/en/arithmetic-functions.html#operator_div for details.


mysql&amp;gt; select 3/2;
+--------+
| 3/2    |
+--------+
| 1.5000 |
+--------+
1 row in set (0.00 sec)

mysql&amp;gt; select 3 div 2;
+---------+
| 3 div 2 |
+---------+
|       1 |
+---------+
1 row in set (0.00 sec)

mysql&amp;gt; select -3 div 2;
+----------+
| -3 div 2 |
+----------+
|       -1 |
+----------+
1 row in set (0.00 sec)

mysql&amp;gt; select -3 div -2;
+-----------+
| -3 div -2 |
+-----------+
|         1 |
+-----------+
1 row in set (0.00 sec)

mysql&amp;gt; select 3 div -2;
+----------+
| 3 div -2 |
+----------+
|       -1 |
+----------+
1 row in set (0.00 sec)

</description>
			<version>0.4.0</version>
			<fixedVersion>0.4.0, 0.5.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFOPDivide.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">778</link>
			<link type="Reference" description="is related to">776</link>
			<link type="Reference" description="is related to">778</link>
		</links>
	</bug>
	<bug id="794" opendate="2009-08-25 08:21:56" fixdate="2009-08-26 05:59:31" resolution="Fixed">
		<buginformation>
			<summary>MergeTask should use COMPRESSRESULT instead of COMPRESSINTERMEDIATE</summary>
			<description>The MergeTask is responsible for merging small output files into bigger files for the final output table.
The compression settings to be used should be COMPRESSRESULT instead of COMPRESSINTERMEDIATE.
GenMRFileSink1.java:172:


    FileSinkOperator newOutput = 
      (FileSinkOperator)OperatorFactory.getAndMakeChild(
         new fileSinkDesc(finalName, ts, 
                          parseCtx.getConf().getBoolVar(HiveConf.ConfVars.COMPRESSINTERMEDIATE)),
         fsRS, extract);


Associated mailing list discussion: http://mail-archives.apache.org/mod_mbox/hadoop-hive-user/200908.mbox/%3C794f042d0908122114o7ddb8d18h4f444c1dfa16fa87@mail.gmail.com%3E</description>
			<version>0.4.0</version>
			<fixedVersion>0.4.0, 0.5.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
		</fixedFiles>
	</bug>
	<bug id="816" opendate="2009-09-04 20:33:41" fixdate="2009-09-04 21:22:05" resolution="Fixed">
		<buginformation>
			<summary>MetastoreClient not being cached</summary>
			<description>In org.apache.hadoop.hive.ql.metadata.Hive.getMSC(), we create a new MetaStoreClient on every call because the result is not getting properly cached in the threadLocal:


  private IMetaStoreClient getMSC() throws MetaException {
    IMetaStoreClient msc = threadLocalMSC.get();
    if(msc == null) {
      msc = this.createMetaStoreClient();
      // THERE SHOULD BE A threadLocalMSC.set here!
    }
    return msc;
  }

</description>
			<version>0.3.0</version>
			<fixedVersion>0.4.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
	</bug>
	<bug id="823" opendate="2009-09-09 19:54:03" fixdate="2009-09-14 21:31:47" resolution="Fixed">
		<buginformation>
			<summary>Make table alise in MAPJOIN hint case insensitive</summary>
			<description>If we use table alias in upper case for MAPJOIN hint, it is ignored. It must be specified in lower case.
Example query:
SELECT /*+ MAPJOIN(N) */ parse_url(ADATA.url,&amp;amp;apos;HOST&amp;amp;apos;) AS domain, N.type AS type
FROM nikeusers N join adserves ADATA on (ADATA.user_id = N.uid)
WHERE ADATA.data_date = &amp;amp;apos;20090901&amp;amp;apos;
This query features reducers in its execution. Attached is output of explain extended.
After changing query to:
SELECT /*+ MAPJOIN */ parse_url(adata.url,&amp;amp;apos;HOST&amp;amp;apos;) AS domain, n.type AS type
FROM nikeusers n join adserves adata on (adata.user_id = N.uid)
WHERE adata.data_date = &amp;amp;apos;20090901&amp;amp;apos;
It executes as expected. Attached is output of explain extended.
Thanks to Zheng for helping and catching this.</description>
			<version>0.4.0</version>
			<fixedVersion>0.5.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
	</bug>
	<bug id="718" opendate="2009-08-03 19:07:23" fixdate="2009-09-15 19:15:26" resolution="Fixed">
		<buginformation>
			<summary>Load data inpath into a new partition without overwrite does not move the file</summary>
			<description>The bug can be reproduced as following. Note that it only happens for partitioned tables. The select after the first load returns nothing, while the second returns the data correctly.
insert.txt in the current local directory contains 3 lines: "a", "b" and "c".


&amp;gt; create table tmp_insert_test (value string) stored as textfile;
&amp;gt; load data local inpath &amp;amp;apos;insert.txt&amp;amp;apos; into table tmp_insert_test;
&amp;gt; select * from tmp_insert_test;
a
b
c
&amp;gt; create table tmp_insert_test_p ( value string) partitioned by (ds string) stored as textfile;
&amp;gt; load data local inpath &amp;amp;apos;insert.txt&amp;amp;apos; into table tmp_insert_test_p partition (ds = &amp;amp;apos;2009-08-01&amp;amp;apos;);
&amp;gt; select * from tmp_insert_test_p where ds= &amp;amp;apos;2009-08-01&amp;amp;apos;;
&amp;gt; load data local inpath &amp;amp;apos;insert.txt&amp;amp;apos; into table tmp_insert_test_p partition (ds = &amp;amp;apos;2009-08-01&amp;amp;apos;);
&amp;gt; select * from tmp_insert_test_p where ds= &amp;amp;apos;2009-08-01&amp;amp;apos;;
a       2009-08-01
b       2009-08-01
d       2009-08-01

</description>
			<version>0.4.0</version>
			<fixedVersion>0.4.0, 0.5.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">829</link>
			<link type="Reference" description="relates to">307</link>
			<link type="dependent" description="depends upon">789</link>
		</links>
	</bug>
	<bug id="841" opendate="2009-09-17 18:12:37" fixdate="2009-09-17 22:31:19" resolution="Fixed">
		<buginformation>
			<summary>Context.java Uses Deleted (previously Deprecated) Hadoop Methods</summary>
			<description>Building Hive against Trunk/Nightly Hadoop Fails (ql/src/java/org/apache/hadoop/hive/ql/Context.java)</description>
			<version>0.5.0</version>
			<fixedVersion>0.5.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.Context.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">4940</link>
		</links>
	</bug>
	<bug id="855" opendate="2009-09-24 23:12:14" fixdate="2009-10-05 09:33:30" resolution="Fixed">
		<buginformation>
			<summary>UDF: Concat should accept multiple arguments</summary>
			<description>According to mysql, concat should accept multiple arguments.
http://dev.mysql.com/doc/refman/5.0/en/string-functions.html#function_concat
</description>
			<version>0.5.0</version>
			<fixedVersion>0.5.0</fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFConcat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">856</link>
		</links>
	</bug>
	<bug id="869" opendate="2009-10-05 21:49:48" fixdate="2009-10-12 19:06:50" resolution="Fixed">
		<buginformation>
			<summary>Allow ScriptOperator to consume not all input data</summary>
			<description>The ScriptOperator (SELECT TRANSFORM(a, b, c) USING &amp;amp;apos;myscript&amp;amp;apos; AS (d, e, f) ...) has a problem:
If the user script exits without consuming all data from standard input, then we will report an error even if the exit code from the user script is 0.
We want to have an option, when enabled, ScriptOperator will return successfully in that case.
If the option is not enabled, then we should stick to the current behavior.
The option can be called: "hive.exec.script.allow.partial.consumption ".</description>
			<version>0.5.0</version>
			<fixedVersion>0.5.0</fixedVersion>
			<type>New Feature</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">386</link>
		</links>
	</bug>
	<bug id="856" opendate="2009-09-24 23:18:59" fixdate="2009-10-13 04:50:19" resolution="Duplicate">
		<buginformation>
			<summary>allow concat to take more than 2 arguments</summary>
			<description>mysql&amp;amp;apos;s concat allows concat(&amp;amp;apos;a&amp;amp;apos;, &amp;amp;apos;b&amp;amp;apos;, &amp;amp;apos;c&amp;amp;apos;), but hive&amp;amp;apos;s currently will accept only two arguments.</description>
			<version>0.5.0</version>
			<fixedVersion>0.5.0</fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFConcat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">855</link>
		</links>
	</bug>
	<bug id="876" opendate="2009-10-14 23:54:09" fixdate="2009-10-15 03:07:29" resolution="Fixed">
		<buginformation>
			<summary>UDFOPNegative should deal with NULL gracefully</summary>
			<description>UDFOPNegative is throwing out NullPointerException. It should return NULL for that.</description>
			<version>0.4.1</version>
			<fixedVersion>0.5.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFOPNegative.java</file>
		</fixedFiles>
	</bug>
	<bug id="878" opendate="2009-10-15 05:06:27" fixdate="2009-10-15 20:53:07" resolution="Fixed">
		<buginformation>
			<summary>Update the hash table entry before flushing in Group By hash aggregation</summary>
			<description>This is a newly introduced bug from r796133.
We should first update the aggregation, and then we can flush the hash table. Otherwise the entry that we update might be already out of the hash table.</description>
			<version>0.4.0</version>
			<fixedVersion>0.4.1, 0.5.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">609</link>
		</links>
	</bug>
	<bug id="893" opendate="2009-10-20 23:02:39" fixdate="2009-10-22 00:02:43" resolution="Fixed">
		<buginformation>
			<summary>Thrift serde doesn&amp;apos;t work with the new version of thrift</summary>
			<description>The new version of thrift rename the __isset to __isset_bit_vector in the generated Thrift java code. This causes __isset_bit_vector passed as a field in ThriftSerDe. </description>
			<version>0.4.0</version>
			<fixedVersion>0.5.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.ThriftStructObjectInspector.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">803</link>
		</links>
	</bug>
	<bug id="883" opendate="2009-10-17 05:51:53" fixdate="2009-10-26 22:38:59" resolution="Fixed">
		<buginformation>
			<summary>URISyntaxException when partition value contains special chars</summary>
			<description>When we try to insert into a partitioned table that the partition value contains special char ":", we will see an exception


stack trace:
java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: ts=2009-10-16 16:14:10
        at org.apache.hadoop.fs.Path.initialize(Path.java:140)
        at org.apache.hadoop.fs.Path.&amp;lt;init&amp;gt;(Path.java:126)
        at org.apache.hadoop.fs.Path.&amp;lt;init&amp;gt;(Path.java:45)
        at org.apache.hadoop.hive.ql.metadata.Partition.initialize(Partition.java:146)
        at org.apache.hadoop.hive.ql.metadata.Partition.&amp;lt;init&amp;gt;(Partition.java:123)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer$tableSpec.&amp;lt;init&amp;gt;(BaseSemanticAnalyzer.java:292)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(SemanticAnalyzer.java:747)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:4383)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:87)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:251)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:283)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:123)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:181)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:251)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:166)
        at org.apache.hadoop.mapred.JobShell.run(JobShell.java:194)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
        at org.apache.hadoop.mapred.JobShell.main(JobShell.java:220)
Caused by: java.net.URISyntaxException: Relative path in absolute URI: ts=2009-10-16 16:14:10
        at java.net.URI.checkPath(URI.java:1787)
        at java.net.URI.&amp;lt;init&amp;gt;(URI.java:735)
        at org.apache.hadoop.fs.Path.initialize(Path.java:137)
        ... 22 more

</description>
			<version>0.4.0</version>
			<fixedVersion>0.4.1, 0.5.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.Warehouse.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
		</fixedFiles>
	</bug>
	<bug id="940" opendate="2009-11-18 21:07:19" fixdate="2009-11-19 00:15:54" resolution="Fixed">
		<buginformation>
			<summary>restrict creation of partitions with empty partition keys</summary>
			<description>create table pc (a int) partitioned by (b string, c string);
alter table pc add partition (b="f", c=&amp;amp;apos;&amp;amp;apos;);
above alter cmd fails but actually creates a partition with name &amp;amp;apos;b=f/c=&amp;amp;apos; but describe partition on the same name fails. creation of such partitions should not be allowed.</description>
			<version>0.3.0</version>
			<fixedVersion>0.5.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
	</bug>
	<bug id="953" opendate="2009-11-25 05:48:01" fixdate="2009-12-01 02:37:20" resolution="Fixed">
		<buginformation>
			<summary>script_broken_pipe3.q broken</summary>
			<description>The negative test script_broken_pipe3.q is broken if we allow partial consumption.
For now, I have disabled partial consumption. Can you take a look ?</description>
			<version>0.5.0</version>
			<fixedVersion>0.5.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
		</fixedFiles>
	</bug>
	<bug id="386" opendate="2009-04-04 01:52:24" fixdate="2009-12-17 23:37:32" resolution="Duplicate">
		<buginformation>
			<summary>Streaming processes should be able to return successfully without consuming all their input data.</summary>
			<description>Currently if a streaming process exits without consuming all data in stdin, it causes a java IOException with Broken pipe. It seems like it should be possible to distinguish between broken pipe and the actual return code of the sub-process; so that broken pipe by itself  should be caught and ignored if the sub-process produced a SUCCESS return code.</description>
			<version>0.5.0</version>
			<fixedVersion>0.5.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">869</link>
		</links>
	</bug>
	<bug id="419" opendate="2009-04-15 17:08:17" fixdate="2009-12-17 23:39:50" resolution="Duplicate">
		<buginformation>
			<summary>Rename HDFS directories when a table is renamed.</summary>
			<description>As the title says. Applies only to internal (or native) tables.</description>
			<version>0.3.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.Warehouse.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">592</link>
		</links>
	</bug>
	<bug id="595" opendate="2009-06-30 03:05:55" fixdate="2009-12-18 19:21:29" resolution="Fixed">
		<buginformation>
			<summary>copyFiles does not report errors in file rename operations</summary>
			<description>ql/../Hive.java:copyFiles() does not catch failures reported by fs.rename. this may cause load commands to look successful when they actually failed</description>
			<version>0.3.0</version>
			<fixedVersion>0.5.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
	</bug>
	<bug id="1006" opendate="2009-12-22 15:12:20" fixdate="2009-12-22 19:48:02" resolution="Fixed">
		<buginformation>
			<summary>getPartitionDescFromPath failing from CombineHiveInputFormat</summary>
			<description>When HiveInputFormat.getPartitionDescFromPath is called from CombineHiveInputFormat, it sometimes fails to return a matching partitionDesc which then causes an Exception down the line since the split doesn&amp;amp;apos;t have an inputFormatClassName.
The issue is that the path format used as the key in pathToPartitionInfo varies between stage - in the first stage it&amp;amp;apos;s the complete path as returned from the table definitions (eg. hdfs://server/path), and then in subsequent stages, it&amp;amp;apos;s the complete path with port (eg. hdfs://server:8020/path) of the result of the previous stage.  This isn&amp;amp;apos;t a problem in HiveInputFormat since the directory you&amp;amp;apos;re looking up always uses the same format as the keys, but in CombineHiveInputFormat, we take that path and look up its children in the file system to get all the block information, and then use one of the returned paths to get the partition info  and that returned path does not include the port.  So, in any stage after the first, we are looking for a path without the port, but all the keys in the map contain a port, so we don&amp;amp;apos;t find a match.
The attached patch may not be ideal  it doesn&amp;amp;apos;t fix the underlying problem of inconsistent path formats in pathToPartitionInfo  it just works around it by walking through the map and looking for a matching path rather than doing a hash lookup.</description>
			<version>0.4.1</version>
			<fixedVersion>0.5.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">1008</link>
			<link type="Reference" description="is related to">1008</link>
		</links>
	</bug>
	<bug id="1001" opendate="2009-12-21 07:43:17" fixdate="2010-01-02 07:55:41" resolution="Fixed">
		<buginformation>
			<summary>CombinedHiveInputFormat should parse the inputpath correctly</summary>
			<description>From David Lerman:
"
I&amp;amp;apos;m running into errors where CombinedHiveInputFormat is combining data from
two different tables which is causing problems because the tables have
different input formats.
It looks like the problem is in
org.apache.hadoop.hive.shims.Hadoop20Shims.getInputPathsShim.  It calls
CombineFileInputFormat.getInputPaths which returns the list of input paths
and then chops off the first 5 characters to remove file: from the
beginning, but the return value I&amp;amp;apos;m getting from getInputPaths is actually
hdfs://domain/path.  So then when it creates the pools using these paths,
none of the input paths match the pools (since they&amp;amp;apos;re just the file path
which protocol or domain).
"
We should use Path.getPath() to get the path part of an URI instead of just chopping off 5 chars.</description>
			<version>0.5.0</version>
			<fixedVersion>0.5.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
		</fixedFiles>
	</bug>
	<bug id="458" opendate="2009-04-29 20:58:49" fixdate="2010-01-06 02:47:42" resolution="Fixed">
		<buginformation>
			<summary>Setting fs.default.name incorrectly leads to meaningless error message</summary>
			<description>In my hadoop-site.xml I accidentally set fs.default.name to 
http://wilbur21.labs.corp.sp1.yahoo.com:8020 
instead of the proper:
hdfs://wilbur21.labs.corp.sp1.yahoo.com:8020
The result was


hive&amp;gt; show tables;
FAILED: Unknown exception : null
FAILED: Unknown exception : null
Time taken: 0.035 seconds
hive&amp;gt;


It should give a meaningful error message.</description>
			<version>0.3.0</version>
			<fixedVersion>0.5.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.Context.java</file>
		</fixedFiles>
	</bug>
	<bug id="1039" opendate="2010-01-11 19:29:20" fixdate="2010-01-13 00:13:10" resolution="Fixed">
		<buginformation>
			<summary>multi-insert doesn&amp;apos;t work for local directories</summary>
			<description>As wd pointed out in hive-user, the following query only load data to the first local directory. Multi-insert to tables works fine. 
hive&amp;gt; from test
    &amp;gt; INSERT OVERWRITE LOCAL DIRECTORY &amp;amp;apos;/home/stefdong/tmp/0&amp;amp;apos; select *
where a = 1
    &amp;gt; INSERT OVERWRITE LOCAL DIRECTORY &amp;amp;apos;/home/stefdong/tmp/1&amp;amp;apos; select *
where a = 3;</description>
			<version>0.5.0</version>
			<fixedVersion>0.5.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
		</fixedFiles>
	</bug>
	<bug id="1045" opendate="2010-01-12 18:37:40" fixdate="2010-01-14 18:17:34" resolution="Fixed">
		<buginformation>
			<summary>(bigint % int) should return bigint instead of double</summary>
			<description>This expression should return bigint instead of double.


CREATE TABLE test (a BIGINT);
EXPLAIN SELECT a % 3 FROM test;


There must be something wrong in FunctionRegistry.getMethodInternal</description>
			<version>0.5.0</version>
			<fixedVersion>0.5.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.ComparisonOpMethodResolver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.NumericOpMethodResolver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFOPDivide.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.UDFMethodResolver.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="is blocked by">1048</link>
		</links>
	</bug>
	<bug id="1046" opendate="2010-01-12 20:05:26" fixdate="2010-01-15 19:44:08" resolution="Fixed">
		<buginformation>
			<summary>Pass build.dir.hive and other properties to subant</summary>
			<description>Currently we are not passing properties like "build.dir.hive" etc to subant.
We should do that, otherwise setting "build.dir.hive" is not useful.
</description>
			<version>0.4.1</version>
			<fixedVersion>0.5.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapRedTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.QTestUtil.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">1051</link>
		</links>
	</bug>
	<bug id="1064" opendate="2010-01-18 11:56:05" fixdate="2010-01-21 00:55:23" resolution="Fixed">
		<buginformation>
			<summary>NPE when operating HiveCLI in distributed mode</summary>
			<description>

hive&amp;gt; select id, name from tab_a;
select id, name from tab_a;
10/01/18 03:55:59 INFO parse.ParseDriver: Parsing command: select id, name from tab_a
10/01/18 03:55:59 INFO parse.ParseDriver: Parse Completed
10/01/18 03:55:59 INFO parse.SemanticAnalyzer: Starting Semantic Analysis
10/01/18 03:55:59 INFO parse.SemanticAnalyzer: Completed phase 1 of Semantic Analysis
10/01/18 03:55:59 INFO parse.SemanticAnalyzer: Get metadata for source tables
10/01/18 03:55:59 INFO metastore.HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
10/01/18 03:55:59 INFO metastore.ObjectStore: ObjectStore, initialize called
10/01/18 03:56:03 INFO metastore.ObjectStore: Initialized ObjectStore
10/01/18 03:56:03 INFO metastore.HiveMetaStore: 0: get_table : db=default tbl=tab_a
10/01/18 03:56:03 INFO hive.log: DDL: struct tab_a { i32 id, string file, string name}
10/01/18 03:56:03 INFO parse.SemanticAnalyzer: Get metadata for subqueries
10/01/18 03:56:03 INFO parse.SemanticAnalyzer: Get metadata for destination tables
10/01/18 03:56:04 INFO parse.SemanticAnalyzer: Completed getting MetaData in Semantic Analysis
10/01/18 03:56:04 INFO ppd.OpProcFactory: Processing for FS(2)
10/01/18 03:56:04 INFO ppd.OpProcFactory: Processing for SEL(1)
10/01/18 03:56:04 INFO ppd.OpProcFactory: Processing for TS(0)
10/01/18 03:56:04 INFO hive.log: DDL: struct tab_a { i32 id, string file, string name}
10/01/18 03:56:04 INFO hive.log: DDL: struct tab_a { i32 id, string file, string name}
10/01/18 03:56:04 INFO parse.SemanticAnalyzer: Completed plan generation
10/01/18 03:56:04 INFO ql.Driver: Semantic Analysis Completed
10/01/18 03:56:04 INFO ql.Driver: Starting command: select id, name from tab_a
Total MapReduce jobs = 1
10/01/18 03:56:04 INFO ql.Driver: Total MapReduce jobs = 1
Launching Job 1 out of 1
10/01/18 03:56:04 INFO ql.Driver: Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there&amp;amp;apos;s no reduce operator
10/01/18 03:56:04 INFO exec.ExecDriver: Number of reduce tasks is set to 0 since there&amp;amp;apos;s no reduce operator
FAILED: Unknown exception : null
10/01/18 03:56:04 ERROR ql.Driver: FAILED: Unknown exception : null
java.lang.NullPointerException
	at org.apache.hadoop.hive.conf.HiveConf.getVar(HiveConf.java:288)
	at org.apache.hadoop.hive.ql.exec.ExecDriver.execute(ExecDriver.java:475)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:103)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:64)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:589)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:469)
	at org.apache.hadoop.hive.ql.Driver.runCommand(Driver.java:329)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:317)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:123)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:181)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:287)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:155)
	at org.apache.hadoop.mapred.JobShell.run(JobShell.java:54)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
	at org.apache.hadoop.mapred.JobShell.main(JobShell.java:68)

hive&amp;gt; 

</description>
			<version>0.6.0</version>
			<fixedVersion>0.6.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.FetchTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Task.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FunctionTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">1080</link>
		</links>
	</bug>
	<bug id="1072" opendate="2010-01-20 22:18:14" fixdate="2010-01-23 02:12:39" resolution="Fixed">
		<buginformation>
			<summary>"show table extended like table partition(xxx) " will show the result of the whole table if the partition does not exist</summary>
			<description>See the following example, we should output an error for the second command.


hive&amp;gt; show table extended like member_count;
OK
tableName:member_count
owner:null
location:/user/hive/member_count
inputformat:org.apache.hadoop.mapred.SequenceFileInputFormat
outputformat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
columns:struct columns { string count}
partitioned:true
partitionColumns:struct partition_columns { string ds}
totalNumberFiles:233933
totalFileSize:32802665
maxFileSize:257
minFileSize:140
lastAccessTime:1264017438860
lastUpdateTime:1263949909703

Time taken: 125.104 seconds

hive&amp;gt; show table extended like member_count partition(ds = &amp;amp;apos;2009-10-11&amp;amp;apos;);
OK
tableName:member_count
owner:null
location:/user/hive/member_count
inputformat:org.apache.hadoop.mapred.SequenceFileInputFormat
outputformat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
columns:struct columns { string count}
partitioned:true
partitionColumns:struct partition_columns { string ds}
totalNumberFiles:233933
totalFileSize:32802665
maxFileSize:257
minFileSize:140
lastAccessTime:1264017438860
lastUpdateTime:1263949909703

Time taken: 24.618 seconds

hive&amp;gt; show table extended like member_count partition(ds = &amp;amp;apos;2009-12-11&amp;amp;apos;);
OK
tableName:member_count
owner:null
location:/user/hive/member_count
inputformat:org.apache.hadoop.mapred.SequenceFileInputFormat
outputformat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
columns:struct columns { string count}
partitioned:true
partitionColumns:struct partition_columns { string ds}
totalNumberFiles:3495
totalFileSize:489417
maxFileSize:257
minFileSize:140
lastAccessTime:1262676533852
lastUpdateTime:1263949909703

Time taken: 0.549 seconds

</description>
			<version>0.4.1</version>
			<fixedVersion>0.5.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
		</fixedFiles>
	</bug>
	<bug id="1089" opendate="2010-01-24 22:40:36" fixdate="2010-01-26 01:25:31" resolution="Duplicate">
		<buginformation>
			<summary>Intermittent test failure in groupby_bigdata.q</summary>
			<description>

ant test -Dtestcase=TestCliDriver -Dqfile=groupby_bigdata.q


This sometimes fail with out-ot-memory exception in java.
We might want to tweak the default hash table size in map-side aggregation.</description>
			<version>0.4.1</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">1097</link>
		</links>
	</bug>
	<bug id="1097" opendate="2010-01-25 21:37:34" fixdate="2010-01-26 16:57:36" resolution="Fixed">
		<buginformation>
			<summary>groupby_bigdata.q sometimes throws out of memory exception</summary>
			<description>I would get out of memory errors like the following when running groupby_bigdata.q.


  
    [junit] plan = /data/users/pyang/task2/trunk/VENDOR.hive/trunk/build/ql/scratchdir/plan38413.xml
    [junit] Exception in thread "Thread-15" java.lang.OutOfMemoryError: Java heap space
    [junit]     at java.util.Arrays.copyOf(Arrays.java:2882)
    [junit]     at java.lang.AbstractStringBuilder.expandCapacity(AbstractStringBuilder.java:100)
    [junit]     at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:390)
    [junit]     at java.lang.StringBuffer.append(StringBuffer.java:224)
    [junit]     at java.io.StringWriter.write(StringWriter.java:84)
    [junit]     at java.io.PrintWriter.newLine(PrintWriter.java:436)
    [junit]     at java.io.PrintWriter.println(PrintWriter.java:585)
    [junit]     at java.io.PrintWriter.println(PrintWriter.java:696)
    [junit]     at java.lang.Throwable.printStackTrace(Throwable.java:512)
    [junit]     at org.apache.hadoop.util.StringUtils.stringifyException(StringUtils.java:60)
    [junit]     at org.apache.hadoop.hive.ql.exec.ScriptOperator$StreamThread.run(ScriptOperator.java:561)
    [junit] Exception in thread "main" java.lang.OutOfMemoryError: Java heap space
    [junit]     at java.nio.HeapCharBuffer.&amp;lt;init&amp;gt;(HeapCharBuffer.java:39)
    [junit]     at java.nio.CharBuffer.allocate(CharBuffer.java:312)
    [junit]     at java.nio.charset.CharsetEncoder.isLegalReplacement(CharsetEncoder.java:319)
    [junit]     at java.nio.charset.CharsetEncoder.replaceWith(CharsetEncoder.java:267)
    [junit]     at java.nio.charset.CharsetEncoder.&amp;lt;init&amp;gt;(CharsetEncoder.java:186)
    [junit]     at java.nio.charset.CharsetEncoder.&amp;lt;init&amp;gt;(CharsetEncoder.java:209)
    [junit]     at sun.nio.cs.ISO_8859_1$Encoder.&amp;lt;init&amp;gt;(ISO_8859_1.java:116)
    [junit]     at sun.nio.cs.ISO_8859_1$Encoder.&amp;lt;init&amp;gt;(ISO_8859_1.java:113)
    [junit]     at sun.nio.cs.ISO_8859_1.newEncoder(ISO_8859_1.java:46)
    [junit]     at java.lang.StringCoding$StringEncoder.&amp;lt;init&amp;gt;(StringCoding.java:215)
    [junit]     at java.lang.StringCoding$StringEncoder.&amp;lt;init&amp;gt;(StringCoding.java:207)
    [junit]     at java.lang.StringCoding.encode(StringCoding.java:266)
    [junit]     at java.lang.String.getBytes(String.java:947)
    [junit]     at java.io.UnixFileSystem.getLength(Native Method)
    [junit]     at java.io.File.length(File.java:848)
    [junit]     at org.apache.hadoop.fs.RawLocalFileSystem$RawLocalFileStatus.&amp;lt;init&amp;gt;(RawLocalFileSystem.java:375)
    [junit]     at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:359)
    [junit]     at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:245)
    [junit]     at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:643)
    [junit]     at org.apache.hadoop.hive.ql.exec.Utilities.clearMapRedWork(Utilities.java:114)
    [junit]     at org.apache.hadoop.hive.ql.exec.ExecDriver.execute(ExecDriver.java:680)
    [junit]     at org.apache.hadoop.hive.ql.exec.ExecDriver.main(ExecDriver.java:936)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit]     at java.lang.reflect.Method.invoke(Method.java:597)
    [junit]     at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
    [junit] Traceback (most recent call last):
    [junit]   File "../data/scripts/dumpdata_script.py", line 6, in &amp;lt;module&amp;gt;
    [junit]     print 20000 * i + k


</description>
			<version>0.4.1</version>
			<fixedVersion>0.5.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">1089</link>
		</links>
	</bug>
	<bug id="763" opendate="2009-08-18 17:00:08" fixdate="2010-01-26 19:46:34" resolution="Fixed">
		<buginformation>
			<summary>getSchema returns invalid column names, getThriftSchema does not return old style string schemas</summary>
			<description>SELECT AVG(total) as avg,STDDEV(total) as stddevr FROM (SELECT COUNT(phrase) as total FROM TABLE GROUP BY phrase) t2
getSchema and getThriftSchema both return
col0: double
col1 : double
expected results
avg : double
stddevr : double
col0 &amp;amp; col1 are useless column names.</description>
			<version>0.4.0</version>
			<fixedVersion>0.6.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
		</fixedFiles>
	</bug>
	<bug id="1112" opendate="2010-01-27 21:12:33" fixdate="2010-01-28 19:58:35" resolution="Fixed">
		<buginformation>
			<summary>Replace instances of StringBuffer/Vector with StringBuilder/ArrayList</summary>
			<description>When possible replace instances of StringBuffer and Vector with their non-synchronized counterparts StringBuilder and ArrayList.</description>
			<version>0.4.0</version>
			<fixedVersion>0.6.0</fixedVersion>
			<type>Task</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.ASTNode.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.dynamic_type.ParseException.java</file>
			<file type="M">org.apache.hadoop.hive.ql.lib.Node.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
			<file type="M">org.apache.hadoop.hive.service.HiveServer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.history.HiveHistory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Table.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.dynamic_type.TokenMgrError.java</file>
			<file type="M">org.apache.hadoop.hive.ql.lib.GraphWalker.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.InputSignature.java</file>
			<file type="M">org.apache.hadoop.hive.ql.QTestUtil.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FetchTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFConv.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.RowResolver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.UDAFEvaluatorResolver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.RowSchema.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.NonSyncDataInputBuffer.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.java</file>
			<file type="M">org.apache.hadoop.hive.cli.CliDriver.java</file>
			<file type="M">org.apache.hadoop.hive.hwi.HWISessionManager.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeFieldList.java</file>
			<file type="M">org.apache.hadoop.hive.hwi.TestHWISessionManager.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.QBJoinTree.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.Warehouse.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.hwi.HWISessionItem.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.RCFileOutputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.GenMRSkewJoinProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.AddPartitionDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Task.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryMap.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.columnar.BytesRefWritable.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java</file>
			<file type="M">org.apache.hadoop.hive.hwi.HWIServer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			<file type="M">org.apache.hadoop.hive.hwi.HWIAuth.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Operator.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFTestLength.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">545</link>
		</links>
	</bug>
	<bug id="1125" opendate="2010-02-02 18:52:44" fixdate="2010-02-02 23:33:46" resolution="Fixed">
		<buginformation>
			<summary>Hive CLI shows &amp;apos;Ended Job=&amp;apos; at the beginning of the job</summary>
			<description>Instead of "Starting Job = ", it prints "Ended Job ="


Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there&amp;amp;apos;s no reduce operator
Ended Job = job_201002012342_2688, Tracking URL = http://silver.data.facebook.com:50030/jobdetails.jsp?jobid=job_201002012342_2688
Kill Command = /data/users/pyang/task2/trunk/dist/shortcuts/silver.trunk/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=silver.data.facebook.com:50029 -kill job_201002012342_2688
2010-02-02 10:47:05,067 Stage-1 map = 0%,  reduce = 0%

</description>
			<version>0.6.0</version>
			<fixedVersion>0.6.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
		</fixedFiles>
	</bug>
	<bug id="803" opendate="2009-08-27 00:55:06" fixdate="2010-02-04 00:53:36" resolution="Duplicate">
		<buginformation>
			<summary>Hive Thrift interface code should ignore fields start with __isset</summary>
			<description>New versions of Thrift generates a field "_isset_bit_vector" instead of "_isset".
We should ignore both cases.
</description>
			<version>0.4.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.ThriftStructObjectInspector.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">893</link>
		</links>
	</bug>
	<bug id="1124" opendate="2010-02-02 08:26:37" fixdate="2010-02-04 19:43:54" resolution="Fixed">
		<buginformation>
			<summary>CREATE VIEW should expand the query text consistently</summary>
			<description>We should expand the omitted alias in the same way in "select" and in "group by".
Hive "Group By" recognize "group by" expressions by comparing the literal string.


hive&amp;gt; create view zshao_view as select d, count(1) as cnt from zshao_tt group by d;
OK
Time taken: 0.286 seconds
hive&amp;gt; select * from zshao_view;
FAILED: Error in semantic analysis: line 1:7 Expression Not In Group By Key d in definition of VIEW zshao_view [
select d, count(1) as `cnt` from `zshao_tt` group by `zshao_tt`.`d`
] used as zshao_view at line 1:14


</description>
			<version>0.6.0</version>
			<fixedVersion>0.6.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.RowResolver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.UnparseTranslator.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">972</link>
		</links>
	</bug>
	<bug id="1129" opendate="2010-02-03 20:22:00" fixdate="2010-02-06 00:18:01" resolution="Fixed">
		<buginformation>
			<summary>Fix Assertion in ExecDriver.execute when assertions are enabled in HADOOP_OPTS</summary>
			<description>I noticed that when running hive CLI, assertions are not enabled, which was causing me some confusion when debugging an issue.
So, I added the following to my environment:
export HADOOP_OPTS="-ea -esa"
This worked, and allowed me to see assertion failures when executing via CLI.
But then I tried to run a test, and got an assertion failure from the following code in ExecDriver.execute:
    // Turn on speculative execution for reducers
    HiveConf.setVar(job, HiveConf.ConfVars.HADOOPSPECULATIVEEXECREDUCERS,
        HiveConf.getVar(job, HiveConf.ConfVars.HIVESPECULATIVEEXECREDUCERS));
The assertion says it should be using getBoolVar/setBoolVar instead.</description>
			<version>0.5.0</version>
			<fixedVersion>0.6.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
		</fixedFiles>
	</bug>
	<bug id="1167" opendate="2010-02-13 01:50:15" fixdate="2010-02-14 02:32:25" resolution="Fixed">
		<buginformation>
			<summary>Use TreeMap instead of Property to make explain extended deterministic</summary>
			<description>In some places in the code, we are using Properties class in "explain extended".
This makes the order of the lines in the "explain extended" undeterministic because Properties are based on Hashtable class.
We should add another function to show the properties in sorted order.</description>
			<version>0.6.0</version>
			<fixedVersion>0.6.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">1117</link>
		</links>
	</bug>
	<bug id="1195" opendate="2010-02-24 23:06:34" fixdate="2010-02-25 04:57:56" resolution="Fixed">
		<buginformation>
			<summary>Increase ObjectInspector[] length on demand</summary>
			<description>

Operator.java
  protected transient ObjectInspector[] inputObjInspectors = new ObjectInspector[Short.MAX_VALUE];


An array of 32K elements takes 256KB memory under 64-bit Java.
We are seeing hive client going out of memory because of that.</description>
			<version>0.5.0</version>
			<fixedVersion>0.6.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.Operator.java</file>
		</fixedFiles>
	</bug>
	<bug id="1184" opendate="2010-02-20 04:20:47" fixdate="2010-02-25 08:40:17" resolution="Fixed">
		<buginformation>
			<summary>Expression Not In Group By Key error is sometimes masked</summary>
			<description>Depending on the order of expressions, the error message for a expression not in group key is not displayed; instead it is null.


hive&amp;gt; select concat(value, concat(value)) from src group by concat(value);
FAILED: Error in semantic analysis: null

hive&amp;gt; select concat(concat(value), value) from src group by concat(value);
FAILED: Error in semantic analysis: line 1:29 Expression Not In Group By Key value


</description>
			<version>0.6.0</version>
			<fixedVersion>0.6.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.TypeCheckCtx.java</file>
		</fixedFiles>
	</bug>
	<bug id="1207" opendate="2010-03-02 01:53:36" fixdate="2010-03-02 03:22:59" resolution="Fixed">
		<buginformation>
			<summary>ScriptOperator AutoProgressor does not set the interval</summary>
			<description>As title. I will show more details in the patch.</description>
			<version>0.5.0</version>
			<fixedVersion>0.6.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.AutoProgressor.java</file>
		</fixedFiles>
	</bug>
	<bug id="1242" opendate="2010-03-11 23:23:53" fixdate="2010-03-15 00:09:42" resolution="Fixed">
		<buginformation>
			<summary>CombineHiveInputFormat does not work for compressed text files</summary>
			<description></description>
			<version>0.5.0</version>
			<fixedVersion>0.6.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">1289</link>
		</links>
	</bug>
	<bug id="1257" opendate="2010-03-18 22:57:12" fixdate="2010-03-23 22:35:02" resolution="Fixed">
		<buginformation>
			<summary>joins between HBase tables and other tables (whether HBase or not) are broken</summary>
			<description>Details in
http://mail-archives.apache.org/mod_mbox/hadoop-hive-user/201003.mbox/%3C9A53DDE1FE082F4D952FDF20AC87E21F021F3EBC@exchange2.t8design.com%3E</description>
			<version>0.6.0</version>
			<fixedVersion>0.6.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">705</link>
		</links>
	</bug>
	<bug id="1273" opendate="2010-03-23 23:09:44" fixdate="2010-03-24 01:01:15" resolution="Fixed">
		<buginformation>
			<summary>UDF_Percentile NullPointerException</summary>
			<description></description>
			<version>0.6.0</version>
			<fixedVersion>0.6.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDAFPercentile.java</file>
		</fixedFiles>
	</bug>
	<bug id="1275" opendate="2010-03-24 17:55:07" fixdate="2010-03-25 22:37:23" resolution="Fixed">
		<buginformation>
			<summary>TestHBaseCliDriver hangs</summary>
			<description>TestHBaseCliDriver hangs after running hbase_joins.q
This can be reproduced by running


ant test -Dtestcase=TestHBaseCliDriver

</description>
			<version>0.6.0</version>
			<fixedVersion>0.6.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.hbase.HBaseQTestUtil.java</file>
			<file type="M">org.apache.hadoop.hive.ql.QTestUtil.java</file>
		</fixedFiles>
	</bug>
	<bug id="1281" opendate="2010-03-24 23:53:04" fixdate="2010-03-29 23:43:47" resolution="Fixed">
		<buginformation>
			<summary>Bucketing column names in create table should be case-insensitive</summary>
			<description>This create table fails because &amp;amp;apos;userId&amp;amp;apos; != &amp;amp;apos;userid&amp;amp;apos;


CREATE TABLE tmp_pyang_bucket3 (userId INT) CLUSTERED BY (userid) INTO 32 BUCKETS;

</description>
			<version>0.5.0</version>
			<fixedVersion>0.6.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
	</bug>
	<bug id="1291" opendate="2010-04-05 20:49:03" fixdate="2010-04-06 06:11:13" resolution="Fixed">
		<buginformation>
			<summary>Fix UDAFPercentile ndexOutOfBoundsException</summary>
			<description>The counts array can be empty. We should directly return null in that case.


org.apache.hadoop.hive.ql.metadata.HiveException: Unable to execute method public org.apache.hadoop.hive.serde2.io.DoubleWritable org.apache.hadoop.hive.ql.udf.UDAFPercentile$PercentileLongEvaluator.terminate()  on object org.apache.hadoop.hive.ql.udf.UDAFPercentile$PercentileLongEvaluator@530d0eae of class org.apache.hadoop.hive.ql.udf.UDAFPercentile$PercentileLongEvaluator with arguments {} of size 0
	at org.apache.hadoop.hive.ql.exec.FunctionRegistry.invoke(FunctionRegistry.java:725)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBridge$GenericUDAFBridgeEvaluator.terminate(GenericUDAFBridge.java:181)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.evaluate(GenericUDAFEvaluator.java:157)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.forward(GroupByOperator.java:838)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.closeOp(GroupByOperator.java:885)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:539)
	at org.apache.hadoop.hive.ql.exec.ExecReducer.close(ExecReducer.java:300)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:474)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:412)
	at org.apache.hadoop.mapred.Child.main(Child.java:159)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hive.ql.exec.FunctionRegistry.invoke(FunctionRegistry.java:701)
	... 9 more
Caused by: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0
	at java.util.ArrayList.RangeCheck(ArrayList.java:547)
	at java.util.ArrayList.get(ArrayList.java:322)
	at org.apache.hadoop.hive.ql.udf.UDAFPercentile.getPercentile(UDAFPercentile.java:97)
	at org.apache.hadoop.hive.ql.udf.UDAFPercentile.access$300(UDAFPercentile.java:44)
	at org.apache.hadoop.hive.ql.udf.UDAFPercentile$PercentileLongEvaluator.terminate(UDAFPercentile.java:196)
	... 14 more

</description>
			<version>0.6.0</version>
			<fixedVersion>0.6.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDAFPercentile.java</file>
		</fixedFiles>
	</bug>
	<bug id="1308" opendate="2010-04-14 19:08:27" fixdate="2010-04-16 01:42:26" resolution="Fixed">
		<buginformation>
			<summary>&lt;boolean&gt; = &lt;boolean&gt; throws NPE</summary>
			<description>Workaround is to just use &amp;lt;boolean&amp;gt; or NOT &amp;lt;boolean&amp;gt;


hive&amp;gt; select true=true from src;
FAILED: Hive Internal Error: java.lang.NullPointerException(null)
java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper.&amp;lt;init&amp;gt;(GenericUDFUtils.java:212)
        at org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.initialize(GenericUDFBridge.java:138)
        at org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.newInstance(ExprNodeGenericFuncDesc.java:153)
        at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.getXpathOrFuncExprNodeDesc(TypeCheckProcFactory.java:587)
        at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.process(TypeCheckProcFactory.java:708)
        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:89)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:88)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:128)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:102)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genExprNodeDesc(SemanticAnalyzer.java:6136)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genSelectPlan(SemanticAnalyzer.java:1831)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genSelectPlan(SemanticAnalyzer.java:1663)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:4911)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:5421)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:5952)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:126)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:304)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:377)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:138)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:197)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:303)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)

</description>
			<version>0.5.0</version>
			<fixedVersion>0.6.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.ComparisonOpMethodResolver.java</file>
		</fixedFiles>
	</bug>
	<bug id="1312" opendate="2010-04-15 07:55:04" fixdate="2010-04-16 03:52:11" resolution="Fixed">
		<buginformation>
			<summary>hive trunk does not compile with hadoop 0.17 any more</summary>
			<description>This is caused by HIVE-1295.


compile:
     [echo] Compiling: hive
    [javac] Compiling 527 source files to /hadoop_hive_trunk/.ptest_0/build/ql/classes
    [javac] /hadoop_hive_trunk/.ptest_0/ql/src/java/org/apache/hadoop/hive/ql/io/HiveNullValueSequenceFileOu\
tputFormat.java:69: cannot find symbol
    [javac] symbol  : method getBytes()
    [javac] location: class org.apache.hadoop.io.BytesWritable
    [javac]           keyWritable.set(bw.getBytes(), 0, bw.getLength());
    [javac]                             ^
    [javac] /hadoop_hive_trunk/.ptest_0/ql/src/java/org/apache/hadoop/hive/ql/io/HiveNullValueSequenceFileOu\
tputFormat.java:69: cannot find symbol
    [javac] symbol  : method getLength()
    [javac] location: class org.apache.hadoop.io.BytesWritable
    [javac]           keyWritable.set(bw.getBytes(), 0, bw.getLength());
    [javac]                                               ^
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] Note: Some input files use unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.
    [javac] 2 errors

</description>
			<version>0.6.0</version>
			<fixedVersion>0.6.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.HiveNullValueSequenceFileOutputFormat.java</file>
		</fixedFiles>
	</bug>
	<bug id="1315" opendate="2010-04-20 05:19:05" fixdate="2010-04-20 19:29:07" resolution="Fixed">
		<buginformation>
			<summary>bucketed sort merge join breaks after dynamic partition insert</summary>
			<description>bucketed sort merge join produces wrong bucket number due to HIVE-1002 patch, which breaks HIVE-1290.</description>
			<version>0.6.0</version>
			<fixedVersion>0.6.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
		</fixedFiles>
	</bug>
	<bug id="1320" opendate="2010-04-22 22:50:52" fixdate="2010-04-23 01:08:53" resolution="Fixed">
		<buginformation>
			<summary>NPE with lineage in a query of union alls on joins.</summary>
			<description>The following query generates a NPE in the lineage ctx code
EXPLAIN
INSERT OVERWRITE TABLE dest_l1
SELECT j.*
FROM (SELECT t1.key, p1.value
      FROM src1 t1
      LEFT OUTER JOIN src p1
      ON (t1.key = p1.key)
      UNION ALL
      SELECT t2.key, p2.value
      FROM src1 t2
      LEFT OUTER JOIN src p2
      ON (t2.key = p2.key)) j;
The stack trace is:
FAILED: Hive Internal Error: java.lang.NullPointerException(null)
java.lang.NullPointerException
at org.apache.hadoop.hive.ql.optimizer.lineage.LineageCtx$Index.mergeDependency(LineageCtx.java:116)
at org.apache.hadoop.hive.ql.optimizer.lineage.OpProcFactory$UnionLineage.process(OpProcFactory.java:396)
at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:89)
at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:88)
at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:54)
at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:59)
at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:59)
at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:59)
at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:59)
at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:59)
at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:102)
at org.apache.hadoop.hive.ql.optimizer.lineage.Generator.transform(Generator.java:72)
at org.apache.hadoop.hive.ql.optimizer.Optimizer.optimize(Optimizer.java:83)
at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:5976)
at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:126)
at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:48)
at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:126)</description>
			<version>0.6.0</version>
			<fixedVersion>0.6.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.lineage.OpProcFactory.java</file>
		</fixedFiles>
	</bug>
	<bug id="1321" opendate="2010-04-22 23:14:26" fixdate="2010-04-26 23:13:02" resolution="Fixed">
		<buginformation>
			<summary>bugs with temp directories, trailing blank fields in HBase bulk load</summary>
			<description>HIVE-1295 had two bugs discovered during testing with production data:
(1) extra directories may be present in the output directory depending on how the cluster is configured; we need to walk down these to find the column family directory
(2) if a record ends with fields which are blank strings, the text format omits the corresponding Control-A delimiters, so we need to fill in blanks for these fields (instead of throwing ArrayIndexOutOfBoundsException)</description>
			<version>0.6.0</version>
			<fixedVersion>0.6.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.hbase.HiveHFileOutputFormat.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">1295</link>
		</links>
	</bug>
	<bug id="1330" opendate="2010-04-29 04:35:06" fixdate="2010-04-29 17:22:27" resolution="Fixed">
		<buginformation>
			<summary>fatal error check omitted for reducer-side operators</summary>
			<description></description>
			<version>0.6.0</version>
			<fixedVersion>0.6.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
		</fixedFiles>
	</bug>
	<bug id="1317" opendate="2010-04-22 01:02:47" fixdate="2010-05-06 19:07:41" resolution="Fixed">
		<buginformation>
			<summary>CombineHiveInputFormat throws exception when partition name contains special characters to URI</summary>
			<description>If a partition name contains characters such as &amp;amp;apos;:&amp;amp;apos; and &amp;amp;apos;|&amp;amp;apos; which have special meaning in URI (hdfs uses URI internally for Path), CombineHiveInputFormat throws an exception. URI was created in CombineHiveInputFormat to compare a path belongs to a partition in partitionToPathInfo. We should bypass URI creation by just string comparisons. </description>
			<version>0.6.0</version>
			<fixedVersion>0.6.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
		</fixedFiles>
	</bug>
	<bug id="1341" opendate="2010-05-11 18:00:07" fixdate="2010-05-12 19:49:28" resolution="Fixed">
		<buginformation>
			<summary>Filter Operator Column Pruning should preserve the column order</summary>
			<description>The column pruning process for the filter operator should preserve the order of input columns, otherwise it could result in miss match in columns in the down stream operators. </description>
			<version>0.6.0</version>
			<fixedVersion>0.6.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryFactory.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryStruct.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">1340</link>
		</links>
	</bug>
	<bug id="1116" opendate="2010-01-28 16:49:49" fixdate="2010-05-12 23:01:46" resolution="Fixed">
		<buginformation>
			<summary>bug with alter table rename when table has property EXTERNAL=FALSE</summary>
			<description>if the location is not an external location - this would be safer.
the problem right now is that it&amp;amp;apos;s tricky to use the drop and rename way of writing new data into a table. consider:
Initialization block:
drop table a_tmp
create table a_tmp like a;
Loading block:
load data &amp;lt;newdata&amp;gt; into a_tmp;
drop table a;
alter table a_tmp rename to a;
this looks safe. but it&amp;amp;apos;s not. if one runs this multiple times - then data is lost (since &amp;amp;apos;a&amp;amp;apos; is pointing to &amp;amp;apos;a_tmp&amp;amp;apos;&amp;amp;apos;s location after any iteration. and dropping table &amp;amp;apos;a&amp;amp;apos; blows away loaded data in the next iteration). 
if the location is being managed by Hive - then &amp;amp;apos;rename&amp;amp;apos; should switch location as well.
</description>
			<version>0.5.0</version>
			<fixedVersion>0.6.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">592</link>
		</links>
	</bug>
	<bug id="1345" opendate="2010-05-14 23:53:33" fixdate="2010-05-18 05:49:15" resolution="Fixed">
		<buginformation>
			<summary>TypedBytesSerDe fails to create table with multiple columns.</summary>
			<description>Creating a table with more than one columns fails when the row format SerDe is TypedBytesSerDe. 


hive&amp;gt; CREATE TABLE test (a STRING, b STRING) ROW FORMAT SERDE &amp;amp;apos;org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe&amp;amp;apos;;      
Found class for org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe                                                       
FAILED: Error in metadata: java.lang.IndexOutOfBoundsException: Index: 1, Size: 1                                           
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask                                          
hive&amp;gt; 


</description>
			<version>0.5.0</version>
			<fixedVersion>0.6.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe.java</file>
		</fixedFiles>
	</bug>
	<bug id="1350" opendate="2010-05-18 22:26:24" fixdate="2010-05-24 20:14:08" resolution="Fixed">
		<buginformation>
			<summary>hive.query.id is not unique </summary>
			<description>if commands are executed by the same user within a second</description>
			<version>0.5.0</version>
			<fixedVersion>0.6.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.QueryPlan.java</file>
		</fixedFiles>
	</bug>
	<bug id="1377" opendate="2010-05-28 21:55:52" fixdate="2010-06-02 04:24:13" resolution="Fixed">
		<buginformation>
			<summary>getPartitionDescFromPath() in CombineHiveInputFormat should handle matching by path</summary>
			<description>The use case is:


dir = hdfs://host:9000/user/warehouse/tableName/abc
pathToPartitionInfo = {/user/warehouse/tableName : myPart}


Then calling 

 
getPartitionDescFromPath(dir, pathToPartitionInfo)


will throw an IOException because /user/warehouse/tableName is not a prefix of hdfs://host:9000/user/warehouse/tableName/abc. Currently, this is not an issue but will come up if CombineFileInputFormat is modified so what the scheme and authority are not stripped out  when generating splits (see MAPREDUCE-1806).
The proposed solution is add a case where matching is done by just the path component of the URI&amp;amp;apos;s.</description>
			<version>0.6.0</version>
			<fixedVersion>0.6.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.java</file>
		</fixedFiles>
	</bug>
	<bug id="1368" opendate="2010-05-25 11:18:20" fixdate="2010-06-09 00:09:44" resolution="Duplicate">
		<buginformation>
			<summary>Hive JDBC Integration with SQuirrel SQL Client support Enhanced</summary>
			<description>Hive JDBC Integration with SQuirrel SQL Client support Enhanced:-
Hive JDBC Client enhanced to browse hive default schema tables through Squirrel SQL Client.
This enhancement help to browse the hive table&amp;amp;apos;s structure i.e. table&amp;amp;apos;s column and their data type in the Squirrel SQL client interface and SQL query can be also performed on the tables through Squirrel SQL client.
To enable this following Hive JDBC Java files are modified and added:-
1.	Methods of org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.java are updated.
2.	Hive org.apache.hadoop.hive.jdbc.ResultSet.java updated and extended (org.apache.hadoop.hive.jdbc.ExtendedHiveResultSet.java) to support additional JDBC metadata 
3.	Methods of org.apache.hadoop.hive.jdbc. HiveResultSetMetaData are updated.
4.	Methods of  org.apache.hadoop.hive.jdbc. HiveConnection are updated.</description>
			<version>0.5.0</version>
			<fixedVersion>0.5.0</fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="D">org.apache.hadoop.hive.jdbc.HiveResultSet.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveCallableStatement.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HivePreparedStatement.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveConnection.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveStatement.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">1126</link>
			<link type="Reference" description="is related to">1126</link>
		</links>
	</bug>
	<bug id="1187" opendate="2010-02-22 16:51:16" fixdate="2010-06-10 16:49:14" resolution="Duplicate">
		<buginformation>
			<summary>Implement ddldump utility for Hive Metastore</summary>
			<description>Implement a ddldump utility for the Hive metastore that will generate the QL DDL necessary to recreate the state of the current metastore on another metastore instance.
A major use case for this utility is migrating a metastore from one database to another database, e.g. from an embedded Derby instanced to a MySQL instance.
The ddldump utility should support the following features:

Ability to generate DDL for specific tables or all tables.
Ability to specify a table name prefix for the generated DDL, which will be useful for resolving table name conflicts.

</description>
			<version>0.6.0</version>
			<fixedVersion></fixedVersion>
			<type>New Feature</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
			<file type="M">org.apache.hadoop.hive.ql.ErrorMsg.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">967</link>
		</links>
	</bug>
	<bug id="88" opendate="2008-12-01 06:53:47" fixdate="2010-06-11 07:05:52" resolution="Duplicate">
		<buginformation>
			<summary>hadoop doesn&amp;apos;t use conf/hive-log4j.properties</summary>
			<description>hadoop-0.20.0-dev-core.jar contains log4j.properties file, and I think that&amp;amp;apos;s the one hadoop is picking up. I modified both conf/hive-log4j.properties and hadoopcore/conf/log4j.properties, but hadoop still printed INFO messages to stderr.
Pasting relevant posts from the mailing list below:
Michi Mutsuzaki &amp;lt;michi@cs.stanford.edu&amp;gt; 	Fri, Nov 28, 2008 at 7:14 PM
To: hive-users@publists.facebook.com
Hello,
When I do "ant test" under ql directory, I get many log messages to stderr.
 [junit] 08/11/28 19:04:14 INFO exec.MapOperator: Got partitions: null
[junit] 08/11/28 19:04:14 INFO exec.ReduceSinkOperator: Initializing Self
[junit] 08/11/28 19:04:14 INFO exec.ReduceSinkOperator: Using tag = -1
[junit] 08/11/28 19:04:14 INFO thrift.TBinarySortableProtocol:
Sort order is ""
[junit] 08/11/28 19:04:14 INFO thrift.TBinarySortableProtocol:
Sort order is ""
   ....
I tried setting log level to ERROR in conf/hive-log4j.properties, but these info lines still show up. How can I get rid of them?
Thanks!
--Michi
Joydeep Sen Sarma &amp;lt;jssarma@facebook.com&amp;gt; 	Fri, Nov 28, 2008 at 10:49 PM
To: "michi@cs.stanford.edu" &amp;lt;michi@cs.stanford.edu&amp;gt;, "hive-users@publists.facebook.com" &amp;lt;hive-users@publists.facebook.com&amp;gt;
When we run the tests - we run in hadoop &amp;amp;apos;local&amp;amp;apos; mode - and in this mode, we run map-reduce jobs by invoking &amp;amp;apos;hadoop jar ... ExecDriver&amp;amp;apos; cmd line. this was done because we had some issues submitting map-reduce jobs directly (from same jvm) in local mode that we could not resolve.
The issue is that when we invoke &amp;amp;apos;hadoop jar ... ExecDriver&amp;amp;apos; - we don&amp;amp;apos;t control log4j via hive-log4j. one thing u can try is changing the hadoop&amp;amp;apos;s log4j.properties that hive is picking up (probably hadoopcore/conf/log4j.properties).
Revisiting this after a long time - I think this can be fixed with some changes to MapRedTask.java (need to add hive-log4j.properties to hadoop classpath here and then reset log4j using this in execdriver). Feel free to file a jira if this is too irritating ..</description>
			<version>0.5.0</version>
			<fixedVersion></fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.common.FileUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapRedTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.session.SessionState.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Context.java</file>
			<file type="M">org.apache.hadoop.hive.cli.CliDriver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">543</link>
			<link type="Duplicate" description="duplicates">517</link>
			<link type="Duplicate" description="is duplicated by">543</link>
		</links>
	</bug>
	<bug id="1409" opendate="2010-06-15 03:26:12" fixdate="2010-06-15 22:04:53" resolution="Fixed">
		<buginformation>
			<summary>File format information is retrieved from first partition</summary>
			<description>Currently, if no partitions match the partition predicate, the first partition is used to retrieve the file format. This can cause an problem if the table is set to use RCFile, but the first partition uses SequenceFile:


java.lang.RuntimeException: java.lang.NoSuchMethodException: org.apache.hadoop.hive.ql.io.RCFile$KeyBuffer.()
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:115)
	at org.apache.hadoop.mapred.SequenceFileRecordReader.createKey(SequenceFileRecordReader.java:65)
	at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.createKey(CombineHiveRecordReader.java:76)
	at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.createKey(CombineHiveRecordReader.java:42)
	at org.apache.hadoop.hive.shims.Hadoop20Shims$CombineFileRecordReader.createKey(Hadoop20Shims.java:212)
	at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.createKey(MapTask.java:167)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:45)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:358)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:307)
	at org.apache.hadoop.mapred.Child.main(Child.java:159)
Caused by: java.lang.NoSuchMethodException: org.apache.hadoop.hive.ql.io.RCFile$KeyBuffer.()
	at java.lang.Class.getConstructor0(Class.java:2706)
	at java.lang.Class.getDeclaredConstructor(Class.java:1985)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:109)
	... 9 more



The proposed change is to use the table&amp;amp;apos;s metadata in such cases.</description>
			<version>0.6.0</version>
			<fixedVersion>0.6.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
		</fixedFiles>
	</bug>
	<bug id="802" opendate="2009-08-26 22:51:15" fixdate="2010-06-16 02:27:41" resolution="Duplicate">
		<buginformation>
			<summary>Bug in DataNucleus prevents Hive from building if inside a dir with &amp;apos;+&amp;apos; in it</summary>
			<description>There&amp;amp;apos;s a bug in DataNucleus that causes this issue:
http://www.jpox.org/servlet/jira/browse/NUCCORE-371
To reproduce, simply put your hive source tree in a directory that contains a &amp;amp;apos;+&amp;amp;apos; character.</description>
			<version>0.5.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">1176</link>
			<link type="dependent" description="depends upon">1176</link>
		</links>
	</bug>
	<bug id="543" opendate="2009-06-04 22:59:42" fixdate="2010-06-17 18:39:32" resolution="Fixed">
		<buginformation>
			<summary>provide option to run hive in local mode</summary>
			<description>this is a little bit more than just mapred.job.tracker=local
when run in this mode - multiple jobs are an issue since writing to same tmp directories is an issue. the following options:
hadoop.tmp.dir
mapred.local.dir
need to be randomized (perhaps based on queryid). </description>
			<version>0.5.0</version>
			<fixedVersion>0.6.0</fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.common.FileUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapRedTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.session.SessionState.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Context.java</file>
			<file type="M">org.apache.hadoop.hive.cli.CliDriver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="is blocked by">88</link>
			<link type="Duplicate" description="duplicates">88</link>
		</links>
	</bug>
	<bug id="1418" opendate="2010-06-19 02:47:26" fixdate="2010-06-19 18:16:05" resolution="Fixed">
		<buginformation>
			<summary>column pruning not working with lateral view</summary>
			<description>select myCol from tmp_pyang_lv LATERAL VIEW explode(array(1,2,3)) myTab as myCol limit 3;</description>
			<version>0.5.0</version>
			<fixedVersion>0.6.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.java</file>
		</fixedFiles>
	</bug>
	<bug id="1412" opendate="2010-06-17 19:44:40" fixdate="2010-06-22 01:18:01" resolution="Fixed">
		<buginformation>
			<summary>CombineHiveInputFormat bug on tablesample</summary>
			<description>CombineHiveInputFormat should combine all files inside one partition to form a split but should not takes files cross partition boundary. This works for regular table and partitions since all input paths are directory. However this breaks when the input is files (in which case tablesample could be the use case). CombineHiveInputFormat should adjust to the case when input could also be non-directories. </description>
			<version>0.6.0</version>
			<fixedVersion>0.6.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.java</file>
		</fixedFiles>
	</bug>
	<bug id="1176" opendate="2010-02-17 18:15:00" fixdate="2010-06-24 05:23:29" resolution="Fixed">
		<buginformation>
			<summary>&amp;apos;create if not exists&amp;apos; fails for a table name with &amp;apos;select&amp;apos; in it</summary>
			<description>hive&amp;gt; create table if not exists tmp_select(s string, c string, n int);
org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Got exception: javax.jdo.JDOUserException JDOQL Single-String query should always start with SELECT)
        at org.apache.hadoop.hive.ql.metadata.Hive.getTablesForDb(Hive.java:441)
        at org.apache.hadoop.hive.ql.metadata.Hive.getTablesByPattern(Hive.java:423)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeCreateTable(SemanticAnalyzer.java:5538)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:5192)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:105)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:275)
        at org.apache.hadoop.hive.ql.Driver.runCommand(Driver.java:320)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:312)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:123)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:181)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:287)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
Caused by: MetaException(message:Got exception: javax.jdo.JDOUserException JDOQL Single-String query should always start with SELECT)
        at org.apache.hadoop.hive.metastore.MetaStoreUtils.logAndThrowMetaException(MetaStoreUtils.java:612)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getTables(HiveMetaStoreClient.java:450)
        at org.apache.hadoop.hive.ql.metadata.Hive.getTablesForDb(Hive.java:439)
        ... 15 more</description>
			<version>0.5.0</version>
			<fixedVersion>0.6.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">802</link>
			<link type="dependent" description="is depended upon by">802</link>
		</links>
	</bug>
	<bug id="1271" opendate="2010-03-23 16:27:04" fixdate="2010-06-25 23:30:40" resolution="Fixed">
		<buginformation>
			<summary>Case sensitiveness of type information specified when using custom reducer causes type mismatch</summary>
			<description>Type information specified  while using a custom reduce script is converted to lower case, and causes type mismatch during query semantic analysis .  The following REDUCE query where field name =  "userId" failed.
hive&amp;gt; CREATE TABLE SS (
   &amp;gt;                     a INT,
   &amp;gt;                     b INT,
   &amp;gt;                     vals ARRAY&amp;lt;STRUCT&amp;lt;userId:INT, y:STRING&amp;gt;&amp;gt;
   &amp;gt;                 );
OK
hive&amp;gt; FROM (select * from srcTable DISTRIBUTE BY id SORT BY id) s
   &amp;gt;     INSERT OVERWRITE TABLE SS
   &amp;gt;     REDUCE *
   &amp;gt;         USING &amp;amp;apos;myreduce.py&amp;amp;apos;
   &amp;gt;         AS
   &amp;gt;                     (a INT,
   &amp;gt;                     b INT,
   &amp;gt;                     vals ARRAY&amp;lt;STRUCT&amp;lt;userId:INT, y:STRING&amp;gt;&amp;gt;
   &amp;gt;                     )
   &amp;gt;         ;
FAILED: Error in semantic analysis: line 2:27 Cannot insert into
target table because column number/types are different SS: Cannot
convert column 2 from array&amp;lt;struct&amp;lt;userId:int,y:string&amp;gt;&amp;gt; to
array&amp;lt;struct&amp;lt;userid:int,y:string&amp;gt;&amp;gt;.
The same query worked fine after changing "userId" to "userid".</description>
			<version>0.5.0</version>
			<fixedVersion>0.6.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.typeinfo.TypeInfo.java</file>
		</fixedFiles>
	</bug>
	<bug id="1440" opendate="2010-06-26 05:57:32" fixdate="2010-06-28 21:38:25" resolution="Fixed">
		<buginformation>
			<summary>FetchOperator(mapjoin) does not work with RCFile</summary>
			<description>RCFile needs column prunning&amp;amp;apos;s results. But when initializing the mapjoin&amp;amp;apos;s fetch operator, the cp&amp;amp;apos;s result is not passed to record reader.</description>
			<version>0.7.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecMapper.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
		</fixedFiles>
	</bug>
	<bug id="287" opendate="2009-02-12 19:31:20" fixdate="2010-07-12 01:24:40" resolution="Fixed">
		<buginformation>
			<summary>support count(*) and count distinct on multiple columns</summary>
			<description>The following query does not work:
select count(distinct col1, col2) from Tbl</description>
			<version>0.6.0</version>
			<fixedVersion>0.6.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFHistogramNumeric.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCount.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFVariance.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBridge.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFResolver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">1459</link>
		</links>
	</bug>
	<bug id="1385" opendate="2010-06-03 01:58:32" fixdate="2010-07-15 21:17:14" resolution="Fixed">
		<buginformation>
			<summary>UDF field() doesn&amp;apos;t work</summary>
			<description>I tried it against one of my table:
hive&amp;gt; desc r;
OK
key int
value string
a string
hive&amp;gt; select * from r;
OK
4 val_356 NULL
4 val_356 NULL
484 val_169 NULL
484 val_169 NULL
2000 val_169 NULL
2000 val_169 NULL
3000 val_169 NULL
3000 val_169 NULL
4000 val_125 NULL
4000 val_125 NULL
hive&amp;gt; select *, field(value, &amp;amp;apos;val_169&amp;amp;apos;) from r; 
OK
4 val_356 NULL 0
4 val_356 NULL 0
484 val_169 NULL 0
484 val_169 NULL 0
2000 val_169 NULL 0
2000 val_169 NULL 0
3000 val_169 NULL 0
3000 val_169 NULL 0
4000 val_125 NULL 0
4000 val_125 NULL 0</description>
			<version>0.5.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFField.java</file>
		</fixedFiles>
	</bug>
	<bug id="1470" opendate="2010-07-16 22:37:11" fixdate="2010-07-27 00:17:27" resolution="Fixed">
		<buginformation>
			<summary>percentile_approx() fails with more than 1 reducer</summary>
			<description>The larger issue is that a UDAF that has variable return types needs two inner Evaluator classes. This patch fixes a NullPointerException bug that is only encountered when partial aggregations are invoked.</description>
			<version>0.6.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentileApprox.java</file>
		</fixedFiles>
	</bug>
	<bug id="1056" opendate="2010-01-16 01:32:08" fixdate="2010-07-27 03:09:34" resolution="Fixed">
		<buginformation>
			<summary>Predicate push down does not work with UDTF&amp;apos;s</summary>
			<description>Predicate push down does not work with UDTF&amp;amp;apos;s in lateral views



hive&amp;gt; SELECT * FROM src LATERAL VIEW explode(array(1,2,3)) myTable AS k WHERE k=1;
FAILED: Unknown exception: null
hive&amp;gt;


</description>
			<version>0.5.0</version>
			<fixedVersion>0.6.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.LateralViewJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.ppd.PredicatePushDown.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.api.OperatorType.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.OperatorFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.UDTFOperator.java</file>
		</fixedFiles>
	</bug>
	<bug id="1489" opendate="2010-07-27 00:48:54" fixdate="2010-07-27 07:20:47" resolution="Fixed">
		<buginformation>
			<summary>TestCliDriver -Doverwrite=true does not put the file in the correct directory</summary>
			<description>When adding a new file in clientpositive with -Doverwrite=true, the output file was in ql/ rather than ql/src/test/results/clientpositive. </description>
			<version>0.7.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.QTestUtil.java</file>
		</fixedFiles>
	</bug>
	<bug id="1471" opendate="2010-07-17 01:05:06" fixdate="2010-07-29 18:55:06" resolution="Fixed">
		<buginformation>
			<summary>CTAS should unescape the column name in the select-clause. </summary>
			<description>The following query 
{{
{
create table T as select `to` from S;
}
}}
failed since `to` should be unescaped before creating the table. </description>
			<version>0.6.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
	</bug>
	<bug id="1126" opendate="2010-02-03 10:51:39" fixdate="2010-07-30 00:12:11" resolution="Fixed">
		<buginformation>
			<summary>Missing some Jdbc functionality like getTables getColumns and HiveResultSet.get* methods based on column name.</summary>
			<description>I&amp;amp;apos;ve been using the hive jdbc driver more and more and was missing some functionality which I added
HiveDatabaseMetaData.getTables
Using "show tables" to get the info from hive.
HiveDatabaseMetaData.getColumns
Using "describe tablename" to get the columns.
This makes using something like SQuirreL a lot nicer since you have the list of tables and just click on the content tab to see what&amp;amp;apos;s in the table.
I also implemented
HiveResultSet.getObject(String columnName) so you call most get* methods based on the column name.</description>
			<version>0.5.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="D">org.apache.hadoop.hive.jdbc.HiveResultSet.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveCallableStatement.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HivePreparedStatement.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveConnection.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveStatement.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">1368</link>
			<link type="Incorporates" description="is part of">576</link>
			<link type="Reference" description="relates to">1368</link>
			<link type="Reference" description="is related to">1378</link>
		</links>
	</bug>
	<bug id="417" opendate="2009-04-15 15:51:02" fixdate="2010-07-30 06:42:14" resolution="Fixed">
		<buginformation>
			<summary>Implement Indexing in Hive</summary>
			<description>Implement indexing on Hive so that lookup and range queries are efficient.</description>
			<version>0.3.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>New Feature</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecMapperContext.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
			<file type="M">org.apache.hadoop.hive.ql.history.HiveHistory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.PartitionDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.lineage.OpProcFactory.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.Warehouse.java</file>
			<file type="M">org.apache.hadoop.hive.common.FileUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ppr.ExprProcFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TaskFactory.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.HiveUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.Schema.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.hooks.PostExecutePrinter.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.Index.java</file>
			<file type="M">org.apache.hadoop.hive.ql.DriverContext.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.RCFile.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecMapper.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.HiveRecordReader.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.BucketizedHiveRecordReader.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ColumnInfo.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.TableType.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.TableScanDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ReduceSinkDeDuplication.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.RawStore.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">837</link>
			<link type="Reference" description="relates to">1803</link>
			<link type="Reference" description="relates to">1904</link>
			<link type="Reference" description="is related to">1501</link>
			<link type="Reference" description="is related to">22</link>
			<link type="Reference" description="is related to">1694</link>
			<link type="Reference" description="is related to">1889</link>
			<link type="Reference" description="is related to">1496</link>
			<link type="Reference" description="is related to">1499</link>
			<link type="Reference" description="is related to">1500</link>
			<link type="Reference" description="is related to">1502</link>
			<link type="Reference" description="is related to">1503</link>
			<link type="Reference" description="is related to">1495</link>
			<link type="Reference" description="is related to">1497</link>
			<link type="Reference" description="is related to">1498</link>
		</links>
	</bug>
	<bug id="1494" opendate="2010-07-30 23:34:24" fixdate="2010-07-31 02:34:37" resolution="Fixed">
		<buginformation>
			<summary>Index followup: remove sort by clause and fix a bug in collect_set udaf</summary>
			<description></description>
			<version>0.7.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCollectSet.java</file>
		</fixedFiles>
	</bug>
	<bug id="1422" opendate="2010-06-22 01:33:52" fixdate="2010-08-02 23:43:02" resolution="Fixed">
		<buginformation>
			<summary>skip counter update when RunningJob.getCounters() returns null</summary>
			<description>Under heavy load circumstances on some Hadoop versions, we may get a NPE from trying to dereference a null Counters object.  I don&amp;amp;apos;t have a unit test which can reproduce it, but here&amp;amp;apos;s an example stack from a production cluster we saw today:
10/06/21 13:01:10 ERROR exec.ExecDriver: Ended Job = job_201005200457_701060 with exception &amp;amp;apos;java.lang.NullPointerException(null)&amp;amp;apos;
java.lang.NullPointerException
at org.apache.hadoop.hive.ql.exec.Operator.updateCounters(Operator.java:999)
at org.apache.hadoop.hive.ql.exec.ExecDriver.updateCounters(ExecDriver.java:503)
at org.apache.hadoop.hive.ql.exec.ExecDriver.progress(ExecDriver.java:390)
at org.apache.hadoop.hive.ql.exec.ExecDriver.execute(ExecDriver.java:697)
at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:107)
at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:55)
at org.apache.hadoop.hive.ql.exec.TaskRunner.run(TaskRunner.java:47)</description>
			<version>0.6.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapRedTask.java</file>
		</fixedFiles>
		<links>
			<link type="Incorporates" description="incorporates">1493</link>
		</links>
	</bug>
	<bug id="1509" opendate="2010-08-04 00:07:51" fixdate="2010-08-05 07:38:58" resolution="Fixed">
		<buginformation>
			<summary>Monitor the working set of the number of files </summary>
			<description></description>
			<version>0.6.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Operator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
	</bug>
	<bug id="1460" opendate="2010-07-12 21:13:50" fixdate="2010-08-12 19:02:54" resolution="Duplicate">
		<buginformation>
			<summary>JOIN should not output rows for NULL values</summary>
			<description>We should filter out rows with NULL keys from the result of this query


SELECT * FROM a JOIN b on a.key = b.key

</description>
			<version>0.6.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.SerDeUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">741</link>
		</links>
	</bug>
	<bug id="1428" opendate="2010-06-23 01:02:22" fixdate="2010-08-14 01:06:59" resolution="Fixed">
		<buginformation>
			<summary>ALTER TABLE ADD PARTITION fails with a remote Thrift metastore</summary>
			<description>If the hive cli is configured to use a remote metastore, ALTER TABLE ... ADD PARTITION commands will fail with an error similar to the following:
[pradeepk@chargesize:~/dev/howl]hive --auxpath ult-serde.jar -e "ALTER TABLE mytable add partition(datestamp = &amp;amp;apos;20091101&amp;amp;apos;, srcid = &amp;amp;apos;10&amp;amp;apos;,action) location &amp;amp;apos;/user/pradeepk/mytable/20091101/10&amp;amp;apos;;"
10/06/16 17:08:59 WARN conf.Configuration: DEPRECATED: hadoop-site.xml found in the classpath. Usage of hadoop-site.xml is deprecated. Instead use core-site.xml, mapred-site.xml and hdfs-site.xml to override properties of core-default.xml, mapred-default.xml and hdfs-default.xml respectively
Hive history file=/tmp/pradeepk/hive_job_log_pradeepk_201006161709_1934304805.txt
FAILED: Error in metadata: org.apache.thrift.TApplicationException: get_partition failed: unknown result
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
[pradeepk@chargesize:~/dev/howl]
This is due to a check that tries to retrieve the partition to see if it exists. If it does not, an attempt is made to pass a null value from the metastore. Since thrift does not support null return values, an exception is thrown.</description>
			<version>0.5.0</version>
			<fixedVersion>0.6.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.RawStore.java</file>
		</fixedFiles>
	</bug>
	<bug id="1547" opendate="2010-08-17 02:09:54" fixdate="2010-08-17 20:58:16" resolution="Fixed">
		<buginformation>
			<summary>Unarchiving operation throws NPE</summary>
			<description>Unarchiving a partition throws a null pointer exception similar to the following:
2010-08-16 12:44:18,801 ERROR exec.DDLTask (SessionState.java:printError(277)) - Failed with exception null
java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.exec.DDLTask.unarchive(DDLTask.java:729)
        at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:195)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:108)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:55)
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:609)
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:478)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:356)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:140)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:199)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:351)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
This error seems to be DFS specific, as local file system in the unit tests don&amp;amp;apos;t catch this.</description>
			<version>0.7.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
		</fixedFiles>
	</bug>
	<bug id="741" opendate="2009-08-08 03:58:45" fixdate="2010-08-24 22:21:00" resolution="Fixed">
		<buginformation>
			<summary>NULL is not handled correctly in join</summary>
			<description>With the following data in table input4_cb:
Key        Value
------       --------
NULL     325
18          NULL
The following query:


select * from input4_cb a join input4_cb b on a.key = b.value;


returns the following result:
NULL    325    18   NULL
The correct result should be empty set.
When &amp;amp;apos;null&amp;amp;apos; is replaced by &amp;amp;apos;&amp;amp;apos; it works.</description>
			<version>0.6.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.SerDeUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
		</fixedFiles>
		<links>
			<link type="Cloners" description="is a clone of">734</link>
			<link type="Duplicate" description="is duplicated by">1460</link>
			<link type="Incorporates" description="incorporates">1552</link>
			<link type="Reference" description="relates to">1552</link>
			<link type="Reference" description="relates to">2810</link>
			<link type="Reference" description="relates to">1544</link>
		</links>
	</bug>
	<bug id="1600" opendate="2010-08-26 21:53:35" fixdate="2010-08-29 06:54:46" resolution="Fixed">
		<buginformation>
			<summary>need to sort hook input/output lists for test result determinism</summary>
			<description>Begin forwarded message:
From: Ning Zhang &amp;lt;nzhang@facebook.com&amp;gt;
Date: August 26, 2010 2:47:26 PM PDT
To: John Sichi &amp;lt;jsichi@facebook.com&amp;gt;
Cc: "hive-dev@hadoop.apache.org" &amp;lt;hive-dev@hadoop.apache.org&amp;gt;
Subject: Re: failure in load_dyn_part1.q
Yes I saw this error before but if it does not repro. So it&amp;amp;apos;s probably an ordering issue in POSTHOOK. 
On Aug 26, 2010, at 2:39 PM, John Sichi wrote:
I&amp;amp;apos;m seeing this failure due to a result diff when running tests on latest trunk:
POSTHOOK: Input: default@srcpart@ds=2008-04-08/hr=12
POSTHOOK: Input: default@srcpart@ds=2008-04-09/hr=11
POSTHOOK: Input: default@srcpart@ds=2008-04-09/hr=12
-POSTHOOK: Output: default@nzhang_part2@ds=2008-12-31/hr=11
-POSTHOOK: Output: default@nzhang_part2@ds=2008-12-31/hr=12
POSTHOOK: Output: default@nzhang_part1@ds=2008-04-08/hr=11
POSTHOOK: Output: default@nzhang_part1@ds=2008-04-08/hr=12
+POSTHOOK: Output: default@nzhang_part2@ds=2008-12-31/hr=11
+POSTHOOK: Output: default@nzhang_part2@ds=2008-12-31/hr=12
Did something change recently?  Or are we missing a Java-level sort on the input/output list for determinism?
JVS
</description>
			<version>0.6.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.hooks.PreExecutePrinter.java</file>
			<file type="M">org.apache.hadoop.hive.ql.hooks.PostExecutePrinter.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="is blocked by">1601</link>
		</links>
	</bug>
	<bug id="1614" opendate="2010-09-03 17:56:24" fixdate="2010-09-04 05:26:11" resolution="Fixed">
		<buginformation>
			<summary>UDTF json_tuple should return null row when input is not a valid JSON string</summary>
			<description>If the input column is not a valid JSON string, json_tuple will not return anything but this will prevent the downstream operators to access the left-hand side table. We should output a NULL row instead, similar to when the input column is a NULL value. </description>
			<version>0.7.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDTFJSONTuple.java</file>
		</fixedFiles>
	</bug>
	<bug id="1628" opendate="2010-09-10 18:35:55" fixdate="2010-09-20 21:41:07" resolution="Fixed">
		<buginformation>
			<summary>Fix Base64TextInputFormat to be compatible with commons codec 1.4</summary>
			<description>Commons-codec 1.4 made an incompatible change to the Base64 class that made line-wrapping default (boo!). This breaks the Base64TextInputFormat in contrib. This patch adds some simple reflection to use the new constructor that uses the old behavior.</description>
			<version>0.6.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.contrib.fileformat.base64.Base64TextOutputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.contrib.fileformat.base64.Base64TextInputFormat.java</file>
		</fixedFiles>
	</bug>
	<bug id="1378" opendate="2010-06-01 19:56:11" fixdate="2010-09-27 19:17:28" resolution="Fixed">
		<buginformation>
			<summary>Return value for map, array, and struct needs to return a string </summary>
			<description>In order to be able to select/display any data from JDBC Hive driver, return value for map, array, and struct needs to return a string</description>
			<version>2.0.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FetchTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.JdbcColumn.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveConnection.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveBaseResultSet.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveQueryResultSet.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">1859</link>
			<link type="Duplicate" description="is duplicated by">1606</link>
			<link type="Duplicate" description="is duplicated by">1860</link>
			<link type="Duplicate" description="is duplicated by">1861</link>
			<link type="Duplicate" description="is duplicated by">1863</link>
			<link type="Reference" description="relates to">1126</link>
		</links>
	</bug>
	<bug id="1676" opendate="2010-09-30 17:39:51" fixdate="2010-09-30 18:39:03" resolution="Duplicate">
		<buginformation>
			<summary>show table extended like does not work well with wildcards</summary>
			<description>As evident from the output below though there are tables that match the wildcard, the output from "show table extended like " does not contain the matches.

bin/hive -e "show tables &amp;amp;apos;foo*&amp;amp;apos;"
Hive history file=/tmp/pradeepk/hive_job_log_pradeepk_201009301037_568707409.txt
OK
foo
foo2
Time taken: 3.417 seconds

bin/hive -e "show table extended like &amp;amp;apos;foo*&amp;amp;apos;"
Hive history file=/tmp/pradeepk/hive_job_log_pradeepk_201009301037_410056681.txt
OK
Time taken: 2.948 seconds

</description>
			<version>0.5.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">1363</link>
		</links>
	</bug>
	<bug id="1673" opendate="2010-09-29 19:55:56" fixdate="2010-10-01 17:16:26" resolution="Fixed">
		<buginformation>
			<summary>Create table bug causes the row format property lost when serde is specified.</summary>
			<description>An example:
create table src_rc_serde_yongqiang(key string, value string) ROW FORMAT  DELIMITED FIELDS TERMINATED BY &amp;amp;apos;0&amp;amp;apos; stored as rcfile; 
will lost the row format information.</description>
			<version>0.7.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
		</fixedFiles>
	</bug>
	<bug id="1376" opendate="2010-05-28 21:10:01" fixdate="2010-10-07 17:02:18" resolution="Fixed">
		<buginformation>
			<summary>Simple UDAFs with more than 1 parameter crash on empty row query </summary>
			<description>Simple UDAFs with more than 1 parameter crash when the query returns no rows. Currently, this only seems to affect the percentile() UDAF where the second parameter is the percentile to be computed (of type double). I&amp;amp;apos;ve also verified the bug by adding a dummy parameter to ExampleMin in contrib. 
On an empty query, Hive seems to be trying to resolve an iterate() method with signature 
{null,null}
 instead of 
{null,double}
. You can reproduce this bug using:
CREATE TABLE pct_test ( val INT );
SELECT percentile(val, 0.5) FROM pct_test;
which produces a lot of errors like: 
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to execute method public boolean org.apache.hadoop.hive.ql.udf.UDAFPercentile$PercentileLongEvaluator.iterate(org.apache.hadoop.io.LongWritable,double)  on object org.apache.hadoop.hive.ql.udf.UDAFPercentile$PercentileLongEvaluator@11d13272 of class org.apache.hadoop.hive.ql.udf.UDAFPercentile$PercentileLongEvaluator with arguments 
{null, null}
 of size 2</description>
			<version>0.6.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDAFPercentile.java</file>
		</fixedFiles>
	</bug>
	<bug id="1658" opendate="2010-09-21 01:19:57" fixdate="2010-10-13 04:28:05" resolution="Fixed">
		<buginformation>
			<summary>Fix describe [extended] column formatting</summary>
			<description>When displaying the column schema, the formatting should follow should be 
name&amp;lt;TAB&amp;gt;type&amp;lt;TAB&amp;gt;comment&amp;lt;NEWLINE&amp;gt;
to be inline with the previous formatting style for backward compatibility.</description>
			<version>0.7.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.hwi.TestHWISessionManager.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.DescTableDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.MetaDataFormatUtils.java</file>
		</fixedFiles>
	</bug>
	<bug id="307" opendate="2009-02-25 21:46:57" fixdate="2010-10-15 04:25:12" resolution="Fixed">
		<buginformation>
			<summary>"LOAD DATA LOCAL INPATH" fails when the table already contains a file of the same name</summary>
			<description>Failed with exception checkPaths: /user/zshao/warehouse/tmp_user_msg_history/test_user_msg_history already exists
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask</description>
			<version>0.5.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">718</link>
		</links>
	</bug>
	<bug id="1681" opendate="2010-10-01 08:29:48" fixdate="2010-10-15 16:45:32" resolution="Fixed">
		<buginformation>
			<summary>ObjectStore.commitTransaction() does not properly handle transactions that have already been rolled back</summary>
			<description>Here&amp;amp;apos;s the code for ObjectStore.commitTransaction() and ObjectStore.rollbackTransaction():


  public boolean commitTransaction() {
    assert (openTrasactionCalls &amp;gt;= 1);
    if (!currentTransaction.isActive()) {
      throw new RuntimeException(
          "Commit is called, but transaction is not active. Either there are"
              + " mismatching open and close calls or rollback was called in the same trasaction");
    }
    openTrasactionCalls--;
    if ((openTrasactionCalls == 0) &amp;amp;&amp;amp; currentTransaction.isActive()) {
      transactionStatus = TXN_STATUS.COMMITED;
      currentTransaction.commit();
    }
    return true;
  }

  public void rollbackTransaction() {
    if (openTrasactionCalls &amp;lt; 1) {
      return;
    }
    openTrasactionCalls = 0;
    if (currentTransaction.isActive()
        &amp;amp;&amp;amp; transactionStatus != TXN_STATUS.ROLLBACK) {
      transactionStatus = TXN_STATUS.ROLLBACK;
      // could already be rolled back
      currentTransaction.rollback();
    }
  }



Now suppose a nested transaction throws an exception which results
in the nested pseudo-transaction calling rollbackTransaction(). This causes
rollbackTransaction() to rollback the actual transaction, as well as to set 
openTransactionCalls=0 and transactionStatus = TXN_STATUS.ROLLBACK.
Suppose also that this nested transaction squelches the original exception.
In this case the stack will unwind and the caller will eventually try to commit the
transaction by calling commitTransaction() which will see that currentTransaction.isActive() returns
FALSE and will throw a RuntimeException. The fix for this problem is
that commitTransaction() needs to first check transactionStatus and return immediately
if transactionStatus==TXN_STATUS.ROLLBACK.</description>
			<version>0.5.0</version>
			<fixedVersion>0.6.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">1710</link>
			<link type="Reference" description="is related to">1710</link>
		</links>
	</bug>
	<bug id="1720" opendate="2010-10-16 00:22:47" fixdate="2010-10-17 04:50:11" resolution="Fixed">
		<buginformation>
			<summary>hbase_stats.q is failing</summary>
			<description>Saw this failure on Hudson and in my own sandbox.
https://hudson.apache.org/hudson/job/Hive-trunk-h0.20/392/</description>
			<version>0.7.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Partition.java</file>
		</fixedFiles>
	</bug>
	<bug id="1340" opendate="2010-05-11 17:56:48" fixdate="2010-10-18 03:26:21" resolution="Duplicate">
		<buginformation>
			<summary>checking VOID type for NULL in LazyBinarySerde</summary>
			<description>NULL was not handled correctly in LazyBinarySerDe. One example is


 insert overwrite table T select &amp;amp;apos;1&amp;amp;apos;, NULL from src limit 1;

</description>
			<version>0.6.0</version>
			<fixedVersion>0.6.0</fixedVersion>
			<type>Sub-task</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryFactory.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryStruct.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">1341</link>
		</links>
	</bug>
	<bug id="1760" opendate="2010-10-29 22:55:34" fixdate="2010-10-30 04:59:02" resolution="Fixed">
		<buginformation>
			<summary>Mismatched open/commit transaction calls in case of connection retry</summary>
			<description>Consider the create table function (parts removed for simplicity):



    private void create_table_core(final RawStore ms, final Table tbl)
        throws AlreadyExistsException, MetaException, InvalidObjectException {

      Path tblPath = null;
      boolean success = false, madeDir = false;
      try {
        ms.openTransaction();

        // get_table checks whether database exists, it should be moved here
        if (is_table_exists(tbl.getDbName(), tbl.getTableName())) {
          throw new AlreadyExistsException("Table " + tbl.getTableName()
              + " already exists");
        }

        ms.createTable(tbl);
        success = ms.commitTransaction();

      } finally {
        if (!success) {
          ms.rollbackTransaction();
          if (madeDir) {
            wh.deleteDir(tblPath, true);
          }
        }
      }
    }



A potential openTransaction() / commitTransaction() mismatch can occur if the is_table_exits() method call experiences a connection failure. 
Since get_table() in is_table_exists() uses executeWithRetry(),  the transaction will be rolled back and get_table() will be called again if the is a connection problem. However, this rollback and retry will reset the global openTransactionCalls counter back to 0, effectively canceling out the openTransaction() call. 
Then later in the method when commitTransaction() is called, Hive will throw an error similar to the following:
Caused by: java.lang.RuntimeException: commitTransaction was called but openTransactionCalls = 0. This probably indicates that there are unbalanced calls to openTransaction/commitTransaction
A similar problem exists with create_type_core()</description>
			<version>0.7.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">4996</link>
		</links>
	</bug>
	<bug id="1753" opendate="2010-10-26 18:21:46" fixdate="2010-11-04 03:19:57" resolution="Fixed">
		<buginformation>
			<summary>HIVE 1633 hit for Stage2 jobs with CombineHiveInputFormat</summary>
			<description>Errors are the same as HIVE-1633 but I see them for Stage-2 jobs.</description>
			<version>0.7.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.Context.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">1633</link>
		</links>
	</bug>
	<bug id="1777" opendate="2010-11-08 23:17:03" fixdate="2010-11-09 19:59:15" resolution="Fixed">
		<buginformation>
			<summary>Outdated comments for GenericUDTF.close()</summary>
			<description>In a GenericUDTF, rows can be forward()&amp;amp;apos;ed on close(), contrary to what the comment says.</description>
			<version>0.7.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDTF.java</file>
		</fixedFiles>
	</bug>
	<bug id="1771" opendate="2010-11-05 21:49:07" fixdate="2010-11-10 06:01:39" resolution="Fixed">
		<buginformation>
			<summary>ROUND(infinity) chokes</summary>
			<description>Since 1-arg ROUND returns an integer, it&amp;amp;apos;s hard to fix this without either losing data (return NULL) or making a backwards-incompatible change (return DOUBLE instead of BIGINT).
In any case, we should definitely fix 2-arg ROUND to preserve infinity/NaN/etc, since it is already returning double.</description>
			<version>0.6.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFRound.java</file>
		</fixedFiles>
	</bug>
	<bug id="1501" opendate="2010-07-31 00:56:12" fixdate="2010-11-10 23:22:01" resolution="Fixed">
		<buginformation>
			<summary>when generating reentrant INSERT for index rebuild, quote identifiers using backticks</summary>
			<description>Yongqiang, you mentioned that you weren&amp;amp;apos;t able to do this due to SORT BY not accepting them.  The SORT BY is gone now as of HIVE-1494 (and SORT BY needs to be fixed anyway).</description>
			<version>0.7.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.HiveUtils.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">417</link>
		</links>
	</bug>
	<bug id="1712" opendate="2010-10-14 17:17:44" fixdate="2010-11-11 18:55:16" resolution="Fixed">
		<buginformation>
			<summary>Migrating metadata from derby to mysql thrown NullPointerException</summary>
			<description>Exported derby data to csv, loaded data into mysql and ran hive query which worked in derby and got the following exception
2010-10-16 08:57:29,080 INFO  metastore.ObjectStore (ObjectStore.java:setConf(106)) - Initialized ObjectStore
2010-10-16 08:57:29,552 INFO  metastore.HiveMetaStore (HiveMetaStore.java:logStartFunction(171)) - 0: get_table : db=default tbl=testimport
2010-10-16 08:57:30,140 ERROR metadata.Hive (Hive.java:getTable(395)) - java.lang.NullPointerException
        at java.util.Hashtable.put(Hashtable.java:394)
        at java.util.Hashtable.putAll(Hashtable.java:466)
        at org.apache.hadoop.hive.metastore.MetaStoreUtils.getSchema(MetaStoreUtils.java:520)
        at org.apache.hadoop.hive.metastore.MetaStoreUtils.getSchema(MetaStoreUtils.java:489)
        at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:381)
        at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:333)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(SemanticAnalyzer.java:683)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:5200)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:105)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:275)
        at org.apache.hadoop.hive.ql.Driver.runCommand(Driver.java:320)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:312)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:123)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:181)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:287)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)</description>
			<version>0.5.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
		</fixedFiles>
	</bug>
	<bug id="1804" opendate="2010-11-22 20:31:30" fixdate="2010-11-30 00:06:51" resolution="Fixed">
		<buginformation>
			<summary>Mapjoin will fail if there are no files associating with the join tables</summary>
			<description>If there are some empty tables without any file associated, the map join will fail.</description>
			<version>0.7.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapredLocalTask.java</file>
		</fixedFiles>
	</bug>
	<bug id="1853" opendate="2010-12-16 06:38:36" fixdate="2010-12-16 21:46:41" resolution="Fixed">
		<buginformation>
			<summary>downgrade JDO version</summary>
			<description>After HIVE-1609, we are seeing some table not found errors intermittently.
We have a test case where 5 processes are concurrently issueing the same query - 
explain extended insert .. select from &amp;lt;T&amp;gt;
and once in a while, we get a error &amp;lt;T&amp;gt; not found - 
When we revert back the JDO version, the error is gone.
We can investigate later to find the JDO bug, but for now this is a show-stopper for facebook, and needs
to be reverted back immediately.
This also means, that the filters will not be pushed to mysql.</description>
			<version>0.7.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">1539</link>
			<link type="Reference" description="relates to">1862</link>
			<link type="Reference" description="is related to">1609</link>
		</links>
	</bug>
	<bug id="1854" opendate="2010-12-19 06:53:33" fixdate="2010-12-20 19:31:44" resolution="Fixed">
		<buginformation>
			<summary>Temporarily disable metastore tests for listPartitionsByFilter()</summary>
			<description>After the JDO downgrade in HIVE-1853, the tests for the disabled function listPartitionByFilter() should be disabled as well.</description>
			<version>0.7.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
		</fixedFiles>
	</bug>
	<bug id="1857" opendate="2010-12-20 22:55:14" fixdate="2010-12-21 04:45:46" resolution="Fixed">
		<buginformation>
			<summary>mixed case tablename on lefthand side of LATERAL VIEW results in query failing with confusing error message</summary>
			<description>For the modified query below in lateral_view.q, the exception "org.apache.hadoop.hive.ql.parse.SemanticException: line 3:7 Invalid Table Alias or Column Reference myCol" is thrown.  The query should succeed.
SELECT myCol from tmp_PYANG_lv LATERAL VIEW explode(array(1,2,3)) myTab as myCol limit 3;</description>
			<version>0.6.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
	</bug>
	<bug id="1856" opendate="2010-12-20 22:19:59" fixdate="2010-12-21 23:19:45" resolution="Fixed">
		<buginformation>
			<summary>Implement DROP TABLE/VIEW ... IF EXISTS </summary>
			<description>This issue combines issues HIVE-1550/1165/1542/1551:

augment DROP TABLE/VIEW with IF EXISTS
signal an error if the table/view doesn&amp;amp;apos;t exist and IF EXISTS wasn&amp;amp;apos;t specified
introduce a flag in the configuration that allows you to turn off the new behavior

</description>
			<version>0.7.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>New Feature</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">1858</link>
			<link type="Duplicate" description="is duplicated by">1550</link>
			<link type="Incorporates" description="incorporates">1542</link>
			<link type="Incorporates" description="incorporates">1551</link>
			<link type="Incorporates" description="incorporates">1550</link>
			<link type="Incorporates" description="incorporates">1165</link>
			<link type="Reference" description="is related to">6754</link>
		</links>
	</bug>
	<bug id="837" opendate="2009-09-16 19:04:18" fixdate="2010-12-23 21:01:48" resolution="Duplicate">
		<buginformation>
			<summary>virtual column support (filename) in hive</summary>
			<description>Copying from some mails:
I am dumping files into a hive partion on five minute intervals. I am using LOAD DATA into a partition.
weblogs
web1.00
web1.05
web1.10
...
web2.00
web2.05
web1.10
....
Things that would be useful..
Select files from the folder with a regex or exact name
select * FROM logs where FILENAME LIKE(WEB1*)
select * FROM LOGS WHERE FILENAME=web2.00
Also it would be nice to be able to select offsets in a file, this would make sense with appends
select * from logs WHERE FILENAME=web2.00 FROMOFFSET=454644 [TOOFFSET=]
select  
substr(filename, 4, 7) as  class_A, 
substr(filename,  8, 10) as class_B
count( x ) as cnt
from FOO
group by
substr(filename, 4, 7), 
substr(filename,  8, 10) ;
Hive should support virtual columns</description>
			<version>0.3.0</version>
			<fixedVersion></fixedVersion>
			<type>New Feature</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecMapperContext.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
			<file type="M">org.apache.hadoop.hive.ql.history.HiveHistory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.PartitionDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.lineage.OpProcFactory.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.Warehouse.java</file>
			<file type="M">org.apache.hadoop.hive.common.FileUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ppr.ExprProcFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TaskFactory.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.HiveUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.Schema.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.hooks.PostExecutePrinter.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.Index.java</file>
			<file type="M">org.apache.hadoop.hive.ql.DriverContext.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.RCFile.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecMapper.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.HiveRecordReader.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.BucketizedHiveRecordReader.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ColumnInfo.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.TableType.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.TableScanDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ReduceSinkDeDuplication.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.RawStore.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">417</link>
		</links>
	</bug>
	<bug id="545" opendate="2009-06-06 00:56:15" fixdate="2010-12-25 15:55:16" resolution="Duplicate">
		<buginformation>
			<summary>Use ArrayList instead of Vector in single-threaded Hive code</summary>
			<description>Most of the Hive code is single-threaded, but sometimes we are using Vector instead of the more efficient ArrayList.
See http://java.sun.com/j2se/1.5.0/docs/api/java/util/ArrayList.html
"This class (ArrayList) is roughly equivalent to Vector, except that it is unsynchronized."</description>
			<version>0.4.0</version>
			<fixedVersion></fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.ASTNode.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.dynamic_type.ParseException.java</file>
			<file type="M">org.apache.hadoop.hive.ql.lib.Node.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
			<file type="M">org.apache.hadoop.hive.service.HiveServer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.history.HiveHistory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Table.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.dynamic_type.TokenMgrError.java</file>
			<file type="M">org.apache.hadoop.hive.ql.lib.GraphWalker.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.InputSignature.java</file>
			<file type="M">org.apache.hadoop.hive.ql.QTestUtil.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FetchTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFConv.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.RowResolver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.UDAFEvaluatorResolver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.RowSchema.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.NonSyncDataInputBuffer.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.java</file>
			<file type="M">org.apache.hadoop.hive.cli.CliDriver.java</file>
			<file type="M">org.apache.hadoop.hive.hwi.HWISessionManager.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeFieldList.java</file>
			<file type="M">org.apache.hadoop.hive.hwi.TestHWISessionManager.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.QBJoinTree.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.Warehouse.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.hwi.HWISessionItem.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.RCFileOutputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.GenMRSkewJoinProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.AddPartitionDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Task.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryMap.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.columnar.BytesRefWritable.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java</file>
			<file type="M">org.apache.hadoop.hive.hwi.HWIServer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			<file type="M">org.apache.hadoop.hive.hwi.HWIAuth.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Operator.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFTestLength.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">1112</link>
		</links>
	</bug>
	<bug id="1874" opendate="2011-01-03 04:31:23" fixdate="2011-01-03 22:13:47" resolution="Fixed">
		<buginformation>
			<summary>fix HBase filter pushdown broken by HIVE-1638</summary>
			<description>See comments at end of HIVE-1660 for what happened.</description>
			<version>0.7.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.java</file>
		</fixedFiles>
	</bug>
	<bug id="1859" opendate="2010-12-21 22:39:20" fixdate="2011-01-04 20:05:28" resolution="Duplicate">
		<buginformation>
			<summary>Hive&amp;apos;s tinyint datatype is not supported by the Hive JDBC driver</summary>
			<description>java.sql.SQLException: Could not create ResultSet: org.apache.hadoop.hive.serde2.dynamic_type.ParseException: Encountered "byte" at line 1, column 47.
Was expecting one of:
    "bool" ...
    "i16" ...
    "i32" ...
    "i64" ...
    "double" ...
    "string" ...
    "map" ...
    "list" ...
    "set" ...
    "required" ...
    "optional" ...
    "skip" ...
    &amp;lt;tok_int_constant&amp;gt; ...
    &amp;lt;IDENTIFIER&amp;gt; ...
    "}" ...
        at org.apache.hadoop.hive.jdbc.HiveResultSet.initDynamicSerde(HiveResultSet.java:120)
        at org.apache.hadoop.hive.jdbc.HiveResultSet.&amp;lt;init&amp;gt;(HiveResultSet.java:74)
        at org.apache.hadoop.hive.jdbc.HiveStatement.executeQuery(HiveStatement.java:178)
        at com.quest.orahive.HiveJdbcClient.main(HiveJdbcClient.java:117)</description>
			<version>0.5.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FetchTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.JdbcColumn.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveConnection.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveBaseResultSet.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveQueryResultSet.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">1378</link>
		</links>
	</bug>
	<bug id="1860" opendate="2010-12-21 22:43:53" fixdate="2011-01-04 20:07:06" resolution="Duplicate">
		<buginformation>
			<summary>Hive&amp;apos;s smallint datatype is not supported by the Hive JDBC driver</summary>
			<description>java.sql.SQLException: Inrecognized column type: i16
        at org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.getColumnType(HiveResultSetMetaData.java:132)</description>
			<version>0.5.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FetchTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.JdbcColumn.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveConnection.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveBaseResultSet.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveQueryResultSet.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">1378</link>
		</links>
	</bug>
	<bug id="1861" opendate="2010-12-21 22:48:20" fixdate="2011-01-04 20:07:47" resolution="Duplicate">
		<buginformation>
			<summary>Hive&amp;apos;s float datatype is not supported by the Hive JDBC driver</summary>
			<description>ERROR: DDL specifying type float which has not been defined
java.lang.RuntimeException: specifying type float which has not been defined
        at org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.FieldType(thrift_grammar.java:1879)
        at org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.Field(thrift_grammar.java:1545)
        at org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.FieldList(thrift_grammar.java:1501)
        at org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.Struct(thrift_grammar.java:1171)
        at org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.TypeDefinition(thrift_grammar.java:497)
        at org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.Definition(thrift_grammar.java:439)
        at org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.Start(thrift_grammar.java:101)
        at org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe.initialize(DynamicSerDe.java:102)
        at org.apache.hadoop.hive.jdbc.HiveResultSet.initDynamicSerde(HiveResultSet.java:117)
        at org.apache.hadoop.hive.jdbc.HiveResultSet.&amp;lt;init&amp;gt;(HiveResultSet.java:74)
        at org.apache.hadoop.hive.jdbc.HiveStatement.executeQuery(HiveStatement.java:178)
        at com.quest.orahive.HiveJdbcClient.main(HiveJdbcClient.java:117)
org.apache.hadoop.hive.serde2.SerDeException: java.lang.RuntimeException: specifying type float which has not been defined
        at org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe.initialize(DynamicSerDe.java:117)
        at org.apache.hadoop.hive.jdbc.HiveResultSet.initDynamicSerde(HiveResultSet.java:117)
        at org.apache.hadoop.hive.jdbc.HiveResultSet.&amp;lt;init&amp;gt;(HiveResultSet.java:74)
        at org.apache.hadoop.hive.jdbc.HiveStatement.executeQuery(HiveStatement.java:178)
        at com.quest.orahive.HiveJdbcClient.main(HiveJdbcClient.java:117)
Caused by: java.lang.RuntimeException: specifying type float which has not been defined
        at org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.FieldType(thrift_grammar.java:1879)
        at org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.Field(thrift_grammar.java:1545)
        at org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.FieldList(thrift_grammar.java:1501)
        at org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.Struct(thrift_grammar.java:1171)
        at org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.TypeDefinition(thrift_grammar.java:497)
        at org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.Definition(thrift_grammar.java:439)
        at org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.Start(thrift_grammar.java:101)
        at org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe.initialize(DynamicSerDe.java:102)
        ... 4 more</description>
			<version>0.5.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FetchTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.JdbcColumn.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveConnection.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveBaseResultSet.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveQueryResultSet.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">1378</link>
		</links>
	</bug>
	<bug id="1863" opendate="2010-12-23 04:02:31" fixdate="2011-01-04 20:09:44" resolution="Duplicate">
		<buginformation>
			<summary>Boolean columns in Hive tables containing NULL are treated as FALSE by the Hive JDBC driver.</summary>
			<description>(1) Using the Hive CLI, create a table using...
create table dt4_boolean
(
        dt4_id          int,
        dt4_testbool    boolean,
        dt4_string      string
)
row format delimited
        fields  terminated by &amp;amp;apos;,&amp;amp;apos;
        lines  terminated by &amp;amp;apos;\n&amp;amp;apos;;
(2) Create a file containing the following text...
1,true,Value is True
2,null,Data says null and must be null
3,,No value that means null
4,NoIdea,Data says NoIdea that&amp;amp;apos;s gonna be null
5,false,Value is FALSE
(3) Load the data in the file into the Hive table...
load data local inpath &amp;amp;apos;&amp;lt;DATA FILE PATH&amp;gt;&amp;amp;apos; overwrite into table dt4_boolean;
(4) Check the table works as expected using the Hive CLI...
hive&amp;gt; select * from dt4_boolean;
OK
1	true	Value is True
2	NULL	Data says null and must be null
3	NULL	No value that means null
4	NULL	Data says NoIdea that&amp;amp;apos;s gonna be null
5	false	Value is FALSE
Time taken: 0.049 seconds
(5) Using the Hive JDBC driver, execute the same Hive query (select * from dt4_boolean)
(5.1) The "row_str" values obtained by the Hive JDBC driver for deserialization are correct...
1	true	Value is True
2	NULL	Data says null and must be null
3	NULL	No value that means null
4	NULL	Data says NoIdea that&amp;amp;apos;s gonna be null
5	false	Value is FALSE
(5.2) However, when these "row_str" are deserialized by the DynamicSerDe to a java.lang.Object, the NULL boolean values are converted to FALSE - instead of being null.
As a consequence, the application making use of the Hive JDBC driver produces this (incorrect) output...
SQL&amp;gt; select dt4_id, dt4_testbool from dt4_boolean;
    DT4_ID DT4_TESTBOOL
---------- ------------
         1            true
         2            false
         3            false
         4            false
         5            false
...instead of producing this (correct) output...
SQL&amp;gt; select dt4_id, dt4_testbool from dt4_boolean;
    DT4_ID DT4_TESTBOOL
---------- ------------
         1            true
         2            NULL
         3            NULL
         4            NULL
         5            false</description>
			<version>2.0.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FetchTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.JdbcColumn.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveConnection.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveBaseResultSet.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveQueryResultSet.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">1378</link>
		</links>
	</bug>
	<bug id="1829" opendate="2010-12-03 21:12:44" fixdate="2011-01-11 17:28:46" resolution="Fixed">
		<buginformation>
			<summary>Fix intermittent failures in TestRemoteMetaStore</summary>
			<description>Notice how Running metastore! appears twice.

test:
    [junit] Running org.apache.hadoop.hive.metastore.TestEmbeddedHiveMetaStore
    [junit] BR.recoverFromMismatchedToken
    [junit] Tests run: 11, Failures: 0, Errors: 0, Time elapsed: 36.697 sec
    [junit] Running org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore
    [junit] Running metastore!
    [junit] Running metastore!
    [junit] org.apache.thrift.transport.TTransportException: Could not create ServerSocket on address 0.0.0.0/0.0.0.0:29083.
    [junit] 	at org.apache.thrift.transport.TServerSocket.&amp;lt;init&amp;gt;(TServerSocket.java:98)
    [junit] 	at org.apache.thrift.transport.TServerSocket.&amp;lt;init&amp;gt;(TServerSocket.java:79)
    [junit] 	at org.apache.hadoop.hive.metastore.TServerSocketKeepAlive.&amp;lt;init&amp;gt;(TServerSocketKeepAlive.java:34)
    [junit] 	at org.apache.hadoop.hive.metastore.HiveMetaStore.main(HiveMetaStore.java:2189)
    [junit] 	at org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore$RunMS.run(TestRemoteHiveMetaStore.java:35)
    [junit] 	at java.lang.Thread.run(Thread.java:619)
    [junit] Running org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec
    [junit] Test org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore FAILED (crashed)

</description>
			<version>0.6.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
	</bug>
	<bug id="1911" opendate="2011-01-13 18:42:43" fixdate="2011-01-13 18:57:20" resolution="Duplicate">
		<buginformation>
			<summary>TestHBaseCliDriver is broken</summary>
			<description>It broken on the current trunk:
$ ant test -Dtestcase=TestHBaseCliDriver 
....
....
test:
[junit] Running org.apache.hadoop.hive.cli.TestHBaseCliDriver
[junit] Exception: Timed out trying to locate root region
[junit] org.apache.hadoop.hbase.client.NoServerForRegionException: Timed out trying to locate root region
[junit]     at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRootRegion(HConnectionManager.java:976)
[junit]     at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:625)
[junit]     at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.relocateRegion(HConnectionManager.java:607)
[junit]     at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegionInMeta(HConnectionManager.java:738)
[junit]     at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:634)
[junit]     at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:601)
[junit]     at org.apache.hadoop.hbase.client.HTable.&amp;lt;init&amp;gt;(HTable.java:128)
[junit]     at org.apache.hadoop.hive.hbase.HBaseTestSetup.setUpFixtures(HBaseTestSetup.java:87)
[junit]     at org.apache.hadoop.hive.hbase.HBaseTestSetup.preTest(HBaseTestSetup.java:59)
[junit]     at org.apache.hadoop.hive.hbase.HBaseQTestUtil.&amp;lt;init&amp;gt;(HBaseQTestUtil.java:31)
[junit]     at org.apache.hadoop.hive.cli.TestHBaseCliDriver.setUp(TestHBaseCliDriver.java:43)
[junit]     at junit.framework.TestCase.runBare(TestCase.java:125)
[junit]     at junit.framework.TestResult$1.protect(TestResult.java:106)
[junit]     at junit.framework.TestResult.runProtected(TestResult.java:124)
[junit]     at junit.framework.TestResult.run(TestResult.java:109)
[junit]     at junit.framework.TestCase.run(TestCase.java:118)
[junit]     at junit.framework.TestSuite.runTest(TestSuite.java:208)
[junit]     at junit.framework.TestSuite.run(TestSuite.java:203)
[junit]     at junit.extensions.TestDecorator.basicRun(TestDecorator.java:22)
[junit]     at junit.extensions.TestSetup$1.protect(TestSetup.java:19)
[junit]     at junit.framework.TestResult.runProtected(TestResult.java:124)
[junit]     at junit.extensions.TestSetup.run(TestSetup.java:23)
[junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:422)
[junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:931)
[junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:785)
[junit] Hive history file=/data/users/nzhang/work/2/apache-hive/build/hbase-handler/tmp/hive_job_log_nzhang_20110</description>
			<version>0.7.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.hbase.HBaseTestSetup.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">1716</link>
		</links>
	</bug>
	<bug id="1914" opendate="2011-01-14 07:07:57" fixdate="2011-01-15 01:21:59" resolution="Duplicate">
		<buginformation>
			<summary>failures in testhbaseclidriver</summary>
			<description>i didnt debug it</description>
			<version>0.7.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.hbase.HBaseTestSetup.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">1716</link>
		</links>
	</bug>
	<bug id="1862" opendate="2010-12-22 21:46:38" fixdate="2011-01-20 03:59:19" resolution="Fixed">
		<buginformation>
			<summary>Revive partition filtering in the Hive MetaStore</summary>
			<description>HIVE-1853 downgraded the JDO version. This makes the feature of partition filtering in the metastore unusable. This jira is to keep track of the lost feature and discussing approaches to bring it back.</description>
			<version>0.7.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.parser.ExpressionTree.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">2048</link>
			<link type="Reference" description="is related to">1539</link>
			<link type="Reference" description="is related to">1853</link>
			<link type="Reference" description="is related to">1609</link>
		</links>
	</bug>
	<bug id="1897" opendate="2011-01-06 14:25:13" fixdate="2011-01-24 23:28:15" resolution="Fixed">
		<buginformation>
			<summary>Alter command execution "when HDFS is down" results in holding stale data in MetaStore </summary>
			<description>Lets  consider, the  "DFS" is down , 
And on executing an alter query say  "alter table firsttable rename to secondtable".  
the query execution fails with the following exception:
 
InvalidOperationException(message:Unable to access old location hdfs://localhost:9000/user/hive/warehouse/firsttable for table default.firsttable)
Now after starting the DFS and then executing the same query , the client gets the following exception:

NoSuchObjectException(message:default.firsttable table not found)
Root Cause
In Alter Query execution flow, first "MetaStore" operation is executed successfully and then "DFS" operation is started. In this scenario, "DFS" is down. As a result, execution of the query failed and partial information of the operation is saved.</description>
			<version>0.6.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
		</fixedFiles>
	</bug>
	<bug id="1908" opendate="2011-01-11 10:38:18" fixdate="2011-01-24 23:31:48" resolution="Fixed">
		<buginformation>
			<summary>FileHandler leak on partial iteration of the resultset. </summary>
			<description>If the "resultset" is not iterated completely ,  one filehandler is leaking
Ex: We need only first row. This case one resource is leaking



ResultSet resultSet = createStatement.executeQuery("select * from sampletable");

if (resultSet.next())
{
	System.out.println(resultSet.getString(1)+"   "+resultSet.getString(2));
} 



Command used for checking the filehandlers


lsof -p {hive_process_id} &amp;gt; runjarlsof.txt


</description>
			<version>0.6.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FetchTask.java</file>
		</fixedFiles>
	</bug>
	<bug id="1716" opendate="2010-10-15 17:38:59" fixdate="2011-02-03 17:47:32" resolution="Fixed">
		<buginformation>
			<summary>make TestHBaseCliDriver use dynamic ports to avoid conflicts with already-running services</summary>
			<description>ant test -Dhadoop.version=0.20.0 -Dtestcase=TestHBaseCliDriver:
.... 
[junit] org.apache.hadoop.hbase.client.NoServerForRegionException: Timed out trying to locate root region
[junit]     at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRootRegion(HConnectionManager.java:976)
[junit]     at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:625)
[junit]     at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.relocateRegion(HConnectionManager.java:607)
[junit]     at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegionInMeta(HConnectionManager.java:738)
[junit]     at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:634)
[junit]     at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:601)
[junit]     at org.apache.hadoop.hbase.client.HTable.&amp;lt;init&amp;gt;(HTable.java:128)
[junit]     at org.apache.hadoop.hive.hbase.HBaseTestSetup.setUpFixtures(HBaseTestSetup.java:87)
[junit]     at org.apache.hadoop.hive.hbase.HBaseTestSetup.preTest(HBaseTestSetup.java:59)
[junit]     at org.apache.hadoop.hive.hbase.HBaseQTestUtil.&amp;lt;init&amp;gt;(HBaseQTestUtil.java:31)
[junit]     at org.apache.hadoop.hive.cli.TestHBaseCliDriver.setUp(TestHBaseCliDriver.java:43)
[junit]     at junit.framework.TestCase.runBare(TestCase.java:125)
[junit]     at junit.framework.TestResult$1.protect(TestResult.java:106)
[junit]     at junit.framework.TestResult.runProtected(TestResult.java:124)
[junit]     at junit.framework.TestResult.run(TestResult.java:109)
[junit]     at junit.framework.TestCase.run(TestCase.java:118)
[junit]     at junit.framework.TestSuite.runTest(TestSuite.java:208)
[junit]     at junit.framework.TestSuite.run(TestSuite.java:203)</description>
			<version>0.7.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.hbase.HBaseTestSetup.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">1911</link>
			<link type="Duplicate" description="is duplicated by">1914</link>
		</links>
	</bug>
	<bug id="1465" opendate="2010-07-14 17:55:28" fixdate="2011-02-12 17:09:54" resolution="Fixed">
		<buginformation>
			<summary>hive-site.xml ${user.name} not replaced for local-file derby metastore connection URL</summary>
			<description>Seems that for this parameter


&amp;lt;property&amp;gt;
&amp;lt;name&amp;gt;javax.jdo.option.ConnectionURL&amp;lt;/name&amp;gt;
&amp;lt;value&amp;gt;jdbc:derby:;databaseName=/var/lib/hive/metastore/${user.name}_db;create=true&amp;lt;/value&amp;gt;
&amp;lt;description&amp;gt;JDBC connect string for a JDBC metastore&amp;lt;/description&amp;gt;
&amp;lt;/property&amp;gt;


${user.name} is never replaced by the actual user name:


$ ls -la /var/lib/hive/metastore/
total 24
drwxrwxrwt 3 root root 4096 Apr 30 12:37 .
drwxr-xr-x 3 root root 4096 Apr 30 12:25 ..
drwxrwxr-x 5 hadoop hadoop 4096 Apr 30 12:37 ${user.name}_db

</description>
			<version>0.5.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
		</fixedFiles>
	</bug>
	<bug id="1896" opendate="2011-01-06 10:21:15" fixdate="2011-02-13 09:30:49" resolution="Fixed">
		<buginformation>
			<summary>HBase and Contrib JAR names are missing version numbers</summary>
			<description>Also, does anyone know why the hbase and contrib JARs use underscores
instead of dashes in their names? Can I change this or will it break something?


./build/dist/lib/hive-anttasks-0.7.0-SNAPSHOT.jar
./build/dist/lib/hive-cli-0.7.0-SNAPSHOT.jar
./build/dist/lib/hive-common-0.7.0-SNAPSHOT.jar
./build/dist/lib/hive-exec-0.7.0-SNAPSHOT.jar
./build/dist/lib/hive-hwi-0.7.0-SNAPSHOT.jar
./build/dist/lib/hive-jdbc-0.7.0-SNAPSHOT.jar
./build/dist/lib/hive-metastore-0.7.0-SNAPSHOT.jar
./build/dist/lib/hive-serde-0.7.0-SNAPSHOT.jar
./build/dist/lib/hive-service-0.7.0-SNAPSHOT.jar
./build/dist/lib/hive-shims-0.7.0-SNAPSHOT.jar
./build/dist/lib/hive_contrib.jar                                   &amp;lt;------
./build/dist/lib/hive_hbase-handler.jar                     &amp;lt;------


</description>
			<version>0.7.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.QTestUtil.java</file>
		</fixedFiles>
	</bug>
	<bug id="1995" opendate="2011-02-16 02:23:25" fixdate="2011-02-16 06:21:09" resolution="Fixed">
		<buginformation>
			<summary>Mismatched open/commit transaction calls when using get_partition()</summary>
			<description>Nested executeWithRetry() calls caused by using HiveMetaStore.get_partition() can result in mis-matched open/commit calls. Fixes the same issue as described in HIVE-1760.</description>
			<version>0.7.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
		</fixedFiles>
	</bug>
	<bug id="1928" opendate="2011-01-26 01:53:02" fixdate="2011-02-17 07:46:13" resolution="Fixed">
		<buginformation>
			<summary>GRANT/REVOKE should handle privileges as tokens, not identifiers</summary>
			<description>The grammar for the GRANT and REVOKE Privileges statements currently handle the list of privileges as a list of
identifiers. Since most of the privileges are also keywords in the HQL grammar this requires users
to individually quote-escape each of the privileges, e.g:


grant `Create` on table authorization_part to user hive_test_user;
grant `Update` on table authorization_part to user hive_test_user;
grant `Drop` on table authorization_part to user hive_test_user;
grant `select` on table src to user hive_test_user;


Both MySQL and the SQL standard treat privileges as tokens. Hive should do the same.</description>
			<version>0.7.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.PrivilegeRegistry.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.Privilege.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">78</link>
		</links>
	</bug>
	<bug id="1974" opendate="2011-02-08 15:32:11" fixdate="2011-02-23 15:01:21" resolution="Fixed">
		<buginformation>
			<summary>In error scenario some opened streams may not closed in ScriptOperator.java, Utilities.java </summary>
			<description>1)In error scenario StreamProcessor may not be closed in ScriptOperator.java
2)In error scenario XMLEncoder may not be closed in Utilities.java</description>
			<version>0.5.0</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
		</fixedFiles>
	</bug>
	<bug id="1973" opendate="2011-02-08 15:29:05" fixdate="2011-02-23 16:01:15" resolution="Fixed">
		<buginformation>
			<summary>Getting error when join on tables where name of table has uppercase letters</summary>
			<description>When execute a join query on tables containing Uppercase letters in the table names hit an exception
 Ex:

  create table a(b int);
  create table tabForJoin(b int,c int);

  select * from a join tabForJoin on(a.b=tabForJoin.b);

  Got an exception like this
  FAILED: Error in semantic analysis:  Invalid Table Alias tabForJoin


But if i give without capital letters ,It is working</description>
			<version>0.5.0</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
	</bug>
	<bug id="2001" opendate="2011-02-22 23:52:13" fixdate="2011-02-25 00:25:18" resolution="Fixed">
		<buginformation>
			<summary>Add inputs and outputs to authorization DDL commands</summary>
			<description>When permissions are changed for a table/partition, the respective object should be present in the read/write entities for hooks to act on.</description>
			<version>0.8.0</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
		</fixedFiles>
	</bug>
	<bug id="2007" opendate="2011-02-25 11:40:19" fixdate="2011-02-26 15:31:41" resolution="Fixed">
		<buginformation>
			<summary>Executing queries using Hive Server is not logging to the log file specified in hive-log4j.properties</summary>
			<description>Start Hive Server by specifying the log details ( filelocation , appender , loglevel ) in hive-log4j.properties, but logging is not happening as per the details provided in the hive-log4j.properties.</description>
			<version>0.7.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.service.HiveServer.java</file>
		</fixedFiles>
	</bug>
	<bug id="1867" opendate="2010-12-24 22:22:21" fixdate="2011-03-15 20:49:24" resolution="Fixed">
		<buginformation>
			<summary>Add mechanism for disabling tests with intermittent failures</summary>
			<description>

[junit] Begin query: dyn_part_empty.q
    [junit] Running org.apache.hadoop.hive.cli.TestNegativeCliDriver
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec
    [junit] Test org.apache.hadoop.hive.cli.TestNegativeCliDriver FAILED (crashed)


dyn_part_empty.q has been intermittently failing on Hudson. I was able to reproduce locally,
and with different versions of JUnit (3.8.1, 4.5, 4.8.2).</description>
			<version>0.7.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ant.QTestGenTask.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">1872</link>
		</links>
	</bug>
	<bug id="2059" opendate="2011-03-16 18:56:15" fixdate="2011-03-17 04:39:58" resolution="Fixed">
		<buginformation>
			<summary>Add datanucleus.identifierFactory property to HiveConf to avoid unintentional MetaStore Schema corruption</summary>
			<description>Hive 0.6.0 we upgraded the version of DataNucleus from 1.0 to 2.0, which changed some of the defaults for how field names get mapped to datastore identifiers. This was problem was resolved in HIVE-1435 by setting datanucleus.identifierFactory=datanucleus in hive-default.xml
However, this property definition was not added to HiveConf. This can result in schema corruption if the user upgrades from Hive 0.5.0 to 0.6.0 or 0.7.0 and retains the Hive 0.5.0 version hive-default.xml on their classpath.</description>
			<version>0.7.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">1435</link>
		</links>
	</bug>
	<bug id="1959" opendate="2011-02-04 13:58:09" fixdate="2011-03-17 19:26:40" resolution="Fixed">
		<buginformation>
			<summary>Potential memory leak when same connection used for long time. TaskInfo and QueryInfo objects are getting accumulated on executing more queries on the same connection.</summary>
			<description>org.apache.hadoop.hive.ql.history.HiveHistory$TaskInfo and org.apache.hadoop.hive.ql.history.HiveHistory$QueryInfo these two objects are getting accumulated on executing more number of queries on the same connection. These objects are getting released only when the connection is closed.</description>
			<version>0.8.0</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.history.HiveHistory.java</file>
		</fixedFiles>
	</bug>
	<bug id="1976" opendate="2011-02-08 15:37:06" fixdate="2011-03-18 22:11:11" resolution="Fixed">
		<buginformation>
			<summary>Exception should be thrown when invalid jar,file,archive is given to add command</summary>
			<description>When executed add command with non existing jar it should throw exception through   HiveStatement
Ex:

  add jar /root/invalidpath/testjar.jar


Here testjar.jar is not exist so it should throw exception.</description>
			<version>0.5.0</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.session.SessionState.java</file>
			<file type="M">org.apache.hadoop.hive.ql.processors.AddResourceProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.service.TestHiveServer.java</file>
		</fixedFiles>
	</bug>
	<bug id="2064" opendate="2011-03-18 17:14:01" fixdate="2011-03-19 01:29:37" resolution="Fixed">
		<buginformation>
			<summary>Make call to SecurityUtil.getServerPrincipal unambiguous</summary>
			<description>HadoopThriftAuthBridge20S calls SecurityUtil.getServerPrincipal and passes null for the 2nd arg. When building against the hadoop security branch this is a compilation error as it matches the signatures of both getServerPrincipal methods (one takes a String for the 2nd arg, one an InetAddress). This call needs to be made unambiguous eg by passing "0.0.0.0" instead of null, which per the getServerPrincipal javadoc is equivalent:

It replaces hostname pattern with hostname, which should be

fully-qualified domain name. If hostname is null or "0.0.0.0", it uses
dynamically looked-up fqdn of the current host instead.

</description>
			<version>0.7.0</version>
			<fixedVersion>0.7.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.java</file>
		</fixedFiles>
	</bug>
	<bug id="2060" opendate="2011-03-17 18:28:01" fixdate="2011-03-21 00:41:01" resolution="Fixed">
		<buginformation>
			<summary>CLI local mode hit NPE when exiting by ^D</summary>
			<description>CLI gets an NPE when running in local mode and hit an ^D to exit it. </description>
			<version>0.8.0</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.cli.CliDriver.java</file>
			<file type="M">org.apache.hadoop.hive.cli.CliSessionState.java</file>
		</fixedFiles>
	</bug>
	<bug id="2042" opendate="2011-03-11 08:23:20" fixdate="2011-03-22 09:55:50" resolution="Fixed">
		<buginformation>
			<summary>In error scenario some opened streams may not closed</summary>
			<description>1) In error scenario PrintStream may not be closed in execute() of  ExplainTask.java
2) In error scenario InputStream may not be closed in checkJobTracker() of Throttle.java </description>
			<version>0.7.0</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.SequenceFileInputFormatChecker.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			<file type="M">org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableOutput.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Throttle.java</file>
		</fixedFiles>
	</bug>
	<bug id="2069" opendate="2011-03-22 20:32:43" fixdate="2011-03-23 17:57:02" resolution="Fixed">
		<buginformation>
			<summary>NullPointerException on getSchemas</summary>
			<description>Calling getSchemas will cause a nullpointerexception</description>
			<version>0.8.0</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveMetaDataResultSet.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">8030</link>
		</links>
	</bug>
	<bug id="2054" opendate="2011-03-15 12:47:47" fixdate="2011-04-05 18:10:25" resolution="Fixed">
		<buginformation>
			<summary>Exception on windows when using the jdbc driver. "IOException: The system cannot find the path specified"</summary>
			<description>It seems something recently changed on the jdbc driver which causes this IOException on windows.
java.lang.RuntimeException: java.io.IOException: The system cannot find the path specified
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:237)
	at org.apache.hadoop.hive.jdbc.HiveConnection.&amp;lt;init&amp;gt;(HiveConnection.java:73)
	at org.apache.hadoop.hive.jdbc.HiveDriver.connect(HiveDriver.java:110)</description>
			<version>0.8.0</version>
			<fixedVersion>0.7.1, 0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveConnection.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.JdbcSessionState.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveStatement.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HivePreparedStatement.java</file>
		</fixedFiles>
	</bug>
	<bug id="1988" opendate="2011-02-14 11:45:29" fixdate="2011-04-06 10:51:02" resolution="Fixed">
		<buginformation>
			<summary>Make the delegation token issued by the MetaStore owned by the right user</summary>
			<description>The &amp;amp;apos;owner&amp;amp;apos; of any delegation token issued by the MetaStore is set to the requesting user. When a delegation token is asked by the user himself during a job submission, this is fine. However, in the case where the token is requested for by services (e.g., Oozie), on behalf of the user, the token&amp;amp;apos;s owner is set to the user the service is running as. Later on, when the token is used by a MapReduce task, the MetaStore treats the incoming request as coming from Oozie and does operations as Oozie. This means any new directory creations (e.g., create_table) on the hdfs by the MetaStore will end up with Oozie as the owner.
Also, the MetaStore doesn&amp;amp;apos;t check whether a user asking for a token on behalf of some other user, is actually authorized to act on behalf of that other user. We should start using the ProxyUser authorization in the MetaStore (HADOOP-6510&amp;amp;apos;s APIs).</description>
			<version>0.7.0</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.thrift.TestHadoop20SAuthBridge.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.DelegationTokenSecretManager.java</file>
		</fixedFiles>
	</bug>
	<bug id="2095" opendate="2011-04-05 21:42:49" fixdate="2011-04-08 22:00:09" resolution="Fixed">
		<buginformation>
			<summary>auto convert map join bug</summary>
			<description>1) 
when considering to choose one table as the big table candidate for a map join, if at compile time, hive can find out that the total known size of all other tables excluding the big table in consideration is bigger than a configured value, this big table candidate is a bad one, and should not put into plan. Otherwise, at runtime to filter this out may cause more time.
2)
added a null check for back up tasks. Otherwise will see NullPointerException
3)
CommonJoinResolver needs to know a full mapping of pathToAliases. Otherwise it will make wrong decision.
4)
changes made to the ConditionalResolverCommonJoin: added pathToAliases, aliasToSize (alias&amp;amp;apos;s input size that is known at compile time, by inputSummary), and intermediate dir path.
So the logic is, go over all the pathToAliases, and for each path, if it is from intermediate dir path, add this path&amp;amp;apos;s size to all aliases. And finally based on the size information and others like aliasToTask to choose the big table. 
5)
Conditional task&amp;amp;apos;s children contains wrong options, which may cause join fail or incorrect results. Basically when getting all possible children for the conditional task, should use a whitelist of big tables. Only tables in this while list can be considered as a big table.
Here is the logic:

Get a list of big table candidates. Only the tables in the returned set can be used as big table in the join operation.
The logic here is to scan the join condition array from left to right.
	
If see a inner join and the bigTableCandidates is empty, add both side of this inner join to big table candidates.
If see a left outer join, and the bigTableCandidates is empty, add the left side to it, and
if the bigTableCandidates is not empty, do nothing (which means the bigTableCandidates is from left side).
If see a right outer join, clear the bigTableCandidates, and add right side to the bigTableCandidates, it means the right side of a right outer join always win.
If see a full outer join, return null immediately (no one can be the big table, can not do a mapjoin).



</description>
			<version>0.7.0</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Task.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinResolver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.LocalMapJoinProcFactory.java</file>
		</fixedFiles>
	</bug>
	<bug id="2031" opendate="2011-03-08 11:38:53" fixdate="2011-04-09 16:26:05" resolution="Fixed">
		<buginformation>
			<summary>Correct the exception message for the better traceability for the scenario load into the partitioned table having 2  partitions by specifying only one partition in the load statement. </summary>
			<description> Load into the partitioned table having 2 partitions by specifying only one partition in the load statement is failing and logging the following exception message.

 org.apache.hadoop.hive.ql.parse.SemanticException: line 1:91 Partition not found &amp;amp;apos;21Oct&amp;amp;apos;
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer$tableSpec.&amp;lt;init&amp;gt;(BaseSemanticAnalyzer.java:685)
	at org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.analyzeInternal(LoadSemanticAnalyzer.java:196)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:238)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:340)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:736)
	at org.apache.hadoop.hive.service.HiveServer$HiveServerHandler.execute(HiveServer.java:151)
	at org.apache.hadoop.hive.service.ThriftHive$Processor$execute.process(ThriftHive.java:764)
	at org.apache.hadoop.hive.service.ThriftHive$Processor.process(ThriftHive.java:742)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)


This needs to be corrected in such a way what is the actual root cause for this.</description>
			<version>0.7.0</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
		</fixedFiles>
	</bug>
	<bug id="2096" opendate="2011-04-06 00:07:36" fixdate="2011-05-20 17:33:24" resolution="Fixed">
		<buginformation>
			<summary>throw a error if the input is larger than a threshold for index input format</summary>
			<description>This can hang for ever.</description>
			<version>0.8.0</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.index.HiveIndexResult.java</file>
			<file type="M">org.apache.hadoop.hive.ql.index.HiveIndexedInputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
	</bug>
	<bug id="2117" opendate="2011-04-18 18:26:23" fixdate="2011-05-25 05:10:06" resolution="Fixed">
		<buginformation>
			<summary>insert overwrite ignoring partition location</summary>
			<description>The following code works differently in 0.5.0 vs 0.7.0.
In 0.5.0 the partition location is respected. 
However in 0.7.0 while the initial partition is create with the specified location "&amp;lt;path&amp;gt;/parta", the "insert overwrite ..." results in the partition written to "&amp;lt;path&amp;gt;/dt=a" (note that &amp;lt;path&amp;gt; is the same in both cases).


create table foo_stg (bar INT, car INT); 
load data local inpath &amp;amp;apos;data.txt&amp;amp;apos; into table foo_stg;
 
create table foo4 (bar INT, car INT) partitioned by (dt STRING) LOCATION &amp;amp;apos;/user/hive/warehouse/foo4&amp;amp;apos;; 
alter table foo4 add partition (dt=&amp;amp;apos;a&amp;amp;apos;) location &amp;amp;apos;/user/hive/warehouse/foo4/parta&amp;amp;apos;;
 
from foo_stg fs insert overwrite table foo4 partition (dt=&amp;amp;apos;a&amp;amp;apos;) select *;


From what I can tell HIVE-1707 introduced this via a change to
org.apache.hadoop.hive.ql.metadata.Hive.loadPartition(Path, String, Map&amp;lt;String, String&amp;gt;, boolean, boolean)
specifically:


+      Path partPath = new Path(tbl.getDataLocation().getPath(),
+          Warehouse.makePartPath(partSpec));
+
+      Path newPartPath = new Path(loadPath.toUri().getScheme(), loadPath
+          .toUri().getAuthority(), partPath.toUri().getPath());


Reading the description on HIVE-1707 it seems that this may have been done purposefully, however given the partition location is explicitly specified for the partition in question it seems like that should be honored (esp give the table location has not changed).
This difference in behavior is causing a regression in existing production Hive based code. I&amp;amp;apos;d like to take a stab at addressing this, any suggestions?
</description>
			<version>0.7.1</version>
			<fixedVersion>0.7.1, 0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.TestMTQueries.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
			<file type="M">org.apache.hadoop.hive.ql.QTestUtil.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">1707</link>
		</links>
	</bug>
	<bug id="1550" opendate="2010-08-17 18:49:32" fixdate="2011-06-04 07:44:49" resolution="Duplicate">
		<buginformation>
			<summary>Implement DROP VIEW IF EXISTS</summary>
			<description>Implement the IF EXISTS clause for the DROP VIEW statement.
See http://dev.mysql.com/doc/refman/5.0/en/drop-view.html
</description>
			<version>0.7.0</version>
			<fixedVersion></fixedVersion>
			<type>New Feature</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">1856</link>
			<link type="Incorporates" description="is part of">1856</link>
			<link type="Reference" description="is related to">1165</link>
		</links>
	</bug>
	<bug id="2237" opendate="2011-06-23 20:05:21" fixdate="2011-06-23 21:16:30" resolution="Fixed">
		<buginformation>
			<summary>hive fails to build in eclipse due to syntax error in BitmapIndexHandler.java</summary>
			<description>I see the following error in helios eclipse with the latest trunk (although build on the command line is fine):
Syntax error on token ";", delete this token
seems to have been introduced by this change in HIVE-2036
+import org.apache.hadoop.hive.ql.index.HiveIndexedInputFormat;;
I have a patch forthcoming.</description>
			<version>0.8.0</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.index.bitmap.BitmapIndexHandler.java</file>
		</fixedFiles>
	</bug>
	<bug id="2045" opendate="2011-03-11 08:39:55" fixdate="2011-07-08 18:32:26" resolution="Fixed">
		<buginformation>
			<summary>TCTLSeparatedProtocol.SimpleTransportTokenizer.nextToken() throws Null Pointer Exception in some cases</summary>
			<description>1) In TCTLSeparatedProtocol.SimpleTransportTokenizer.nextToken() is doing null check for the tokenizer.
If tokenizer is null, fillTokenizer() method is called to get the tokenizer object. But fillTokenizer() method also can update the tokenizer with NULL , so NULL check should be done before using the tokenizer.
2) Also improved some logging in TCTLSeparatedProtocol.java</description>
			<version>0.7.0</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.serde2.TestTCTLSeparatedProtocol.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.java</file>
		</fixedFiles>
	</bug>
	<bug id="2204" opendate="2011-06-07 13:21:38" fixdate="2011-07-14 18:08:09" resolution="Fixed">
		<buginformation>
			<summary>unable to get column names for a specific table that has &amp;apos;_&amp;apos; as part of its table name</summary>
			<description>I have a table age_group and I am trying to get list of columns for this table name. As underscore and &amp;amp;apos;%&amp;amp;apos; have special meaning in table search pattern according to JDBC searchPattern string specification, I escape the &amp;amp;apos;_&amp;amp;apos; in my table name when I call getColumns for this single table. But HIVE does not return any columns. My call to getColumns is as follows
catalog	&amp;lt;null&amp;gt;
schemaPattern	"%"
tableNamePattern  "age_group"
columnNamePattern  "%"
If I don&amp;amp;apos;t escape the &amp;amp;apos;_&amp;amp;apos; in my tableNamePattern, I am able to get the list of columns.</description>
			<version>0.8.0</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.java</file>
		</fixedFiles>
	</bug>
	<bug id="2260" opendate="2011-07-05 22:42:17" fixdate="2011-07-16 15:28:56" resolution="Fixed">
		<buginformation>
			<summary>ExecDriver::addInputPaths should pass the table properties to the record writer</summary>
			<description>Currently when ExecDriver encounters a non-existent partition, it creates an empty file so that the query will be valid (and return 0 results).  However, when it does this and calls getHiveRecordWriter(), it creates a new instance of Properties, rather than providing the Properties associated with the table.
This causes RecordWriters that pull information from the table through the Properties to fail (such as Haivvreo).  The RecordWriter should be provided the table&amp;amp;apos;s Properties, as it is in all other cases where it&amp;amp;apos;s called.</description>
			<version>0.7.0</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
		</fixedFiles>
	</bug>
	<bug id="1218" opendate="2010-03-09 23:23:31" fixdate="2011-07-18 19:01:15" resolution="Fixed">
		<buginformation>
			<summary>CREATE TABLE t LIKE some_view should create a new empty base table, but instead creates a copy of view</summary>
			<description>I think it should copy only the column definitions from the view metadata.  Currently it is copying the entire descriptor, resulting in a new view instead of a new base table.</description>
			<version>0.7.0</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.CreateTableLikeDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">2268</link>
			<link type="Reference" description="relates to">2086</link>
			<link type="Reference" description="relates to">972</link>
		</links>
	</bug>
	<bug id="2198" opendate="2011-06-06 13:08:38" fixdate="2011-07-20 04:40:12" resolution="Fixed">
		<buginformation>
			<summary>While using Hive in server mode, HiveConnection.close() is not cleaning up server side resources</summary>
			<description>org.apache.hadoop.hive.service.ThriftHive.Client.clean() method is called for every session end in CLI mode for the cleanup but in HiveServer mode this is not called.
So this can be integrate with the HiveConnection.close()</description>
			<version>0.5.0</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveConnection.java</file>
			<file type="M">org.apache.hadoop.hive.service.TestHiveServer.java</file>
		</fixedFiles>
	</bug>
	<bug id="1884" opendate="2011-01-05 09:52:48" fixdate="2011-07-20 19:54:41" resolution="Fixed">
		<buginformation>
			<summary>Potential risk of resource leaks in Hive</summary>
			<description>There are couple of resource leaks.
For example,
In CliDriver.java, Method :- processReader() the buffered reader is not closed.
Also there are risk(s) of  resource(s) getting leaked , in such cases we need to re factor the code to move closing of resources in finally block.
For Example :- 
In Throttle.java   Method:- checkJobTracker() , the following code snippet might cause resource leak.


InputStream in = url.openStream();
in.read(buffer);
in.close();


Ideally and as per the best coding practices it should be like below



InputStream in=null;
try   {
        in = url.openStream();
        int numRead = in.read(buffer);
}
finally {
       IOUtils.closeStream(in);
}



Similar cases, were found in ExplainTask.java, DDLTask.java etc.Need to re factor all such occurrences.
</description>
			<version>0.3.0</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.cli.CliDriver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.RCFileInputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableInput.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
		</fixedFiles>
	</bug>
	<bug id="2086" opendate="2011-03-31 18:05:43" fixdate="2011-07-21 19:32:02" resolution="Fixed">
		<buginformation>
			<summary>Add test coverage for external table data loss issue</summary>
			<description>Data loss when using "create external table like" statement. 
1) Set up an external table S, point to location L. Populate data in S.
2) Create another external table T, using statement like this:
    create external table T like S location L
   Make sure table T point to the same location as the original table S.
3) Query table T, see the same set of data in S.
4) drop table T.
5) Query table S will return nothing, and location L is deleted. </description>
			<version>0.7.0</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.QTestUtil.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">1218</link>
		</links>
	</bug>
	<bug id="2296" opendate="2011-07-20 23:13:23" fixdate="2011-07-22 20:21:24" resolution="Fixed">
		<buginformation>
			<summary>bad compressed file names from insert into</summary>
			<description>When INSERT INTO is run on a table with compressed output (hive.exec.compress.output=true) and existing files in the table, it may copy the new files in bad file names:
Before INSERT INTO:
000000_0.gz
After INSERT INTO:
000000_0.gz
000000_0.gz_copy_1
This causes corrupted output when doing a SELECT * on the table.
Correct behavior should be to pick a valid filename such as:
000000_0_copy_1.gz</description>
			<version>0.8.0</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
	</bug>
	<bug id="2309" opendate="2011-07-26 22:11:58" fixdate="2011-07-27 23:23:31" resolution="Fixed">
		<buginformation>
			<summary>Incorrect regular expression for extracting task id from filename</summary>
			<description>For producing the correct filenames for bucketed tables, there is a method in Utilities.java that extracts out the task id from the filename and replaces it with the bucket number. There is a bug in the regex that is used to extract this value for attempt numbers &amp;gt;= 10:


&amp;gt;&amp;gt;&amp;gt; re.match("^.*?([0-9]+)(_[0-9])?(\\..*)?$", &amp;amp;apos;attempt_201107090429_64965_m_001210_10&amp;amp;apos;).group(1)
&amp;amp;apos;10&amp;amp;apos;
&amp;gt;&amp;gt;&amp;gt; re.match("^.*?([0-9]+)(_[0-9])?(\\..*)?$", &amp;amp;apos;attempt_201107090429_64965_m_001210_9&amp;amp;apos;).group(1)
&amp;amp;apos;001210&amp;amp;apos;

</description>
			<version>0.7.1</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">6309</link>
		</links>
	</bug>
	<bug id="2080" opendate="2011-03-29 06:42:52" fixdate="2011-07-29 18:52:04" resolution="Fixed">
		<buginformation>
			<summary>Few code improvements in the ql and serde packages.</summary>
			<description>Few code improvements in the ql and serde packages.
1) Little performance Improvements 
2) Null checks to avoid NPEs
3) Effective varaible management.</description>
			<version>0.7.0</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.UnionOperator.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeFunction.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeFieldType.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.ASTNode.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FilterOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.SelectOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TaskFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Operator.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeField.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
		</fixedFiles>
	</bug>
	<bug id="2183" opendate="2011-05-25 06:28:19" fixdate="2011-07-29 18:58:05" resolution="Fixed">
		<buginformation>
			<summary>In Task class and its subclasses logger is initialized in constructor</summary>
			<description>In Task class and its subclasses logger is initialized in constructor. Log object no need to initialize every time in the constructor, Log object can make it as static object.

Ex:
  public ExecDriver() {
    super();
    LOG = LogFactory.getLog(this.getClass().getName());
    console = new LogHelper(LOG);
    this.jobExecHelper = new HadoopJobExecHelper(job, console, this, this);
  }


Need to change like this

private static final Log LOG = LogFactory.getLog(ExecDriver.class);


</description>
			<version>0.5.0</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.FunctionSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FetchTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Task.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FunctionTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.StatsTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.CopyTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapredLocalTask.java</file>
		</fixedFiles>
	</bug>
	<bug id="2298" opendate="2011-07-21 19:03:10" fixdate="2011-08-05 04:10:12" resolution="Fixed">
		<buginformation>
			<summary>Fix UDAFPercentile to tolerate null percentiles</summary>
			<description>UDAFPercentile when passed null percentile list will throw a null pointer exception.
Submitting a small fix for that.</description>
			<version>0.7.0</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDAFPercentile.java</file>
		</fixedFiles>
	</bug>
	<bug id="1631" opendate="2010-09-11 00:44:19" fixdate="2011-08-08 17:42:45" resolution="Fixed">
		<buginformation>
			<summary>JDBC driver returns wrong precision, scale, or column size for some data types</summary>
			<description>For some data types, these methods return values that do not conform to the JDBC spec:
org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.getPrecision(int)
org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.getScale(int)
org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.getColumnDisplaySize(int)
org.apache.hadoop.hive.jdbc.JdbcColumn.getColumnSize()</description>
			<version>0.7.0</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.JdbcColumn.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="is blocked by">2358</link>
		</links>
	</bug>
	<bug id="2138" opendate="2011-05-01 00:19:23" fixdate="2011-08-08 19:07:51" resolution="Fixed">
		<buginformation>
			<summary>Exception when no splits returned from index</summary>
			<description>Running a query that uses indexing but doesn&amp;amp;apos;t return any results give an exception.

 java.lang.IllegalArgumentException: Can not create a Path from an empty string
at org.apache.hadoop.fs.Path.checkPathArg(Path.java:82)
at org.apache.hadoop.fs.Path.&amp;lt;init&amp;gt;(Path.java:90)
at org.apache.hadoop.util.StringUtils.stringToPath(StringUtils.java:224)
at org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:282)
at org.apache.hadoop.hive.ql.index.HiveIndexedInputFormat.getSplits(HiveIndexedInputFormat.java:123) 

This could potentially be fixed by creating a new empty file to use for the splits.
Once this is fixed, the index_auto_test_if_used.q can be used.</description>
			<version>0.8.0</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.index.HiveIndexedInputFormat.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">1644</link>
		</links>
	</bug>
	<bug id="2344" opendate="2011-08-03 21:44:35" fixdate="2011-08-11 20:08:56" resolution="Fixed">
		<buginformation>
			<summary>filter is removed due to regression of HIVE-1538</summary>
			<description> select * from 
 (
 select type_bucket,randum123
 from (SELECT *, cast(rand() as double) AS randum123 FROM tbl where ds = ...) a
 where randum123 &amp;lt;=0.1)s where s.randum123&amp;gt;0.1 limit 20;
This is returning results...
and 
 explain
 select type_bucket,randum123
 from (SELECT *, cast(rand() as double) AS randum123 FROM tbl where ds = ...) a
 where randum123 &amp;lt;=0.1
shows that there is no filter.</description>
			<version>0.8.0</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">1538</link>
		</links>
	</bug>
	<bug id="2358" opendate="2011-08-08 21:43:51" fixdate="2011-08-15 07:23:48" resolution="Fixed">
		<buginformation>
			<summary>JDBC DatabaseMetaData and ResultSetMetaData need to match for particular types</summary>
			<description>My patch for HIVE-1631 did not ensure the following (from comment on 1631):
-------------
Mythili Gopalakrishnan added a comment - 08/Aug/11 08:42
Just tested this fix and does NOT work correctly. Here are my findings on a FLOAT column
Without Patch on a FLOAT Column
--------------------------------
DatabaseMetaData.getColumns () COLUMN_SIZE returns 12
DatabaseMetaData.getColumns () DECIMAL_DIGITS - returns 0
ResultSetMetaData.getPrecision() returns 0
ResultSetMetaData.getScale() returns 0
With Patch on a FLOAT Column
----------------------------
DatabaseMetaData.getColumns () COLUMN_SIZE returns 24
DatabaseMetaData.getColumns () DECIMAL_DIGITS - returns 0
ResultSetMetaData.getPrecision() returns 7
ResultSetMetaData.getScale() returns 7
Also both DatabaseMetadata and ResultSetMetaData must return the same information for Precision and Scale for FLOAT,DOUBLE types.</description>
			<version>0.8.0</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.JdbcColumn.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">1631</link>
		</links>
	</bug>
	<bug id="2315" opendate="2011-07-27 16:46:38" fixdate="2011-08-18 21:39:27" resolution="Fixed">
		<buginformation>
			<summary>DatabaseMetadata.getColumns() does not return partition column names for a table</summary>
			<description>getColumns() method of DatabaseMetadata for HIVE JDBC Driver does not return the partition column names. Where as from HIVE CLI, if you do a &amp;amp;apos;describe tablename&amp;amp;apos; you get all columns including the partition columns. It would be nice if getColumns() method returns all columns.</description>
			<version>0.7.1</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">1573</link>
			<link type="Duplicate" description="is duplicated by">1573</link>
			<link type="Reference" description="relates to">1573</link>
		</links>
	</bug>
	<bug id="1573" opendate="2010-08-20 00:33:41" fixdate="2011-08-18 21:40:31" resolution="Duplicate">
		<buginformation>
			<summary>select * from partitioned table via JDBC does not return partition columns</summary>
			<description>select * from partitioned table via JDBC does not return partition columns. This behavior is different from via CLI.</description>
			<version>0.7.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">2315</link>
			<link type="Duplicate" description="is duplicated by">2315</link>
			<link type="Reference" description="is related to">2315</link>
		</links>
	</bug>
	<bug id="2334" opendate="2011-08-02 21:57:29" fixdate="2011-08-19 18:55:57" resolution="Fixed">
		<buginformation>
			<summary>DESCRIBE TABLE causes NPE when hive.cli.print.header=true</summary>
			<description></description>
			<version>0.7.1</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.cli.CliDriver.java</file>
		</fixedFiles>
	</bug>
	<bug id="2382" opendate="2011-08-17 00:55:51" fixdate="2011-08-30 22:18:24" resolution="Fixed">
		<buginformation>
			<summary>Invalid predicate pushdown from incorrect column expression map for select operator generated by GROUP BY operation</summary>
			<description>When a GROUP BY is specified, a select operator is added before the GROUP BY in SemanticAnalyzer.insertSelectAllPlanForGroupBy.  Currently, the column expression map for this is set to the column expression map for the parent operator.  This behavior is incorrect as, for example, the parent operator could rearrange the order of the columns (_col0 =&amp;gt; _col0, _col1 =&amp;gt; _col2, _col2 =&amp;gt; _col1) and the new operator should not repeat this.
The predicate pushdown optimization uses the column expression map to track which columns a filter expression refers to at different operators.  This results in a filter on incorrect columns.
Here is a simple case of this going wrong: Using

create table invites (id int, foo int, bar int);


executing the query

explain select * from (select foo, bar from (select bar, foo from invites c union all select bar, foo from invites d) b) a group by bar, foo having bar=1;


results in

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Alias -&amp;gt; Map Operator Tree:
        a-subquery1:b-subquery1:c 
          TableScan
            alias: c
            Filter Operator
              predicate:
                  expr: (foo = 1)
                  type: boolean
              Select Operator
                expressions:
                      expr: bar
                      type: int
                      expr: foo
                      type: int
                outputColumnNames: _col0, _col1
                Union
                  Select Operator
                    expressions:
                          expr: _col1
                          type: int
                          expr: _col0
                          type: int
                    outputColumnNames: _col0, _col1
                    Select Operator
                      expressions:
                            expr: _col0
                            type: int
                            expr: _col1
                            type: int
                      outputColumnNames: _col0, _col1
                      Group By Operator
                        bucketGroup: false
                        keys:
                              expr: _col1
                              type: int
                              expr: _col0
                              type: int
                        mode: hash
                        outputColumnNames: _col0, _col1
                        Reduce Output Operator
                          key expressions:
                                expr: _col0
                                type: int
                                expr: _col1
                                type: int
                          sort order: ++
                          Map-reduce partition columns:
                                expr: _col0
                                type: int
                                expr: _col1
                                type: int
                          tag: -1
        a-subquery2:b-subquery2:d 
          TableScan
            alias: d
            Filter Operator
              predicate:
                  expr: (foo = 1)
                  type: boolean
              Select Operator
                expressions:
                      expr: bar
                      type: int
                      expr: foo
                      type: int
                outputColumnNames: _col0, _col1
                Union
                  Select Operator
                    expressions:
                          expr: _col1
                          type: int
                          expr: _col0
                          type: int
                    outputColumnNames: _col0, _col1
                    Select Operator
                      expressions:
                            expr: _col0
                            type: int
                            expr: _col1
                            type: int
                      outputColumnNames: _col0, _col1
                      Group By Operator
                        bucketGroup: false
                        keys:
                              expr: _col1
                              type: int
                              expr: _col0
                              type: int
                        mode: hash
                        outputColumnNames: _col0, _col1
                        Reduce Output Operator
                          key expressions:
                                expr: _col0
                                type: int
                                expr: _col1
                                type: int
                          sort order: ++
                          Map-reduce partition columns:
                                expr: _col0
                                type: int
                                expr: _col1
                                type: int
                          tag: -1
      Reduce Operator Tree:
        Group By Operator
          bucketGroup: false
          keys:
                expr: KEY._col0
                type: int
                expr: KEY._col1
                type: int
          mode: mergepartial
          outputColumnNames: _col0, _col1
          Select Operator
            expressions:
                  expr: _col0
                  type: int
                  expr: _col1
                  type: int
            outputColumnNames: _col0, _col1
            File Output Operator
              compressed: false
              GlobalTableId: 0
              table:
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat

  Stage: Stage-0
    Fetch Operator
      limit: -1


Note that the filter is now "foo = 1", while the correct behavior is to have "bar = 1".  If we remove the group by, the behavior is correct.</description>
			<version>0.6.0</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
		<links>
			<link type="Incorporates" description="is part of">1342</link>
		</links>
	</bug>
	<bug id="2184" opendate="2011-05-25 06:54:24" fixdate="2011-08-31 16:22:52" resolution="Fixed">
		<buginformation>
			<summary>Few improvements in org.apache.hadoop.hive.ql.metadata.Hive.close()</summary>
			<description>1)Hive.close() will call HiveMetaStoreClient.close() in this method the variable "standAloneClient" is never become true then client.shutdown() never call.
2)Hive.close() After calling metaStoreClient.close() need to make metaStoreClient=null</description>
			<version>0.5.0</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">3067</link>
			<link type="Reference" description="relates to">3057</link>
		</links>
	</bug>
	<bug id="2383" opendate="2011-08-17 00:55:55" fixdate="2011-08-31 23:42:00" resolution="Fixed">
		<buginformation>
			<summary>Incorrect alias filtering for predicate pushdown</summary>
			<description>The predicate pushdown optimizer starts at the topmost operators traverses the operator tree, at each stage collecting predicates to be pushed down.  At each operator, ive.ql.ppd.OpProcFactory.DefaultPPD.mergeWithChildrenPred is called, which merges the predicates of the children nodes into the current node.  The predicates are stored in hive.ql.ppd.ExprWalkerInfo.pushdownPreds as a map from the alias a predicate refers to (a predicate may only refer to one alias at a time as only such predicates can be pushed) to a list of such predicates.  Since at each stage the alias the predicate refers to may change (subqueries may change aliases), this is updated for each operator (hive.ql.ppd.ExprWalkerProcFactory.extractPushdownPreds is called which walks the ExprNodeDesc for each predicate). When a JoinOperator is encountered, mergeWithChildrenPred is passed an optional parameter "aliases" which contains a set of aliases that can be pushed per ansi semantics (see hive.ql.ppd.OpProcFactory.JoinPPD.getQualifiedAliases).  The part that is incorrect is that aliases are filtered in mergeWithChildrenPred before extractPushdownPreds is called, which associates the predicates with the correct alias in the current operator&amp;amp;apos;s context while the filtering should happen after.
In test case Q2 below, when the predicate "a.bar=3" comes into the JoinOperator, the alias is "a" coming in so it is accepted for pushdown.  When brought into the JoinOperator&amp;amp;apos;s context, however, since the predicate refers to b.foo in the inner scope, we should not actually accept this for pushdown.
With the test cases

-- Q1: predicate should not be pushed on the right side of a left outer join (this is correct in trunk)
explain
SELECT a.foo as foo1, b.foo as foo2, b.bar
FROM pokes a LEFT OUTER JOIN pokes2 b
ON a.foo=b.foo
WHERE b.bar=3;

-- Q2: predicate should not be pushed on the right side of a left outer join (this is broken in trunk)
explain
SELECT * FROM
    (SELECT a.foo as foo1, b.foo as foo2, b.bar
    FROM pokes a LEFT OUTER JOIN pokes2 b
    ON a.foo=b.foo) a
WHERE a.bar=3;

-- Q3: predicate should be pushed (this is correct in trunk)
explain
SELECT * FROM
    (SELECT a.foo as foo1, b.foo as foo2, a.bar
    FROM pokes a JOIN pokes2 b
    ON a.foo=b.foo) a
WHERE a.bar=3;


The current output is

hive&amp;gt; 
    &amp;gt; -- Q1: predicate should not be pushed on the right side of a left outer join
    &amp;gt; explain
    &amp;gt; SELECT a.foo as foo1, b.foo as foo2, b.bar
    &amp;gt; FROM pokes a LEFT OUTER JOIN pokes2 b
    &amp;gt; ON a.foo=b.foo
    &amp;gt; WHERE b.bar=3;
OK
ABSTRACT SYNTAX TREE:
  (TOK_QUERY (TOK_FROM (TOK_LEFTOUTERJOIN (TOK_TABREF (TOK_TABNAME pokes) a) (TOK_TABREF (TOK_TABNAME pokes2) b) (= (. (TOK_TABLE_OR_COL a) foo) (. (TOK_TABLE_OR_COL b) foo)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (. (TOK_TABLE_OR_COL a) foo) foo1) (TOK_SELEXPR (. (TOK_TABLE_OR_COL b) foo) foo2) (TOK_SELEXPR (. (TOK_TABLE_OR_COL b) bar))) (TOK_WHERE (= (. (TOK_TABLE_OR_COL b) bar) 3))))

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Alias -&amp;gt; Map Operator Tree:
        a 
          TableScan
            alias: a
            Reduce Output Operator
              key expressions:
                    expr: foo
                    type: int
              sort order: +
              Map-reduce partition columns:
                    expr: foo
                    type: int
              tag: 0
              value expressions:
                    expr: foo
                    type: int
        b 
          TableScan
            alias: b
            Reduce Output Operator
              key expressions:
                    expr: foo
                    type: int
              sort order: +
              Map-reduce partition columns:
                    expr: foo
                    type: int
              tag: 1
              value expressions:
                    expr: foo
                    type: int
                    expr: bar
                    type: int
      Reduce Operator Tree:
        Join Operator
          condition map:
               Left Outer Join0 to 1
          condition expressions:
            0 {VALUE._col0}
            1 {VALUE._col0} {VALUE._col1}
          handleSkewJoin: false
          outputColumnNames: _col0, _col4, _col5
          Filter Operator
            predicate:
                expr: (_col5 = 3)
                type: boolean
            Select Operator
              expressions:
                    expr: _col0
                    type: int
                    expr: _col4
                    type: int
                    expr: _col5
                    type: int
              outputColumnNames: _col0, _col1, _col2
              File Output Operator
                compressed: false
                GlobalTableId: 0
                table:
                    input format: org.apache.hadoop.mapred.TextInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat

  Stage: Stage-0
    Fetch Operator
      limit: -1


Time taken: 0.113 seconds
hive&amp;gt; 
    &amp;gt; -- Q2: predicate should not be pushed on the right side of a left outer join
    &amp;gt; explain
    &amp;gt; SELECT * FROM
    &amp;gt;     (SELECT a.foo as foo1, b.foo as foo2, b.bar
    &amp;gt;     FROM pokes a LEFT OUTER JOIN pokes2 b
    &amp;gt;     ON a.foo=b.foo) a
    &amp;gt; WHERE a.bar=3;
OK
ABSTRACT SYNTAX TREE:
  (TOK_QUERY (TOK_FROM (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_LEFTOUTERJOIN (TOK_TABREF (TOK_TABNAME pokes) a) (TOK_TABREF (TOK_TABNAME pokes2) b) (= (. (TOK_TABLE_OR_COL a) foo) (. (TOK_TABLE_OR_COL b) foo)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (. (TOK_TABLE_OR_COL a) foo) foo1) (TOK_SELEXPR (. (TOK_TABLE_OR_COL b) foo) foo2) (TOK_SELEXPR (. (TOK_TABLE_OR_COL b) bar))))) a)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (= (. (TOK_TABLE_OR_COL a) bar) 3))))

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Alias -&amp;gt; Map Operator Tree:
        a:a 
          TableScan
            alias: a
            Reduce Output Operator
              key expressions:
                    expr: foo
                    type: int
              sort order: +
              Map-reduce partition columns:
                    expr: foo
                    type: int
              tag: 0
              value expressions:
                    expr: foo
                    type: int
        a:b 
          TableScan
            alias: b
            Filter Operator
              predicate:
                  expr: (bar = 3)
                  type: boolean
              Reduce Output Operator
                key expressions:
                      expr: foo
                      type: int
                sort order: +
                Map-reduce partition columns:
                      expr: foo
                      type: int
                tag: 1
                value expressions:
                      expr: foo
                      type: int
                      expr: bar
                      type: int
      Reduce Operator Tree:
        Join Operator
          condition map:
               Left Outer Join0 to 1
          condition expressions:
            0 {VALUE._col0}
            1 {VALUE._col0} {VALUE._col1}
          handleSkewJoin: false
          outputColumnNames: _col0, _col4, _col5
          Select Operator
            expressions:
                  expr: _col0
                  type: int
                  expr: _col4
                  type: int
                  expr: _col5
                  type: int
            outputColumnNames: _col0, _col1, _col2
            Select Operator
              expressions:
                    expr: _col0
                    type: int
                    expr: _col1
                    type: int
                    expr: _col2
                    type: int
              outputColumnNames: _col0, _col1, _col2
              File Output Operator
                compressed: false
                GlobalTableId: 0
                table:
                    input format: org.apache.hadoop.mapred.TextInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat

  Stage: Stage-0
    Fetch Operator
      limit: -1


Time taken: 0.101 seconds
hive&amp;gt; 
    &amp;gt; -- Q3: predicate should be pushed
    &amp;gt; explain
    &amp;gt; SELECT * FROM
    &amp;gt;     (SELECT a.foo as foo1, b.foo as foo2, a.bar
    &amp;gt;     FROM pokes a JOIN pokes2 b
    &amp;gt;     ON a.foo=b.foo) a
    &amp;gt; WHERE a.bar=3;
OK
ABSTRACT SYNTAX TREE:
  (TOK_QUERY (TOK_FROM (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_TABREF (TOK_TABNAME pokes) a) (TOK_TABREF (TOK_TABNAME pokes2) b) (= (. (TOK_TABLE_OR_COL a) foo) (. (TOK_TABLE_OR_COL b) foo)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (. (TOK_TABLE_OR_COL a) foo) foo1) (TOK_SELEXPR (. (TOK_TABLE_OR_COL b) foo) foo2) (TOK_SELEXPR (. (TOK_TABLE_OR_COL a) bar))))) a)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (= (. (TOK_TABLE_OR_COL a) bar) 3))))

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Alias -&amp;gt; Map Operator Tree:
        a:a 
          TableScan
            alias: a
            Filter Operator
              predicate:
                  expr: (bar = 3)
                  type: boolean
              Reduce Output Operator
                key expressions:
                      expr: foo
                      type: int
                sort order: +
                Map-reduce partition columns:
                      expr: foo
                      type: int
                tag: 0
                value expressions:
                      expr: foo
                      type: int
                      expr: bar
                      type: int
        a:b 
          TableScan
            alias: b
            Reduce Output Operator
              key expressions:
                    expr: foo
                    type: int
              sort order: +
              Map-reduce partition columns:
                    expr: foo
                    type: int
              tag: 1
              value expressions:
                    expr: foo
                    type: int
      Reduce Operator Tree:
        Join Operator
          condition map:
               Inner Join 0 to 1
          condition expressions:
            0 {VALUE._col0} {VALUE._col1}
            1 {VALUE._col0}
          handleSkewJoin: false
          outputColumnNames: _col0, _col1, _col4
          Select Operator
            expressions:
                  expr: _col0
                  type: int
                  expr: _col4
                  type: int
                  expr: _col1
                  type: int
            outputColumnNames: _col0, _col1, _col2
            Select Operator
              expressions:
                    expr: _col0
                    type: int
                    expr: _col1
                    type: int
                    expr: _col2
                    type: int
              outputColumnNames: _col0, _col1, _col2
              File Output Operator
                compressed: false
                GlobalTableId: 0
                table:
                    input format: org.apache.hadoop.mapred.TextInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat

  Stage: Stage-0
    Fetch Operator
      limit: -1


Note that Q2 is incorrect because the predicate "bar = 3" is incorrectly pushed to the right side of the left outer join (Q1 and Q3 are correct).</description>
			<version>0.6.0</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.ppd.ExprWalkerInfo.java</file>
			<file type="M">org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.ppd.ExprWalkerProcFactory.java</file>
		</fixedFiles>
		<links>
			<link type="Incorporates" description="is part of">1342</link>
		</links>
	</bug>
	<bug id="2369" opendate="2011-08-11 17:42:30" fixdate="2011-09-07 03:04:42" resolution="Fixed">
		<buginformation>
			<summary>Minor typo in error message in HiveConnection.java (JDBC)</summary>
			<description>There is a minor typo issue in HiveConnection.java (jdbc) :

throw new SQLException("Could not establish connecton to "
            + uri + ": " + e.getMessage(), "08S01");

It seems like there&amp;amp;apos;s a "i" missing.
I know it&amp;amp;apos;s a very minor typo but I report it anyway. I won&amp;amp;apos;t attach a patch because it would be too long for me to SVN checkout just for 1 letter.</description>
			<version>0.7.1</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveConnection.java</file>
		</fixedFiles>
	</bug>
	<bug id="2402" opendate="2011-08-24 13:55:02" fixdate="2011-09-08 23:38:09" resolution="Fixed">
		<buginformation>
			<summary>Function like with empty string is throwing null pointer exception</summary>
			<description>select emp.ename from emp where ename like &amp;amp;apos;&amp;amp;apos;
This query is throwing null pointer exception</description>
			<version>0.8.0</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFLike.java</file>
		</fixedFiles>
	</bug>
	<bug id="2182" opendate="2011-05-25 06:17:08" fixdate="2011-09-14 17:13:27" resolution="Fixed">
		<buginformation>
			<summary>Avoid null pointer exception when executing UDF</summary>
			<description>For using UDF&amp;amp;apos;s executed following steps

add jar /home/udf/udf.jar;
create temporary function grade as &amp;amp;apos;udf.Grade&amp;amp;apos;;
select m.userid,m.name,grade(m.maths,m.physics,m.chemistry) from marks m;


But from the above steps if we miss the first step (add jar) and execute remaining steps

create temporary function grade as &amp;amp;apos;udf.Grade&amp;amp;apos;;
select m.userid,m.name,grade(m.maths,m.physics,m.chemistry) from marks m;


In tasktracker it is throwing this exception

Caused by: java.lang.RuntimeException: Map operator initialization failed
		 at org.apache.hadoop.hive.ql.exec.ExecMapper.configure(ExecMapper.java:121)
		 ... 18 more
Caused by: java.lang.RuntimeException: java.lang.NullPointerException
		 at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:115)
		 at org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.initialize(GenericUDFBridge.java:126)
		 at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator.initialize(ExprNodeGenericFuncEvaluator.java:133)
		 at org.apache.hadoop.hive.ql.exec.Operator.initEvaluators(Operator.java:878)
		 at org.apache.hadoop.hive.ql.exec.Operator.initEvaluatorsAndReturnStruct(Operator.java:904)
		 at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:60)
		 at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:357)
		 at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:433)
		 at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:389)
		 at org.apache.hadoop.hive.ql.exec.TableScanOperator.initializeOp(TableScanOperator.java:133)
		 at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:357)
		 at org.apache.hadoop.hive.ql.exec.MapOperator.initializeOp(MapOperator.java:444)
		 at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:357)
		 at org.apache.hadoop.hive.ql.exec.ExecMapper.configure(ExecMapper.java:98)
		 ... 18 more
Caused by: java.lang.NullPointerException
		 at java.util.concurrent.ConcurrentHashMap.get(ConcurrentHashMap.java:768)
		 at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:107)
		 ... 31 more


Instead of null pointer exception it should throw meaning full exception</description>
			<version>0.5.0</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.java</file>
		</fixedFiles>
	</bug>
	<bug id="2398" opendate="2011-08-19 22:18:23" fixdate="2011-09-15 20:16:55" resolution="Fixed">
		<buginformation>
			<summary>Hive server doesn&amp;apos;t return schema for &amp;apos;set&amp;apos; command</summary>
			<description>The Hive server does process the CLI commands like &amp;amp;apos;set&amp;amp;apos;, &amp;amp;apos;set -v&amp;amp;apos; sent by ODBC or JDBC clients. But currently only the data is returned to client but not schema for that resultset. This makes it unusable for a ODBC or JDBC client to use this option.</description>
			<version>0.7.1</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.service.HiveServer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.processors.SetProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.java</file>
		</fixedFiles>
	</bug>
	<bug id="2181" opendate="2011-05-24 09:47:28" fixdate="2011-09-23 17:21:29" resolution="Fixed">
		<buginformation>
			<summary> Clean up the scratch.dir (tmp/hive-root) while restarting Hive server. </summary>
			<description>Now queries leaves the map outputs under scratch.dir after execution. If the hive server is stopped we need not keep the stopped server&amp;amp;apos;s map oputputs. So whle starting the server we can clear the scratch.dir. This can help in improved disk usage.</description>
			<version>0.8.0</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.service.TestHiveServer.java</file>
			<file type="M">org.apache.hadoop.hive.service.HiveServer.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">10415</link>
			<link type="Reference" description="is related to">13429</link>
		</links>
	</bug>
	<bug id="2455" opendate="2011-09-18 07:49:19" fixdate="2011-10-11 18:26:17" resolution="Fixed">
		<buginformation>
			<summary>Pass correct remoteAddress in proxy user authentication</summary>
			<description></description>
			<version>0.7.1</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.thrift.TestHadoop20SAuthBridge.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.java</file>
		</fixedFiles>
	</bug>
	<bug id="2499" opendate="2011-10-12 08:24:00" fixdate="2011-10-13 20:36:36" resolution="Fixed">
		<buginformation>
			<summary>small table filesize for automapjoin is not consistent in HiveConf.java and hive-default.xml</summary>
			<description></description>
			<version>0.7.1</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
	</bug>
	<bug id="2497" opendate="2011-10-12 02:27:43" fixdate="2011-10-25 03:45:13" resolution="Fixed">
		<buginformation>
			<summary>partition pruning  prune some right partition under specific conditions</summary>
			<description>create table src3(key string, value string) partitioned by (pt string)
row format delimited fields terminated by &amp;amp;apos;,&amp;amp;apos;;
ALTER TABLE src3 ADD IF NOT EXISTS PARTITION (pt=&amp;amp;apos;20110911000000&amp;amp;apos;) ;
ALTER TABLE src3 ADD IF NOT EXISTS PARTITION (pt=&amp;amp;apos;20110912000000&amp;amp;apos;) ;
ALTER TABLE src3 ADD IF NOT EXISTS PARTITION (pt=&amp;amp;apos;20110913000000&amp;amp;apos;) ;
explain extended
select user_id 
from
 (
   select 
    cast(key as int) as user_id
    ,case when (value like &amp;amp;apos;aaa%&amp;amp;apos; or value like &amp;amp;apos;vvv%&amp;amp;apos;)
            then 1
            else 0  end as tag_student
   from src3
 ) sub
where sub.tag_student &amp;gt; 0;
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 is a root stage
STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Alias -&amp;gt; Map Operator Tree:
        sub:src3 
          TableScan
            alias: src3
            Filter Operator
              isSamplingPred: false
              predicate:
                  expr: (CASE WHEN (((value like &amp;amp;apos;aaa%&amp;amp;apos;) or (value like &amp;amp;apos;vvv%&amp;amp;apos;))) THEN (1) ELSE (0) END &amp;gt; 0)
                  type: boolean
              Select Operator
                expressions:
                      expr: UDFToInteger(key)
                      type: int
                      expr: CASE WHEN (((value like &amp;amp;apos;aaa%&amp;amp;apos;) or (value like &amp;amp;apos;vvv%&amp;amp;apos;))) THEN (1) ELSE (0) END
                      type: int
                outputColumnNames: _col0, _col1
                Filter Operator
                  isSamplingPred: false
                  predicate:
                      expr: (_col1 &amp;gt; 0)
                      type: boolean
                  Select Operator
                    expressions:
                          expr: _col0
                          type: int
                    outputColumnNames: _col0
                    File Output Operator
                      compressed: false
                      GlobalTableId: 0
                      directory: hdfs://localhost:54310/tmp/hive-tianzhao/hive_2011-10-11_19-26-12_894_9085644225727185586/-ext-10001
                      NumFilesPerFileSink: 1
                      table:
                          input format: org.apache.hadoop.mapred.TextInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                          properties:
                            columns _col0
                            columns.types int
                            serialization.format 1
                      TotalFiles: 1
                      MultiFileSpray: false
      Needs Tagging: false
  Stage: Stage-0
    Fetch Operator
      limit: -1
if we set hive.optimize.ppd=false;
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 is a root stage
STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Alias -&amp;gt; Map Operator Tree:
        sub:src3 
          TableScan
            alias: src3
            Select Operator
              expressions:
                    expr: UDFToInteger(key)
                    type: int
                    expr: CASE WHEN (((value like &amp;amp;apos;aaa%&amp;amp;apos;) or (value like &amp;amp;apos;vvv%&amp;amp;apos;))) THEN (1) ELSE (0) END
                    type: int
              outputColumnNames: _col0, _col1
              Filter Operator
                isSamplingPred: false
                predicate:
                    expr: (_col1 &amp;gt; 0)
                    type: boolean
                Select Operator
                  expressions:
                        expr: _col0
                        type: int
                  outputColumnNames: _col0
                  File Output Operator
                    compressed: false
                    GlobalTableId: 0
                    directory: hdfs://localhost:54310/tmp/hive-tianzhao/hive_2011-10-11_19-27-22_527_1729287213481398480/-ext-10001
                    NumFilesPerFileSink: 1
                    table:
                        input format: org.apache.hadoop.mapred.TextInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                        properties:
                          columns _col0
                          columns.types int
                          serialization.format 1
                    TotalFiles: 1
                    MultiFileSpray: false
      Needs Tagging: false
      Path -&amp;gt; Alias:
        hdfs://localhost:54310/user/hive/warehouse/src3/pt=20110911000000 [sub:src3]
        hdfs://localhost:54310/user/hive/warehouse/src3/pt=20110912000000 [sub:src3]
        hdfs://localhost:54310/user/hive/warehouse/src3/pt=20110913000000 [sub:src3]
      Path -&amp;gt; Partition:
        hdfs://localhost:54310/user/hive/warehouse/src3/pt=20110911000000 
          Partition
            base file name: pt=20110911000000
</description>
			<version>0.7.1</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ppr.ExprProcFactory.java</file>
		</fixedFiles>
	</bug>
	<bug id="1850" opendate="2010-12-14 13:18:20" fixdate="2011-10-26 04:50:03" resolution="Fixed">
		<buginformation>
			<summary>alter table set serdeproperties bypasses regexps checks (leaves table in a non-recoverable state?)</summary>
			<description>

create table aa ( test STRING )
  ROW FORMAT SERDE &amp;amp;apos;org.apache.hadoop.hive.contrib.serde2.RegexSerDe&amp;amp;apos;
  WITH SERDEPROPERTIES ("input.regex" = "[^\\](.*)", "output.format.string" = "$1s");


This will fail. Great!


create table aa ( test STRING )
  ROW FORMAT SERDE &amp;amp;apos;org.apache.hadoop.hive.contrib.serde2.RegexSerDe&amp;amp;apos;
  WITH SERDEPROPERTIES ("input.regex" = "(.*)", "output.format.string" = "$1s");


Works, no problem there.


alter table aa set serdeproperties ("input.regex" = "[^\\](.*)", "output.format.string" = "$1s");


Wups... I can set that without any problems!


alter table aa set serdeproperties ("input.regex" = "(.*)", "output.format.string" = "$1s");
FAILED: Hive Internal Error: java.util.regex.PatternSyntaxException(Unclosed character class near index 7
[^\](.*)
       ^)
java.util.regex.PatternSyntaxException: Unclosed character class near index 7
[^\](.*)
       ^
	at java.util.regex.Pattern.error(Pattern.java:1713)
	at java.util.regex.Pattern.clazz(Pattern.java:2254)
	at java.util.regex.Pattern.sequence(Pattern.java:1818)
	at java.util.regex.Pattern.expr(Pattern.java:1752)
	at java.util.regex.Pattern.compile(Pattern.java:1460)
	at java.util.regex.Pattern.&amp;lt;init&amp;gt;(Pattern.java:1133)
	at java.util.regex.Pattern.compile(Pattern.java:847)
	at org.apache.hadoop.hive.contrib.serde2.RegexSerDe.initialize(RegexSerDe.java:101)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:199)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:253)
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:484)
	at org.apache.hadoop.hive.ql.metadata.Table.checkValidity(Table.java:161)
	at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:803)
	at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableSerdeProps(DDLSemanticAnalyzer.java:558)
	at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeInternal(DDLSemanticAnalyzer.java:232)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:238)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:335)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:686)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:142)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:216)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:370)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:156)


After this, all further commands on the table fails, including drop table 
1. The alter table command should probably check the regexp just like the create table command does
2. Even though the regexp is bad, it should be possible to do things like set the regexp again or drop the table.</description>
			<version>0.7.0</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Table.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
		</fixedFiles>
	</bug>
	<bug id="1975" opendate="2011-02-08 15:34:46" fixdate="2011-10-29 00:51:33" resolution="Fixed">
		<buginformation>
			<summary>"insert overwrite directory" Not able to insert data with multi level directory path</summary>
			<description>Below query execution is failed
Ex:

   insert overwrite directory &amp;amp;apos;/HIVEFT25686/chinna/&amp;amp;apos; select * from dept_j;

</description>
			<version>0.5.0</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
	</bug>
	<bug id="1592" opendate="2010-08-24 22:54:20" fixdate="2011-10-29 18:19:06" resolution="Fixed">
		<buginformation>
			<summary>ProxyFileSystem.close calls super.close twice.</summary>
			<description>  public void close() throws IOException 
{
    super.close();
    super.close();
  }</description>
			<version>0.7.0</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.fs.ProxyFileSystem.java</file>
		</fixedFiles>
	</bug>
	<bug id="2465" opendate="2011-09-23 08:28:10" fixdate="2011-11-03 17:17:22" resolution="Fixed">
		<buginformation>
			<summary>Primitive Data Types returning null if the data is out of range of the data type.</summary>
			<description>Primitive Data Types returning null if the input data is out of range of the data type. In this case it is better to log the message with the proper message and actual data then user get to know some data is missing.</description>
			<version>0.9.0</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyDouble.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyInteger.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyPrimitive.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyBoolean.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyShort.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyLong.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyByte.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyTimestamp.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyBinary.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyFloat.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.TestLazyPrimitive.java</file>
		</fixedFiles>
	</bug>
	<bug id="2214" opendate="2011-06-10 21:00:55" fixdate="2011-11-03 19:17:15" resolution="Fixed">
		<buginformation>
			<summary>CommandNeedRetryException.java is missing ASF header</summary>
			<description>Please add one.</description>
			<version>0.8.0</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.CommandNeedRetryException.java</file>
		</fixedFiles>
	</bug>
	<bug id="11" opendate="2008-09-19 08:46:59" fixdate="2011-11-06 21:22:58" resolution="Fixed">
		<buginformation>
			<summary>better error code from Hive describe command</summary>
			<description>cryptic, non-informative error message
hive&amp;gt; describe hive1_scribeloadertest
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
in this case the table was missing. better say that.
</description>
			<version>0.3.0</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">1302</link>
			<link type="Duplicate" description="duplicates">2290</link>
		</links>
	</bug>
	<bug id="1302" opendate="2010-04-12 22:30:25" fixdate="2011-11-06 21:57:07" resolution="Fixed">
		<buginformation>
			<summary>describe parse_url throws an error</summary>
			<description>descHive history file=/tmp/njain/hive_job_log_njain_201004121528_1840617354.txt
hive&amp;gt; describe parse_url;
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
hive&amp;gt; describe extended parse_url;
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
hive&amp;gt; [njain@dev029 clientpositive]$ </description>
			<version>0.3.0</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">11</link>
		</links>
	</bug>
	<bug id="2290" opendate="2011-07-18 21:47:13" fixdate="2011-11-06 21:58:22" resolution="Fixed">
		<buginformation>
			<summary>Improve error messages for DESCRIBE command</summary>
			<description></description>
			<version>0.3.0</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">11</link>
			<link type="Reference" description="is related to">1977</link>
		</links>
	</bug>
	<bug id="2178" opendate="2011-05-24 06:35:44" fixdate="2011-11-07 17:56:58" resolution="Fixed">
		<buginformation>
			<summary>Log related Check style Comments fixes</summary>
			<description>Fix Log related Check style Comments</description>
			<version>0.5.0</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveConnection.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Partition.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.dynamic_type.SimpleCharStream.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveDataSource.java</file>
		</fixedFiles>
	</bug>
	<bug id="2466" opendate="2011-09-23 16:39:01" fixdate="2011-11-08 05:53:22" resolution="Fixed">
		<buginformation>
			<summary>mapjoin_subquery  dump small table (mapjoin table) to the same file</summary>
			<description>in mapjoin_subquery.q  there is a query
SELECT /*+ MAPJOIN(z) */ subq.key1, z.value
FROM
(SELECT /*+ MAPJOIN */ x.key as key1, x.value as value1, y.key as key2, y.value as value2 
 FROM src1 x JOIN src y ON (x.key = y.key)) subq
 JOIN srcpart z ON (subq.key1 = z.key and z.ds=&amp;amp;apos;2008-04-08&amp;amp;apos; and z.hr=11);
when dump x and z to a local file,there all dump to the same file, so we lost the data of x</description>
			<version>0.7.1</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.GenMRSkewJoinProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.MapJoinDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapredLocalTask.java</file>
		</fixedFiles>
	</bug>
	<bug id="2196" opendate="2011-06-04 07:33:06" fixdate="2011-11-15 17:59:47" resolution="Fixed">
		<buginformation>
			<summary>Ensure HiveConf includes all properties defined in hive-default.xml</summary>
			<description>There are a bunch of properties that are defined in hive-default.xml but not in HiveConf.</description>
			<version>0.8.0</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">1984</link>
			<link type="Incorporates" description="incorporates">1571</link>
			<link type="Reference" description="relates to">2596</link>
			<link type="Reference" description="relates to">6037</link>
		</links>
	</bug>
	<bug id="2253" opendate="2011-07-04 07:45:19" fixdate="2011-12-01 20:28:05" resolution="Fixed">
		<buginformation>
			<summary>Merge failing of join tree in exceptional case</summary>
			<description>In some very exceptional cases, SemanticAnayzer fails to merge join tree. Example is below.
create table a (val1 int, val2 int)
create table b (val1 int, val2 int)
create table c (val1 int, val2 int)
create table d (val1 int, val2 int)
create table e (val1 int, val2 int)
1. all same(single) join key --&amp;gt; one MR, good
select * from a join b on a.val1=b.val1 join c on a.val1=c.val1 join d on a.val1=d.val1 join e on a.val1=e.val1
2. two join keys --&amp;gt; expected to have two MR, but resulted to three MR
select * from a join b on a.val1=b.val1 join c on a.val1=c.val1 join d on a.val1=d.val1 join e on a.val2=e.val2
3. by changing the join order, we could attain two MR as first-expectation.
select * from a join e on a.val2=e.val2 join c on a.val1=c.val1 join d on a.val1=d.val1 join b on a.val1=b.val1</description>
			<version>0.9.0</version>
			<fixedVersion>0.8.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
	</bug>
	<bug id="2520" opendate="2011-10-21 03:55:36" fixdate="2011-12-10 06:28:27" resolution="Fixed">
		<buginformation>
			<summary>left semi join will duplicate data</summary>
			<description>CREATE TABLE sales (name STRING, id INT)
ROW FORMAT DELIMITED FIELDS TERMINATED BY &amp;amp;apos;\t&amp;amp;apos;;
CREATE TABLE things (id INT, name STRING)
ROW FORMAT DELIMITED FIELDS TERMINATED BY &amp;amp;apos;\t&amp;amp;apos;;
The &amp;amp;apos;sales&amp;amp;apos; table has data in a file: sales.txt, and the data is
Joe 2
Hank 2
The &amp;amp;apos;things&amp;amp;apos; table has data int two files: things.txt and things2.txt
The content of things.txt is :
2 Tie
The content of things2.txt is :
2 Tie
SELECT * FROM sales LEFT SEMI JOIN things ON (sales.id = things.id);
will output
Joe 2
Joe 2
Hank 2
Hank 2
so the result is wrong.
In CommonJoinOperator left semi join should use " genObject(null, 0, new IntermediateObject(new ArrayList[numAliases], 0), true); " to generate data.
but now it uses " genUniqueJoinObject(0, 0); " to generate data.
This patch will solve this problem.</description>
			<version>0.7.0</version>
			<fixedVersion>0.9.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
		</fixedFiles>
	</bug>
	<bug id="2631" opendate="2011-12-07 01:14:05" fixdate="2011-12-16 21:17:13" resolution="Fixed">
		<buginformation>
			<summary>Make Hive work with Hadoop 1.0.0</summary>
			<description>With Hadoop 1.0.0 around the corner ( http://mail-archives.apache.org/mod_mbox/hadoop-general/201111.mbox/%3C9D6B6144-F4E0-4A31-883F-2AC504727A1F%40hortonworks.com%3E ), it will be useful to make Hive work with it.</description>
			<version>0.8.0</version>
			<fixedVersion>0.8.1, 0.9.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.shims.ShimLoader.java</file>
		</fixedFiles>
	</bug>
	<bug id="2705" opendate="2012-01-10 21:08:47" fixdate="2012-01-11 15:28:26" resolution="Fixed">
		<buginformation>
			<summary>SemanticAnalyzer twice swallows an exception it shouldn&amp;apos;t</summary>
			<description>Twice SemanticAnalyzer catches an exception and drops it, just passing on the original message&amp;amp;apos;s in a new SemanticException. This means that those that see the message in the output cannot tell what generated the original exception.  These original exceptions should be wrapped, as they are in other parts of the code.</description>
			<version>0.7.1</version>
			<fixedVersion>0.9.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
	</bug>
	<bug id="2746" opendate="2012-01-24 22:49:36" fixdate="2012-01-26 02:38:24" resolution="Fixed">
		<buginformation>
			<summary>Metastore client doesn&amp;apos;t log properly in case of connection failure to server</summary>
			<description>LOG.error(e.getStackTrace()) in current code prints memory location of StackTraceElement[] instead of message.</description>
			<version>0.8.0</version>
			<fixedVersion>0.9.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
		</fixedFiles>
		<links>
			<link type="dependent" description="is depended upon by">231</link>
		</links>
	</bug>
	<bug id="2735" opendate="2012-01-22 22:10:33" fixdate="2012-01-29 16:35:15" resolution="Fixed">
		<buginformation>
			<summary>PlanUtils.configureTableJobPropertiesForStorageHandler() is not called for partitioned table</summary>
			<description>As a result, if there is a query which results in a MR job which needs to be configured via storage handler, it returns in failure.</description>
			<version>0.7.0</version>
			<fixedVersion>0.9.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
	</bug>
	<bug id="2792" opendate="2012-02-09 02:25:34" fixdate="2012-02-21 07:43:36" resolution="Fixed">
		<buginformation>
			<summary>SUBSTR(CAST(&lt;string&gt; AS BINARY)) produces unexpected results</summary>
			<description></description>
			<version>0.8.0</version>
			<fixedVersion>0.9.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFSubstr.java</file>
		</fixedFiles>
	</bug>
	<bug id="1444" opendate="2010-06-30 21:26:03" fixdate="2012-03-07 00:33:43" resolution="Fixed">
		<buginformation>
			<summary>"hdfs" is hardcoded in few places in the code which inhibits use of other file systems</summary>
			<description>In quite a few places "hdfs" is hardcoded, which is OK for majority of the cases, except when it is not really hdfs, but s3 or any other file system.
The place where it really breaks is:
in ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java :
method: private void applyConstraints(URI fromURI, URI toURI, Tree ast, boolean isLocal)
First few lines are check for file system:
    if (!fromURI.getScheme().equals("file")
        &amp;amp;&amp;amp; !fromURI.getScheme().equals("hdfs")) 
{
      throw new SemanticException(ErrorMsg.INVALID_PATH.getMsg(ast,
          "only \"file\" or \"hdfs\" file systems accepted"));
    }

"hdfs" is hardcoded. 
I don&amp;amp;apos;t think you need to have this check at all as you are checking whether filesystem is local or not later on anyway and in regards to non locla file system - if one would be bad one you would get problems or have it look like local before you even come to "applyConstraints" method.
</description>
			<version>0.3.0</version>
			<fixedVersion>0.9.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
		</fixedFiles>
	</bug>
	<bug id="2857" opendate="2012-03-09 03:33:11" fixdate="2012-03-10 17:11:32" resolution="Fixed">
		<buginformation>
			<summary>QTestUtil.cleanUp() fails with FileNotException on 0.23</summary>
			<description></description>
			<version>0.9.0</version>
			<fixedVersion>0.9.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.QTestUtil.java</file>
		</fixedFiles>
	</bug>
	<bug id="2748" opendate="2012-01-25 17:17:29" fixdate="2012-03-15 06:41:23" resolution="Fixed">
		<buginformation>
			<summary>Upgrade Hbase and ZK dependcies</summary>
			<description>Both softwares have moved forward with significant improvements. Lets bump compile time dependency to keep up</description>
			<version>0.7.0</version>
			<fixedVersion>0.9.0</fixedVersion>
			<type>Task</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.thrift.MemoryTokenStore.java</file>
			<file type="M">org.apache.hadoop.hive.hbase.HBaseTestSetup.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.DelegationTokenStore.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.TestZooKeeperTokenStore.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">2764</link>
			<link type="Duplicate" description="is duplicated by">2077</link>
		</links>
	</bug>
	<bug id="2856" opendate="2012-03-09 00:19:24" fixdate="2012-03-15 19:30:30" resolution="Fixed">
		<buginformation>
			<summary>Fix TestCliDriver escape1.q failure on MR2</summary>
			<description>Additional &amp;amp;apos;^&amp;amp;apos; in escape test:
[junit] Begin query: escape1.q
[junit] Copying file: file:/home/cloudera/Code/hive/data/files/escapetest.txt
[junit] 12/01/23 15:22:15 WARN conf.Configuration: mapred.system.dir is deprecated. Instead, use mapreduce.jobtracker.system.dir
[junit] 12/01/23 15:22:15 WARN conf.Configuration: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir
[junit] diff -a -I file: -I pfile: -I hdfs: -I /tmp/ -I invalidscheme: -I lastUpdateTime -I lastAccessTime -I [Oo]wner -I CreateTime -I LastAccessTime -I Location -I LOCATION &amp;amp;apos; -I transient_lastDdlTime -I last_modified_ -I java.lang.RuntimeException -I at org -I at sun -I at java -I at junit -I Caused by: -I LOCK_QUERYID: -I LOCK_TIME: -I grantTime -I [.][.][.] [0-9]* more -I job_[0-9]_[0-9] -I USING &amp;amp;apos;java -cp /home/cloudera/Code/hive/build/ql/test/logs/clientpositive/escape1.q.out /home/cloudera/Code/hive/ql/src/test/results/clientpositive/escape1.q.out
[junit] 893d892
[junit] &amp;lt; 1	1	^
[junit] junit.framework.AssertionFailedError: Client execution results failed with error code = 1
[junit] See build/ql/tmp/hive.log, or try "ant test ... -Dtest.silent=false" to get more logs.
[junit] at junit.framework.Assert.fail(Assert.java:50)
[junit] at org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_escape1(TestCliDriver.java:131)
[junit] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[junit] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
[junit] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[junit] at java.lang.reflect.Method.invoke(Method.java:616)
[junit] at junit.framework.TestCase.runTest(TestCase.java:168)
[junit] at junit.framework.TestCase.runBare(TestCase.java:134)
[junit] at junit.framework.TestResult$1.protect(TestResult.java:110)
[junit] at junit.framework.TestResult.runProtected(TestResult.java:128)
[junit] at junit.framework.TestResult.run(TestResult.java:113)
[junit] at junit.framework.TestCase.run(TestCase.java:124)
[junit] at junit.framework.TestSuite.runTest(TestSuite.java:243)
[junit] at junit.framework.TestSuite.run(TestSuite.java:238)
[junit] at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)
[junit] at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)
[junit] at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:768)
[junit] Exception: Client execution results failed with error code = 1
[junit] See build/ql/tmp/hive.log, or try "ant test ... -Dtest.silent=false" to get more logs.
[junit] See build/ql/tmp/hive.log, or try "ant test ... -Dtest.silent=false" to get more logs.)</description>
			<version>0.8.0</version>
			<fixedVersion>0.9.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.common.FileUtils.java</file>
		</fixedFiles>
	</bug>
	<bug id="2778" opendate="2012-02-06 02:36:13" fixdate="2012-03-16 01:50:42" resolution="Fixed">
		<buginformation>
			<summary>Fail on table sampling </summary>
			<description>Trying table sampling on any non-empty table throws NPE. This does not occur by test on mini-MR.

select count(*) from emp tablesample (0.1 percent);     
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=&amp;lt;number&amp;gt;
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=&amp;lt;number&amp;gt;
In order to set a constant number of reducers:
  set mapred.reduce.tasks=&amp;lt;number&amp;gt;
java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.sampleSplits(CombineHiveInputFormat.java:450)
	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getSplits(CombineHiveInputFormat.java:403)
	at org.apache.hadoop.mapred.JobClient.writeOldSplits(JobClient.java:971)
	at org.apache.hadoop.mapred.JobClient.writeSplits(JobClient.java:963)
	at org.apache.hadoop.mapred.JobClient.access$500(JobClient.java:170)
	at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:880)
	at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:833)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1127)
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:833)
	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:807)
	at org.apache.hadoop.hive.ql.exec.ExecDriver.execute(ExecDriver.java:432)
	at org.apache.hadoop.hive.ql.exec.MapRedTask.execute(MapRedTask.java:136)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:134)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:57)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1332)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1123)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:931)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:255)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:212)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:671)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:554)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:186)
Job Submission failed with exception &amp;amp;apos;java.lang.NullPointerException(null)&amp;amp;apos;
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MapRedTask


</description>
			<version>0.9.0</version>
			<fixedVersion>0.9.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">2784</link>
			<link type="Reference" description="relates to">2737</link>
			<link type="Reference" description="is related to">3257</link>
		</links>
	</bug>
	<bug id="2503" opendate="2011-10-13 02:01:45" fixdate="2012-03-16 14:58:39" resolution="Fixed">
		<buginformation>
			<summary>HiveServer should provide per session configuration</summary>
			<description>Currently ThriftHiveProcessorFactory returns same HiveConf instance to HiveServerHandler, making impossible to use per sesssion configuration. Just wrapping &amp;amp;apos;conf&amp;amp;apos; -&amp;gt; &amp;amp;apos;new HiveConf(conf)&amp;amp;apos; seemed to solve this problem.</description>
			<version>0.9.0</version>
			<fixedVersion>0.9.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.service.HiveServer.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">2573</link>
		</links>
	</bug>
	<bug id="2784" opendate="2012-02-08 01:17:39" fixdate="2012-03-16 23:04:46" resolution="Duplicate">
		<buginformation>
			<summary>Integrating with MapReduce2 get NPE throwed when executing a query with a "TABLESAMPLE(x percent)" clause</summary>
			<description>the following TestCliDriver testcases fail:
sample_islocalmode_hook
split_sample
[junit] java.lang.NullPointerException
[junit] 	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.sampleSplits(CombineHiveInputFormat.java:450)
[junit] 	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getSplits(CombineHiveInputFormat.java:403)
[junit] 	at org.apache.hadoop.mapreduce.JobSubmitter.writeOldSplits(JobSubmitter.java:472)
[junit] 	at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:464)
[junit] 	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:360)
[junit] 	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1215)
[junit] 	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1212)
[junit] 	at java.security.AccessController.doPrivileged(Native Method)
[junit] 	at javax.security.auth.Subject.doAs(Subject.java:396)
[junit] 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1157)
[junit] 	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1212)
[junit] 	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:592)
[junit] 	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:587)
[junit] 	at java.security.AccessController.doPrivileged(Native Method)
[junit] 	at javax.security.auth.Subject.doAs(Subject.java:396)
[junit] 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1157)
[junit] 	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:587)
[junit] 	at org.apache.hadoop.hive.ql.exec.ExecDriver.execute(ExecDriver.java:452)
[junit] 	at org.apache.hadoop.hive.ql.exec.ExecDriver.main(ExecDriver.java:710)
[junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
[junit] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
[junit] 	at java.lang.reflect.Method.invoke(Method.java:597)
[junit] 	at org.apache.hadoop.util.RunJar.main(RunJar.java:200)
There are other qfiles which pass which use TABLESAMPLE without specifying a percent</description>
			<version>0.9.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">2778</link>
			<link type="Reference" description="is related to">3257</link>
		</links>
	</bug>
	<bug id="2913" opendate="2012-03-30 09:38:21" fixdate="2012-04-03 10:48:06" resolution="Fixed">
		<buginformation>
			<summary>BlockMergeTask Doesn&amp;apos;t Honor Job Configuration Properties when used directly</summary>
			<description>BlockMergeTask has a main() and when used directly (instead of say partition concatenate feature), the -jobconf arguments are not honored. This is not something most people directly use.
Usage:
BlockMergeTask -input &amp;lt;colon seperated input paths&amp;gt;  -outputDir outputDir [-jobconf k1=v1 [-jobconf k2=v2] ...] 
To reproduce:
Run BlockMergeTask with say -jobconf mapred.job.name=test and launched job will have a different name.</description>
			<version>0.8.1</version>
			<fixedVersion>0.9.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.rcfile.merge.BlockMergeTask.java</file>
		</fixedFiles>
	</bug>
	<bug id="2923" opendate="2012-04-03 00:40:20" fixdate="2012-04-06 15:44:56" resolution="Fixed">
		<buginformation>
			<summary>testAclPositive in TestZooKeeperTokenStore failing in clean checkout when run on Mac</summary>
			<description>When running testAclPositive in TestZooKeeperTokenStore in a clean checkout, it fails with the error:
Failed to validate token path. 
org.apache.hadoop.hive.thrift.DelegationTokenStore$TokenStoreException: Failed to validate token path.
at org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.init(ZooKeeperTokenStore.java:207)
at org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.setConf(ZooKeeperTokenStore.java:225)
at org.apache.hadoop.hive.thrift.TestZooKeeperTokenStore.testAclPositive(TestZooKeeperTokenStore.java:170)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at junit.framework.TestCase.runTest(TestCase.java:168)
at junit.framework.TestCase.runBare(TestCase.java:134)
at junit.framework.TestResult$1.protect(TestResult.java:110)
at junit.framework.TestResult.runProtected(TestResult.java:128)
at junit.framework.TestResult.run(TestResult.java:113)
at junit.framework.TestCase.run(TestCase.java:124)
at junit.framework.TestSuite.runTest(TestSuite.java:232)
at junit.framework.TestSuite.run(TestSuite.java:227)
at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:518)
at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1052)
at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:906)
Caused by: org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /zktokenstore-testAcl
at org.apache.zookeeper.KeeperException.create(KeeperException.java:99)
at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:778)
at org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.ensurePath(ZooKeeperTokenStore.java:119)
at org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.init(ZooKeeperTokenStore.java:204)
... 17 more
This message is also printed to standard out:
Unable to load realm mapping info from SCDynamicStore
The test seems to run fine in Linux, but more than one developer has reported this on a Mac.</description>
			<version>0.9.0</version>
			<fixedVersion>0.9.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.TestZooKeeperTokenStore.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.java</file>
		</fixedFiles>
	</bug>
	<bug id="2907" opendate="2012-03-26 18:18:11" fixdate="2012-04-10 16:56:37" resolution="Fixed">
		<buginformation>
			<summary>Hive error when dropping a table with large number of partitions</summary>
			<description>Running into an "Out Of Memory" error when trying to drop a table with 128K partitions.
The methods dropTable in metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java 
and dropTable in ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java encounter out of memory errors 
when dropping tables with lots of partitions because they try to load the metadata for every partition into memory.</description>
			<version>0.9.0</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
	</bug>
	<bug id="2931" opendate="2012-04-06 18:48:04" fixdate="2012-04-13 06:11:33" resolution="Duplicate">
		<buginformation>
			<summary>conf settings may be ignored</summary>
			<description>This is a pretty serious problem.
If a conf variable is changed, Hive may not pick up the variable unless the metastore variables are changed.
When any session variables are changed, it might be simpler to update the corresponding Hive conf.</description>
			<version>0.7.1</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">2918</link>
		</links>
	</bug>
	<bug id="2829" opendate="2012-02-28 17:07:34" fixdate="2012-04-13 21:15:41" resolution="Duplicate">
		<buginformation>
			<summary>SET hive.exec.max.dynamic.partitions.pernode is ignored</summary>
			<description>SET hive.exec.max.dynamic.partitions.pernode=12345;
in a Hive script is ignored. The default value of 100 coming from hive-default.xml is always passed.</description>
			<version>0.7.1</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">2918</link>
		</links>
	</bug>
	<bug id="2077" opendate="2011-03-26 03:07:24" fixdate="2012-04-16 23:34:59" resolution="Duplicate">
		<buginformation>
			<summary>Allow HBaseStorageHandler to work with hbase 0.90.1</summary>
			<description>Currently HBase handler works with hbase 0.89
We should make it work with 0.90.1 and utilize new features of 0.90.1</description>
			<version>0.7.0</version>
			<fixedVersion></fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.thrift.MemoryTokenStore.java</file>
			<file type="M">org.apache.hadoop.hive.hbase.HBaseTestSetup.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.DelegationTokenStore.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.TestZooKeeperTokenStore.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">2748</link>
		</links>
	</bug>
	<bug id="2958" opendate="2012-04-17 15:02:38" fixdate="2012-04-20 00:51:15" resolution="Fixed">
		<buginformation>
			<summary>GROUP BY causing ClassCastException [LazyDioInteger cannot be cast LazyInteger]</summary>
			<description>This relates to https://issues.apache.org/jira/browse/HIVE-1634.
The following work fine:


CREATE EXTERNAL TABLE tim_hbase_occurrence ( 
  id int,
  scientific_name string,
  data_resource_id int
) STORED BY &amp;amp;apos;org.apache.hadoop.hive.hbase.HBaseStorageHandler&amp;amp;apos; WITH SERDEPROPERTIES (
  "hbase.columns.mapping" = ":key#b,v:scientific_name#s,v:data_resource_id#b"
) TBLPROPERTIES(
  "hbase.table.name" = "mini_occurrences", 
  "hbase.table.default.storage.type" = "binary"
);
SELECT * FROM tim_hbase_occurrence LIMIT 3;
SELECT * FROM tim_hbase_occurrence WHERE data_resource_id=1081 LIMIT 3;


However, the following fails:


SELECT data_resource_id, count(*) FROM tim_hbase_occurrence GROUP BY data_resource_id;


The error given:


0 TS
2012-04-17 16:58:45,693 INFO org.apache.hadoop.hive.ql.exec.MapOperator: Initialization Done 7 MAP
2012-04-17 16:58:45,714 INFO org.apache.hadoop.hive.ql.exec.MapOperator: Processing alias tim_hbase_occurrence for file hdfs://c1n2.gbif.org/user/hive/warehouse/tim_hbase_occurrence
2012-04-17 16:58:45,714 INFO org.apache.hadoop.hive.ql.exec.MapOperator: 7 forwarding 1 rows
2012-04-17 16:58:45,714 INFO org.apache.hadoop.hive.ql.exec.TableScanOperator: 0 forwarding 1 rows
2012-04-17 16:58:45,716 INFO org.apache.hadoop.hive.ql.exec.SelectOperator: 1 forwarding 1 rows
2012-04-17 16:58:45,723 FATAL ExecMapper: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {"id":1444,"scientific_name":null,"data_resource_id":1081}
	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:548)
	at org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:143)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:391)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:325)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:270)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1157)
	at org.apache.hadoop.mapred.Child.main(Child.java:264)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.lazydio.LazyDioInteger cannot be cast to org.apache.hadoop.hive.serde2.lazy.LazyInteger
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.processOp(GroupByOperator.java:737)
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:762)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:762)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:83)
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:762)
	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:529)
	... 9 more
Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.lazydio.LazyDioInteger cannot be cast to org.apache.hadoop.hive.serde2.lazy.LazyInteger
	at org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyIntObjectInspector.copyObject(LazyIntObjectInspector.java:43)
	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.copyToStandardObject(ObjectInspectorUtils.java:239)
	at org.apache.hadoop.hive.ql.exec.KeyWrapperFactory$ListKeyWrapper.deepCopyElements(KeyWrapperFactory.java:150)
	at org.apache.hadoop.hive.ql.exec.KeyWrapperFactory$ListKeyWrapper.deepCopyElements(KeyWrapperFactory.java:142)
	at org.apache.hadoop.hive.ql.exec.KeyWrapperFactory$ListKeyWrapper.copyKey(KeyWrapperFactory.java:119)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.processHashAggr(GroupByOperator.java:750)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.processOp(GroupByOperator.java:722)
	... 18 more


</description>
			<version>0.9.0</version>
			<fixedVersion>0.9.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.serde2.lazydio.LazyDioInteger.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazydio.LazyDioFloat.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazydio.LazyDioDouble.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazydio.LazyDioByte.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazydio.LazyDioBoolean.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazydio.LazyDioLong.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazydio.LazyDioShort.java</file>
		</fixedFiles>
	</bug>
	<bug id="2803" opendate="2012-02-12 11:50:03" fixdate="2012-04-23 23:23:18" resolution="Fixed">
		<buginformation>
			<summary>utc_from_timestamp and utc_to_timestamp returns incorrect results.</summary>
			<description>How to reproduce:

$ echo "2011-12-25 09:00:00.123456" &amp;gt; /tmp/data5.txt
hive&amp;gt; create table ts1(t1 timestamp);
hive&amp;gt; load data local inpath &amp;amp;apos;/tmp/data5.txt&amp;amp;apos; overwrite into table ts1;
hive&amp;gt; select t1, from_utc_timestamp(t1, &amp;amp;apos;JST&amp;amp;apos;), from_utc_timestamp(t1, &amp;amp;apos;JST&amp;amp;apos;) from ts1 limit 1;


The following result is expected:

 2011-12-25 09:00:00.123456      2011-12-25 18:00:00.123456      2011-12-25 18:00:00.123456


However, the above query return incorrect result like this:

 2011-12-26 03:00:00.492456      2011-12-26 03:00:00.492456      2011-12-26 03:00:00.492456


This is because GenericUDFFromUtcTimestamp.applyOffset() does setTime() improperly.
On evaluating query, timestamp argument always returns the same instance.
GenericUDFFromUtcTimestamp.applyOffset() does setTime() on the instance.
That means it adds all offsets in the query.</description>
			<version>0.8.0</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFFromUtcTimestamp.java</file>
		</fixedFiles>
	</bug>
	<bug id="2883" opendate="2012-03-20 17:59:45" fixdate="2012-04-24 19:30:31" resolution="Fixed">
		<buginformation>
			<summary>Metastore client doesnt close connection properly</summary>
			<description>While closing connection, it always fail with following trace. Seemingly, it doesnt have any harmful effects.


12/03/20 10:55:02 ERROR hive.metastore: Unable to shutdown local metastore client
org.apache.thrift.transport.TTransportException: Cannot write to null outputStream
	at org.apache.thrift.transport.TIOStreamTransport.write(TIOStreamTransport.java:142)
	at org.apache.thrift.protocol.TBinaryProtocol.writeI32(TBinaryProtocol.java:163)
	at org.apache.thrift.protocol.TBinaryProtocol.writeMessageBegin(TBinaryProtocol.java:91)
	at org.apache.thrift.TServiceClient.sendBase(TServiceClient.java:62)
	at com.facebook.fb303.FacebookService$Client.send_shutdown(FacebookService.java:421)
	at com.facebook.fb303.FacebookService$Client.shutdown(FacebookService.java:415)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.close(HiveMetaStoreClient.java:310)

</description>
			<version>0.9.0</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">236</link>
		</links>
	</bug>
	<bug id="2918" opendate="2012-04-02 11:08:59" fixdate="2012-04-25 17:00:32" resolution="Fixed">
		<buginformation>
			<summary>Hive Dynamic Partition Insert - move task not considering &amp;apos;hive.exec.max.dynamic.partitions&amp;apos; from CLI</summary>
			<description>Dynamic Partition insert showing an error with the number of partitions created even after the default value of &amp;amp;apos;hive.exec.max.dynamic.partitions&amp;amp;apos; is bumped high to 2000.
Error Message:
"Failed with exception Number of dynamic partitions created is 1413, which is more than 1000. To solve this try to set hive.exec.max.dynamic.partitions to at least 1413."
These are the following properties set on hive CLI
hive&amp;gt; set hive.exec.dynamic.partition=true;
hive&amp;gt; set hive.exec.dynamic.partition.mode=nonstrict;
hive&amp;gt; set hive.exec.max.dynamic.partitions=2000;
hive&amp;gt; set hive.exec.max.dynamic.partitions.pernode=2000;
This is the query with console error log
hive&amp;gt; 
    &amp;gt; INSERT OVERWRITE TABLE partn_dyn Partition (pobox)
    &amp;gt; SELECT country,state,pobox FROM non_partn_dyn;
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks is set to 0 since there&amp;amp;apos;s no reduce operator
Starting Job = job_201204021529_0002, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_201204021529_0002
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_201204021529_0002
2012-04-02 16:05:28,619 Stage-1 map = 0%,  reduce = 0%
2012-04-02 16:05:39,701 Stage-1 map = 100%,  reduce = 0%
2012-04-02 16:05:50,800 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201204021529_0002
Ended Job = 248865587, job is filtered out (removed at runtime).
Moving data to: hdfs://0.0.0.0/tmp/hive-cloudera/hive_2012-04-02_16-05-24_919_5976014408587784412/-ext-10000
Loading data to table default.partn_dyn partition (pobox=null)
Failed with exception Number of dynamic partitions created is 1413, which is more than 1000. To solve this try to set hive.exec.max.dynamic.partitions to at least 1413.
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask
I checked the job.xml of the first map only job, there the value hive.exec.max.dynamic.partitions=2000 is reflected but the move task is taking the default value from hive-site.xml . If I change the value in hive-site.xml then the job completes successfully. Bottom line,the property &amp;amp;apos;hive.exec.max.dynamic.partitions&amp;amp;apos;set on CLI is not being considered by move task
</description>
			<version>0.7.1</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">2829</link>
			<link type="Duplicate" description="is duplicated by">2931</link>
			<link type="Duplicate" description="is duplicated by">2963</link>
			<link type="Reference" description="relates to">2984</link>
		</links>
	</bug>
	<bug id="2721" opendate="2012-01-17 01:31:52" fixdate="2012-04-27 00:50:31" resolution="Fixed">
		<buginformation>
			<summary>ability to select a view qualified by the database / schema name</summary>
			<description>HIVE-1517 added support for selecting tables from different databases (aka schemas) by qualifying the tables with the database name. The feature work did not however extend this support to views. Note that this point came up in the earlier JIRA, but was not addressed. See the following two comments:
https://issues.apache.org/jira/browse/HIVE-1517?focusedCommentId=12996641&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-12996641
https://issues.apache.org/jira/browse/HIVE-1517?focusedCommentId=12996679&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-12996679</description>
			<version>0.7.0</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.UnparseTranslator.java</file>
		</fixedFiles>
	</bug>
	<bug id="3000" opendate="2012-05-03 00:53:06" fixdate="2012-05-07 16:37:59" resolution="Fixed">
		<buginformation>
			<summary>Potential infinite loop / log spew in ZookeeperHiveLockManager</summary>
			<description>See ZookeeperHiveLockManger.lock()
If Zookeeper is in a bad state, it&amp;amp;apos;s possible to get an exception (e.g. org.apache.zookeeper.KeeperException$SessionExpiredException) when we call lockPrimitive(). There is a bug in the exception handler where the loop does not exit because the break in the switch statement gets out the switch, not the do..while loop. Because tryNum was not incremented due to the exception, lockPrimitive() will be called in an infinite loop, as fast as possible. Since the exception is printed for each call, Hive will produce significant log spew.</description>
			<version>0.9.0</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.java</file>
		</fixedFiles>
	</bug>
	<bug id="2757" opendate="2012-01-27 02:21:08" fixdate="2012-05-10 21:31:31" resolution="Fixed">
		<buginformation>
			<summary>hive can&amp;apos;t find hadoop executor scripts without HADOOP_HOME set</summary>
			<description>The trouble is that in Hadoop 0.23 HADOOP_HOME has been deprecated. I think it would be really nice if bin/hive can be modified to capture the which hadoop
and pass that as a property into the JVM.</description>
			<version>0.8.0</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
		<links>
			<link type="Regression" description="breaks">3014</link>
		</links>
	</bug>
	<bug id="2732" opendate="2012-01-21 00:44:07" fixdate="2012-05-15 20:06:48" resolution="Fixed">
		<buginformation>
			<summary>Reduce Sink deduplication fails if the child reduce sink is followed by a join</summary>
			<description>set hive.optimize.reducededuplication=true;
set hive.auto.convert.join=true;
explain select * from (select * from src distribute by key sort by key) a join src b on a.key = b.key;
fails with the following exception
java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.SelectOperator cannot be cast to org.apache.hadoop.hive.ql.exec.ReduceSinkOperator
	at org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.convertMapJoin(MapJoinProcessor.java:313)
	at org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.genMapJoinOpAndLocalWork(MapJoinProcessor.java:226)
	at org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinResolver$CommonJoinTaskDispatcher.processCurrentTask(CommonJoinResolver.java:174)
	at org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinResolver$CommonJoinTaskDispatcher.dispatch(CommonJoinResolver.java:287)
	at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.dispatch(TaskGraphWalker.java:111)
	at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.walk(TaskGraphWalker.java:194)
	at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.startWalking(TaskGraphWalker.java:139)
	at org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinResolver.resolve(CommonJoinResolver.java:68)
	at org.apache.hadoop.hive.ql.optimizer.physical.PhysicalOptimizer.optimize(PhysicalOptimizer.java:72)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genMapRedTasks(SemanticAnalyzer.java:7019)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:7312)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:243)
	at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:48)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:243)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:430)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:337)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:889)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:255)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:212)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:671)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:554)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
If hive.auto.convert.join is set to false, it produces an incorrect plan where the two halves of the join are processed in two separate map reduce tasks, and the reducers of these two tasks both contain the join operator resulting in an exception.</description>
			<version>0.10.0</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ReduceSinkDeDuplication.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">2329</link>
		</links>
	</bug>
	<bug id="2963" opendate="2012-04-19 01:32:29" fixdate="2012-05-18 02:54:47" resolution="Duplicate">
		<buginformation>
			<summary>metastore delegation token is not getting used by hive commandline</summary>
			<description>When metastore delegation tokens are used to run hive (or hcat) commands, the delegation token does not end up getting used.
This is because new Hive object is not created with value of hive.metastore.token.signature in its conf. This config parameter is missing in the list of HiveConf variables whose change results in metastore recreation.</description>
			<version>0.8.1</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">2918</link>
			<link type="Reference" description="relates to">13363</link>
		</links>
	</bug>
	<bug id="2372" opendate="2011-08-12 09:07:34" fixdate="2012-05-25 23:55:02" resolution="Fixed">
		<buginformation>
			<summary>java.io.IOException: error=7, Argument list too long</summary>
			<description>I execute a huge query on a table with a lot of 2-level partitions. There is a perl reducer in my query. Maps worked ok, but every reducer fails with the following exception:
2011-08-11 04:58:29,865 INFO org.apache.hadoop.hive.ql.exec.ScriptOperator: Executing [/usr/bin/perl, &amp;lt;reducer.pl&amp;gt;, &amp;lt;my_argument&amp;gt;]
2011-08-11 04:58:29,866 INFO org.apache.hadoop.hive.ql.exec.ScriptOperator: tablename=null
2011-08-11 04:58:29,866 INFO org.apache.hadoop.hive.ql.exec.ScriptOperator: partname=null
2011-08-11 04:58:29,866 INFO org.apache.hadoop.hive.ql.exec.ScriptOperator: alias=null
2011-08-11 04:58:29,935 FATAL ExecReducer: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {"key":
{"reducesinkkey0":129390185139228,"reducesinkkey1":"00008AF10000000063CA6F"}
,"value":
{"_col0":"00008AF10000000063CA6F","_col1":"2011-07-27 22:48:52","_col2":129390185139228,"_col3":2006,"_col4":4100,"_col5":"10017388=6","_col6":1063,"_col7":"NULL","_col8":"address.com","_col9":"NULL","_col10":"NULL"}
,"alias":0}
	at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:256)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:468)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:416)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:268)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1115)
	at org.apache.hadoop.mapred.Child.main(Child.java:262)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Cannot initialize ScriptOperator
	at org.apache.hadoop.hive.ql.exec.ScriptOperator.processOp(ScriptOperator.java:320)
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:744)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:744)
	at org.apache.hadoop.hive.ql.exec.ExtractOperator.processOp(ExtractOperator.java:45)
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)
	at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:247)
	... 7 more
Caused by: java.io.IOException: Cannot run program "/usr/bin/perl": java.io.IOException: error=7, Argument list too long
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:460)
	at org.apache.hadoop.hive.ql.exec.ScriptOperator.processOp(ScriptOperator.java:279)
	... 15 more
Caused by: java.io.IOException: java.io.IOException: error=7, Argument list too long
	at java.lang.UNIXProcess.&amp;lt;init&amp;gt;(UNIXProcess.java:148)
	at java.lang.ProcessImpl.start(ProcessImpl.java:65)
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:453)
	... 16 more
It seems to me, I found the cause. ScriptOperator.java puts a lot of configs as environment variables to the child reduce process. One of variables is mapred.input.dir, which in my case more than 150KB. There are a huge amount of input directories in this variable. In short, the problem is that Linux (up to 2.6.23 kernel version) limits summary size of environment variables for child processes to 132KB. This problem could be solved by upgrading the kernel. But strings limitations still be 132KB per string in environment variable. So such huge variable doesn&amp;amp;apos;t work even on my home computer (2.6.32). You can read more information on (http://www.kernel.org/doc/man-pages/online/pages/man2/execve.2.html).
For now all our work has been stopped because of this problem and I can&amp;amp;apos;t find the solution. The only solution, which seems to me more reasonable is to get rid of this variable in reducers.
</description>
			<version>0.12.0</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.TestOperators.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
	</bug>
	<bug id="2540" opendate="2011-11-01 17:32:28" fixdate="2012-05-27 12:08:25" resolution="Fixed">
		<buginformation>
			<summary>LATERAL VIEW with EXPLODE produces ConcurrentModificationException</summary>
			<description>The following produces ConcurrentModificationException on the for loop inside EXPLODE:


create table foo as select array(1, 2) a from src limit 1;
select a, x.b from foo lateral view explode(a) x as b;

</description>
			<version>0.7.1</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyMap.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyArray.java</file>
		</fixedFiles>
		<links>
			<link type="Container" description="Is contained by">10089</link>
			<link type="Duplicate" description="is duplicated by">2939</link>
		</links>
	</bug>
	<bug id="3057" opendate="2012-05-26 00:17:34" fixdate="2012-05-31 16:12:10" resolution="Fixed">
		<buginformation>
			<summary>metastore.HiveMetaStore$HMSHandler should set the thread local raw store to null in shutdown()</summary>
			<description>The shutdown() function of metastore.HiveMetaStore$HMSHandler does not set the thread local RawStore variable (in threadLocalMS) to null. Subsequent getMS() calls may get the wrong RawStore object.</description>
			<version>0.8.1</version>
			<fixedVersion>0.9.1, 0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">3067</link>
			<link type="Reference" description="is related to">2184</link>
			<link type="Reference" description="is related to">1195</link>
		</links>
	</bug>
	<bug id="3052" opendate="2012-05-25 09:34:28" fixdate="2012-05-31 16:20:09" resolution="Fixed">
		<buginformation>
			<summary>TestHadoop20SAuthBridge always uses the same port</summary>
			<description>Similar to https://issues.apache.org/jira/browse/HIVE-2959
TestHadoop20SAuthBridge uses fixed port 10000 (and 10010) for testing which is default port of hive server, making test fail if someone is testing it(hive server).</description>
			<version>0.10.0</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.thrift.TestHadoop20SAuthBridge.java</file>
		</fixedFiles>
	</bug>
	<bug id="2736" opendate="2012-01-23 14:22:05" fixdate="2012-06-07 14:56:36" resolution="Fixed">
		<buginformation>
			<summary>Hive UDFs cannot emit binary constants</summary>
			<description>I recently wrote a UDF which emits BINARY values (as implemented in HIVE-2380). When testing this, I encountered the following exception (because I was evaluating f(g(constant string))) and g() was emitting a BytesWritable type.
FAILED: Hive Internal Error: java.lang.RuntimeException(Internal error: Cannot find ConstantObjectInspector for BINARY)
java.lang.RuntimeException: Internal error: Cannot find ConstantObjectInspector for BINARY
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.getPrimitiveWritableConstantObjectInspector(PrimitiveObjectInspectorFactory.java:196)
	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.getConstantObjectInspector(ObjectInspectorUtils.java:899)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDF.initializeAndFoldConstants(GenericUDF.java:128)
	at org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.newInstance(ExprNodeGenericFuncDesc.java:214)
	at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.getXpathOrFuncExprNodeDesc(TypeCheckProcFactory.java:684)
	at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.process(TypeCheckProcFactory.java:805)
	at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:89)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:88)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:125)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:102)
	at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.genExprNode(TypeCheckProcFactory.java:161)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genExprNodeDesc(SemanticAnalyzer.java:7708)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genSelectPlan(SemanticAnalyzer.java:2301)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genSelectPlan(SemanticAnalyzer.java:2103)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPostGroupByBodyPlan(SemanticAnalyzer.java:6126)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:6097)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:6723)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:7484)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:243)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:430)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:337)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:889)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:255)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:212)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:671)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:554)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
It looks like a pretty simple fix - add a case for BINARY in PrimitiveObjectInspectorFactory.getPrimitiveWritableConstantObjectInspector() and implement a WritableConstantByteArrayObjectInspector class (almost identical to the others). I&amp;amp;apos;m happy to do this, although this is my first foray into the world of contributing to FOSS so I might end up asking a few stupid questions.</description>
			<version>0.9.0</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.java</file>
		</fixedFiles>
	</bug>
	<bug id="3099" opendate="2012-06-07 07:27:04" fixdate="2012-06-10 01:30:20" resolution="Fixed">
		<buginformation>
			<summary>add findbugs in build.xml</summary>
			<description>see HIVE-1172 and HIVE-2169 
it was used default rules.
exclude filter file is in findbugs\findbugs-exclude.xml
the result of xml files to html file is not added because no style files.</description>
			<version>0.9.0</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">2169</link>
			<link type="Duplicate" description="duplicates">1172</link>
		</links>
	</bug>
	<bug id="3081" opendate="2012-06-04 03:09:02" fixdate="2012-06-12 23:13:00" resolution="Fixed">
		<buginformation>
			<summary>ROFL Moment. Numberator and denaminator typos</summary>
			<description></description>
			<version>0.9.0</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.ErrorMsg.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
	</bug>
	<bug id="3090" opendate="2012-06-06 02:56:52" fixdate="2012-06-13 15:38:46" resolution="Fixed">
		<buginformation>
			<summary>Timestamp type values not having nano-second part breaks row</summary>
			<description>Timestamp values are reading additional one byte if nano-sec part is zero, breaking following columns.  

&amp;gt;create table timestamp_1 (t timestamp, key string, value string);
&amp;gt;insert overwrite table timestamp_1 select cast(&amp;amp;apos;2011-01-01 01:01:01&amp;amp;apos; as timestamp), key, value from src limit 5;

&amp;gt;select t,key,value from timestamp_1;
2011-01-01 01:01:01		238
2011-01-01 01:01:01		86
2011-01-01 01:01:01		311
2011-01-01 01:01:01		27
2011-01-01 01:01:01		165

&amp;gt;select t,key,value from timestamp_1 distribute by t;
2011-01-01 01:01:01		
2011-01-01 01:01:01		
2011-01-01 01:01:01		
2011-01-01 01:01:01		
2011-01-01 01:01:01		

</description>
			<version>0.10.0</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.serde2.io.TimestampWritable.java</file>
		</fixedFiles>
	</bug>
	<bug id="3062" opendate="2012-05-29 08:15:08" fixdate="2012-06-16 06:42:02" resolution="Fixed">
		<buginformation>
			<summary>Insert into table overwrites existing table if table name contains uppercase character</summary>
			<description>"Insert into table &amp;lt;table-name&amp;gt; ~~" is expected to append query result into the table. But when the table name contains uppercase character, it overwrite existing table.</description>
			<version>0.8.0</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">3939</link>
			<link type="Duplicate" description="is duplicated by">3064</link>
			<link type="Reference" description="relates to">3465</link>
		</links>
	</bug>
	<bug id="2955" opendate="2012-04-17 05:31:41" fixdate="2012-06-22 04:27:09" resolution="Fixed">
		<buginformation>
			<summary>Queries consists of metadata-only-query returns always empty value</summary>
			<description>For partitioned table, simple query on partition column returns always null or empty value, for example,


create table emppart(empno int, ename string) partitioned by (deptno int);
.. load partitions..

select distinct deptno from emppart; // empty
select min(deptno), max(deptno) from emppart;  // NULL and NULL

</description>
			<version>0.8.1</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.MetadataOnlyOptimizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">4386</link>
			<link type="Duplicate" description="is duplicated by">3108</link>
			<link type="Duplicate" description="is duplicated by">4386</link>
		</links>
	</bug>
	<bug id="3125" opendate="2012-06-13 11:06:43" fixdate="2012-06-23 21:34:12" resolution="Fixed">
		<buginformation>
			<summary>sort_array doesn&amp;apos;t work with LazyPrimitive</summary>
			<description>The sort_array function doesn&amp;amp;apos;t work against data that&amp;amp;apos;s actually come out of a table. The test suite only covers constants given in the query.
If you try and use sort_array on an array from a table, then you get a ClassCastException that you can&amp;amp;apos;t convert LazyX to Comparable.</description>
			<version>0.9.0</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFSortArray.java</file>
		</fixedFiles>
	</bug>
	<bug id="3127" opendate="2012-06-13 17:52:20" fixdate="2012-06-28 00:44:59" resolution="Fixed">
		<buginformation>
			<summary>Pass hconf values as XML instead of command line arguments to child JVM</summary>
			<description>The maximum length of the DOS command string is 8191 characters (in Windows latest versions http://support.microsoft.com/kb/830473). This limit will be exceeded easily when it appends individual hconf values to the command string. To work around this problem, Write all changed hconf values to a temp file and pass the temp file path to the child jvm to read and initialize the -hconf parameters from file.</description>
			<version>0.9.0</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.rcfile.merge.BlockMergeTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapRedTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapredLocalTask.java</file>
		</fixedFiles>
		<links>
			<link type="Incorporates" description="is part of">2998</link>
		</links>
	</bug>
	<bug id="3206" opendate="2012-06-28 03:40:13" fixdate="2012-07-03 00:00:38" resolution="Fixed">
		<buginformation>
			<summary>FileUtils.tar assumes wrong directory in some cases</summary>
			<description>Bucket mapjoin throws exception archiving stored hashtables. 

hive&amp;gt; set hive.optimize.bucketmapjoin = true;
hive&amp;gt; select /*+mapjoin(a)*/ a.key, a.value, b.value 
    &amp;gt; from srcbucket_mapjoin_part a join srcbucket_mapjoin_part_2 b 
    &amp;gt; on a.key=b.key;
Total MapReduce jobs = 1
12/06/28 12:36:18 WARN conf.HiveConf: DEPRECATED: Ignoring hive-default.xml found on the CLASSPATH at /home/navis/hive/conf/hive-default.xml
Execution log at: /tmp/navis/navis_20120628123636_5298a863-605c-4b98-bbb3-0a132c85c5a3.log
2012-06-28 12:36:18	Starting to launch local task to process map join;	maximum memory = 932118528
2012-06-28 12:36:18	Processing rows:	153	Hashtable size:	153	Memory usage:	1771376	rate:	0.002
2012-06-28 12:36:18	Dump the hashtable into file: file:/tmp/navis/hive_2012-06-28_12-36-17_003_3016196240171705142/-local-10002/HashTable-Stage-1/MapJoin-a-00-srcbucket22.txt.hashtable
2012-06-28 12:36:18	Upload 1 File to: file:/tmp/navis/hive_2012-06-28_12-36-17_003_3016196240171705142/-local-10002/HashTable-Stage-1/MapJoin-a-00-srcbucket22.txt.hashtable File size: 9644
2012-06-28 12:36:19	Processing rows:	309	Hashtable size:	156	Memory usage:	1844568	rate:	0.002
2012-06-28 12:36:19	Dump the hashtable into file: file:/tmp/navis/hive_2012-06-28_12-36-17_003_3016196240171705142/-local-10002/HashTable-Stage-1/MapJoin-a-00-srcbucket23.txt.hashtable
2012-06-28 12:36:19	Upload 1 File to: file:/tmp/navis/hive_2012-06-28_12-36-17_003_3016196240171705142/-local-10002/HashTable-Stage-1/MapJoin-a-00-srcbucket23.txt.hashtable File size: 10023
2012-06-28 12:36:19	End of local task; Time Taken: 0.773 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there&amp;amp;apos;s no reduce operator
java.io.IOException: This archives contains unclosed entries.
	at org.apache.commons.compress.archivers.tar.TarArchiveOutputStream.finish(TarArchiveOutputStream.java:214)
	at org.apache.hadoop.hive.common.FileUtils.tar(FileUtils.java:276)
	at org.apache.hadoop.hive.ql.exec.ExecDriver.execute(ExecDriver.java:391)
	at org.apache.hadoop.hive.ql.exec.MapRedTask.execute(MapRedTask.java:137)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:134)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:57)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1324)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1110)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:944)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:258)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:215)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:406)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:744)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:607)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:186)
Job Submission failed with exception &amp;amp;apos;java.io.IOException(This archives contains unclosed entries.)&amp;amp;apos;
java.lang.IllegalArgumentException: Can not create a Path from an empty string
	at org.apache.hadoop.fs.Path.checkPathArg(Path.java:82)
	at org.apache.hadoop.fs.Path.&amp;lt;init&amp;gt;(Path.java:90)
	at org.apache.hadoop.hive.ql.exec.Utilities.getHiveJobID(Utilities.java:380)
	at org.apache.hadoop.hive.ql.exec.Utilities.clearMapRedWork(Utilities.java:193)
	at org.apache.hadoop.hive.ql.exec.ExecDriver.execute(ExecDriver.java:460)
	at org.apache.hadoop.hive.ql.exec.MapRedTask.execute(MapRedTask.java:137)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:134)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:57)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1324)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1110)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:944)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:258)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:215)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:406)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:744)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:607)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:186)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MapRedTask


Seemed to be regression from HIVE-3128.</description>
			<version>0.10.0</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.common.FileUtils.java</file>
		</fixedFiles>
	</bug>
	<bug id="2498" opendate="2011-10-12 05:20:52" fixdate="2012-07-05 19:53:58" resolution="Fixed">
		<buginformation>
			<summary>Group by operator does not estimate size of Timestamp &amp; Binary data correctly</summary>
			<description>It currently defaults to default case and returns constant value, whereas we can do better by getting actual size at runtime.</description>
			<version>0.8.0</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
		</fixedFiles>
	</bug>
	<bug id="3232" opendate="2012-07-05 19:45:07" fixdate="2012-07-10 23:03:37" resolution="Fixed">
		<buginformation>
			<summary>Resource Leak: Fix the File handle leak in EximUtil.java</summary>
			<description>1) Not closing the file handle EximUtil after reading the metadata from the file.
2) Nit: Get the path from URI to handle the Windows paths.</description>
			<version>0.8.0</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.EximUtil.java</file>
		</fixedFiles>
		<links>
			<link type="Required" description="is required by">2998</link>
		</links>
	</bug>
	<bug id="3168" opendate="2012-06-21 02:53:16" fixdate="2012-07-12 16:53:41" resolution="Fixed">
		<buginformation>
			<summary>LazyBinaryObjectInspector.getPrimitiveJavaObject copies beyond length of underlying BytesWritable</summary>
			<description>LazyBinaryObjectInspector.getPrimitiveJavaObject copies the full capacity of the LazyBinary&amp;amp;apos;s underlying BytesWritable object, which can be greater than the size of the actual contents. 
This leads to additional characters at the end of the ByteArrayRef returned. When the LazyBinary object gets re-used, there can be remnants of the later portion of previous entry. 
This was not seen while reading through hive queries, which I think is because a copy elsewhere seems to create LazyBinary with length == capacity. (probably LazyBinary copy constructor). This was seen when MR or pig used Hcatalog to read the data.</description>
			<version>0.9.0</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyUtils.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazybinary.TestLazyBinarySerDe.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyBinaryObjectInspector.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableBinaryObjectInspector.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaBinaryObjectInspector.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">430</link>
		</links>
	</bug>
	<bug id="3064" opendate="2012-05-30 09:43:53" fixdate="2012-07-14 12:14:08" resolution="Duplicate">
		<buginformation>
			<summary>in "insert into tablename" statement,if the "tablename" contains uppercase characters this statement will overwrite the table</summary>
			<description>in "insert into tablename" statement,
if the "tablename" contains uppercase characters this statement will overwrite the table.
For Example:
hive&amp;gt; desc dual;
OK
dummy   string  
Time taken: 1.856 seconds
hive&amp;gt; select * from dual;
OK
dummy
Time taken: 3.133 seconds
drop table if exists tmp_test_1 ;
create EXTERNAL table tmp_test_1 (dummy string) partitioned by (dt string, hr string);
insert into table tmp_test_1 partition (dt=&amp;amp;apos;1&amp;amp;apos;, hr=&amp;amp;apos;1&amp;amp;apos;)  select * from dual;
insert into table tmp_TEST_1 partition (dt=&amp;amp;apos;1&amp;amp;apos;, hr=&amp;amp;apos;1&amp;amp;apos;) select count from dual;
select * from tmp_test_1;
Result :
OK
1       1       1
Time taken: 0.121 seconds</description>
			<version>0.8.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">3062</link>
		</links>
	</bug>
	<bug id="3247" opendate="2012-07-09 22:08:55" fixdate="2012-07-17 04:16:30" resolution="Fixed">
		<buginformation>
			<summary>Sorted by order of table not respected</summary>
			<description>When a table a sorted by a column or columns, and data is inserted with hive.enforce.sorting=true, regardless of whether the metadata says the table is sorted in ascending or descending order, the data will be sorted in ascending order.
e.g.
create table table_desc(key string, value string) clustered by (key) sorted by (key DESC) into 1 BUCKETS;
create table table_asc(key string, value string) clustered by (key) sorted by (key ASC) into 1 BUCKETS;
insert overwrite table table_desc select key, value from src;
insert overwrite table table_asc select key, value from src;
select * from table_desc;
...
96	val_96
97	val_97
97	val_97
98	val_98
98	val_98
select * from table_asc;
...
96	val_96
97	val_97
97	val_97
98	val_98
98	val_98</description>
			<version>0.10.0</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
	</bug>
	<bug id="3248" opendate="2012-07-10 12:24:46" fixdate="2012-07-17 04:55:02" resolution="Fixed">
		<buginformation>
			<summary>lack of semi-colon in .q file leads to missing the next statement</summary>
			<description>set hive.check.par=1
select count(1) from src;
select count(1) from src;
If the above .q file is executed, the first statement is lost.
Found this while reviewing https://issues.apache.org/jira/browse/HIVE-2848</description>
			<version>0.9.0</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.processors.SetProcessor.java</file>
		</fixedFiles>
	</bug>
	<bug id="2544" opendate="2011-11-02 16:44:30" fixdate="2012-07-17 06:12:30" resolution="Fixed">
		<buginformation>
			<summary>Nullpointer on registering udfs.</summary>
			<description>Currently the Function registry can throw NullPointers when multiple threads are trying to register the same function. The normal put() will replace the existing registered function object even if it&amp;amp;apos;s exactly the same function.</description>
			<version>0.9.0</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">3263</link>
			<link type="Reference" description="is related to">2573</link>
		</links>
	</bug>
	<bug id="3230" opendate="2012-07-05 17:33:18" fixdate="2012-07-18 06:36:39" resolution="Fixed">
		<buginformation>
			<summary>Make logging of plan progress in HadoopJobExecHelper configurable</summary>
			<description>Currently, by default, every second a job is run a massive JSON string containing the query plan, the tasks, and some counters is logged to the hive_job_log.  For large, long running jobs that can easily reach gigabytes of data. This logging should be configurable as average user doesn&amp;amp;apos;t need this logging.</description>
			<version>0.10.0</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.HadoopJobExecHelper.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
	</bug>
	<bug id="3246" opendate="2012-07-09 21:30:42" fixdate="2012-07-19 17:13:22" resolution="Fixed">
		<buginformation>
			<summary>java primitive type for binary datatype should be byte[]</summary>
			<description>PrimitiveObjectInspector.getPrimitiveJavaObject is supposed to return a java object. But in case of binary datatype, it returns ByteArrayRef (not java standard type). The suitable java object for it would be byte[]. </description>
			<version>0.9.0</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyUtils.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.binarysortable.TestBinarySortableSerDe.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyBinaryObjectInspector.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantBinaryObjectInspector.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.TestStatsSerde.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.TestStandardObjectInspectors.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazybinary.MyTestClassBigger.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableBinaryObjectInspector.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaBinaryObjectInspector.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.primitive.BinaryObjectInspector.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazybinary.TestLazyBinarySerDe.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.columnar.TestLazyBinaryColumnarSerDe.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.binarysortable.MyTestClass.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableBinaryObjectInspector.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">3266</link>
		</links>
	</bug>
	<bug id="3205" opendate="2012-06-28 00:46:32" fixdate="2012-07-20 03:51:14" resolution="Fixed">
		<buginformation>
			<summary>Bucketed mapjoin on partitioned table which has no partition throws NPE</summary>
			<description>

create table hive_test_smb_bucket1 (key int, value string) partitioned by (ds string) clustered by (key) sorted by (key) into 2 buckets;
create table hive_test_smb_bucket2 (key int, value string) partitioned by (ds string) clustered by (key) sorted by (key) into 2 buckets;

set hive.optimize.bucketmapjoin = true;
set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;

explain
SELECT /* + MAPJOIN(b) */ b.key as k1, b.value, b.ds, a.key as k2
FROM hive_test_smb_bucket1 a JOIN
hive_test_smb_bucket2 b
ON a.key = b.key WHERE a.ds = &amp;amp;apos;2010-10-15&amp;amp;apos; and b.ds=&amp;amp;apos;2010-10-15&amp;amp;apos; and  b.key IS NOT NULL;


throws NPE

2012-06-28 08:59:13,459 ERROR ql.Driver (SessionState.java:printError(400)) - FAILED: NullPointerException null
java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.optimizer.BucketMapJoinOptimizer$BucketMapjoinOptProc.process(BucketMapJoinOptimizer.java:269)
	at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:89)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:88)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:125)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:102)
	at org.apache.hadoop.hive.ql.optimizer.BucketMapJoinOptimizer.transform(BucketMapJoinOptimizer.java:100)
	at org.apache.hadoop.hive.ql.optimizer.Optimizer.optimize(Optimizer.java:87)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:7564)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:245)
	at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:50)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:245)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:430)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:335)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:902)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:258)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:215)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:406)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:744)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:607)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:186)

</description>
			<version>0.10.0</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.plan.MapredLocalWork.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.FetchWork.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.BucketMapJoinOptimizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.GenMRTableScan1.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.PrunedPartitionList.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapredLocalTask.java</file>
		</fixedFiles>
	</bug>
	<bug id="3070" opendate="2012-05-31 09:45:43" fixdate="2012-07-21 03:39:48" resolution="Fixed">
		<buginformation>
			<summary>Filter on outer join condition removed while merging join tree</summary>
			<description>should the result of query A: 
select s.aa, s.bb, c.key keyc from (select a.key aa, b.key bb from src a left outer join src b on a.key=b.key) s left outer join src c on s.bb=c.key and s.bb&amp;lt;10 where s.aa&amp;lt;20;
be the same as query B:
select a.key keya, b.key keyb, c.key keyc from src a left outer join src b on a.key=b.key left outer join src c on b.key=c.key and b.key&amp;lt;10 where a.key&amp;lt;20;
?
Currently, the result is different, query B gets wrong result!
In SemanticAnalyzer.java, mergeJoins():
ArrayList&amp;lt;ArrayList&amp;lt;ASTNode&amp;gt;&amp;gt; filters = target.getFilters();
for (int i = 0; i &amp;lt; nodeRightAliases.length; i++) 
{
  filters.add(node.getFilters().get(i + 1));
}

filters in node.getFilters().get(0) are lost.</description>
			<version>0.8.1</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
	</bug>
	<bug id="3225" opendate="2012-07-03 02:29:46" fixdate="2012-07-24 00:25:39" resolution="Fixed">
		<buginformation>
			<summary>NPE on a join query with authorization enabled</summary>
			<description>when performing a join query which filters by a non-existent partition in the where clause (ie):
select t1.a as a1, t2.a as a2 from t1 join t2 on t1.a=t2.a where t2.part="non-existent";
It returns an NPE. It seems that the partition since non-existent is not part of the list of inputs (or maybe optimized out?). But the TableScanOperator still has a reference to it which causes an NPE after tableUsePartLevelAuth.get() returns null.
FAILED: Hive Internal Error: java.lang.NullPointerException(null)java.lang.NullPointerException        at org.apache.hadoop.hive.ql.Driver.doAuthorization(Driver.java:617)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:486)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:336)        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:909)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:258)        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:215)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:406)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:689)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:557)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)</description>
			<version>0.9.0</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
		</fixedFiles>
	</bug>
	<bug id="3295" opendate="2012-07-24 23:24:42" fixdate="2012-07-25 06:26:24" resolution="Fixed">
		<buginformation>
			<summary>HIVE-3128 introduced bug causing dynamic partitioning to fail</summary>
			<description>HIVE-3128 introduced a new commons-compress jar and imports classes from it in FileUtils.java  The FileUtils class is accessed by dynamic partitioning in the map reduce cluster where the jar is not available, causing the query to fail.</description>
			<version>0.10.0</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
			<file type="M">org.apache.hadoop.hive.common.FileUtils.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">3423</link>
			<link type="Regression" description="is broken by">3128</link>
		</links>
	</bug>
	<bug id="3126" opendate="2012-06-13 17:41:16" fixdate="2012-07-25 06:48:39" resolution="Fixed">
		<buginformation>
			<summary>Generate &amp; build the velocity based Hive tests on windows by fixing the path issues</summary>
			<description>1)Escape the backward slash in Canonical Path if unit test runs on windows.
2)Diff comparison  
     a.	Ignore the extra spacing on windows
     b.	Ignore the different line endings on windows &amp;amp; Unix
     c.	Convert the file paths to windows specific. (Handle spaces etc..)
3)Set the right file scheme &amp;amp; class path separators while invoking the junit task from </description>
			<version>0.9.0</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.contrib.mr.TestGenericMR.java</file>
			<file type="M">org.apache.hadoop.hive.ql.QTestUtil.java</file>
			<file type="M">org.apache.hadoop.hive.ant.QTestGenTask.java</file>
			<file type="M">org.apache.hadoop.fs.ProxyFileSystem.java</file>
			<file type="M">org.apache.hadoop.fs.ProxyLocalFileSystem.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Context.java</file>
		</fixedFiles>
		<links>
			<link type="Incorporates" description="is part of">2998</link>
			<link type="Required" description="requires">3172</link>
		</links>
	</bug>
	<bug id="2101" opendate="2011-04-08 19:11:01" fixdate="2012-07-27 07:50:48" resolution="Fixed">
		<buginformation>
			<summary>mapjoin sometimes gives wrong results if there is a filter in the on condition</summary>
			<description>"SELECT / * + mapjoin(src1, src2) * / * FROM src src1 RIGHT OUTER JOIN src src2 ON (src1.key = src2.key AND src1.key &amp;lt; 10 AND src2.key &amp;gt; 10) JOIN src src3 ON (src2.key = src3.key AND src3.key &amp;lt; 10) SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;" will give wrong results in today&amp;amp;apos;s hive</description>
			<version>0.10.0</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
		</fixedFiles>
	</bug>
	<bug id="3218" opendate="2012-06-30 00:33:51" fixdate="2012-07-30 07:22:30" resolution="Fixed">
		<buginformation>
			<summary>Stream table of SMBJoin/BucketMapJoin with two or more partitions is not handled properly</summary>
			<description>

drop table hive_test_smb_bucket1;
drop table hive_test_smb_bucket2;

create table hive_test_smb_bucket1 (key int, value string) partitioned by (ds string) clustered by (key) sorted by (key) into 2 buckets;
create table hive_test_smb_bucket2 (key int, value string) partitioned by (ds string) clustered by (key) sorted by (key) into 2 buckets;

set hive.enforce.bucketing = true;
set hive.enforce.sorting = true;

insert overwrite table hive_test_smb_bucket1 partition (ds=&amp;amp;apos;2010-10-14&amp;amp;apos;) select key, value from src;
insert overwrite table hive_test_smb_bucket1 partition (ds=&amp;amp;apos;2010-10-15&amp;amp;apos;) select key, value from src;
insert overwrite table hive_test_smb_bucket2 partition (ds=&amp;amp;apos;2010-10-15&amp;amp;apos;) select key, value from src;


set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;

SELECT /* + MAPJOIN(b) */ * FROM hive_test_smb_bucket1 a JOIN hive_test_smb_bucket2 b ON a.key = b.key;


which make bucket join context..

Alias Bucket Output File Name Mapping:
        hdfs://localhost:9000/user/hive/warehouse/hive_test_smb_bucket1/ds=2010-10-14/000000_0 0
        hdfs://localhost:9000/user/hive/warehouse/hive_test_smb_bucket1/ds=2010-10-14/000001_0 1
        hdfs://localhost:9000/user/hive/warehouse/hive_test_smb_bucket1/ds=2010-10-15/000000_0 0
        hdfs://localhost:9000/user/hive/warehouse/hive_test_smb_bucket1/ds=2010-10-15/000001_0 1


fails with exception

java.lang.RuntimeException: Hive Runtime Error while closing operators
	at org.apache.hadoop.hive.ql.exec.ExecMapper.close(ExecMapper.java:226)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:57)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:391)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:325)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:270)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1127)
	at org.apache.hadoop.mapred.Child.main(Child.java:264)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to rename output from: hdfs://localhost:9000/tmp/hive-navis/hive_2012-06-29_22-17-49_574_6018646381714861925/_task_tmp.-ext-10001/_tmp.000001_0 to: hdfs://localhost:9000/tmp/hive-navis/hive_2012-06-29_22-17-49_574_6018646381714861925/_tmp.-ext-10001/000001_0
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator$FSPaths.commit(FileSinkOperator.java:198)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator$FSPaths.access$300(FileSinkOperator.java:100)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.closeOp(FileSinkOperator.java:717)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:557)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:566)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:566)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:566)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:566)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:566)
	at org.apache.hadoop.hive.ql.exec.ExecMapper.close(ExecMapper.java:193)
	... 8 more

</description>
			<version>0.10.0</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.BucketMapJoinOptimizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecMapperContext.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.MapJoinDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.BucketMatcher.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.MapJoinResolver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.MapredLocalWork.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.SortedMergeBucketMapJoinOptimizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.DefaultBucketMatcher.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapredLocalTask.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">3171</link>
			<link type="Regression" description="breaks">3429</link>
		</links>
	</bug>
	<bug id="2925" opendate="2012-04-04 04:08:42" fixdate="2012-08-17 18:29:31" resolution="Fixed">
		<buginformation>
			<summary>Support non-MR fetching for simple queries with select/limit/filter operations only</summary>
			<description>It&amp;amp;apos;s trivial but frequently asked by end-users. Currently, select queries with simple conditions or limit should run MR job which takes some time especially for big tables, making the people irritated.
For that kind of simple queries, using fetch task would make them happy.</description>
			<version>0.10.0</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.IOContext.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.FetchWork.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.GlobalLimitCtx.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FetchTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecMapperContext.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.PrunedPartitionList.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.QB.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">3045</link>
			<link type="Duplicate" description="duplicates">887</link>
			<link type="Reference" description="is related to">3990</link>
			<link type="Reference" description="is related to">5718</link>
		</links>
	</bug>
	<bug id="3226" opendate="2012-07-05 02:08:39" fixdate="2012-08-24 12:04:13" resolution="Fixed">
		<buginformation>
			<summary>ColumnPruner is not working on LateralView</summary>
			<description>Column pruning is not applied to LVJ and SEL operator, which makes exceptions at various stages. For example,

drop table array_valued_src;
create table array_valued_src (key string, value array&amp;lt;string&amp;gt;);
insert overwrite table array_valued_src select key, array(value) from src;

select sum(val) from (select a.key as key, b.value as array_val from src a join array_valued_src b on a.key=b.key) i lateral view explode (array_val) c as val;

... 9 more
Caused by: java.lang.RuntimeException: Reduce operator initialization failed
	at org.apache.hadoop.hive.ql.exec.ExecReducer.configure(ExecReducer.java:157)
	... 14 more
Caused by: java.lang.RuntimeException: cannot find field _col0 from [0:_col5]
	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.getStandardStructFieldRef(ObjectInspectorUtils.java:345)
	at org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getStructFieldRef(StandardStructObjectInspector.java:143)
	at org.apache.hadoop.hive.ql.exec.ExprNodeColumnEvaluator.initialize(ExprNodeColumnEvaluator.java:57)
	at org.apache.hadoop.hive.ql.exec.Operator.initEvaluators(Operator.java:896)
	at org.apache.hadoop.hive.ql.exec.Operator.initEvaluatorsAndReturnStruct(Operator.java:922)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:60)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:357)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:433)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:389)
	at org.apache.hadoop.hive.ql.exec.JoinOperator.initializeOp(JoinOperator.java:62)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:357)
	at org.apache.hadoop.hive.ql.exec.ExecReducer.configure(ExecReducer.java:150)

</description>
			<version>0.10.0</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcCtx.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.LateralViewJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ColumnPruner.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">1901</link>
		</links>
	</bug>
	<bug id="3423" opendate="2012-09-01 01:11:32" fixdate="2012-09-02 16:20:55" resolution="Duplicate">
		<buginformation>
			<summary>merge_dynamic_partition.q is failing when running hive on real cluster</summary>
			<description>merge_dynamic_partition (and a number of other qfiles) is failing when running the current hive on a real cluster:
java.lang.RuntimeException: java.lang.NoClassDefFoundError: org/apache/commons/compress/compressors/gzip/GzipCompressorOutputStream
	at org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:161)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:393)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:327)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:268)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1332)
	at org.apache.hadoop.mapred.Child.main(Child.java:262)
Caused by: java.lang.NoClassDefFoundError: org/apache/commons/compress/compressors/gzip/GzipCompressorOutputStream
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.getDynPartDirectory(FileSinkOperator.java:644)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.getDynOutPaths(FileSinkOperator.java:613)
	at </description>
			<version>0.10.0</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
			<file type="M">org.apache.hadoop.hive.common.FileUtils.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">3295</link>
			<link type="Reference" description="relates to">3128</link>
		</links>
	</bug>
	<bug id="3098" opendate="2012-06-06 18:34:56" fixdate="2012-09-07 14:23:08" resolution="Fixed">
		<buginformation>
			<summary>Memory leak from large number of FileSystem instances in FileSystem.CACHE</summary>
			<description>The problem manifested from stress-testing HCatalog 0.4.1 (as part of testing the Oracle backend).
The HCatalog server ran out of memory (-Xmx2048m) when pounded by 60-threads, in under 24 hours. The heap-dump indicates that hadoop::FileSystem.CACHE had 1000000 instances of FileSystem, whose combined retained-mem consumed the entire heap.
It boiled down to hadoop::UserGroupInformation::equals() being implemented such that the "Subject" member is compared for equality ("=="), and not equivalence (".equals()"). This causes equivalent UGI instances to compare as unequal, and causes a new FileSystem instance to be created and cached.
The UGI.equals() is so implemented, incidentally, as a fix for yet another problem (HADOOP-6670); so it is unlikely that that implementation can be modified.
The solution for this is to check for UGI equivalence in HCatalog (i.e. in the Hive metastore), using an cache for UGI instances in the shims.
I have a patch to fix this. I&amp;amp;apos;ll upload it shortly. I just ran an overnight test to confirm that the memory-leak has been arrested.</description>
			<version>0.9.0</version>
			<fixedVersion>0.9.1, 0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.TUGIBasedProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.shims.HadoopShimsSecure.java</file>
			<file type="M">org.apache.hadoop.hive.shims.HadoopShims.java</file>
			<file type="M">org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">3545</link>
			<link type="Reference" description="relates to">4501</link>
			<link type="Reference" description="relates to">5296</link>
			<link type="Reference" description="relates to">3513</link>
			<link type="Reference" description="is related to">9234</link>
		</links>
	</bug>
	<bug id="3171" opendate="2012-06-21 18:09:50" fixdate="2012-09-07 17:41:00" resolution="Fixed">
		<buginformation>
			<summary>Bucketed sort merge join doesn&amp;apos;t work when multiple files exist for small alias</summary>
			<description>Executing a query with the MAPJOIN hint and the bucketed sort merge join optimizations enabled:

set hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;


works fine with partitioned tables if there is only one partition in the table. However, if you add a second partition, Hive attempts to do a regular map-side join which can fail because the tables are too large. Hive ought to be able to still do the bucketed sort merge join with partitions.</description>
			<version>0.10.0</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.plan.FetchWork.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.ColumnProjectionUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecMapperContext.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.SortedMergeBucketMapJoinOptimizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapredLocalTask.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">3290</link>
			<link type="Blocker" description="is blocked by">3218</link>
			<link type="Blocker" description="is blocked by">3210</link>
		</links>
	</bug>
	<bug id="2957" opendate="2012-04-17 14:25:07" fixdate="2012-09-13 17:51:11" resolution="Fixed">
		<buginformation>
			<summary>Hive JDBC doesn&amp;apos;t support TIMESTAMP column</summary>
			<description>Steps to replicate:
1. Create a table with at least one column of type TIMESTAMP
2. Do a DatabaseMetaData.getColumns () such that this TIMESTAMP column is part of the resultset.
3. When you iterate over the TIMESTAMP column it would fail, throwing the below exception:
Exception in thread "main" java.sql.SQLException: Unrecognized column type: timestamp
	at org.apache.hadoop.hive.jdbc.Utils.hiveTypeToSqlType(Utils.java:56)
	at org.apache.hadoop.hive.jdbc.JdbcColumn.getSqlType(JdbcColumn.java:62)
	at org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData$2.next(HiveDatabaseMetaData.java:244)</description>
			<version>0.8.1</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.jdbc.Utils.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.JdbcColumn.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveBaseResultSet.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HivePreparedStatement.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">11748</link>
		</links>
	</bug>
	<bug id="967" opendate="2009-12-02 19:50:39" fixdate="2012-10-16 17:41:00" resolution="Fixed">
		<buginformation>
			<summary>Implement "show create table"</summary>
			<description>SHOW CREATE TABLE would be very useful in cases where you are trying to figure out the partitioning and/or bucketing scheme for a table. Perhaps this could be implemented by having new tables automatically SET PROPERTIES (create_command=&amp;amp;apos;raw text of the create statement&amp;amp;apos;)?</description>
			<version>0.6.0</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>New Feature</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
			<file type="M">org.apache.hadoop.hive.ql.ErrorMsg.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">1187</link>
			<link type="Reference" description="relates to">4659</link>
			<link type="Reference" description="relates to">11706</link>
			<link type="Reference" description="is related to">4210</link>
			<link type="Reference" description="is related to">2816</link>
		</links>
	</bug>
	<bug id="2715" opendate="2012-01-12 19:41:02" fixdate="2012-11-08 09:53:26" resolution="Fixed">
		<buginformation>
			<summary>Upgrade Thrift dependency to 0.9.0</summary>
			<description>I work on HCatalog (0.2). Recently, we ran into HCat_server running out of memory every few days, and it boiled down to a bug in thrift, (THRIFT-1468, recently fixed).
HCat-0.2-branch depends on Hive-0.8, which in turn depends on thrift-0.5.0. (The bug also exists on 0.7.0.)
May I please enquire if Hive can&amp;amp;apos;t depend on a more current version of thrift? (Does it break the metastore?) I&amp;amp;apos;m afraid I&amp;amp;apos;m not privy to the reasoning behind Hive&amp;amp;apos;s dependency on a slightly dated thrift-lib. </description>
			<version>0.8.0</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.ArchiveUtils.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.Order.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFFormatNumber.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.InvalidObjectException.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.HiveObjectType.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.thrift.test.IntString.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.thrift.test.MiniStruct.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.UnknownDBException.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ListSinkOperator.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.thrift.test.Complex.java</file>
			<file type="M">org.apache.hadoop.hive.ql.processors.SetProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TestExecDriver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDTFParseUrlTuple.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.FieldSchema.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.NoSuchObjectException.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.StringColumnStatsData.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ConfigValSecurityException.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.TestHive.java</file>
			<file type="M">org.apache.hadoop.hive.hbase.HBaseSerDe.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyUtils.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.SerDeInfo.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeList.java</file>
			<file type="M">org.apache.hadoop.hive.contrib.serde2.RegexSerDe.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.PrincipalType.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.IndexAlreadyExistsException.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFSortArray.java</file>
			<file type="M">org.apache.hadoop.hive.service.HiveClusterStatus.java</file>
			<file type="M">org.apache.hadoop.hive.service.TestHiveServer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFEvaluateNPE.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.api.QueryPlan.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.UnknownPartitionException.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.JoinUtil.java</file>
			<file type="D">org.apache.hadoop.hive.metastore.api.Constants.java</file>
			<file type="M">org.apache.hadoop.hive.hbase.TestHBaseSerDe.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.EnvironmentContext.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.PartitionEventType.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.TestRCFile.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFArrayContains.java</file>
			<file type="M">org.apache.hadoop.hive.ql.QTestUtil.java</file>
			<file type="M">org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.Schema.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.TestHiveMetaTool.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.api.Stage.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.parser.ExpressionTree.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.DelimitedJSONSerDe.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.Database.java</file>
			<file type="M">org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.TestCrossMapEqualComparer.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.TestTCTLSeparatedProtocol.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.columnar.TestLazyBinaryColumnarSerDe.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe.java</file>
			<file type="M">org.apache.hadoop.hive.service.JobTrackerState.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.typeinfo.UnionTypeInfo.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFConcatWS.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFWhen.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.AlreadyExistsException.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.PrivilegeBag.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveQueryResultSet.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.StorageDescriptor.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFIf.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.StandardMapObjectInspector.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDTFJSONTuple.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
			<file type="M">org.apache.hadoop.hive.hbase.HBaseStorageHandler.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.api.Adjacency.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.java</file>
			<file type="M">org.apache.hadoop.hive.service.HiveServerException.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.TestSimpleMapEqualComparer.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.MetaException.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ColumnStatistics.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFTestTranslate.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.StandardListObjectInspector.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.api.Operator.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.HiveObjectRef.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.Type.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.Role.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.api.OperatorType.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.Partition.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.dynamic_type.TestDynamicSerDe.java</file>
			<file type="M">org.apache.hadoop.hive.contrib.genericudf.example.GenericUDFDBOutput.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.LongColumnStatsData.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.SkewedInfo.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.Index.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.api.AdjacencyType.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFSize.java</file>
			<file type="M">org.apache.hadoop.hive.service.ThriftHive.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.TestLazySimpleSerDe.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.TestHiveMetaStoreChecker.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFPrintf.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.PartitionDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.api.Query.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Table.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.binarysortable.TestBinarySortableSerDe.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyMapObjectInspector.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.UnknownTableException.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyListObjectInspector.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			<file type="D">org.apache.hadoop.hive.serde.Constants.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.java</file>
			<file type="M">org.apache.hadoop.hive.serde.test.InnerStruct.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.InvalidInputException.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.api.Task.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.InvalidPartitionException.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.api.NodeType.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazybinary.TestLazyBinarySerDe.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.RegexSerDe.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.thrift.test.MyEnum.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.Table.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.Version.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.InvalidOperationException.java</file>
			<file type="M">org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordReader.java</file>
			<file type="M">org.apache.hadoop.hive.serde.test.ThriftTestObj.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.TestStatsSerde.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.api.StageType.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.TableDesc.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.api.Graph.java</file>
			<file type="M">org.apache.hadoop.hive.contrib.serde2.TestRegexSerDe.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.api.TaskType.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TestExpressionEvaluator.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">183</link>
			<link type="Reference" description="is related to">1468</link>
		</links>
	</bug>
	<bug id="3480" opendate="2012-09-18 15:02:32" fixdate="2012-11-08 16:32:12" resolution="Fixed">
		<buginformation>
			<summary>&lt;Resource leak&gt;: Fix the file handle leaks in Symbolic &amp; Symlink related input formats.</summary>
			<description>Noticed these file handle leaks while fixing the Symlink related unit test failures on Windows.</description>
			<version>0.9.1</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.SymbolicInputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.TestSymlinkTextInputFormat.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">3335</link>
		</links>
	</bug>
	<bug id="3243" opendate="2012-07-09 03:24:31" fixdate="2012-11-13 16:03:12" resolution="Fixed">
		<buginformation>
			<summary>ignore white space between entries of hive/hbase table mapping</summary>
			<description>In hive/hbase integration, when creating a hive/hbase table, white space is not ignored in hbase.columns.mapping. 
e.g. "cf:foo, cf:bar" will create two column families "cf" and " cf" in the underlying hbase table, which is certainly not what the user want and make them confused.</description>
			<version>0.9.0</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.hbase.HBaseSerDe.java</file>
		</fixedFiles>
	</bug>
	<bug id="3197" opendate="2012-06-25 18:19:32" fixdate="2012-11-28 08:00:28" resolution="Fixed">
		<buginformation>
			<summary>Hive compile errors under Java 7 (JDBC 4.1)</summary>
			<description>Hi, I&amp;amp;apos;ve been trying to compile Hive trunk from source and getting failures:


    [javac] hive-svn/jdbc/src/java/org/apache/hadoop/hive/jdbc/HiveCallableStatement.java:48: error: HiveCallableStatement is not abstract and does not override abstract method &amp;lt;T&amp;gt;getObject(String,Class&amp;lt;T&amp;gt;) in CallableStatement
    [javac] public class HiveCallableStatement implements java.sql.CallableStatement {
    [javac]        ^
    [javac]   where T is a type-variable:
    [javac]     T extends Object declared in method &amp;lt;T&amp;gt;getObject(String,Class&amp;lt;T&amp;gt;)


I think this is because JDBC 4.1 is part of Java 7, and is not source-compatible with older JDBC versions. Any chance you guys could add JDBC 4.1 support?</description>
			<version>0.10.0</version>
			<fixedVersion>0.11.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveCallableStatement.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HivePreparedStatement.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveConnection.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveDriver.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveStatement.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveBaseResultSet.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveDataSource.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveQueryResultSet.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">3384</link>
		</links>
	</bug>
	<bug id="3709" opendate="2012-11-14 01:39:35" fixdate="2012-11-29 04:50:44" resolution="Fixed">
		<buginformation>
			<summary>Stop storing default ConfVars in temp file</summary>
			<description>To work around issues with Hadoop&amp;amp;apos;s Configuration object, specifically it&amp;amp;apos;s addResource(InputStream), default configurations are written to a temp file (I think HIVE-2362 introduced this).
This, however, introduces the problem that once that file is deleted from /tmp the client crashes.  This is particularly problematic for long running services like the metastore server.
Writing a custom InputStream to deal with the problems in the Configuration object should provide a work around, which does not introduce a time bomb into Hive.</description>
			<version>0.10.0</version>
			<fixedVersion>0.11.0</fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">3596</link>
		</links>
	</bug>
	<bug id="3596" opendate="2012-10-18 12:11:46" fixdate="2012-11-29 04:52:35" resolution="Duplicate">
		<buginformation>
			<summary>Regression - HiveConf static variable causes issues in long running JVM instances with /tmp/ data</summary>
			<description>With Hive 0.8.x, HiveConf was changed to utilize the private, static member "confVarURL" which points to /tmp/hive-&amp;lt;user&amp;gt;-&amp;lt;tmp_number&amp;gt;.xml for job configuration settings. 
During long running JVMs, such as a Beeswax server, which creates multiple HiveConf objects over time this variable does not properly get updated between jobs and can cause job failure if the OS cleans /tmp/ during a cron job. </description>
			<version>0.8.0</version>
			<fixedVersion>0.8.1, 0.9.0, 0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">3709</link>
		</links>
	</bug>
	<bug id="3045" opendate="2012-05-23 13:08:12" fixdate="2012-12-12 01:40:58" resolution="Duplicate">
		<buginformation>
			<summary>Partition column values are not valid if any of virtual columns is selected</summary>
			<description>For example,


hive&amp;gt; select * from srcpart where key &amp;lt; 5;

0	val_0	2008-04-08	11
4	val_4	2008-04-08	11
0	val_0	2008-04-08	11
0	val_0	2008-04-08	11
2	val_2	2008-04-08	11
0	val_1	2008-04-09	12
4	val_5	2008-04-09	12
3	val_4	2008-04-09	12
2	val_3	2008-04-09	12
0	val_1	2008-04-09	12
1	val_2	2008-04-09	12

hive&amp;gt; select *, BLOCK__OFFSET__INSIDE__FILE from srcpart where key &amp;lt; 5;

0	val_0	2008-04-09	11	968
4	val_4	2008-04-09	11	1218
0	val_0	2008-04-09	11	2088
0	val_0	2008-04-09	11	2632
2	val_2	2008-04-09	11	4004
0	val_1	2008-04-09	11	682
4	val_5	2008-04-09	11	1131
3	val_4	2008-04-09	11	1163
2	val_3	2008-04-09	11	2629
0	val_1	2008-04-09	11	4367
1	val_2	2008-04-09	11	5669

</description>
			<version>0.10.0</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.IOContext.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.FetchWork.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.GlobalLimitCtx.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FetchTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecMapperContext.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.PrunedPartitionList.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.QB.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">2925</link>
		</links>
	</bug>
	<bug id="3853" opendate="2013-01-04 02:08:05" fixdate="2013-01-08 18:20:17" resolution="Fixed">
		<buginformation>
			<summary>UDF unix_timestamp is deterministic if an argument is given, but it treated as non-deterministic preventing PPD</summary>
			<description>unix_timestamp is declared as a non-deterministic function. But if user provides an argument, it makes deterministic result and eligible to PPD.</description>
			<version>0.4.1</version>
			<fixedVersion>0.11.0</fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">1986</link>
		</links>
	</bug>
	<bug id="1649" opendate="2010-09-17 04:32:03" fixdate="2013-01-12 00:04:10" resolution="Duplicate">
		<buginformation>
			<summary>Ability to update counters and status from TRANSFORM scripts</summary>
			<description>Hadoop Streaming supports the ability to update counters and status by writing specially coded messages to the script&amp;amp;apos;s stderr stream.
A streaming process can use the stderr to emit counter information. reporter:counter:&amp;lt;group&amp;gt;,&amp;lt;counter&amp;gt;,&amp;lt;amount&amp;gt; should be sent to stderr to update the counter.
A streaming process can use the stderr to emit status information. To set a status, reporter:status:&amp;lt;message&amp;gt; should be sent to stderr.
Hive should support these same features with its TRANSFORM mechanism.</description>
			<version>0.6.0</version>
			<fixedVersion></fixedVersion>
			<type>New Feature</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">305</link>
		</links>
	</bug>
	<bug id="2820" opendate="2012-02-24 01:40:32" fixdate="2013-01-16 16:16:52" resolution="Fixed">
		<buginformation>
			<summary>Invalid tag is used for MapJoinProcessor</summary>
			<description>Testing HIVE-2810, I&amp;amp;apos;ve found tag and alias are used in very confusing manner. For example, query below fails..


hive&amp;gt; set hive.auto.convert.join=true;                                                                                     
hive&amp;gt; select /*+ STREAMTABLE(a) */ * from myinput1 a join myinput1 b on a.key=b.key join myinput1 c on a.key=c.key;        
Total MapReduce jobs = 4
Ended Job = 1667415037, job is filtered out (removed at runtime).
Ended Job = 1739566906, job is filtered out (removed at runtime).
Ended Job = 1113337780, job is filtered out (removed at runtime).
12/02/24 10:27:14 WARN conf.HiveConf: DEPRECATED: Ignoring hive-default.xml found on the CLASSPATH at /home/navis/hive/conf/hive-default.xml
Execution log at: /tmp/navis/navis_20120224102727_cafe0d8d-9b21-441d-bd4e-b83303b31cdc.log
2012-02-24 10:27:14	Starting to launch local task to process map join;	maximum memory = 932118528
java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.processOp(HashTableSinkOperator.java:312)
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:762)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:83)
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)
	at org.apache.hadoop.hive.ql.exec.MapredLocalTask.startForward(MapredLocalTask.java:325)
	at org.apache.hadoop.hive.ql.exec.MapredLocalTask.executeFromChildJVM(MapredLocalTask.java:272)
	at org.apache.hadoop.hive.ql.exec.ExecDriver.main(ExecDriver.java:685)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:186)
Execution failed with exit status: 2
Obtaining error information


Failed task has a plan which doesn&amp;amp;apos;t make sense.

  Stage: Stage-8
    Map Reduce Local Work
      Alias -&amp;gt; Map Local Tables:
        b 
          Fetch Operator
            limit: -1
        c 
          Fetch Operator
            limit: -1
      Alias -&amp;gt; Map Local Operator Tree:
        b 
          TableScan
            alias: b
            HashTable Sink Operator
              condition expressions:
                0 {key} {value}
                1 {key} {value}
                2 {key} {value}
              handleSkewJoin: false
              keys:
                0 [Column[key]]
                1 [Column[key]]
                2 [Column[key]]
              Position of Big Table: 0
        c 
          TableScan
            alias: c
            Map Join Operator
              condition map:
                   Inner Join 0 to 1
                   Inner Join 0 to 2
              condition expressions:
                0 {key} {value}
                1 {key} {value}
                2 {key} {value}
              handleSkewJoin: false
              keys:
                0 [Column[key]]
                1 [Column[key]]
                2 [Column[key]]
              outputColumnNames: _col0, _col1, _col4, _col5, _col8, _col9
              Position of Big Table: 0
              Select Operator
                expressions:
                      expr: _col0
                      type: int
                      expr: _col1
                      type: int
                      expr: _col4
                      type: int
                      expr: _col5
                      type: int
                      expr: _col8
                      type: int
                      expr: _col9
                      type: int
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                File Output Operator
                  compressed: false
                  GlobalTableId: 0
                  table:
                      input format: org.apache.hadoop.mapred.TextInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat

  Stage: Stage-4
    Map Reduce
      Alias -&amp;gt; Map Operator Tree:
        a 
          TableScan
            alias: a
            HashTable Sink Operator
              condition expressions:
                0 {key} {value}
                1 {key} {value}
                2 {key} {value}
              handleSkewJoin: false
              keys:
                0 [Column[key]]
                1 [Column[key]]
                2 [Column[key]]
              Position of Big Table: 0
      Local Work:
        Map Reduce Local Work

</description>
			<version>0.9.0</version>
			<fixedVersion>0.11.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.JoinDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.LocalMapJoinProcFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ConditionalTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.JoinUtil.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.java</file>
		</fixedFiles>
	</bug>
	<bug id="3699" opendate="2012-11-10 09:45:21" fixdate="2013-01-19 03:54:46" resolution="Fixed">
		<buginformation>
			<summary>Multiple insert overwrite into multiple tables query stores same results in all tables</summary>
			<description>(Note: This might be related to HIVE-2750)
I am doing a query with multiple INSERT OVERWRITE to multiple tables in order to scan the dataset only 1 time, and i end up having all these tables with the same content ! It seems the GROUP BY query that returns results is overwriting all the temp tables.
Weird enough, if i had further GROUP BY queries into additional temp tables, grouped by a different field, then all temp tables, even the ones that would have been wrong content are all correctly populated.
This is the misbehaving query:
    FROM nikon
    INSERT OVERWRITE TABLE e1
    SELECT qs_cs_s_aid AS Emplacements, COUNT AS Impressions
    WHERE qs_cs_s_cat=&amp;amp;apos;PRINT&amp;amp;apos; GROUP BY qs_cs_s_aid
    INSERT OVERWRITE TABLE e2
    SELECT qs_cs_s_aid AS Emplacements, COUNT AS Vues
    WHERE qs_cs_s_cat=&amp;amp;apos;VIEW&amp;amp;apos; GROUP BY qs_cs_s_aid
    ;
It launches only one MR job and here are the results. Why does table &amp;amp;apos;e1&amp;amp;apos; contains results from table &amp;amp;apos;e2&amp;amp;apos; ?! Table &amp;amp;apos;e1&amp;amp;apos; should have been empty (see individual SELECTs further below)
    hive&amp;gt; SELECT * from e1;
    OK
    NULL    2
    1627575 25
    1627576 70
    1690950 22
    1690952 42
    1696705 199
    1696706 66
    1696730 229
    1696759 85
    1696893 218
    Time taken: 0.229 seconds
    hive&amp;gt; SELECT * from e2;
    OK
    NULL    2
    1627575 25
    1627576 70
    1690950 22
    1690952 42
    1696705 199
    1696706 66
    1696730 229
    1696759 85
    1696893 218
    Time taken: 0.11 seconds
Here is are the result to the indiviual queries (only the second query returns a result set):
    hive&amp;gt; SELECT qs_cs_s_aid AS Emplacements, COUNT AS Impressions FROM nikon
    WHERE qs_cs_s_cat=&amp;amp;apos;PRINT&amp;amp;apos; GROUP BY qs_cs_s_aid;
    (...)
    OK
          &amp;lt;- There are no results, this is normal
    Time taken: 41.471 seconds
    hive&amp;gt; SELECT qs_cs_s_aid AS Emplacements, COUNT AS Vues FROM nikon
    WHERE qs_cs_s_cat=&amp;amp;apos;VIEW&amp;amp;apos; GROUP BY qs_cs_s_aid;
    (...)
    OK
    NULL  2
    1627575 25
    1627576 70
    1690950 22
    1690952 42
    1696705 199
    1696706 66
    1696730 229
    1696759 85
    1696893 218
    Time taken: 39.607 seconds
</description>
			<version>0.8.1</version>
			<fixedVersion>0.11.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">4173</link>
		</links>
	</bug>
	<bug id="2332" opendate="2011-08-02 05:58:18" fixdate="2013-01-20 07:10:58" resolution="Fixed">
		<buginformation>
			<summary>If all of the parameters of distinct functions are exists in group by columns, query fails in runtime</summary>
			<description>select sum(key_int1), sum(distinct key_int1) from t1 group by key_int1;
fails with message..


FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.MapRedTask


hadoop says..


Caused by: java.lang.IndexOutOfBoundsException: Index: 1, Size: 1
	at java.util.ArrayList.RangeCheck(ArrayList.java:547)
	at java.util.ArrayList.get(ArrayList.java:322)
	at org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.init(StandardStructObjectInspector.java:95)
	at org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.(StandardStructObjectInspector.java:86)
	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector(ObjectInspectorFactory.java:252)
	at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.initEvaluatorsAndReturnStruct(ReduceSinkOperator.java:188)
	at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.processOp(ReduceSinkOperator.java:197)
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:744)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:85)
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:744)
	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:532)


I think the deficient number of key expression, compared to number of key column, is the problem, which should be equal or more. 
Would it be solved if add some key expression? I&amp;amp;apos;ll try.</description>
			<version>0.9.0</version>
			<fixedVersion>0.11.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
		</fixedFiles>
	</bug>
	<bug id="4000" opendate="2013-02-08 00:14:40" fixdate="2013-02-18 02:20:43" resolution="Fixed">
		<buginformation>
			<summary>Hive client goes into infinite loop at 100% cpu</summary>
			<description>The Hive client starts multiple threads to track the progress of the MapReduce jobs. Unfortunately those threads access several static HashMaps that are not protected by locks. When the HashMaps are modified, they sometimes cause race conditions that lead to the client threads getting stuck in infinite loops.</description>
			<version>0.9.0</version>
			<fixedVersion>0.11.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.history.HiveHistory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.QueryPlan.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">3408</link>
		</links>
	</bug>
	<bug id="3428" opendate="2012-09-04 22:55:54" fixdate="2013-02-27 07:22:16" resolution="Fixed">
		<buginformation>
			<summary>Fix log4j configuration errors when running hive on hadoop23</summary>
			<description>There are log4j configuration errors when running hive on hadoop23, some of them may fail testcases, since the following log4j error message could printed to console, or to output file, which diffs from the expected output:
[junit] &amp;lt; log4j:ERROR Could not find value for key log4j.appender.NullAppender
[junit] &amp;lt; log4j:ERROR Could not instantiate appender named "NullAppender".
[junit] &amp;lt; 12/09/04 11:34:42 WARN conf.HiveConf: hive-site.xml not found on CLASSPATH</description>
			<version>0.10.0</version>
			<fixedVersion>0.11.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.shims.ShimLoader.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">4049</link>
			<link type="Reference" description="relates to">3886</link>
		</links>
	</bug>
	<bug id="3628" opendate="2012-10-27 11:46:43" fixdate="2013-02-28 17:27:41" resolution="Fixed">
		<buginformation>
			<summary>Provide a way to use counters in Hive through UDF</summary>
			<description>Currently it is not possible to generate counters through UDF. We should support this. 
Pig currently allows this.</description>
			<version>0.9.0</version>
			<fixedVersion>0.11.0</fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDF.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDTF.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecReducer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecMapper.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.UDTFOperator.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">1016</link>
			<link type="Duplicate" description="is duplicated by">2261</link>
		</links>
	</bug>
	<bug id="2261" opendate="2011-07-06 07:44:52" fixdate="2013-03-08 02:24:43" resolution="Duplicate">
		<buginformation>
			<summary>Add cleanup stages for UDFs</summary>
			<description>In some cases, we bind values at last stage of big SQL from other sources, especially from memcached. I made that kind of UDFs for internal-use.
I found &amp;amp;apos;initialize&amp;amp;apos; method of GenericUDF class is good place for making connections to memcached cluster, but failed to find  somewhere to close/cleanup the connections. If there is cleaup method in GenericUDF class, things can be more neat. If initializing entity like map/reduce/fetch could be also providable to life-cycles(init/close), that makes perfect.</description>
			<version>0.9.0</version>
			<fixedVersion></fixedVersion>
			<type>Wish</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDF.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDTF.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecReducer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecMapper.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.UDTFOperator.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">3628</link>
			<link type="Reference" description="relates to">1016</link>
		</links>
	</bug>
	<bug id="2935" opendate="2012-04-09 18:59:12" fixdate="2013-03-12 18:24:13" resolution="Fixed">
		<buginformation>
			<summary>Implement HiveServer2</summary>
			<description></description>
			<version>0.9.0</version>
			<fixedVersion>0.11.0</fixedVersion>
			<type>New Feature</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.shims.HadoopShimsSecure.java</file>
			<file type="M">org.apache.hadoop.hive.ql.QTestUtil.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
			<file type="M">org.apache.hadoop.hive.ant.QTestGenTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.processors.SetProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.shims.HadoopShims.java</file>
			<file type="D">org.apache.hive.jdbc.beeline.OptionsProcessor.java</file>
			<file type="D">org.apache.hive.jdbc.beeline.HiveBeeline.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.CopyTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
		<links>
			<link type="Container" description="contains">3905</link>
			<link type="Duplicate" description="is duplicated by">3122</link>
			<link type="Duplicate" description="is duplicated by">3545</link>
			<link type="Duplicate" description="is duplicated by">3546</link>
			<link type="Duplicate" description="is duplicated by">3547</link>
			<link type="Duplicate" description="is duplicated by">3548</link>
			<link type="Duplicate" description="is duplicated by">3785</link>
			<link type="Duplicate" description="is duplicated by">3217</link>
			<link type="Reference" description="relates to">4239</link>
			<link type="Reference" description="relates to">1869</link>
			<link type="Reference" description="is related to">3785</link>
			<link type="Reference" description="is related to">4356</link>
			<link type="Reference" description="is related to">7676</link>
			<link type="Reference" description="is related to">9158</link>
			<link type="Reference" description="is related to">80</link>
			<link type="Regression" description="breaks">4188</link>
			<link type="Required" description="is required by">3805</link>
			<link type="dependent" description="depends upon">2264</link>
		</links>
	</bug>
	<bug id="3122" opendate="2012-06-12 22:09:05" fixdate="2013-03-12 18:51:01" resolution="Duplicate">
		<buginformation>
			<summary>Implement JDBC driver for HiveServer2 API</summary>
			<description></description>
			<version>0.9.0</version>
			<fixedVersion></fixedVersion>
			<type>Sub-task</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.shims.HadoopShimsSecure.java</file>
			<file type="M">org.apache.hadoop.hive.ql.QTestUtil.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
			<file type="M">org.apache.hadoop.hive.ant.QTestGenTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.processors.SetProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.shims.HadoopShims.java</file>
			<file type="D">org.apache.hive.jdbc.beeline.OptionsProcessor.java</file>
			<file type="D">org.apache.hive.jdbc.beeline.HiveBeeline.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.CopyTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">2935</link>
			<link type="Reference" description="relates to">3100</link>
		</links>
	</bug>
	<bug id="3546" opendate="2012-10-07 22:06:00" fixdate="2013-03-12 18:51:38" resolution="Duplicate">
		<buginformation>
			<summary>Implement TestBeeLineDriver</summary>
			<description></description>
			<version>0.9.0</version>
			<fixedVersion></fixedVersion>
			<type>Sub-task</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.shims.HadoopShimsSecure.java</file>
			<file type="M">org.apache.hadoop.hive.ql.QTestUtil.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
			<file type="M">org.apache.hadoop.hive.ant.QTestGenTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.processors.SetProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.shims.HadoopShims.java</file>
			<file type="D">org.apache.hive.jdbc.beeline.OptionsProcessor.java</file>
			<file type="D">org.apache.hive.jdbc.beeline.HiveBeeline.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.CopyTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">2935</link>
		</links>
	</bug>
	<bug id="3785" opendate="2012-12-10 06:36:23" fixdate="2013-03-12 18:52:19" resolution="Duplicate">
		<buginformation>
			<summary>Core hive changes for HiveServer2 implementation</summary>
			<description>The subtask to track changes in the core hive components for HiveServer2 implementation</description>
			<version>0.10.0</version>
			<fixedVersion></fixedVersion>
			<type>Sub-task</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.shims.HadoopShimsSecure.java</file>
			<file type="M">org.apache.hadoop.hive.ql.QTestUtil.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
			<file type="M">org.apache.hadoop.hive.ant.QTestGenTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.processors.SetProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.shims.HadoopShims.java</file>
			<file type="D">org.apache.hive.jdbc.beeline.OptionsProcessor.java</file>
			<file type="D">org.apache.hive.jdbc.beeline.HiveBeeline.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.CopyTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">2935</link>
			<link type="Reference" description="relates to">2935</link>
		</links>
	</bug>
	<bug id="2264" opendate="2011-07-06 14:05:06" fixdate="2013-03-29 05:49:23" resolution="Fixed">
		<buginformation>
			<summary>Hive server is SHUTTING DOWN when invalid queries beeing executed.</summary>
			<description>When invalid query is beeing executed, Hive server is shutting down.

"CREATE TABLE SAMPLETABLE(IP STRING , showtime BIGINT ) partitioned by (ds string,ipz int) ROW FORMAT DELIMITED FIELDS TERMINATED BY &amp;amp;apos;\040&amp;amp;apos;"

"ALTER TABLE SAMPLETABLE add Partition(ds=&amp;amp;apos;sf&amp;amp;apos;) location &amp;amp;apos;/user/hive/warehouse&amp;amp;apos; Partition(ipz=100) location &amp;amp;apos;/user/hive/warehouse&amp;amp;apos;"

</description>
			<version>0.9.0</version>
			<fixedVersion>0.11.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Task.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapRedTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapredLocalTask.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">1872</link>
			<link type="Reference" description="is related to">2017</link>
			<link type="dependent" description="is depended upon by">2935</link>
		</links>
	</bug>
	<bug id="3408" opendate="2012-08-27 07:56:33" fixdate="2013-04-02 01:40:57" resolution="Duplicate">
		<buginformation>
			<summary>A race condition is caused within QueryPlan class</summary>
			<description>Hive&amp;amp;apos;s threads are stopped at HashMap.getEntry(..) that is used within QueryPlan#extractCounters() and QueryPlan#updateCountersInQueryPlan().  It seems that a race condition problem of a HashSet object is caused when extractCounters() and updateCountersInQueryPlan() are concurrently executed.  I hit the problem with Hive 0.7.1 but, I think that it also is caused with 0.8.1.
The problem is reported by several persons on mailing list.
http://mail-archives.apache.org/mod_mbox/hive-dev/201201.mbox/%3CCAKTRiE+3x31FDy+3F0c+jZSXQrhxBgT4DOyfZddm7sdX+cu=Zg@mail.gmail.com%3E
http://mail-archives.apache.org/mod_mbox/hive-user/201202.mbox/%3CFC28CCD9-9C75-4F8D-B272-3D50F663A634@gmail.com%3E
The following is a part of my thread dump.

"Thread-1091" prio=10 tid=0x00007fd17112b000 nid=0x1100 runnable [0x00007fd175f60000]
   java.lang.Thread.State: RUNNABLE
   at java.util.HashMap.getEntry(HashMap.java:347)
   at java.util.HashMap.containsKey(HashMap.java:335)
   at java.util.HashSet.contains(HashSet.java:184)
   at org.apache.hadoop.hive.ql.QueryPlan.extractCounters(QueryPlan.java:342)
   at org.apache.hadoop.hive.ql.QueryPlan.getQueryPlan(QueryPlan.java:419)
   at org.apache.hadoop.hive.ql.QueryPlan.toString(QueryPlan.java:592)
   at org.apache.hadoop.hive.ql.history.HiveHistory.logPlanProgress(HiveHistory.java:493)
   at org.apache.hadoop.hive.ql.exec.ExecDriver.progress(ExecDriver.java:395)
   at org.apache.hadoop.hive.ql.exec.ExecDriver.execute(ExecDriver.java:686)
   at org.apache.hadoop.hive.ql.exec.MapRedTask.execute(MapRedTask.java:123)
   at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:130)
   at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:57)
   at org.apache.hadoop.hive.ql.exec.TaskRunner.run(TaskRunner.java:47)
"Thread-1090" prio=10 tid=0x00007fd17012f000 nid=0x10ff runnable [0x00007fd175152000]
   java.lang.Thread.State: RUNNABLE
   at java.util.HashMap.getEntry(HashMap.java:347)
   at java.util.HashMap.containsKey(HashMap.java:335)
   at java.util.HashSet.contains(HashSet.java:184)
   at org.apache.hadoop.hive.ql.QueryPlan.updateCountersInQueryPlan(QueryPlan.java:297)
   at org.apache.hadoop.hive.ql.QueryPlan.getQueryPlan(QueryPlan.java:420)
   at org.apache.hadoop.hive.ql.QueryPlan.toString(QueryPlan.java:592)
   at org.apache.hadoop.hive.ql.history.HiveHistory.logPlanProgress(HiveHistory.java:493)
   at org.apache.hadoop.hive.ql.exec.ExecDriver.progress(ExecDriver.java:395)
   at org.apache.hadoop.hive.ql.exec.ExecDriver.execute(ExecDriver.java:686)
   at org.apache.hadoop.hive.ql.exec.MapRedTask.execute(MapRedTask.java:123)
   at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:130)
   at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:57)
   at org.apache.hadoop.hive.ql.exec.TaskRunner.run(TaskRunner.java:47)</description>
			<version>0.7.1</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.history.HiveHistory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.QueryPlan.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">4000</link>
		</links>
	</bug>
	<bug id="4049" opendate="2013-02-21 22:01:39" fixdate="2013-04-03 09:29:17" resolution="Duplicate">
		<buginformation>
			<summary>local_mapred_error_cache.q with hadoop 23.x fails with additional warning messages</summary>
			<description>When run on branch10 with 23.x, the test fails. An additional warning message leads to failure. The test should be independent of these things.
Diff output:
[junit] 16d15
[junit] &amp;lt; WARNING: org.apache.hadoop.metrics.jvm.EventCounter is deprecated. Please use org.apache.hadoop.log.metrics.EventCounter in all the log4j.properties files.</description>
			<version>0.10.0</version>
			<fixedVersion>0.10.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.shims.ShimLoader.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">3428</link>
		</links>
	</bug>
	<bug id="3308" opendate="2012-07-27 13:52:27" fixdate="2013-04-09 00:45:09" resolution="Fixed">
		<buginformation>
			<summary>Mixing avro and snappy gives null values</summary>
			<description>On default hive uses LazySimpleSerDe for output.
When I now enable compression and "select count from avrotable" the output is a file with the .avro extension but this then will display null values since the file is in reality not an avro file but a file created by LazySimpleSerDe using compression so should be a .snappy file.
This causes any job (exception select * from avrotable is that not truly a job) to show null values.
If you use any serde other then avro you can temporarily fix this by setting "set hive.output.file.extension=.snappy" and it will correctly work again but this won&amp;amp;apos;t work on avro since it overwrites the hive.output.file.extension during initializing.
When you dump the query result into a table with "create table bla as" you can rename the .avro file into .snappy and the "select from bla" will also magiacally work again.
Input and Ouput serdes don&amp;amp;apos;t always match so when I use avro as an input format it should not set the hive.output.file.extension.
Onces it&amp;amp;apos;s set all queries will use it and fail making the connection useless to reuse.</description>
			<version>0.10.0</version>
			<fixedVersion>0.11.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.serde2.avro.AvroSerDe.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">4195</link>
		</links>
	</bug>
	<bug id="3179" opendate="2012-06-22 12:25:47" fixdate="2013-04-15 07:10:44" resolution="Fixed">
		<buginformation>
			<summary>HBase Handler doesn&amp;apos;t handle NULLs properly</summary>
			<description>We found a quite severe issue in the HBase Handler which actually means that Hive potentially returns incorrect data if a column has NULL values in HBase (which means the cell doesn&amp;amp;apos;t even exist)
In HBase Shell:

create &amp;amp;apos;hive_hbase_test&amp;amp;apos;, &amp;amp;apos;test&amp;amp;apos;
put &amp;amp;apos;hive_hbase_test&amp;amp;apos;, &amp;amp;apos;1&amp;amp;apos;, &amp;amp;apos;test:c1&amp;amp;apos;, &amp;amp;apos;c1-1&amp;amp;apos;
put &amp;amp;apos;hive_hbase_test&amp;amp;apos;, &amp;amp;apos;1&amp;amp;apos;, &amp;amp;apos;test:c2&amp;amp;apos;, &amp;amp;apos;c2-1&amp;amp;apos;
put &amp;amp;apos;hive_hbase_test&amp;amp;apos;, &amp;amp;apos;1&amp;amp;apos;, &amp;amp;apos;test:c3&amp;amp;apos;, &amp;amp;apos;c3-1&amp;amp;apos;
put &amp;amp;apos;hive_hbase_test&amp;amp;apos;, &amp;amp;apos;2&amp;amp;apos;, &amp;amp;apos;test:c1&amp;amp;apos;, &amp;amp;apos;c1-2&amp;amp;apos;


In Hive:

DROP TABLE IF EXISTS hive_hbase_test;
CREATE EXTERNAL TABLE hive_hbase_test (
  id int,
  c1 string,
  c2 string,
  c3 string
)
STORED BY &amp;amp;apos;org.apache.hadoop.hive.hbase.HBaseStorageHandler&amp;amp;apos;
WITH SERDEPROPERTIES ("hbase.columns.mapping" =
":key#s,test:c1#s,test:c2#s,test:c3#s")
TBLPROPERTIES("hbase.table.name" = "hive_hbase_test");

hive&amp;gt; select * from hive_hbase_test;
OK
1	c1-1	c2-1	c3-1
2	c1-2	NULL	NULL

hive&amp;gt; select c1 from hive_hbase_test;
c1-1
c1-2

hive&amp;gt; select c1, c2 from hive_hbase_test;
c1-1	c2-1
c1-2	NULL


So far everything is correct but now:

hive&amp;gt; select c1, c2, c2 from hive_hbase_test;
c1-1	c2-1	c2-1
c1-2	NULL	c2-1


Selecting c2 twice works the first time but the second time we
actually get the value from the previous row.

hive&amp;gt; select c1, c3, c2, c2, c3, c3, c1 from hive_hbase_test;
c1-1	c3-1	c2-1	c2-1	c3-1	c3-1	c1-1
c1-2	NULL	NULL	c2-1	c3-1	c3-1	c1-2


We&amp;amp;apos;ve narrowed this down to an early initialization of fieldsInited[fieldID] = true in LazyHBaseRow#uncheckedGetField and we&amp;amp;apos;ll try to provide a patch which surely needs review.</description>
			<version>0.9.0</version>
			<fixedVersion>0.11.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.hbase.TestLazyHBaseObject.java</file>
			<file type="M">org.apache.hadoop.hive.hbase.LazyHBaseRow.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">4057</link>
		</links>
	</bug>
	<bug id="2379" opendate="2011-08-15 23:45:28" fixdate="2013-04-23 06:10:13" resolution="Fixed">
		<buginformation>
			<summary>Hive/HBase integration could be improved</summary>
			<description>  For now any Hive/HBase queries would require the following jars to be explicitly added via hive&amp;amp;apos;s add jar command:
add jar /usr/lib/hive/lib/hbase-0.90.1-cdh3u0.jar;
add jar /usr/lib/hive/lib/hive-hbase-handler-0.7.0-cdh3u0.jar;
add jar /usr/lib/hive/lib/zookeeper-3.3.1.jar;
add jar /usr/lib/hive/lib/guava-r06.jar;
the longer term solution, perhaps, should be to have the code at submit time call hbase&amp;amp;apos;s 
TableMapREduceUtil.addDependencyJar(job, HBaseStorageHandler.class) to ship it in distributedcache.</description>
			<version>0.7.1</version>
			<fixedVersion>0.12.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.MapredWork.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			<file type="M">org.apache.hcatalog.mapreduce.HCatStorageHandler.java</file>
			<file type="M">org.apache.hadoop.hive.hbase.HBaseStorageHandler.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.HiveStorageHandler.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.DefaultStorageHandler.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="is blocked by">3991</link>
			<link type="Reference" description="is related to">8386</link>
		</links>
	</bug>
	<bug id="1016" opendate="2009-12-28 20:34:33" fixdate="2013-04-24 11:55:39" resolution="Duplicate">
		<buginformation>
			<summary>Ability to access DistributedCache from UDFs</summary>
			<description>There have been several requests on the mailing list for
information about how to access the DistributedCache from UDFs, e.g.:
http://www.mail-archive.com/hive-user@hadoop.apache.org/msg01650.html
http://www.mail-archive.com/hive-user@hadoop.apache.org/msg01926.html
While responses to these emails suggested several workarounds, the only correct
way of accessing the distributed cache is via the static methods of Hadoop&amp;amp;apos;s
DistributedCache class, and all of these methods require that the JobConf be passed
in as a parameter. Hence, giving UDFs access to the distributed cache
reduces to giving UDFs access to the JobConf.
I propose the following changes to GenericUDF/UDAF/UDTF:

Add an exec_init(Configuration conf) method that is called during Operator initialization at runtime.
Change the name of the "initialize" method to "compile_init" to make it clear that this method is called at compile-time.

</description>
			<version>0.9.0</version>
			<fixedVersion></fixedVersion>
			<type>New Feature</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDF.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDTF.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecReducer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecMapper.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.UDTFOperator.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">2312</link>
			<link type="Duplicate" description="duplicates">3628</link>
			<link type="Reference" description="relates to">1360</link>
			<link type="Reference" description="is related to">2261</link>
		</links>
	</bug>
	<bug id="4425" opendate="2013-04-26 17:18:49" fixdate="2013-04-26 18:40:51" resolution="Duplicate">
		<buginformation>
			<summary>HiveSessionImpl contains hard-coded version number</summary>
			<description>As a result doing getInfo() call on HiveServer2 currently returns current hard coded value which is 0.10.0</description>
			<version>0.11.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
			<file type="M">org.apache.hive.jdbc.TestJdbcDriver2.java</file>
			<file type="M">org.apache.hive.jdbc.HiveDatabaseMetaData.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">4373</link>
		</links>
	</bug>
	<bug id="4373" opendate="2013-04-17 22:20:56" fixdate="2013-04-29 18:28:42" resolution="Fixed">
		<buginformation>
			<summary>Hive Version returned by HiveDatabaseMetaData.getDatabaseProductVersion is incorrect</summary>
			<description>When running beeline


% beeline -u &amp;amp;apos;jdbc:hive2://localhost:10000&amp;amp;apos; -n hive -p passwd -d org.apache.hive.jdbc.HiveDriver
Connecting to jdbc:hive2://localhost:10000
Connected to: Hive (version 0.10.0)
Driver: Hive (version 0.11.0)
Transaction isolation: TRANSACTION_REPEATABLE_READ


The Hive version in the "Connected to: " string says 0.10.0 instead of 0.11.0.
Looking at the code it seems that the version is hardcoded at two places:
line 250 in jdbc/src/java/org/apache/hive/jdbc/HiveDatabaseMetaData.java
line 833 in jdbc/src/test/org/apache/hive/jdbc/TestJdbcDriver2.java</description>
			<version>0.11.0</version>
			<fixedVersion>0.11.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
			<file type="M">org.apache.hive.jdbc.TestJdbcDriver2.java</file>
			<file type="M">org.apache.hive.jdbc.HiveDatabaseMetaData.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">4425</link>
		</links>
	</bug>
	<bug id="3682" opendate="2012-11-07 07:50:25" fixdate="2013-04-29 22:10:19" resolution="Fixed">
		<buginformation>
			<summary>when output hive table to file,users should could have a separator of their own choice</summary>
			<description>By default,when output hive table to file ,columns of the Hive table are separated by ^A character (that is \001).
But indeed users should have the right to set a seperator of their own choice.
Usage Example:
create table for_test (key string, value string);
load data local inpath &amp;amp;apos;./in1.txt&amp;amp;apos; into table for_test
select * from for_test;
UT-01default separator is \001 line separator is \n
insert overwrite local directory &amp;amp;apos;./test-01&amp;amp;apos; 
select * from src ;
create table array_table (a array&amp;lt;string&amp;gt;, b array&amp;lt;string&amp;gt;)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY &amp;amp;apos;\t&amp;amp;apos;
COLLECTION ITEMS TERMINATED BY &amp;amp;apos;,&amp;amp;apos;;
load data local inpath "../hive/examples/files/arraytest.txt" overwrite into table table2;
CREATE TABLE map_table (foo STRING , bar MAP&amp;lt;STRING, STRING&amp;gt;)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY &amp;amp;apos;\t&amp;amp;apos;
COLLECTION ITEMS TERMINATED BY &amp;amp;apos;,&amp;amp;apos;
MAP KEYS TERMINATED BY &amp;amp;apos;:&amp;amp;apos;
STORED AS TEXTFILE;
UT-02defined field separator as &amp;amp;apos;:&amp;amp;apos;
insert overwrite local directory &amp;amp;apos;./test-02&amp;amp;apos; 
row format delimited 
FIELDS TERMINATED BY &amp;amp;apos;:&amp;amp;apos; 
select * from src ;
UT-03: line separator DO NOT ALLOWED to define as other separator 
insert overwrite local directory &amp;amp;apos;./test-03&amp;amp;apos; 
row format delimited 
FIELDS TERMINATED BY &amp;amp;apos;:&amp;amp;apos; 
select * from src ;
UT-04: define map separators 
insert overwrite local directory &amp;amp;apos;./test-04&amp;amp;apos; 
row format delimited 
FIELDS TERMINATED BY &amp;amp;apos;\t&amp;amp;apos;
COLLECTION ITEMS TERMINATED BY &amp;amp;apos;,&amp;amp;apos;
MAP KEYS TERMINATED BY &amp;amp;apos;:&amp;amp;apos;
select * from src;</description>
			<version>0.8.1</version>
			<fixedVersion>0.11.0</fixedVersion>
			<type>New Feature</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.QB.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">634</link>
			<link type="Duplicate" description="is duplicated by">268</link>
			<link type="Reference" description="relates to">4346</link>
			<link type="Reference" description="is related to">5672</link>
			<link type="Reference" description="is related to">6410</link>
		</links>
	</bug>
	<bug id="4491" opendate="2013-05-03 18:46:03" fixdate="2013-05-03 18:51:49" resolution="Duplicate">
		<buginformation>
			<summary>Grouping by a struct throws an exception</summary>
			<description>Queries that require a shuffle with a struct as the key result in an exception: 

Caused by: java.lang.RuntimeException: Hash code on complex types not supported yet.
	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.hashCode(ObjectInspectorUtils.java:528)
	at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.processOp(ReduceSinkOperator.java:226)
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:531)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:859)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.forward(GroupByOperator.java:1066)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.closeOp(GroupByOperator.java:1118)
	... 13 more

</description>
			<version>0.12.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">2517</link>
		</links>
	</bug>
	<bug id="4507" opendate="2013-05-06 16:10:14" fixdate="2013-05-06 18:11:51" resolution="Duplicate">
		<buginformation>
			<summary>Fix jdbc to compile under openjdk 7</summary>
			<description>The newer Linux distros are shipping with just openjdk 7. Currently, the jdbc module doesn&amp;amp;apos;t compile because some new methods aren&amp;amp;apos;t implemented.</description>
			<version>0.12.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.jdbc.HiveConnection.java</file>
			<file type="M">org.apache.hive.jdbc.HiveDatabaseMetaData.java</file>
			<file type="M">org.apache.hive.jdbc.HiveDataSource.java</file>
			<file type="M">org.apache.hive.jdbc.HiveCallableStatement.java</file>
			<file type="M">org.apache.hive.jdbc.HiveQueryResultSet.java</file>
			<file type="M">org.apache.hive.jdbc.HiveDriver.java</file>
			<file type="M">org.apache.hive.jdbc.HiveStatement.java</file>
			<file type="M">org.apache.hive.jdbc.HivePreparedStatement.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">4496</link>
		</links>
	</bug>
	<bug id="3384" opendate="2012-08-14 08:54:51" fixdate="2013-05-06 19:59:22" resolution="Fixed">
		<buginformation>
			<summary>HIVE JDBC module won&amp;apos;t compile under JDK1.7 as new methods added in JDBC specification</summary>
			<description>jdbc module couldn&amp;amp;apos;t be compiled with jdk7 as it adds some abstract method in the JDBC specification 
some error info:
 error: HiveCallableStatement is not abstract and does not override abstract
method &amp;lt;T&amp;gt;getObject(String,Class&amp;lt;T&amp;gt;) in CallableStatement
.
.
.
</description>
			<version>0.10.0</version>
			<fixedVersion>0.11.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveCallableStatement.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HivePreparedStatement.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveConnection.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveDriver.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveStatement.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveBaseResultSet.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveDataSource.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveQueryResultSet.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">3197</link>
			<link type="Reference" description="relates to">4496</link>
			<link type="Reference" description="is related to">3967</link>
			<link type="dependent" description="is depended upon by">3630</link>
			<link type="dependent" description="is depended upon by">3631</link>
			<link type="dependent" description="is depended upon by">4583</link>
		</links>
	</bug>
	<bug id="4195" opendate="2013-03-16 00:44:46" fixdate="2013-05-06 21:22:05" resolution="Duplicate">
		<buginformation>
			<summary>Avro SerDe causes incorrect behavior in unrelated tables</summary>
			<description>When I run a file that first creates an Avro table using the Avro SerDe, then immediately creates an LZO text table and inserts data into the LZO table, the resulting LZO table contain Avro data files. When I remove the Avro CREATE TABLE statement, the LZO table contains .lzo files as expected.

DROP TABLE IF EXISTS avro_table;
CREATE EXTERNAL TABLE avro_table
ROW FORMAT SERDE &amp;amp;apos;org.apache.hadoop.hive.serde2.avro.AvroSerDe&amp;amp;apos;
STORED AS
INPUTFORMAT &amp;amp;apos;org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat&amp;amp;apos;
OUTPUTFORMAT &amp;amp;apos;org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat&amp;amp;apos;
TBLPROPERTIES (&amp;amp;apos;avro.schema.literal&amp;amp;apos; = &amp;amp;apos;{
"namespace": "testing.hive.avro.serde",
"name": "test_record",
"type": "record",
"fields": [
{"name":"int1", "type":"long"},
{"name":"string1", "type":"string"}
]
}&amp;amp;apos;);

DROP TABLE IF EXISTS lzo_table;
CREATE EXTERNAL TABLE lzo_table (
id int,
bool_col boolean,
tinyint_col tinyint,
smallint_col smallint,
int_col int,
bigint_col bigint,
float_col float,
double_col double,
date_string_col string,
string_col string,
timestamp_col timestamp)
STORED AS 
INPUTFORMAT &amp;amp;apos;com.hadoop.mapred.DeprecatedLzoTextInputFormat&amp;amp;apos;
OUTPUTFORMAT &amp;amp;apos;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&amp;amp;apos;
;

SET hive.exec.compress.output=true;
SET mapred.output.compression.type=BLOCK;
SET mapred.output.compression.codec=com.hadoop.compression.lzo.LzopCodec;
SET hive.exec.dynamic.partition.mode=nonstrict;
SET hive.exec.dynamic.partition=true;
SET mapred.max.split.size=256000000;
SET hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
insert overwrite table lzo_table SELECT id, bool_col, tinyint_col, smallint_col, int_col, bigint_col, float_col, double_col, date_string_col, string_col, timestamp_col FROM src_table;

</description>
			<version>0.10.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.serde2.avro.AvroSerDe.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">3308</link>
		</links>
	</bug>
	<bug id="4490" opendate="2013-05-03 18:44:20" fixdate="2013-05-16 01:52:58" resolution="Duplicate">
		<buginformation>
			<summary>HS2 - &amp;apos;select null ..&amp;apos; fails with NPE</summary>
			<description>Eg, from beeline 


&amp;gt; select null, i from t1 ;
Error: Error running query: java.lang.NullPointerException (state=,code=0)
Error: Error running query: java.lang.NullPointerException (state=,code=0)


In HS2 log
org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.NullPointerException
        at org.apache.hive.service.cli.operation.SQLOperation.run(SQLOperation.java:113)
        at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(HiveSessionImpl.java:169)
        at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:62)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1178)
        at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:524)
        at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:57)
        at $Proxy8.executeStatement(Unknown Source)
        at org.apache.hive.service.cli.CLIService.executeStatement(CLIService.java:148)
        at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:203)
        at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1133)
        at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1118)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
        at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge20S.java:565)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)</description>
			<version>0.11.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.api.SkewedValueList.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.SkewedInfo.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
			<file type="M">org.apache.hive.service.cli.Row.java</file>
			<file type="M">org.apache.hive.jdbc.HiveBaseResultSet.java</file>
			<file type="M">org.apache.hive.service.cli.Type.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyFactory.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.TTypeId.java</file>
			<file type="M">org.apache.hive.service.cli.ColumnValue.java</file>
			<file type="M">org.apache.hive.jdbc.HiveResultSetMetaData.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.TCLIServiceConstants.java</file>
			<file type="M">org.apache.hive.jdbc.TestJdbcDriver2.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">4172</link>
		</links>
	</bug>
	<bug id="4633" opendate="2013-05-30 13:35:25" fixdate="2013-05-31 03:31:19" resolution="Duplicate">
		<buginformation>
			<summary>MR Jobs execution failed.</summary>
			<description>I am running Hive-0.11.0 + Hadoop-0.23 version. All queries that spawn MR jobs got failed. When I look into logs, below exception is thrown in hive.log

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.ql.metadata.HiveException: Configuration and input path are inconsistent
	at org.apache.hadoop.hive.ql.exec.MapOperator.setChildren(MapOperator.java:522)
	at org.apache.hadoop.hive.ql.exec.ExecMapper.configure(ExecMapper.java:90)


</description>
			<version>0.11.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">4619</link>
		</links>
	</bug>
	<bug id="268" opendate="2009-02-03 19:43:26" fixdate="2013-06-07 03:37:18" resolution="Duplicate">
		<buginformation>
			<summary>"Insert Overwrite Directory" to accept configurable table row format</summary>
			<description>There is no way for the users to control the file format when they are outputting the result into a directory.
We should allow:

INSERT OVERWRITE DIRECTORY "/user/zshao/result"
ROW FORMAT DELIMITED
FIELDS TERMINATED BY &amp;amp;apos;9&amp;amp;apos;
SELECT tablea.* from tablea;

</description>
			<version>0.8.1</version>
			<fixedVersion></fixedVersion>
			<type>New Feature</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.QB.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">3682</link>
			<link type="Duplicate" description="is duplicated by">634</link>
		</links>
	</bug>
	<bug id="634" opendate="2009-07-14 16:51:29" fixdate="2013-06-07 03:38:00" resolution="Duplicate">
		<buginformation>
			<summary>ctrl-A is the only output delimiter used, regardless of the Hive table structure</summary>
			<description>No matter what the table format, INSERT OVERWRITE LOCAL DIRECTORY will always use ctrl-A delimiters (&amp;amp;apos;\001&amp;amp;apos; ).
INSERT OVERWRITE LOCAL DIRECTORY &amp;amp;apos;/mnt/daily_timelines&amp;amp;apos; SELECT * FROM daily_timelines;
where daily_timelines is defined as tab delimited
CREATE TABLE daily_timelines (
    page_id BIGINT, 
    dates STRING, 
    pageviews STRING, 
    total_pageviews BIGINT) 
  ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY &amp;amp;apos;\t&amp;amp;apos; 
  STORED AS TEXTFILE;
This page also indicates Hive uses a fixed delimiter, and should be updated: 
http://wiki.apache.org/hadoop/Hive/LanguageManual/DML
</description>
			<version>0.3.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.QB.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">268</link>
			<link type="Duplicate" description="duplicates">3682</link>
			<link type="Reference" description="relates to">669</link>
		</links>
	</bug>
	<bug id="4571" opendate="2013-05-16 10:55:04" fixdate="2013-06-19 16:00:38" resolution="Duplicate">
		<buginformation>
			<summary>Reinvestigate HIVE-337 induced limit on number of separator characters in LazySerDe</summary>
			<description>HIVE-337 added support for complex data structures and also oddly added in a limit of the # of separator characters required to make that happen.
When using an Avro-based table that has more than 8-10 levels of nesting in records, this limit gets hit and such tables can&amp;amp;apos;t be queried.
We either need to remove such a limit or raise it to a high-enough value to support such nested data structures.</description>
			<version>0.10.0</version>
			<fixedVersion></fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyUtils.java</file>
			<file type="M">org.apache.hadoop.hive.hbase.TestLazyHBaseObject.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyFactory.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.TestLazyArrayMapStruct.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">3253</link>
		</links>
	</bug>
	<bug id="4172" opendate="2013-03-14 06:03:39" fixdate="2013-06-21 02:27:41" resolution="Fixed">
		<buginformation>
			<summary>JDBC2 does not support VOID type</summary>
			<description>In beeline, "select key, null from src" fails with exception,

org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.NullPointerException
	at org.apache.hive.service.cli.operation.SQLOperation.run(SQLOperation.java:112)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(HiveSessionImpl.java:166)
	at org.apache.hive.service.cli.CLIService.executeStatement(CLIService.java:148)
	at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:183)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1133)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1118)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:39)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)

</description>
			<version>0.11.0</version>
			<fixedVersion>0.12.0</fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.api.SkewedValueList.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.SkewedInfo.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
			<file type="M">org.apache.hive.service.cli.Row.java</file>
			<file type="M">org.apache.hive.jdbc.HiveBaseResultSet.java</file>
			<file type="M">org.apache.hive.service.cli.Type.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyFactory.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.TTypeId.java</file>
			<file type="M">org.apache.hive.service.cli.ColumnValue.java</file>
			<file type="M">org.apache.hive.jdbc.HiveResultSetMetaData.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.TCLIServiceConstants.java</file>
			<file type="M">org.apache.hive.jdbc.TestJdbcDriver2.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="is blocked by">4300</link>
			<link type="Duplicate" description="is duplicated by">2615</link>
			<link type="Duplicate" description="is duplicated by">4490</link>
		</links>
	</bug>
	<bug id="4496" opendate="2013-05-03 23:28:06" fixdate="2013-06-25 01:42:04" resolution="Fixed">
		<buginformation>
			<summary>JDBC2 won&amp;apos;t compile with JDK7</summary>
			<description>HiveServer2 related JDBC does not compile with JDK7. Related to HIVE-3384.</description>
			<version>0.12.0</version>
			<fixedVersion>0.12.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.jdbc.HiveConnection.java</file>
			<file type="M">org.apache.hive.jdbc.HiveDatabaseMetaData.java</file>
			<file type="M">org.apache.hive.jdbc.HiveDataSource.java</file>
			<file type="M">org.apache.hive.jdbc.HiveCallableStatement.java</file>
			<file type="M">org.apache.hive.jdbc.HiveQueryResultSet.java</file>
			<file type="M">org.apache.hive.jdbc.HiveDriver.java</file>
			<file type="M">org.apache.hive.jdbc.HiveStatement.java</file>
			<file type="M">org.apache.hive.jdbc.HivePreparedStatement.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">4507</link>
			<link type="Reference" description="is related to">3384</link>
			<link type="dependent" description="is depended upon by">4583</link>
		</links>
	</bug>
	<bug id="2517" opendate="2011-10-20 04:40:20" fixdate="2013-07-02 01:45:24" resolution="Fixed">
		<buginformation>
			<summary>Support group by on struct type</summary>
			<description>Currently group by on struct and union types are not supported. This issue will enable support for those.</description>
			<version>0.8.0</version>
			<fixedVersion>0.12.0</fixedVersion>
			<type>New Feature</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">4491</link>
			<link type="Reference" description="relates to">2390</link>
		</links>
	</bug>
	<bug id="3253" opendate="2012-07-12 13:26:30" fixdate="2013-07-03 15:53:46" resolution="Fixed">
		<buginformation>
			<summary>ArrayIndexOutOfBounds exception for deeply nested structs</summary>
			<description>It was observed that while creating table with deeply nested structs might throw this exception:


java.lang.ArrayIndexOutOfBoundsException: 9
        at org.apache.hadoop.hive.serde2.lazy.LazyFactory.createLazyObjectInspector(LazyFactory.java:281)
	at org.apache.hadoop.hive.serde2.lazy.LazyFactory.createLazyObjectInspector(LazyFactory.java:263)
	at org.apache.hadoop.hive.serde2.lazy.LazyFactory.createLazyObjectInspector(LazyFactory.java:276)
	at org.apache.hadoop.hive.serde2.lazy.LazyFactory.createLazyObjectInspector(LazyFactory.java:263)
	at org.apache.hadoop.hive.serde2.lazy.LazyFactory.createLazyObjectInspector(LazyFactory.java:276)
	at org.apache.hadoop.hive.serde2.lazy.LazyFactory.createLazyObjectInspector(LazyFactory.java:263)
	at org.apache.hadoop.hive.serde2.lazy.LazyFactory.createLazyObjectInspector(LazyFactory.java:276)
	at org.apache.hadoop.hive.serde2.lazy.LazyFactory.createLazyObjectInspector(LazyFactory.java:263)
	at org.apache.hadoop.hive.serde2.lazy.LazyFactory.createLazyObjectInspector(LazyFactory.java:276)
	at org.apache.hadoop.hive.serde2.lazy.LazyFactory.createLazyStructInspector(LazyFactory.java:354)


The reason being that currently the separators array has been hardcoded to be of size 8 in the LazySimpleSerde.


// Read the separators: We use 8 levels of separators by default, but we
// should change this when we allow users to specify more than 10 levels
// of separators through DDL.
serdeParams.separators = new byte[8];


If possible, we should increase this size or at least make it configurable to properly handle deeply nested structs.</description>
			<version>0.9.0</version>
			<fixedVersion>0.12.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyUtils.java</file>
			<file type="M">org.apache.hadoop.hive.hbase.TestLazyHBaseObject.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyFactory.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.TestLazyArrayMapStruct.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">4571</link>
			<link type="Reference" description="is related to">9500</link>
		</links>
	</bug>
	<bug id="3094" opendate="2012-06-06 16:19:38" fixdate="2013-07-04 01:07:34" resolution="Duplicate">
		<buginformation>
			<summary>new partition files and directories should inherit file permissions from parent partition/table dir</summary>
			<description>In HIVE-2936 changes were made for warehouse table sub directories to inherit the permissions from parent directory. But this applies only to directories created by metastore. 
When directories (in case of dynamic partitioning) or files are created from the MR job, it uses the default 
But new partition files or directories created from the MR jobs don&amp;amp;apos;t inherit the permissions. 
This means that even if the permissions have been granted on table directory for a group, the group will not have permissions on the new partitions. </description>
			<version>0.9.0</version>
			<fixedVersion></fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">3756</link>
			<link type="Reference" description="relates to">3756</link>
		</links>
	</bug>
	<bug id="4222" opendate="2013-03-23 08:37:50" fixdate="2013-07-25 02:40:30" resolution="Fixed">
		<buginformation>
			<summary>Timestamp type constants cannot be deserialized in JDK 1.6 or less</summary>
			<description>For example,

ExprNodeConstantDesc constant = new ExprNodeConstantDesc(TypeInfoFactory.timestampTypeInfo, new Timestamp(100));
String serialized = Utilities.serializeExpression(constant);
ExprNodeConstantDesc deserilized = (ExprNodeConstantDesc) Utilities.deserializeExpression(serialized, new Configuration());


logs error message

java.lang.InstantiationException: java.sql.Timestamp
Continuing ...
java.lang.RuntimeException: failed to evaluate: &amp;lt;unbound&amp;gt;=Class.new();
Continuing ...


and makes NPE in final.</description>
			<version>0.9.0</version>
			<fixedVersion>0.12.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.TestUtilities.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">3739</link>
			<link type="Reference" description="relates to">5263</link>
		</links>
	</bug>
	<bug id="3756" opendate="2012-11-29 18:11:36" fixdate="2013-07-26 04:39:40" resolution="Fixed">
		<buginformation>
			<summary>"LOAD DATA" does not honor permission inheritence</summary>
			<description>When a "LOAD DATA" operation is performed the resulting data in hdfs for the table does not maintain permission inheritance. This remains true even with the "hive.warehouse.subdir.inherit.perms" set to true.
The issue is easily reproducible by creating a table and loading some data into it. After the load is complete just do a "dfs -ls -R" on the warehouse directory and you will see that the inheritance of permissions worked for the table directory but not for the data. </description>
			<version>0.9.0</version>
			<fixedVersion>0.12.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">3094</link>
			<link type="Incorporates" description="is part of">6892</link>
			<link type="Reference" description="is related to">3094</link>
			<link type="Regression" description="breaks">6209</link>
		</links>
	</bug>
	<bug id="2905" opendate="2012-03-26 04:35:03" fixdate="2013-07-27 15:02:06" resolution="Fixed">
		<buginformation>
			<summary>Desc table can&amp;apos;t show non-ascii comments</summary>
			<description>When desc a table with command line or hive jdbc way, the table&amp;amp;apos;s comment can&amp;amp;apos;t be read.
1. I have updated javax.jdo.option.ConnectionURL parameter in hive-site.xml file.
   jdbc:mysql://...:3306/hive?characterEncoding=UTF-8
2. In mysql database, the comment field of COLUMNS table can be read normally.</description>
			<version>0.7.0</version>
			<fixedVersion>0.12.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.metadata.formatting.TextMetaDataFormatter.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">3914</link>
			<link type="Reference" description="is related to">5682</link>
		</links>
	</bug>
	<bug id="305" opendate="2009-02-24 20:31:30" fixdate="2013-07-29 12:46:28" resolution="Fixed">
		<buginformation>
			<summary>Port Hadoop streaming&amp;apos;s counters/status reporters to Hive Transforms</summary>
			<description>https://issues.apache.org/jira/browse/HADOOP-1328
" Introduced a way for a streaming process to update global counters and status using stderr stream to emit information. Use "reporter:counter:&amp;lt;group&amp;gt;,&amp;lt;counter&amp;gt;,&amp;lt;amount&amp;gt; " to update  a counter. Use "reporter:status:&amp;lt;message&amp;gt;" to update status. "
</description>
			<version>0.6.0</version>
			<fixedVersion>0.12.0</fixedVersion>
			<type>New Feature</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">1649</link>
		</links>
	</bug>
	<bug id="2702" opendate="2012-01-10 07:18:00" fixdate="2013-07-30 17:33:32" resolution="Fixed">
		<buginformation>
			<summary>Enhance listPartitionsByFilter to add support for integral types both for equality and non-equality</summary>
			<description>listPartitionsByFilter supports only non-string partitions. This is because its explicitly specified in generateJDOFilterOverPartitions in ExpressionTree.java. 
//Can only support partitions whose types are string
      if( ! table.getPartitionKeys().get(partitionColumnIndex).
          getType().equals(org.apache.hadoop.hive.serde.Constants.STRING_TYPE_NAME) ) 
{
        throw new MetaException
        ("Filtering is supported only on partition keys of type string");
      }</description>
			<version>0.8.1</version>
			<fixedVersion>0.12.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.parser.ExpressionTree.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="is blocked by">4929</link>
			<link type="Duplicate" description="is duplicated by">7164</link>
			<link type="Reference" description="relates to">347</link>
			<link type="Reference" description="relates to">23</link>
			<link type="Reference" description="relates to">4888</link>
			<link type="Reference" description="is related to">2048</link>
		</links>
	</bug>
	<bug id="4893" opendate="2013-07-19 16:37:09" fixdate="2013-08-07 17:58:32" resolution="Duplicate">
		<buginformation>
			<summary>[WebHCat] HTTP 500 errors should be mapped to 400 for bad request</summary>
			<description></description>
			<version>0.11.0</version>
			<fixedVersion>0.12.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hcatalog.templeton.TestWebHCatE2e.java</file>
			<file type="M">org.apache.hcatalog.templeton.CatchallExceptionMapper.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">4586</link>
		</links>
	</bug>
	<bug id="4911" opendate="2013-07-22 20:03:02" fixdate="2013-08-08 20:05:41" resolution="Fixed">
		<buginformation>
			<summary>Enable QOP configuration for Hive Server 2 thrift transport</summary>
			<description>The QoP for hive server 2 should be configurable to enable encryption. A new configuration should be exposed "hive.server2.thrift.sasl.qop". This would give greater control configuring hive server 2 service.</description>
			<version>0.11.0</version>
			<fixedVersion>0.12.0</fixedVersion>
			<type>New Feature</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.jdbc.HiveConnection.java</file>
			<file type="M">org.apache.hive.service.auth.HiveAuthFactory.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.java</file>
			<file type="M">org.apache.hive.service.auth.KerberosSaslHelper.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.TestHadoop20SAuthBridge.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">4225</link>
			<link type="Reference" description="is related to">5120</link>
			<link type="Supercedes" description="supercedes">4225</link>
		</links>
	</bug>
	<bug id="3191" opendate="2012-06-24 15:33:01" fixdate="2013-08-09 06:54:01" resolution="Fixed">
		<buginformation>
			<summary>timestamp - timestamp causes null pointer exception</summary>
			<description>select tts.rnum, tts.cts - tts.cts from cert.tts tts
Error: Query returned non-zero code: 12, cause: FAILED: Hive Internal Error: java.lang.NullPointerException(null)
SQLState:  42000
ErrorCode: 12
create table if not exists CERT.TTS ( RNUM int , CTS timestamp) 
stored as sequencefile;</description>
			<version>0.8.0</version>
			<fixedVersion>0.12.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.NumericOpMethodResolver.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">5021</link>
		</links>
	</bug>
	<bug id="3189" opendate="2012-06-24 15:21:23" fixdate="2013-08-14 16:33:43" resolution="Fixed">
		<buginformation>
			<summary>cast ( &lt;string type&gt; as bigint) returning null values</summary>
			<description>select rnum, c1, cast(c1 as bigint) from cert.tsdchar tsdchar where rnum in (0,1,2)
create table if not exists CERT.TSDCHAR ( RNUM int , C1 string)
row format sequencefile
rnum	c1	_c2
0	-1                         	&amp;lt;null&amp;gt;
1	0                          	&amp;lt;null&amp;gt;
2	10                         	&amp;lt;null&amp;gt;</description>
			<version>0.8.0</version>
			<fixedVersion>0.12.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.udf.TestUDFDateAdd.java</file>
		</fixedFiles>
	</bug>
	<bug id="4940" opendate="2013-07-26 04:25:12" fixdate="2013-08-16 16:24:00" resolution="Fixed">
		<buginformation>
			<summary>udaf_percentile_approx.q is not deterministic</summary>
			<description>Makes different result for 20(S) and 23.</description>
			<version>1.0.0</version>
			<fixedVersion>0.12.0</fixedVersion>
			<type>Sub-task</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.api.OpenTxnRequest.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.LockRequest.java</file>
			<file type="M">org.apache.hive.beeline.HiveSchemaTool.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.CheckLockRequest.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ShowLocksRequest.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.TxnInfo.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">3911</link>
			<link type="Duplicate" description="duplicates">9833</link>
		</links>
	</bug>
	<bug id="4225" opendate="2013-03-25 06:44:19" fixdate="2013-08-19 17:14:05" resolution="Duplicate">
		<buginformation>
			<summary>HiveServer2 does not support SASL QOP</summary>
			<description>HiveServer2 implements Kerberos authentication through SASL framework, but does not support setting QOP.</description>
			<version>0.11.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.jdbc.HiveConnection.java</file>
			<file type="M">org.apache.hive.service.auth.HiveAuthFactory.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.java</file>
			<file type="M">org.apache.hive.service.auth.KerberosSaslHelper.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.TestHadoop20SAuthBridge.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">4911</link>
			<link type="Reference" description="relates to">4232</link>
			<link type="Supercedes" description="is superceded by">4911</link>
		</links>
	</bug>
	<bug id="5149" opendate="2013-08-26 01:35:43" fixdate="2013-09-03 19:34:18" resolution="Fixed">
		<buginformation>
			<summary>ReduceSinkDeDuplication can pick the wrong partitioning columns</summary>
			<description>https://mail-archives.apache.org/mod_mbox/hive-user/201308.mbox/%3CCAG6Lhyex5XPwszpihKqkPRpzri2k=m4QGc+cpAR5yVR8SJtM4Q@mail.gmail.com%3E</description>
			<version>0.11.0</version>
			<fixedVersion>0.12.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">5237</link>
		</links>
	</bug>
	<bug id="4586" opendate="2013-05-22 00:04:06" fixdate="2013-09-03 19:56:31" resolution="Fixed">
		<buginformation>
			<summary>[HCatalog] WebHCat should return 404 error for undefined resource</summary>
			<description></description>
			<version>0.11.0</version>
			<fixedVersion>0.12.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hcatalog.templeton.TestWebHCatE2e.java</file>
			<file type="M">org.apache.hcatalog.templeton.CatchallExceptionMapper.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">4893</link>
		</links>
	</bug>
	<bug id="4266" opendate="2013-03-29 19:08:50" fixdate="2013-09-04 23:36:49" resolution="Duplicate">
		<buginformation>
			<summary>Refactor HCatalog code to org.apache.hive.hcatalog</summary>
			<description>Currently HCatalog code is in packages org.apache.hcatalog.  It needs to now move to org.apache.hive.hcatalog.  Shell classes/interface need to be created for public facing classes so that user&amp;amp;apos;s code does not break.</description>
			<version>0.11.0</version>
			<fixedVersion>0.12.0</fixedVersion>
			<type>Sub-task</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.hcatalog.mapreduce.TestHCatInputFormat.java</file>
			<file type="M">org.apache.hive.hcatalog.data.HCatRecordable.java</file>
			<file type="M">org.apache.hive.hcatalog.data.TestReaderWriter.java</file>
			<file type="M">org.apache.hive.hcatalog.data.JsonSerDe.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.HBaseInputFormat.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.HBaseDirectOutputFormat.java</file>
			<file type="M">org.apache.hive.hcatalog.rcfile.RCFileMapReduceInputFormat.java</file>
			<file type="M">org.apache.hive.hcatalog.har.HarOutputCommitterPostProcessor.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.HDFSStorage.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.HBaseBulkOutputFormat.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.ZKBasedRevisionManager.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.IDGenerator.java</file>
			<file type="M">org.apache.hive.hcatalog.cli.HCatDriver.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.HCatSplit.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.ProgressReporter.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.IDGenClient.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.ReadText.java</file>
			<file type="M">org.apache.hive.hcatalog.data.HCatRecord.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.ExecService.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.RevisionManagerFactory.java</file>
			<file type="M">org.apache.hive.hcatalog.messaging.json.JSONCreateDatabaseMessage.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.TestHCatExternalHCatNonPartitioned.java</file>
			<file type="M">org.apache.hive.hcatalog.pig.TestPigHCatUtil.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.PartInfo.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.ResultConverter.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.HBaseAuthorizationProvider.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.DataReaderMaster.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.ImportSequenceFile.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.NotAuthorizedException.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.lock.TestWriteLock.java</file>
			<file type="M">org.apache.hive.hcatalog.common.TestHiveClientCache.java</file>
			<file type="M">org.apache.hive.hcatalog.fileformats.TestOrcDynamicPartitioned.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.StorerInfo.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.NotFoundException.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.QueueException.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.HcatException.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.HCatMapReduceTest.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.GroupByAge.java</file>
			<file type="M">org.apache.hive.hcatalog.pig.HCatStorerWrapper.java</file>
			<file type="M">org.apache.hive.hcatalog.NoExitSecurityManager.java</file>
			<file type="M">org.apache.hive.hcatalog.api.HCatClient.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.TempletonControllerJob.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.ReadWrite.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.HcatDelegator.java</file>
			<file type="M">org.apache.hive.hcatalog.data.transfer.HCatWriter.java</file>
			<file type="M">org.apache.hive.hcatalog.storagehandler.DummyHCatAuthProvider.java</file>
			<file type="M">org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzerBase.java</file>
			<file type="M">org.apache.hive.hcatalog.common.TestHCatUtil.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.DatabaseDesc.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.BusyException.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.DeleteDelegator.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.HBaseConstants.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.MaxByteArrayOutputStream.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.ReadJson.java</file>
			<file type="M">org.apache.hive.hcatalog.data.LazyHCatRecord.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.TestTempletonUtils.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.InitializeInput.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.TempletonStorage.java</file>
			<file type="M">org.apache.hive.hcatalog.common.HCatConstants.java</file>
			<file type="M">org.apache.hive.hcatalog.cli.TestPermsGrp.java</file>
			<file type="M">org.apache.hive.hcatalog.data.HCatRecordSerDe.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.HCatBaseOutputFormat.java</file>
			<file type="M">org.apache.hive.hcatalog.messaging.json.JSONDropTableMessage.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.HBaseReadWrite.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.lock.TestZNodeName.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.HCatStorageHandler.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.StoreDemo.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.TestHBaseDirectOutputFormat.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.TestSequenceFileReadWrite.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.lock.WriteLock.java</file>
			<file type="M">org.apache.hive.hcatalog.api.HCatPartition.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.NullRecordReader.java</file>
			<file type="M">org.apache.hive.hcatalog.api.HCatTable.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.PigDelegator.java</file>
			<file type="M">org.apache.hive.hcatalog.data.transfer.impl.HCatInputFormatReader.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.transaction.thrift.StoreFamilyRevisionList.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.TableSnapshot.java</file>
			<file type="M">org.apache.hive.hcatalog.data.schema.HCatSchema.java</file>
			<file type="M">org.apache.hive.hcatalog.pig.HCatStorer.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.TablePropertyDesc.java</file>
			<file type="M">org.apache.hive.hcatalog.data.DataType.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.lock.ZooKeeperOperation.java</file>
			<file type="M">org.apache.hive.hcatalog.api.HCatClientHMSImpl.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.RevisionManagerConfiguration.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.PathUtil.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.TestDesc.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.HCatTableSnapshot.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.HCatRecordReader.java</file>
			<file type="M">org.apache.hive.hcatalog.pig.TestHCatStorerWrapper.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.OutputJobInfo.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.WriteTextPartitioned.java</file>
			<file type="M">org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.java</file>
			<file type="M">org.apache.hive.hcatalog.data.schema.TestHCatSchemaUtils.java</file>
			<file type="M">org.apache.hive.hcatalog.pig.HCatBaseStorer.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.PartitionDesc.java</file>
			<file type="M">org.apache.hive.hcatalog.rcfile.RCFileMapReduceRecordReader.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.ExecServiceImpl.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.InternalUtil.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.Security.java</file>
			<file type="M">org.apache.hive.hcatalog.cli.TestSemanticAnalysis.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.TempletonDelegator.java</file>
			<file type="M">org.apache.hive.hcatalog.data.HCatRecordObjectInspector.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.ListDelegator.java</file>
			<file type="M">org.apache.hive.hcatalog.security.StorageDelegationAuthorizationProvider.java</file>
			<file type="M">org.apache.hive.hcatalog.api.HCatCreateTableDesc.java</file>
			<file type="M">org.apache.hive.hcatalog.pig.MyPigStorage.java</file>
			<file type="M">org.apache.hive.hcatalog.pig.TestHCatStorerMulti.java</file>
			<file type="M">org.apache.hive.hcatalog.security.TestHdfsAuthorizationProvider.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.RevisionManagerProtocol.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.HDFSCleanup.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.StatusDelegator.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.package-info.java</file>
			<file type="M">org.apache.hive.hcatalog.messaging.json.JSONDropDatabaseMessage.java</file>
			<file type="M">org.apache.hive.hcatalog.cli.HCatCli.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.CompleteBean.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.TestHBaseBulkOutputFormat.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.TestHCatDynamicPartitioned.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.LauncherDelegator.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.lock.LockListener.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.RevisionManagerEndpointClient.java</file>
			<file type="M">org.apache.hive.hcatalog.common.HiveClientCache.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.InputJobInfo.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.HBaseRevisionManagerUtil.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.HCatTypeCheckHive.java</file>
			<file type="M">org.apache.hive.hcatalog.pig.TestHCatLoader.java</file>
			<file type="M">org.apache.hive.hcatalog.data.schema.TestHCatSchema.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.TestIDGenerator.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.SecureProxySupport.java</file>
			<file type="M">org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter.java</file>
			<file type="M">org.apache.hive.hcatalog.common.HCatException.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.CallbackFailedException.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer.java</file>
			<file type="M">org.apache.hive.hcatalog.data.transfer.HCatReader.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.TableLikeDesc.java</file>
			<file type="M">org.apache.hive.hcatalog.api.TestHCatClient.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.HBaseHCatStorageHandler.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.MultiOutputFormat.java</file>
			<file type="M">org.apache.hive.hcatalog.data.transfer.ReadEntity.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.DefaultOutputCommitterContainer.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.FosterStorageHandler.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.ZooKeeperCleanup.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.mock.MockServer.java</file>
			<file type="M">org.apache.hive.hcatalog.cli.TestUseDatabase.java</file>
			<file type="M">org.apache.hive.hcatalog.messaging.jms.MessagingUtils.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.TestHCatHiveThriftCompatibility.java</file>
			<file type="M">org.apache.hive.hcatalog.messaging.json.JSONDropPartitionMessage.java</file>
			<file type="M">org.apache.hive.hcatalog.pig.HCatLoader.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.lock.ZNodeName.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.Transaction.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.ZKUtil.java</file>
			<file type="M">org.apache.hive.hcatalog.messaging.DropPartitionMessage.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.AppConfig.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.mock.MockUriInfo.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.StoreComplex.java</file>
			<file type="M">org.apache.hive.hcatalog.messaging.AddPartitionMessage.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.SumNumbers.java</file>
			<file type="M">org.apache.hive.hcatalog.pig.TestHCatStorer.java</file>
			<file type="M">org.apache.hive.hcatalog.data.transfer.WriteEntity.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.TestInputJobInfo.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.ManyMiniCluster.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.TestThriftSerialization.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.DataReaderSlave.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.ColumnDesc.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.JobStateTracker.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.TestZNodeSetUp.java</file>
			<file type="M">org.apache.hive.hcatalog.messaging.json.JSONMessageDeserializer.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.HCatOutputFormat.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.Server.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.TempletonUtils.java</file>
			<file type="M">org.apache.hive.hcatalog.listener.NotificationListener.java</file>
			<file type="M">org.apache.hive.hcatalog.messaging.DropDatabaseMessage.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.TestHCatNonPartitioned.java</file>
			<file type="M">org.apache.hive.hcatalog.security.HdfsAuthorizationProvider.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.CompleteDelegator.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.TestTrivialExecService.java</file>
			<file type="M">org.apache.hive.hcatalog.data.HCatRecordObjectInspectorFactory.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.TestRevisionManager.java</file>
			<file type="M">org.apache.hive.hcatalog.api.HCatAddPartitionDesc.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.TestHBaseInputFormat.java</file>
			<file type="M">org.apache.hive.hcatalog.pig.TestOrcHCatStorer.java</file>
			<file type="M">org.apache.hive.hcatalog.pig.TestOrcHCatLoader.java</file>
			<file type="M">org.apache.hive.hcatalog.data.transfer.DataTransferFactory.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.JobState.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.DataWriterMaster.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.RMConstants.java</file>
			<file type="M">org.apache.hive.hcatalog.pig.TestOrcHCatLoaderComplexSchema.java</file>
			<file type="M">org.apache.hive.hcatalog.data.TestDefaultHCatRecord.java</file>
			<file type="M">org.apache.hive.hcatalog.cli.SemanticAnalysis.CreateTableHook.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.TrivialExecService.java</file>
			<file type="M">org.apache.hive.hcatalog.messaging.MessageDeserializer.java</file>
			<file type="M">org.apache.hive.hcatalog.messaging.json.JSONAddPartitionMessage.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer.java</file>
			<file type="M">org.apache.hive.hcatalog.pig.TestE2EScenarios.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.JsonBuilder.java</file>
			<file type="M">org.apache.hive.hcatalog.data.ReaderWriter.java</file>
			<file type="M">org.apache.hive.hcatalog.pig.PigHCatUtil.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.TestHCatExternalDynamicPartitioned.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.NullSplit.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat.java</file>
			<file type="M">org.apache.hive.hcatalog.pig.TestHCatLoaderStorer.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.HBaseBaseOutputFormat.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.SimpleExceptionMapper.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.mock.MockExecService.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.EnqueueBean.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.Main.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.HCatTypeCheck.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.DataWriterSlave.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.Util.java</file>
			<file type="M">org.apache.hive.hcatalog.api.HCatDatabase.java</file>
			<file type="M">org.apache.hive.hcatalog.pig.TestHCatLoaderComplexSchema.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.TestHCatExternalPartitioned.java</file>
			<file type="M">org.apache.hive.hcatalog.data.transfer.ReaderContext.java</file>
			<file type="M">org.apache.hive.hcatalog.messaging.CreateDatabaseMessage.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.QueueStatusBean.java</file>
			<file type="M">org.apache.hive.hcatalog.cli.DummyStorageHandler.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.TestSnapshots.java</file>
			<file type="M">org.apache.hive.hcatalog.data.transfer.state.StateProvider.java</file>
			<file type="M">org.apache.hive.hcatalog.api.HCatCreateDBDesc.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.TestPassProperties.java</file>
			<file type="M">org.apache.hive.hcatalog.api.ConnectionFailureException.java</file>
			<file type="M">org.apache.hive.hcatalog.common.ErrorType.java</file>
			<file type="M">org.apache.hive.hcatalog.data.schema.HCatFieldSchema.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.HCatTableInfo.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.WadlConfig.java</file>
			<file type="M">org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer.java</file>
			<file type="M">org.apache.hive.hcatalog.listener.TestNotificationListener.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.ZooKeeperStorage.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.RevisionManagerEndpoint.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.DefaultOutputFormatContainer.java</file>
			<file type="M">org.apache.hive.hcatalog.api.ObjectNotFoundException.java</file>
			<file type="M">org.apache.hive.hcatalog.ExitException.java</file>
			<file type="M">org.apache.hive.hcatalog.pig.MockLoader.java</file>
			<file type="M">org.apache.hive.hcatalog.messaging.DropTableMessage.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.StreamingDelegator.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.SkeletonHBaseTest.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.RecordWriterContainer.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.UgiFactory.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.ReadRC.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.HCatTestDriver.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.HiveDelegator.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.HbaseSnapshotRecordReader.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.DefaultRecordWriterContainer.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.TestMultiOutputFormat.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.HBaseUtil.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.TableDesc.java</file>
			<file type="M">org.apache.hive.hcatalog.MiniCluster.java</file>
			<file type="M">org.apache.hive.hcatalog.data.Pair.java</file>
			<file type="M">org.apache.hive.hcatalog.messaging.MessageFactory.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.GroupPermissionsDesc.java</file>
			<file type="M">org.apache.hive.hcatalog.rcfile.TestRCFileMapReduceInputFormat.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.TestRevisionManagerEndpoint.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.SimpleWebException.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.transaction.thrift.StoreFamilyRevision.java</file>
			<file type="M">org.apache.hive.hcatalog.common.HCatContext.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.SimpleRead.java</file>
			<file type="M">org.apache.hive.hcatalog.oozie.JavaAction.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.CatchallExceptionMapper.java</file>
			<file type="M">org.apache.hive.hcatalog.data.TestHCatRecordSerDe.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.TestHCatPartitioned.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.SingleInputFormat.java</file>
			<file type="M">org.apache.hive.hcatalog.data.DefaultHCatRecord.java</file>
			<file type="M">org.apache.hive.hcatalog.messaging.json.JSONCreateTableMessage.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.TestServer.java</file>
			<file type="M">org.apache.hive.hcatalog.pig.HCatBaseLoader.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.lock.ProtocolSupport.java</file>
			<file type="M">org.apache.hive.hcatalog.common.HCatUtil.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.TypeDataCheck.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.WriteRC.java</file>
			<file type="M">org.apache.hive.hcatalog.data.HCatDataCheckUtil.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.BadParam.java</file>
			<file type="M">org.apache.hive.hcatalog.data.transfer.state.DefaultStateProvider.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.OutputCommitterContainer.java</file>
			<file type="M">org.apache.hive.hcatalog.data.TestLazyHCatRecord.java</file>
			<file type="M">org.apache.hive.hcatalog.data.transfer.EntityBase.java</file>
			<file type="M">org.apache.hive.hcatalog.messaging.HCatEventMessage.java</file>
			<file type="M">org.apache.hive.hcatalog.data.transfer.WriterContext.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.TestRevisionManagerConfiguration.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.TestHCatPartitionPublish.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.OutputFormatContainer.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.HCatBaseTest.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.TestWebHCatE2e.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.ExecBean.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.WriteText.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.TestHBaseHCatStorageHandler.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.TestHCatOutputFormat.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.TestHCatHiveCompatibility.java</file>
			<file type="M">org.apache.hive.hcatalog.HcatTestUtils.java</file>
			<file type="M">org.apache.hive.hcatalog.cli.SemanticAnalysis.CreateDatabaseHook.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.RevisionManager.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.WriteJson.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.StoreNumbers.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.FamilyRevision.java</file>
			<file type="M">org.apache.hive.hcatalog.messaging.json.JSONMessageFactory.java</file>
			<file type="M">org.apache.hive.hcatalog.rcfile.RCFileMapReduceOutputFormat.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.HCatInputFormat.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.JarDelegator.java</file>
			<file type="M">org.apache.hive.hcatalog.messaging.CreateTableMessage.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.ProxyUserSupport.java</file>
			<file type="M">org.apache.hive.hcatalog.data.TestJsonSerDe.java</file>
			<file type="M">org.apache.hive.hcatalog.listener.TestMsgBusConnection.java</file>
			<file type="D">org.apache.hcatalog.templeton.BadParam.java</file>
			<file type="D">org.apache.hcatalog.pig.TestHCatLoaderStorer.java</file>
			<file type="D">org.apache.hcatalog.hbase.HBaseDirectOutputFormat.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.MultiOutputFormat.java</file>
			<file type="D">org.apache.hcatalog.templeton.PartitionDesc.java</file>
			<file type="D">org.apache.hcatalog.templeton.LauncherDelegator.java</file>
			<file type="D">org.apache.hcatalog.data.transfer.WriteEntity.java</file>
			<file type="D">org.apache.hcatalog.templeton.tool.SingleInputFormat.java</file>
			<file type="D">org.apache.hcatalog.messaging.json.JSONAddPartitionMessage.java</file>
			<file type="D">org.apache.hcatalog.pig.TestOrcHCatStorer.java</file>
			<file type="D">org.apache.hcatalog.data.ReaderWriter.java</file>
			<file type="D">org.apache.hcatalog.data.DataType.java</file>
			<file type="D">org.apache.hcatalog.cli.TestSemanticAnalysis.java</file>
			<file type="D">org.apache.hcatalog.messaging.MessageFactory.java</file>
			<file type="D">org.apache.hcatalog.pig.HCatBaseStorer.java</file>
			<file type="D">org.apache.hcatalog.templeton.QueueException.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.TestIDGenerator.java</file>
			<file type="D">org.apache.hcatalog.data.HCatDataCheckUtil.java</file>
			<file type="D">org.apache.hcatalog.pig.MyPigStorage.java</file>
			<file type="D">org.apache.hcatalog.templeton.tool.TempletonUtils.java</file>
			<file type="D">org.apache.hcatalog.api.ObjectNotFoundException.java</file>
			<file type="D">org.apache.hcatalog.utils.DataWriterMaster.java</file>
			<file type="D">org.apache.hcatalog.hbase.ResultConverter.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.TestHCatOutputFormat.java</file>
			<file type="D">org.apache.hcatalog.rcfile.RCFileMapReduceOutputFormat.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.Transaction.java</file>
			<file type="D">org.apache.hcatalog.templeton.MaxByteArrayOutputStream.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.lock.LockListener.java</file>
			<file type="D">org.apache.hcatalog.templeton.ExecServiceImpl.java</file>
			<file type="D">org.apache.hcatalog.common.HCatUtil.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.RevisionManagerEndpointClient.java</file>
			<file type="D">org.apache.hcatalog.utils.WriteJson.java</file>
			<file type="D">org.apache.hcatalog.hbase.HbaseSnapshotRecordReader.java</file>
			<file type="D">org.apache.hcatalog.data.HCatRecordSerDe.java</file>
			<file type="D">org.apache.hcatalog.templeton.NotAuthorizedException.java</file>
			<file type="D">org.apache.hcatalog.utils.StoreDemo.java</file>
			<file type="D">org.apache.hcatalog.hbase.HBaseAuthorizationProvider.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.TestInputJobInfo.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.lock.ZooKeeperOperation.java</file>
			<file type="D">org.apache.hcatalog.common.HiveClientCache.java</file>
			<file type="D">org.apache.hcatalog.data.LazyHCatRecord.java</file>
			<file type="D">org.apache.hcatalog.data.transfer.state.DefaultStateProvider.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.HCatMapRedUtil.java</file>
			<file type="D">org.apache.hcatalog.hbase.TestHBaseInputFormat.java</file>
			<file type="D">org.apache.hcatalog.templeton.TableLikeDesc.java</file>
			<file type="D">org.apache.hcatalog.cli.SemanticAnalysis.CreateTableHook.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.PathUtil.java</file>
			<file type="D">org.apache.hcatalog.security.StorageDelegationAuthorizationProvider.java</file>
			<file type="D">org.apache.hcatalog.security.HdfsAuthorizationProvider.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.HCatBaseInputFormat.java</file>
			<file type="D">org.apache.hcatalog.data.transfer.ReaderContext.java</file>
			<file type="D">org.apache.hcatalog.data.transfer.state.StateProvider.java</file>
			<file type="D">org.apache.hcatalog.api.TestHCatClient.java</file>
			<file type="D">org.apache.hcatalog.templeton.HcatDelegator.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.DefaultOutputCommitterContainer.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.TestRevisionManagerConfiguration.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.lock.ProtocolSupport.java</file>
			<file type="D">org.apache.hcatalog.api.HCatAddPartitionDesc.java</file>
			<file type="D">org.apache.hcatalog.templeton.tool.HDFSCleanup.java</file>
			<file type="D">org.apache.hcatalog.data.schema.TestHCatSchemaUtils.java</file>
			<file type="D">org.apache.hcatalog.templeton.CatchallExceptionMapper.java</file>
			<file type="D">org.apache.hcatalog.data.transfer.DataTransferFactory.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.RMConstants.java</file>
			<file type="D">org.apache.hcatalog.common.TestHiveClientCache.java</file>
			<file type="D">org.apache.hcatalog.templeton.tool.JobState.java</file>
			<file type="D">org.apache.hcatalog.templeton.tool.TempletonControllerJob.java</file>
			<file type="D">org.apache.hcatalog.messaging.DropTableMessage.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.TestHCatExternalDynamicPartitioned.java</file>
			<file type="D">org.apache.hcatalog.data.transfer.WriterContext.java</file>
			<file type="D">org.apache.hcatalog.hbase.HBaseRevisionManagerUtil.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.FileRecordWriterContainer.java</file>
			<file type="D">org.apache.hcatalog.hbase.HBaseUtil.java</file>
			<file type="D">org.apache.hcatalog.utils.Util.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.HCatRecordReader.java</file>
			<file type="D">org.apache.hcatalog.utils.DataReaderSlave.java</file>
			<file type="D">org.apache.hcatalog.templeton.QueueStatusBean.java</file>
			<file type="D">org.apache.hcatalog.utils.DataReaderMaster.java</file>
			<file type="D">org.apache.hcatalog.api.HCatTable.java</file>
			<file type="D">org.apache.hcatalog.hbase.TestHBaseBulkOutputFormat.java</file>
			<file type="D">org.apache.hcatalog.rcfile.RCFileMapReduceInputFormat.java</file>
			<file type="D">org.apache.hcatalog.utils.ReadText.java</file>
			<file type="D">org.apache.hcatalog.data.transfer.impl.HCatOutputFormatWriter.java</file>
			<file type="D">org.apache.hcatalog.templeton.TestWebHCatE2e.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.TestRevisionManagerEndpoint.java</file>
			<file type="D">org.apache.hcatalog.templeton.BusyException.java</file>
			<file type="D">org.apache.hcatalog.templeton.tool.HDFSStorage.java</file>
			<file type="D">org.apache.hcatalog.templeton.TestServer.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.PartInfo.java</file>
			<file type="D">org.apache.hcatalog.data.schema.TestHCatSchema.java</file>
			<file type="D">org.apache.hcatalog.templeton.SimpleWebException.java</file>
			<file type="D">org.apache.hcatalog.hbase.ManyMiniCluster.java</file>
			<file type="D">org.apache.hcatalog.data.HCatRecordObjectInspectorFactory.java</file>
			<file type="D">org.apache.hcatalog.templeton.mock.MockExecService.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.HCatBaseTest.java</file>
			<file type="D">org.apache.hcatalog.data.JsonSerDe.java</file>
			<file type="D">org.apache.hcatalog.utils.WriteTextPartitioned.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.HCatSplit.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.TestHCatInputFormat.java</file>
			<file type="D">org.apache.hcatalog.api.HCatPartition.java</file>
			<file type="D">org.apache.hcatalog.data.Pair.java</file>
			<file type="D">org.apache.hcatalog.listener.TestNotificationListener.java</file>
			<file type="D">org.apache.hcatalog.pig.TestPigHCatUtil.java</file>
			<file type="D">org.apache.hcatalog.utils.DataWriterSlave.java</file>
			<file type="D">org.apache.hcatalog.utils.WriteRC.java</file>
			<file type="D">org.apache.hcatalog.cli.SemanticAnalysis.CreateDatabaseHook.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.IDGenerator.java</file>
			<file type="D">org.apache.hcatalog.data.transfer.impl.HCatInputFormatReader.java</file>
			<file type="D">org.apache.hcatalog.pig.TestOrcHCatLoader.java</file>
			<file type="D">org.apache.hcatalog.hbase.HBaseBaseOutputFormat.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.RecordWriterContainer.java</file>
			<file type="D">org.apache.hcatalog.messaging.CreateTableMessage.java</file>
			<file type="D">org.apache.hcatalog.data.schema.HCatSchemaUtils.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.RevisionManagerEndpoint.java</file>
			<file type="D">org.apache.hcatalog.common.HCatException.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.TestMultiOutputFormat.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.HCatOutputFormat.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.RevisionManager.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.HCatInputFormat.java</file>
			<file type="D">org.apache.hcatalog.data.HCatRecord.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.RevisionManagerProtocol.java</file>
			<file type="D">org.apache.hcatalog.hbase.HCatTableSnapshot.java</file>
			<file type="D">org.apache.hcatalog.har.HarOutputCommitterPostProcessor.java</file>
			<file type="D">org.apache.hcatalog.cli.TestPermsGrp.java</file>
			<file type="D">org.apache.hcatalog.templeton.HiveDelegator.java</file>
			<file type="D">org.apache.hcatalog.hbase.TestHBaseHCatStorageHandler.java</file>
			<file type="D">org.apache.hcatalog.templeton.ColumnDesc.java</file>
			<file type="D">org.apache.hcatalog.templeton.EnqueueBean.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.OutputJobInfo.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.FileOutputFormatContainer.java</file>
			<file type="D">org.apache.hcatalog.templeton.WadlConfig.java</file>
			<file type="D">org.apache.hcatalog.storagehandler.DummyHCatAuthProvider.java</file>
			<file type="D">org.apache.hcatalog.data.HCatRecordable.java</file>
			<file type="D">org.apache.hcatalog.pig.HCatStorer.java</file>
			<file type="D">org.apache.hcatalog.utils.StoreNumbers.java</file>
			<file type="D">org.apache.hcatalog.templeton.ProxyUserSupport.java</file>
			<file type="D">org.apache.hcatalog.messaging.DropDatabaseMessage.java</file>
			<file type="D">org.apache.hcatalog.pig.TestHCatLoaderComplexSchema.java</file>
			<file type="D">org.apache.hcatalog.oozie.JavaAction.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.TestHCatDynamicPartitioned.java</file>
			<file type="D">org.apache.hcatalog.templeton.Server.java</file>
			<file type="D">org.apache.hcatalog.messaging.DropPartitionMessage.java</file>
			<file type="D">org.apache.hcatalog.listener.NotificationListener.java</file>
			<file type="D">org.apache.hcatalog.api.HCatCreateTableDesc.java</file>
			<file type="D">org.apache.hcatalog.messaging.json.JSONMessageDeserializer.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.HCatTableInfo.java</file>
			<file type="D">org.apache.hcatalog.messaging.json.JSONDropDatabaseMessage.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.TestHCatMultiOutputFormat.java</file>
			<file type="D">org.apache.hcatalog.pig.TestHCatStorerWrapper.java</file>
			<file type="D">org.apache.hcatalog.pig.TestOrcHCatLoaderComplexSchema.java</file>
			<file type="D">org.apache.hcatalog.templeton.SimpleExceptionMapper.java</file>
			<file type="D">org.apache.hcatalog.cli.TestUseDatabase.java</file>
			<file type="D">org.apache.hcatalog.pig.TestHCatStorer.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.ProgressReporter.java</file>
			<file type="D">org.apache.hcatalog.data.schema.HCatSchema.java</file>
			<file type="D">org.apache.hcatalog.cli.HCatDriver.java</file>
			<file type="D">org.apache.hcatalog.pig.MockLoader.java</file>
			<file type="D">org.apache.hcatalog.templeton.tool.NullSplit.java</file>
			<file type="D">org.apache.hcatalog.rcfile.TestRCFileMapReduceInputFormat.java</file>
			<file type="D">org.apache.hcatalog.utils.HCatTestDriver.java</file>
			<file type="D">org.apache.hcatalog.data.transfer.EntityBase.java</file>
			<file type="D">org.apache.hcatalog.templeton.PigDelegator.java</file>
			<file type="D">org.apache.hcatalog.templeton.AppConfig.java</file>
			<file type="D">org.apache.hcatalog.hbase.ImportSequenceFile.java</file>
			<file type="D">org.apache.hcatalog.templeton.CallbackFailedException.java</file>
			<file type="D">org.apache.hcatalog.messaging.CreateDatabaseMessage.java</file>
			<file type="D">org.apache.hcatalog.utils.GroupByAge.java</file>
			<file type="D">org.apache.hcatalog.messaging.MessageDeserializer.java</file>
			<file type="D">org.apache.hcatalog.data.TestReaderWriter.java</file>
			<file type="D">org.apache.hcatalog.templeton.StreamingDelegator.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.lock.ZNodeName.java</file>
			<file type="D">org.apache.hcatalog.rcfile.RCFileMapReduceRecordReader.java</file>
			<file type="D">org.apache.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer.java</file>
			<file type="D">org.apache.hcatalog.cli.HCatCli.java</file>
			<file type="D">org.apache.hcatalog.HcatTestUtils.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.HCatStorageHandler.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.Security.java</file>
			<file type="D">org.apache.hcatalog.templeton.DatabaseDesc.java</file>
			<file type="D">org.apache.hcatalog.templeton.TableDesc.java</file>
			<file type="D">org.apache.hcatalog.data.HCatRecordObjectInspector.java</file>
			<file type="D">org.apache.hcatalog.templeton.ListDelegator.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.TestHCatPartitioned.java</file>
			<file type="D">org.apache.hcatalog.templeton.tool.TrivialExecService.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.ZKUtil.java</file>
			<file type="D">org.apache.hcatalog.pig.PigHCatUtil.java</file>
			<file type="D">org.apache.hcatalog.api.HCatClientHMSImpl.java</file>
			<file type="D">org.apache.hcatalog.data.transfer.ReadEntity.java</file>
			<file type="D">org.apache.hcatalog.api.HCatDatabase.java</file>
			<file type="D">org.apache.hcatalog.templeton.tool.JobStateTracker.java</file>
			<file type="D">org.apache.hcatalog.templeton.TestDesc.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.lock.TestWriteLock.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.TestHCatHiveCompatibility.java</file>
			<file type="D">org.apache.hcatalog.templeton.tool.NotFoundException.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.transaction.thrift.StoreFamilyRevisionList.java</file>
			<file type="D">org.apache.hcatalog.data.transfer.HCatReader.java</file>
			<file type="D">org.apache.hcatalog.templeton.GroupPermissionsDesc.java</file>
			<file type="D">org.apache.hcatalog.common.ErrorType.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.TestZNodeSetUp.java</file>
			<file type="D">org.apache.hcatalog.data.schema.HCatFieldSchema.java</file>
			<file type="D">org.apache.hcatalog.templeton.TempletonDelegator.java</file>
			<file type="D">org.apache.hcatalog.templeton.mock.MockUriInfo.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.package-info.java</file>
			<file type="D">org.apache.hcatalog.hbase.TestSnapshots.java</file>
			<file type="D">org.apache.hcatalog.ExitException.java</file>
			<file type="D">org.apache.hcatalog.utils.SumNumbers.java</file>
			<file type="D">org.apache.hcatalog.utils.TypeDataCheck.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.HCatMapReduceTest.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.FileOutputCommitterContainer.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.TestHCatNonPartitioned.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.FamilyRevision.java</file>
			<file type="D">org.apache.hcatalog.hbase.HBaseHCatStorageHandler.java</file>
			<file type="D">org.apache.hcatalog.pig.TestHCatStorerMulti.java</file>
			<file type="D">org.apache.hcatalog.listener.TestMsgBusConnection.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.TestThriftSerialization.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.TestPassProperties.java</file>
			<file type="D">org.apache.hcatalog.hbase.HBaseInputFormat.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.FosterStorageHandler.java</file>
			<file type="D">org.apache.hcatalog.messaging.HCatEventMessage.java</file>
			<file type="D">org.apache.hcatalog.api.HCatCreateDBDesc.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.RevisionManagerConfiguration.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.InputJobInfo.java</file>
			<file type="D">org.apache.hcatalog.messaging.json.JSONDropPartitionMessage.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.HCatBaseOutputFormat.java</file>
			<file type="D">org.apache.hcatalog.templeton.ExecService.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.DefaultOutputFormatContainer.java</file>
			<file type="D">org.apache.hcatalog.common.TestHCatUtil.java</file>
			<file type="D">org.apache.hcatalog.data.TestLazyHCatRecord.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.ZKBasedRevisionManager.java</file>
			<file type="D">org.apache.hcatalog.utils.HCatTypeCheck.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.transaction.thrift.StoreFamilyRevision.java</file>
			<file type="D">org.apache.hcatalog.templeton.JsonBuilder.java</file>
			<file type="D">org.apache.hcatalog.templeton.DeleteDelegator.java</file>
			<file type="D">org.apache.hcatalog.templeton.JarDelegator.java</file>
			<file type="D">org.apache.hcatalog.data.TestHCatRecordSerDe.java</file>
			<file type="D">org.apache.hcatalog.templeton.HcatException.java</file>
			<file type="D">org.apache.hcatalog.data.TestDefaultHCatRecord.java</file>
			<file type="D">org.apache.hcatalog.utils.HBaseReadWrite.java</file>
			<file type="D">org.apache.hcatalog.templeton.tool.TempletonStorage.java</file>
			<file type="D">org.apache.hcatalog.templeton.tool.TestTrivialExecService.java</file>
			<file type="D">org.apache.hcatalog.pig.TestHCatLoader.java</file>
			<file type="D">org.apache.hcatalog.fileformats.TestOrcDynamicPartitioned.java</file>
			<file type="D">org.apache.hcatalog.templeton.TablePropertyDesc.java</file>
			<file type="D">org.apache.hcatalog.templeton.tool.TestTempletonUtils.java</file>
			<file type="D">org.apache.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzerBase.java</file>
			<file type="D">org.apache.hcatalog.messaging.jms.MessagingUtils.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.StorerInfo.java</file>
			<file type="D">org.apache.hcatalog.pig.HCatBaseLoader.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.OutputCommitterContainer.java</file>
			<file type="D">org.apache.hcatalog.pig.HCatStorerWrapper.java</file>
			<file type="D">org.apache.hcatalog.templeton.CompleteBean.java</file>
			<file type="D">org.apache.hcatalog.messaging.json.JSONMessageFactory.java</file>
			<file type="D">org.apache.hcatalog.common.HCatContext.java</file>
			<file type="D">org.apache.hcatalog.templeton.mock.MockServer.java</file>
			<file type="D">org.apache.hcatalog.hbase.TestHBaseDirectOutputFormat.java</file>
			<file type="D">org.apache.hcatalog.data.transfer.HCatWriter.java</file>
			<file type="D">org.apache.hcatalog.templeton.tool.ZooKeeperCleanup.java</file>
			<file type="D">org.apache.hcatalog.utils.HCatTypeCheckHive.java</file>
			<file type="D">org.apache.hcatalog.templeton.CompleteDelegator.java</file>
			<file type="D">org.apache.hcatalog.utils.StoreComplex.java</file>
			<file type="D">org.apache.hcatalog.messaging.AddPartitionMessage.java</file>
			<file type="D">org.apache.hcatalog.utils.ReadWrite.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.TestHCatExternalHCatNonPartitioned.java</file>
			<file type="D">org.apache.hcatalog.api.HCatClient.java</file>
			<file type="D">org.apache.hcatalog.pig.HCatLoader.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.TestSequenceFileReadWrite.java</file>
			<file type="D">org.apache.hcatalog.templeton.UgiFactory.java</file>
			<file type="D">org.apache.hcatalog.MiniCluster.java</file>
			<file type="D">org.apache.hcatalog.hbase.HBaseConstants.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.IDGenClient.java</file>
			<file type="D">org.apache.hcatalog.templeton.SecureProxySupport.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.TableSnapshot.java</file>
			<file type="D">org.apache.hcatalog.NoExitSecurityManager.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.TestHCatHiveThriftCompatibility.java</file>
			<file type="D">org.apache.hcatalog.utils.WriteText.java</file>
			<file type="D">org.apache.hcatalog.templeton.ExecBean.java</file>
			<file type="D">org.apache.hcatalog.api.ConnectionFailureException.java</file>
			<file type="D">org.apache.hcatalog.cli.DummyStorageHandler.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.OutputFormatContainer.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.TestRevisionManager.java</file>
			<file type="D">org.apache.hcatalog.utils.SimpleRead.java</file>
			<file type="D">org.apache.hcatalog.templeton.Main.java</file>
			<file type="D">org.apache.hcatalog.messaging.json.JSONCreateTableMessage.java</file>
			<file type="D">org.apache.hcatalog.hbase.HBaseBulkOutputFormat.java</file>
			<file type="D">org.apache.hcatalog.security.TestHdfsAuthorizationProvider.java</file>
			<file type="D">org.apache.hcatalog.hbase.SkeletonHBaseTest.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.DefaultRecordWriterContainer.java</file>
			<file type="D">org.apache.hcatalog.utils.ReadRC.java</file>
			<file type="D">org.apache.hcatalog.utils.ReadJson.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.TestHCatExternalPartitioned.java</file>
			<file type="D">org.apache.hcatalog.data.DefaultHCatRecord.java</file>
			<file type="D">org.apache.hcatalog.templeton.tool.NullRecordReader.java</file>
			<file type="D">org.apache.hcatalog.messaging.json.JSONCreateDatabaseMessage.java</file>
			<file type="D">org.apache.hcatalog.templeton.StatusDelegator.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.RevisionManagerFactory.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.InternalUtil.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.lock.TestZNodeName.java</file>
			<file type="D">org.apache.hcatalog.data.TestJsonSerDe.java</file>
			<file type="D">org.apache.hcatalog.templeton.tool.ZooKeeperStorage.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.lock.WriteLock.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.TestHCatPartitionPublish.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.InitializeInput.java</file>
			<file type="D">org.apache.hcatalog.common.HCatConstants.java</file>
			<file type="D">org.apache.hcatalog.pig.TestE2EScenarios.java</file>
			<file type="D">org.apache.hcatalog.messaging.json.JSONDropTableMessage.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">4895</link>
			<link type="Reference" description="is related to">4869</link>
		</links>
	</bug>
	<bug id="4895" opendate="2013-07-19 17:33:35" fixdate="2013-09-05 03:11:51" resolution="Fixed">
		<buginformation>
			<summary>Move all HCatalog classes to org.apache.hive.hcatalog</summary>
			<description>make sure to preserve history in SCM</description>
			<version>0.12.0</version>
			<fixedVersion>0.12.0</fixedVersion>
			<type>Sub-task</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.hcatalog.mapreduce.TestHCatInputFormat.java</file>
			<file type="M">org.apache.hive.hcatalog.data.HCatRecordable.java</file>
			<file type="M">org.apache.hive.hcatalog.data.TestReaderWriter.java</file>
			<file type="M">org.apache.hive.hcatalog.data.JsonSerDe.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.HBaseInputFormat.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.HBaseDirectOutputFormat.java</file>
			<file type="M">org.apache.hive.hcatalog.rcfile.RCFileMapReduceInputFormat.java</file>
			<file type="M">org.apache.hive.hcatalog.har.HarOutputCommitterPostProcessor.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.HDFSStorage.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.HBaseBulkOutputFormat.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.ZKBasedRevisionManager.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.IDGenerator.java</file>
			<file type="M">org.apache.hive.hcatalog.cli.HCatDriver.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.HCatSplit.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.ProgressReporter.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.IDGenClient.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.ReadText.java</file>
			<file type="M">org.apache.hive.hcatalog.data.HCatRecord.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.ExecService.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.RevisionManagerFactory.java</file>
			<file type="M">org.apache.hive.hcatalog.messaging.json.JSONCreateDatabaseMessage.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.TestHCatExternalHCatNonPartitioned.java</file>
			<file type="M">org.apache.hive.hcatalog.pig.TestPigHCatUtil.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.PartInfo.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.ResultConverter.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.HBaseAuthorizationProvider.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.DataReaderMaster.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.ImportSequenceFile.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.NotAuthorizedException.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.lock.TestWriteLock.java</file>
			<file type="M">org.apache.hive.hcatalog.common.TestHiveClientCache.java</file>
			<file type="M">org.apache.hive.hcatalog.fileformats.TestOrcDynamicPartitioned.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.StorerInfo.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.NotFoundException.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.QueueException.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.HcatException.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.HCatMapReduceTest.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.GroupByAge.java</file>
			<file type="M">org.apache.hive.hcatalog.pig.HCatStorerWrapper.java</file>
			<file type="M">org.apache.hive.hcatalog.NoExitSecurityManager.java</file>
			<file type="M">org.apache.hive.hcatalog.api.HCatClient.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.TempletonControllerJob.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.ReadWrite.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.HcatDelegator.java</file>
			<file type="M">org.apache.hive.hcatalog.data.transfer.HCatWriter.java</file>
			<file type="M">org.apache.hive.hcatalog.storagehandler.DummyHCatAuthProvider.java</file>
			<file type="M">org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzerBase.java</file>
			<file type="M">org.apache.hive.hcatalog.common.TestHCatUtil.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.DatabaseDesc.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.BusyException.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.DeleteDelegator.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.HBaseConstants.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.MaxByteArrayOutputStream.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.ReadJson.java</file>
			<file type="M">org.apache.hive.hcatalog.data.LazyHCatRecord.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.TestTempletonUtils.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.InitializeInput.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.TempletonStorage.java</file>
			<file type="M">org.apache.hive.hcatalog.common.HCatConstants.java</file>
			<file type="M">org.apache.hive.hcatalog.cli.TestPermsGrp.java</file>
			<file type="M">org.apache.hive.hcatalog.data.HCatRecordSerDe.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.HCatBaseOutputFormat.java</file>
			<file type="M">org.apache.hive.hcatalog.messaging.json.JSONDropTableMessage.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.HBaseReadWrite.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.lock.TestZNodeName.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.HCatStorageHandler.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.StoreDemo.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.TestHBaseDirectOutputFormat.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.TestSequenceFileReadWrite.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.lock.WriteLock.java</file>
			<file type="M">org.apache.hive.hcatalog.api.HCatPartition.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.NullRecordReader.java</file>
			<file type="M">org.apache.hive.hcatalog.api.HCatTable.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.PigDelegator.java</file>
			<file type="M">org.apache.hive.hcatalog.data.transfer.impl.HCatInputFormatReader.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.transaction.thrift.StoreFamilyRevisionList.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.TableSnapshot.java</file>
			<file type="M">org.apache.hive.hcatalog.data.schema.HCatSchema.java</file>
			<file type="M">org.apache.hive.hcatalog.pig.HCatStorer.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.TablePropertyDesc.java</file>
			<file type="M">org.apache.hive.hcatalog.data.DataType.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.lock.ZooKeeperOperation.java</file>
			<file type="M">org.apache.hive.hcatalog.api.HCatClientHMSImpl.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.RevisionManagerConfiguration.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.PathUtil.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.TestDesc.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.HCatTableSnapshot.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.HCatRecordReader.java</file>
			<file type="M">org.apache.hive.hcatalog.pig.TestHCatStorerWrapper.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.OutputJobInfo.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.WriteTextPartitioned.java</file>
			<file type="M">org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.java</file>
			<file type="M">org.apache.hive.hcatalog.data.schema.TestHCatSchemaUtils.java</file>
			<file type="M">org.apache.hive.hcatalog.pig.HCatBaseStorer.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.PartitionDesc.java</file>
			<file type="M">org.apache.hive.hcatalog.rcfile.RCFileMapReduceRecordReader.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.ExecServiceImpl.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.InternalUtil.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.Security.java</file>
			<file type="M">org.apache.hive.hcatalog.cli.TestSemanticAnalysis.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.TempletonDelegator.java</file>
			<file type="M">org.apache.hive.hcatalog.data.HCatRecordObjectInspector.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.ListDelegator.java</file>
			<file type="M">org.apache.hive.hcatalog.security.StorageDelegationAuthorizationProvider.java</file>
			<file type="M">org.apache.hive.hcatalog.api.HCatCreateTableDesc.java</file>
			<file type="M">org.apache.hive.hcatalog.pig.MyPigStorage.java</file>
			<file type="M">org.apache.hive.hcatalog.pig.TestHCatStorerMulti.java</file>
			<file type="M">org.apache.hive.hcatalog.security.TestHdfsAuthorizationProvider.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.RevisionManagerProtocol.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.HDFSCleanup.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.StatusDelegator.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.package-info.java</file>
			<file type="M">org.apache.hive.hcatalog.messaging.json.JSONDropDatabaseMessage.java</file>
			<file type="M">org.apache.hive.hcatalog.cli.HCatCli.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.CompleteBean.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.TestHBaseBulkOutputFormat.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.TestHCatDynamicPartitioned.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.LauncherDelegator.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.lock.LockListener.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.RevisionManagerEndpointClient.java</file>
			<file type="M">org.apache.hive.hcatalog.common.HiveClientCache.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.InputJobInfo.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.HBaseRevisionManagerUtil.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.HCatTypeCheckHive.java</file>
			<file type="M">org.apache.hive.hcatalog.pig.TestHCatLoader.java</file>
			<file type="M">org.apache.hive.hcatalog.data.schema.TestHCatSchema.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.TestIDGenerator.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.SecureProxySupport.java</file>
			<file type="M">org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter.java</file>
			<file type="M">org.apache.hive.hcatalog.common.HCatException.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.CallbackFailedException.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer.java</file>
			<file type="M">org.apache.hive.hcatalog.data.transfer.HCatReader.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.TableLikeDesc.java</file>
			<file type="M">org.apache.hive.hcatalog.api.TestHCatClient.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.HBaseHCatStorageHandler.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.MultiOutputFormat.java</file>
			<file type="M">org.apache.hive.hcatalog.data.transfer.ReadEntity.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.DefaultOutputCommitterContainer.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.FosterStorageHandler.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.ZooKeeperCleanup.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.mock.MockServer.java</file>
			<file type="M">org.apache.hive.hcatalog.cli.TestUseDatabase.java</file>
			<file type="M">org.apache.hive.hcatalog.messaging.jms.MessagingUtils.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.TestHCatHiveThriftCompatibility.java</file>
			<file type="M">org.apache.hive.hcatalog.messaging.json.JSONDropPartitionMessage.java</file>
			<file type="M">org.apache.hive.hcatalog.pig.HCatLoader.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.lock.ZNodeName.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.Transaction.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.ZKUtil.java</file>
			<file type="M">org.apache.hive.hcatalog.messaging.DropPartitionMessage.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.AppConfig.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.mock.MockUriInfo.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.StoreComplex.java</file>
			<file type="M">org.apache.hive.hcatalog.messaging.AddPartitionMessage.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.SumNumbers.java</file>
			<file type="M">org.apache.hive.hcatalog.pig.TestHCatStorer.java</file>
			<file type="M">org.apache.hive.hcatalog.data.transfer.WriteEntity.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.TestInputJobInfo.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.ManyMiniCluster.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.TestThriftSerialization.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.DataReaderSlave.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.ColumnDesc.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.JobStateTracker.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.TestZNodeSetUp.java</file>
			<file type="M">org.apache.hive.hcatalog.messaging.json.JSONMessageDeserializer.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.HCatOutputFormat.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.Server.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.TempletonUtils.java</file>
			<file type="M">org.apache.hive.hcatalog.listener.NotificationListener.java</file>
			<file type="M">org.apache.hive.hcatalog.messaging.DropDatabaseMessage.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.TestHCatNonPartitioned.java</file>
			<file type="M">org.apache.hive.hcatalog.security.HdfsAuthorizationProvider.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.CompleteDelegator.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.TestTrivialExecService.java</file>
			<file type="M">org.apache.hive.hcatalog.data.HCatRecordObjectInspectorFactory.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.TestRevisionManager.java</file>
			<file type="M">org.apache.hive.hcatalog.api.HCatAddPartitionDesc.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.TestHBaseInputFormat.java</file>
			<file type="M">org.apache.hive.hcatalog.pig.TestOrcHCatStorer.java</file>
			<file type="M">org.apache.hive.hcatalog.pig.TestOrcHCatLoader.java</file>
			<file type="M">org.apache.hive.hcatalog.data.transfer.DataTransferFactory.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.JobState.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.DataWriterMaster.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.RMConstants.java</file>
			<file type="M">org.apache.hive.hcatalog.pig.TestOrcHCatLoaderComplexSchema.java</file>
			<file type="M">org.apache.hive.hcatalog.data.TestDefaultHCatRecord.java</file>
			<file type="M">org.apache.hive.hcatalog.cli.SemanticAnalysis.CreateTableHook.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.TrivialExecService.java</file>
			<file type="M">org.apache.hive.hcatalog.messaging.MessageDeserializer.java</file>
			<file type="M">org.apache.hive.hcatalog.messaging.json.JSONAddPartitionMessage.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer.java</file>
			<file type="M">org.apache.hive.hcatalog.pig.TestE2EScenarios.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.JsonBuilder.java</file>
			<file type="M">org.apache.hive.hcatalog.data.ReaderWriter.java</file>
			<file type="M">org.apache.hive.hcatalog.pig.PigHCatUtil.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.TestHCatExternalDynamicPartitioned.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.NullSplit.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat.java</file>
			<file type="M">org.apache.hive.hcatalog.pig.TestHCatLoaderStorer.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.HBaseBaseOutputFormat.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.SimpleExceptionMapper.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.mock.MockExecService.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.EnqueueBean.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.Main.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.HCatTypeCheck.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.DataWriterSlave.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.Util.java</file>
			<file type="M">org.apache.hive.hcatalog.api.HCatDatabase.java</file>
			<file type="M">org.apache.hive.hcatalog.pig.TestHCatLoaderComplexSchema.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.TestHCatExternalPartitioned.java</file>
			<file type="M">org.apache.hive.hcatalog.data.transfer.ReaderContext.java</file>
			<file type="M">org.apache.hive.hcatalog.messaging.CreateDatabaseMessage.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.QueueStatusBean.java</file>
			<file type="M">org.apache.hive.hcatalog.cli.DummyStorageHandler.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.TestSnapshots.java</file>
			<file type="M">org.apache.hive.hcatalog.data.transfer.state.StateProvider.java</file>
			<file type="M">org.apache.hive.hcatalog.api.HCatCreateDBDesc.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.TestPassProperties.java</file>
			<file type="M">org.apache.hive.hcatalog.api.ConnectionFailureException.java</file>
			<file type="M">org.apache.hive.hcatalog.common.ErrorType.java</file>
			<file type="M">org.apache.hive.hcatalog.data.schema.HCatFieldSchema.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.HCatTableInfo.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.WadlConfig.java</file>
			<file type="M">org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer.java</file>
			<file type="M">org.apache.hive.hcatalog.listener.TestNotificationListener.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.ZooKeeperStorage.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.RevisionManagerEndpoint.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.DefaultOutputFormatContainer.java</file>
			<file type="M">org.apache.hive.hcatalog.api.ObjectNotFoundException.java</file>
			<file type="M">org.apache.hive.hcatalog.ExitException.java</file>
			<file type="M">org.apache.hive.hcatalog.pig.MockLoader.java</file>
			<file type="M">org.apache.hive.hcatalog.messaging.DropTableMessage.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.StreamingDelegator.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.SkeletonHBaseTest.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.RecordWriterContainer.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.UgiFactory.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.ReadRC.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.HCatTestDriver.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.HiveDelegator.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.HbaseSnapshotRecordReader.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.DefaultRecordWriterContainer.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.TestMultiOutputFormat.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.HBaseUtil.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.TableDesc.java</file>
			<file type="M">org.apache.hive.hcatalog.MiniCluster.java</file>
			<file type="M">org.apache.hive.hcatalog.data.Pair.java</file>
			<file type="M">org.apache.hive.hcatalog.messaging.MessageFactory.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.GroupPermissionsDesc.java</file>
			<file type="M">org.apache.hive.hcatalog.rcfile.TestRCFileMapReduceInputFormat.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.TestRevisionManagerEndpoint.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.SimpleWebException.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.transaction.thrift.StoreFamilyRevision.java</file>
			<file type="M">org.apache.hive.hcatalog.common.HCatContext.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.SimpleRead.java</file>
			<file type="M">org.apache.hive.hcatalog.oozie.JavaAction.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.CatchallExceptionMapper.java</file>
			<file type="M">org.apache.hive.hcatalog.data.TestHCatRecordSerDe.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.TestHCatPartitioned.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.SingleInputFormat.java</file>
			<file type="M">org.apache.hive.hcatalog.data.DefaultHCatRecord.java</file>
			<file type="M">org.apache.hive.hcatalog.messaging.json.JSONCreateTableMessage.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.TestServer.java</file>
			<file type="M">org.apache.hive.hcatalog.pig.HCatBaseLoader.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.lock.ProtocolSupport.java</file>
			<file type="M">org.apache.hive.hcatalog.common.HCatUtil.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.TypeDataCheck.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.WriteRC.java</file>
			<file type="M">org.apache.hive.hcatalog.data.HCatDataCheckUtil.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.BadParam.java</file>
			<file type="M">org.apache.hive.hcatalog.data.transfer.state.DefaultStateProvider.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.OutputCommitterContainer.java</file>
			<file type="M">org.apache.hive.hcatalog.data.TestLazyHCatRecord.java</file>
			<file type="M">org.apache.hive.hcatalog.data.transfer.EntityBase.java</file>
			<file type="M">org.apache.hive.hcatalog.messaging.HCatEventMessage.java</file>
			<file type="M">org.apache.hive.hcatalog.data.transfer.WriterContext.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.TestRevisionManagerConfiguration.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.TestHCatPartitionPublish.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.OutputFormatContainer.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.HCatBaseTest.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.TestWebHCatE2e.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.ExecBean.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.WriteText.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.TestHBaseHCatStorageHandler.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.TestHCatOutputFormat.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.TestHCatHiveCompatibility.java</file>
			<file type="M">org.apache.hive.hcatalog.HcatTestUtils.java</file>
			<file type="M">org.apache.hive.hcatalog.cli.SemanticAnalysis.CreateDatabaseHook.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.RevisionManager.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.WriteJson.java</file>
			<file type="M">org.apache.hive.hcatalog.utils.StoreNumbers.java</file>
			<file type="M">org.apache.hive.hcatalog.hbase.snapshot.FamilyRevision.java</file>
			<file type="M">org.apache.hive.hcatalog.messaging.json.JSONMessageFactory.java</file>
			<file type="M">org.apache.hive.hcatalog.rcfile.RCFileMapReduceOutputFormat.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.HCatInputFormat.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.JarDelegator.java</file>
			<file type="M">org.apache.hive.hcatalog.messaging.CreateTableMessage.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.ProxyUserSupport.java</file>
			<file type="M">org.apache.hive.hcatalog.data.TestJsonSerDe.java</file>
			<file type="M">org.apache.hive.hcatalog.listener.TestMsgBusConnection.java</file>
			<file type="D">org.apache.hcatalog.templeton.BadParam.java</file>
			<file type="D">org.apache.hcatalog.pig.TestHCatLoaderStorer.java</file>
			<file type="D">org.apache.hcatalog.hbase.HBaseDirectOutputFormat.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.MultiOutputFormat.java</file>
			<file type="D">org.apache.hcatalog.templeton.PartitionDesc.java</file>
			<file type="D">org.apache.hcatalog.templeton.LauncherDelegator.java</file>
			<file type="D">org.apache.hcatalog.data.transfer.WriteEntity.java</file>
			<file type="D">org.apache.hcatalog.templeton.tool.SingleInputFormat.java</file>
			<file type="D">org.apache.hcatalog.messaging.json.JSONAddPartitionMessage.java</file>
			<file type="D">org.apache.hcatalog.pig.TestOrcHCatStorer.java</file>
			<file type="D">org.apache.hcatalog.data.ReaderWriter.java</file>
			<file type="D">org.apache.hcatalog.data.DataType.java</file>
			<file type="D">org.apache.hcatalog.cli.TestSemanticAnalysis.java</file>
			<file type="D">org.apache.hcatalog.messaging.MessageFactory.java</file>
			<file type="D">org.apache.hcatalog.pig.HCatBaseStorer.java</file>
			<file type="D">org.apache.hcatalog.templeton.QueueException.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.TestIDGenerator.java</file>
			<file type="D">org.apache.hcatalog.data.HCatDataCheckUtil.java</file>
			<file type="D">org.apache.hcatalog.pig.MyPigStorage.java</file>
			<file type="D">org.apache.hcatalog.templeton.tool.TempletonUtils.java</file>
			<file type="D">org.apache.hcatalog.api.ObjectNotFoundException.java</file>
			<file type="D">org.apache.hcatalog.utils.DataWriterMaster.java</file>
			<file type="D">org.apache.hcatalog.hbase.ResultConverter.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.TestHCatOutputFormat.java</file>
			<file type="D">org.apache.hcatalog.rcfile.RCFileMapReduceOutputFormat.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.Transaction.java</file>
			<file type="D">org.apache.hcatalog.templeton.MaxByteArrayOutputStream.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.lock.LockListener.java</file>
			<file type="D">org.apache.hcatalog.templeton.ExecServiceImpl.java</file>
			<file type="D">org.apache.hcatalog.common.HCatUtil.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.RevisionManagerEndpointClient.java</file>
			<file type="D">org.apache.hcatalog.utils.WriteJson.java</file>
			<file type="D">org.apache.hcatalog.hbase.HbaseSnapshotRecordReader.java</file>
			<file type="D">org.apache.hcatalog.data.HCatRecordSerDe.java</file>
			<file type="D">org.apache.hcatalog.templeton.NotAuthorizedException.java</file>
			<file type="D">org.apache.hcatalog.utils.StoreDemo.java</file>
			<file type="D">org.apache.hcatalog.hbase.HBaseAuthorizationProvider.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.TestInputJobInfo.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.lock.ZooKeeperOperation.java</file>
			<file type="D">org.apache.hcatalog.common.HiveClientCache.java</file>
			<file type="D">org.apache.hcatalog.data.LazyHCatRecord.java</file>
			<file type="D">org.apache.hcatalog.data.transfer.state.DefaultStateProvider.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.HCatMapRedUtil.java</file>
			<file type="D">org.apache.hcatalog.hbase.TestHBaseInputFormat.java</file>
			<file type="D">org.apache.hcatalog.templeton.TableLikeDesc.java</file>
			<file type="D">org.apache.hcatalog.cli.SemanticAnalysis.CreateTableHook.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.PathUtil.java</file>
			<file type="D">org.apache.hcatalog.security.StorageDelegationAuthorizationProvider.java</file>
			<file type="D">org.apache.hcatalog.security.HdfsAuthorizationProvider.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.HCatBaseInputFormat.java</file>
			<file type="D">org.apache.hcatalog.data.transfer.ReaderContext.java</file>
			<file type="D">org.apache.hcatalog.data.transfer.state.StateProvider.java</file>
			<file type="D">org.apache.hcatalog.api.TestHCatClient.java</file>
			<file type="D">org.apache.hcatalog.templeton.HcatDelegator.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.DefaultOutputCommitterContainer.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.TestRevisionManagerConfiguration.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.lock.ProtocolSupport.java</file>
			<file type="D">org.apache.hcatalog.api.HCatAddPartitionDesc.java</file>
			<file type="D">org.apache.hcatalog.templeton.tool.HDFSCleanup.java</file>
			<file type="D">org.apache.hcatalog.data.schema.TestHCatSchemaUtils.java</file>
			<file type="D">org.apache.hcatalog.templeton.CatchallExceptionMapper.java</file>
			<file type="D">org.apache.hcatalog.data.transfer.DataTransferFactory.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.RMConstants.java</file>
			<file type="D">org.apache.hcatalog.common.TestHiveClientCache.java</file>
			<file type="D">org.apache.hcatalog.templeton.tool.JobState.java</file>
			<file type="D">org.apache.hcatalog.templeton.tool.TempletonControllerJob.java</file>
			<file type="D">org.apache.hcatalog.messaging.DropTableMessage.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.TestHCatExternalDynamicPartitioned.java</file>
			<file type="D">org.apache.hcatalog.data.transfer.WriterContext.java</file>
			<file type="D">org.apache.hcatalog.hbase.HBaseRevisionManagerUtil.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.FileRecordWriterContainer.java</file>
			<file type="D">org.apache.hcatalog.hbase.HBaseUtil.java</file>
			<file type="D">org.apache.hcatalog.utils.Util.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.HCatRecordReader.java</file>
			<file type="D">org.apache.hcatalog.utils.DataReaderSlave.java</file>
			<file type="D">org.apache.hcatalog.templeton.QueueStatusBean.java</file>
			<file type="D">org.apache.hcatalog.utils.DataReaderMaster.java</file>
			<file type="D">org.apache.hcatalog.api.HCatTable.java</file>
			<file type="D">org.apache.hcatalog.hbase.TestHBaseBulkOutputFormat.java</file>
			<file type="D">org.apache.hcatalog.rcfile.RCFileMapReduceInputFormat.java</file>
			<file type="D">org.apache.hcatalog.utils.ReadText.java</file>
			<file type="D">org.apache.hcatalog.data.transfer.impl.HCatOutputFormatWriter.java</file>
			<file type="D">org.apache.hcatalog.templeton.TestWebHCatE2e.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.TestRevisionManagerEndpoint.java</file>
			<file type="D">org.apache.hcatalog.templeton.BusyException.java</file>
			<file type="D">org.apache.hcatalog.templeton.tool.HDFSStorage.java</file>
			<file type="D">org.apache.hcatalog.templeton.TestServer.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.PartInfo.java</file>
			<file type="D">org.apache.hcatalog.data.schema.TestHCatSchema.java</file>
			<file type="D">org.apache.hcatalog.templeton.SimpleWebException.java</file>
			<file type="D">org.apache.hcatalog.hbase.ManyMiniCluster.java</file>
			<file type="D">org.apache.hcatalog.data.HCatRecordObjectInspectorFactory.java</file>
			<file type="D">org.apache.hcatalog.templeton.mock.MockExecService.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.HCatBaseTest.java</file>
			<file type="D">org.apache.hcatalog.data.JsonSerDe.java</file>
			<file type="D">org.apache.hcatalog.utils.WriteTextPartitioned.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.HCatSplit.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.TestHCatInputFormat.java</file>
			<file type="D">org.apache.hcatalog.api.HCatPartition.java</file>
			<file type="D">org.apache.hcatalog.data.Pair.java</file>
			<file type="D">org.apache.hcatalog.listener.TestNotificationListener.java</file>
			<file type="D">org.apache.hcatalog.pig.TestPigHCatUtil.java</file>
			<file type="D">org.apache.hcatalog.utils.DataWriterSlave.java</file>
			<file type="D">org.apache.hcatalog.utils.WriteRC.java</file>
			<file type="D">org.apache.hcatalog.cli.SemanticAnalysis.CreateDatabaseHook.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.IDGenerator.java</file>
			<file type="D">org.apache.hcatalog.data.transfer.impl.HCatInputFormatReader.java</file>
			<file type="D">org.apache.hcatalog.pig.TestOrcHCatLoader.java</file>
			<file type="D">org.apache.hcatalog.hbase.HBaseBaseOutputFormat.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.RecordWriterContainer.java</file>
			<file type="D">org.apache.hcatalog.messaging.CreateTableMessage.java</file>
			<file type="D">org.apache.hcatalog.data.schema.HCatSchemaUtils.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.RevisionManagerEndpoint.java</file>
			<file type="D">org.apache.hcatalog.common.HCatException.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.TestMultiOutputFormat.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.HCatOutputFormat.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.RevisionManager.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.HCatInputFormat.java</file>
			<file type="D">org.apache.hcatalog.data.HCatRecord.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.RevisionManagerProtocol.java</file>
			<file type="D">org.apache.hcatalog.hbase.HCatTableSnapshot.java</file>
			<file type="D">org.apache.hcatalog.har.HarOutputCommitterPostProcessor.java</file>
			<file type="D">org.apache.hcatalog.cli.TestPermsGrp.java</file>
			<file type="D">org.apache.hcatalog.templeton.HiveDelegator.java</file>
			<file type="D">org.apache.hcatalog.hbase.TestHBaseHCatStorageHandler.java</file>
			<file type="D">org.apache.hcatalog.templeton.ColumnDesc.java</file>
			<file type="D">org.apache.hcatalog.templeton.EnqueueBean.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.OutputJobInfo.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.FileOutputFormatContainer.java</file>
			<file type="D">org.apache.hcatalog.templeton.WadlConfig.java</file>
			<file type="D">org.apache.hcatalog.storagehandler.DummyHCatAuthProvider.java</file>
			<file type="D">org.apache.hcatalog.data.HCatRecordable.java</file>
			<file type="D">org.apache.hcatalog.pig.HCatStorer.java</file>
			<file type="D">org.apache.hcatalog.utils.StoreNumbers.java</file>
			<file type="D">org.apache.hcatalog.templeton.ProxyUserSupport.java</file>
			<file type="D">org.apache.hcatalog.messaging.DropDatabaseMessage.java</file>
			<file type="D">org.apache.hcatalog.pig.TestHCatLoaderComplexSchema.java</file>
			<file type="D">org.apache.hcatalog.oozie.JavaAction.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.TestHCatDynamicPartitioned.java</file>
			<file type="D">org.apache.hcatalog.templeton.Server.java</file>
			<file type="D">org.apache.hcatalog.messaging.DropPartitionMessage.java</file>
			<file type="D">org.apache.hcatalog.listener.NotificationListener.java</file>
			<file type="D">org.apache.hcatalog.api.HCatCreateTableDesc.java</file>
			<file type="D">org.apache.hcatalog.messaging.json.JSONMessageDeserializer.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.HCatTableInfo.java</file>
			<file type="D">org.apache.hcatalog.messaging.json.JSONDropDatabaseMessage.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.TestHCatMultiOutputFormat.java</file>
			<file type="D">org.apache.hcatalog.pig.TestHCatStorerWrapper.java</file>
			<file type="D">org.apache.hcatalog.pig.TestOrcHCatLoaderComplexSchema.java</file>
			<file type="D">org.apache.hcatalog.templeton.SimpleExceptionMapper.java</file>
			<file type="D">org.apache.hcatalog.cli.TestUseDatabase.java</file>
			<file type="D">org.apache.hcatalog.pig.TestHCatStorer.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.ProgressReporter.java</file>
			<file type="D">org.apache.hcatalog.data.schema.HCatSchema.java</file>
			<file type="D">org.apache.hcatalog.cli.HCatDriver.java</file>
			<file type="D">org.apache.hcatalog.pig.MockLoader.java</file>
			<file type="D">org.apache.hcatalog.templeton.tool.NullSplit.java</file>
			<file type="D">org.apache.hcatalog.rcfile.TestRCFileMapReduceInputFormat.java</file>
			<file type="D">org.apache.hcatalog.utils.HCatTestDriver.java</file>
			<file type="D">org.apache.hcatalog.data.transfer.EntityBase.java</file>
			<file type="D">org.apache.hcatalog.templeton.PigDelegator.java</file>
			<file type="D">org.apache.hcatalog.templeton.AppConfig.java</file>
			<file type="D">org.apache.hcatalog.hbase.ImportSequenceFile.java</file>
			<file type="D">org.apache.hcatalog.templeton.CallbackFailedException.java</file>
			<file type="D">org.apache.hcatalog.messaging.CreateDatabaseMessage.java</file>
			<file type="D">org.apache.hcatalog.utils.GroupByAge.java</file>
			<file type="D">org.apache.hcatalog.messaging.MessageDeserializer.java</file>
			<file type="D">org.apache.hcatalog.data.TestReaderWriter.java</file>
			<file type="D">org.apache.hcatalog.templeton.StreamingDelegator.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.lock.ZNodeName.java</file>
			<file type="D">org.apache.hcatalog.rcfile.RCFileMapReduceRecordReader.java</file>
			<file type="D">org.apache.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer.java</file>
			<file type="D">org.apache.hcatalog.cli.HCatCli.java</file>
			<file type="D">org.apache.hcatalog.HcatTestUtils.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.HCatStorageHandler.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.Security.java</file>
			<file type="D">org.apache.hcatalog.templeton.DatabaseDesc.java</file>
			<file type="D">org.apache.hcatalog.templeton.TableDesc.java</file>
			<file type="D">org.apache.hcatalog.data.HCatRecordObjectInspector.java</file>
			<file type="D">org.apache.hcatalog.templeton.ListDelegator.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.TestHCatPartitioned.java</file>
			<file type="D">org.apache.hcatalog.templeton.tool.TrivialExecService.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.ZKUtil.java</file>
			<file type="D">org.apache.hcatalog.pig.PigHCatUtil.java</file>
			<file type="D">org.apache.hcatalog.api.HCatClientHMSImpl.java</file>
			<file type="D">org.apache.hcatalog.data.transfer.ReadEntity.java</file>
			<file type="D">org.apache.hcatalog.api.HCatDatabase.java</file>
			<file type="D">org.apache.hcatalog.templeton.tool.JobStateTracker.java</file>
			<file type="D">org.apache.hcatalog.templeton.TestDesc.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.lock.TestWriteLock.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.TestHCatHiveCompatibility.java</file>
			<file type="D">org.apache.hcatalog.templeton.tool.NotFoundException.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.transaction.thrift.StoreFamilyRevisionList.java</file>
			<file type="D">org.apache.hcatalog.data.transfer.HCatReader.java</file>
			<file type="D">org.apache.hcatalog.templeton.GroupPermissionsDesc.java</file>
			<file type="D">org.apache.hcatalog.common.ErrorType.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.TestZNodeSetUp.java</file>
			<file type="D">org.apache.hcatalog.data.schema.HCatFieldSchema.java</file>
			<file type="D">org.apache.hcatalog.templeton.TempletonDelegator.java</file>
			<file type="D">org.apache.hcatalog.templeton.mock.MockUriInfo.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.package-info.java</file>
			<file type="D">org.apache.hcatalog.hbase.TestSnapshots.java</file>
			<file type="D">org.apache.hcatalog.ExitException.java</file>
			<file type="D">org.apache.hcatalog.utils.SumNumbers.java</file>
			<file type="D">org.apache.hcatalog.utils.TypeDataCheck.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.HCatMapReduceTest.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.FileOutputCommitterContainer.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.TestHCatNonPartitioned.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.FamilyRevision.java</file>
			<file type="D">org.apache.hcatalog.hbase.HBaseHCatStorageHandler.java</file>
			<file type="D">org.apache.hcatalog.pig.TestHCatStorerMulti.java</file>
			<file type="D">org.apache.hcatalog.listener.TestMsgBusConnection.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.TestThriftSerialization.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.TestPassProperties.java</file>
			<file type="D">org.apache.hcatalog.hbase.HBaseInputFormat.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.FosterStorageHandler.java</file>
			<file type="D">org.apache.hcatalog.messaging.HCatEventMessage.java</file>
			<file type="D">org.apache.hcatalog.api.HCatCreateDBDesc.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.RevisionManagerConfiguration.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.InputJobInfo.java</file>
			<file type="D">org.apache.hcatalog.messaging.json.JSONDropPartitionMessage.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.HCatBaseOutputFormat.java</file>
			<file type="D">org.apache.hcatalog.templeton.ExecService.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.DefaultOutputFormatContainer.java</file>
			<file type="D">org.apache.hcatalog.common.TestHCatUtil.java</file>
			<file type="D">org.apache.hcatalog.data.TestLazyHCatRecord.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.ZKBasedRevisionManager.java</file>
			<file type="D">org.apache.hcatalog.utils.HCatTypeCheck.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.transaction.thrift.StoreFamilyRevision.java</file>
			<file type="D">org.apache.hcatalog.templeton.JsonBuilder.java</file>
			<file type="D">org.apache.hcatalog.templeton.DeleteDelegator.java</file>
			<file type="D">org.apache.hcatalog.templeton.JarDelegator.java</file>
			<file type="D">org.apache.hcatalog.data.TestHCatRecordSerDe.java</file>
			<file type="D">org.apache.hcatalog.templeton.HcatException.java</file>
			<file type="D">org.apache.hcatalog.data.TestDefaultHCatRecord.java</file>
			<file type="D">org.apache.hcatalog.utils.HBaseReadWrite.java</file>
			<file type="D">org.apache.hcatalog.templeton.tool.TempletonStorage.java</file>
			<file type="D">org.apache.hcatalog.templeton.tool.TestTrivialExecService.java</file>
			<file type="D">org.apache.hcatalog.pig.TestHCatLoader.java</file>
			<file type="D">org.apache.hcatalog.fileformats.TestOrcDynamicPartitioned.java</file>
			<file type="D">org.apache.hcatalog.templeton.TablePropertyDesc.java</file>
			<file type="D">org.apache.hcatalog.templeton.tool.TestTempletonUtils.java</file>
			<file type="D">org.apache.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzerBase.java</file>
			<file type="D">org.apache.hcatalog.messaging.jms.MessagingUtils.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.StorerInfo.java</file>
			<file type="D">org.apache.hcatalog.pig.HCatBaseLoader.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.OutputCommitterContainer.java</file>
			<file type="D">org.apache.hcatalog.pig.HCatStorerWrapper.java</file>
			<file type="D">org.apache.hcatalog.templeton.CompleteBean.java</file>
			<file type="D">org.apache.hcatalog.messaging.json.JSONMessageFactory.java</file>
			<file type="D">org.apache.hcatalog.common.HCatContext.java</file>
			<file type="D">org.apache.hcatalog.templeton.mock.MockServer.java</file>
			<file type="D">org.apache.hcatalog.hbase.TestHBaseDirectOutputFormat.java</file>
			<file type="D">org.apache.hcatalog.data.transfer.HCatWriter.java</file>
			<file type="D">org.apache.hcatalog.templeton.tool.ZooKeeperCleanup.java</file>
			<file type="D">org.apache.hcatalog.utils.HCatTypeCheckHive.java</file>
			<file type="D">org.apache.hcatalog.templeton.CompleteDelegator.java</file>
			<file type="D">org.apache.hcatalog.utils.StoreComplex.java</file>
			<file type="D">org.apache.hcatalog.messaging.AddPartitionMessage.java</file>
			<file type="D">org.apache.hcatalog.utils.ReadWrite.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.TestHCatExternalHCatNonPartitioned.java</file>
			<file type="D">org.apache.hcatalog.api.HCatClient.java</file>
			<file type="D">org.apache.hcatalog.pig.HCatLoader.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.TestSequenceFileReadWrite.java</file>
			<file type="D">org.apache.hcatalog.templeton.UgiFactory.java</file>
			<file type="D">org.apache.hcatalog.MiniCluster.java</file>
			<file type="D">org.apache.hcatalog.hbase.HBaseConstants.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.IDGenClient.java</file>
			<file type="D">org.apache.hcatalog.templeton.SecureProxySupport.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.TableSnapshot.java</file>
			<file type="D">org.apache.hcatalog.NoExitSecurityManager.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.TestHCatHiveThriftCompatibility.java</file>
			<file type="D">org.apache.hcatalog.utils.WriteText.java</file>
			<file type="D">org.apache.hcatalog.templeton.ExecBean.java</file>
			<file type="D">org.apache.hcatalog.api.ConnectionFailureException.java</file>
			<file type="D">org.apache.hcatalog.cli.DummyStorageHandler.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.OutputFormatContainer.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.TestRevisionManager.java</file>
			<file type="D">org.apache.hcatalog.utils.SimpleRead.java</file>
			<file type="D">org.apache.hcatalog.templeton.Main.java</file>
			<file type="D">org.apache.hcatalog.messaging.json.JSONCreateTableMessage.java</file>
			<file type="D">org.apache.hcatalog.hbase.HBaseBulkOutputFormat.java</file>
			<file type="D">org.apache.hcatalog.security.TestHdfsAuthorizationProvider.java</file>
			<file type="D">org.apache.hcatalog.hbase.SkeletonHBaseTest.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.DefaultRecordWriterContainer.java</file>
			<file type="D">org.apache.hcatalog.utils.ReadRC.java</file>
			<file type="D">org.apache.hcatalog.utils.ReadJson.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.TestHCatExternalPartitioned.java</file>
			<file type="D">org.apache.hcatalog.data.DefaultHCatRecord.java</file>
			<file type="D">org.apache.hcatalog.templeton.tool.NullRecordReader.java</file>
			<file type="D">org.apache.hcatalog.messaging.json.JSONCreateDatabaseMessage.java</file>
			<file type="D">org.apache.hcatalog.templeton.StatusDelegator.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.RevisionManagerFactory.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.InternalUtil.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.lock.TestZNodeName.java</file>
			<file type="D">org.apache.hcatalog.data.TestJsonSerDe.java</file>
			<file type="D">org.apache.hcatalog.templeton.tool.ZooKeeperStorage.java</file>
			<file type="D">org.apache.hcatalog.hbase.snapshot.lock.WriteLock.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.TestHCatPartitionPublish.java</file>
			<file type="D">org.apache.hcatalog.mapreduce.InitializeInput.java</file>
			<file type="D">org.apache.hcatalog.common.HCatConstants.java</file>
			<file type="D">org.apache.hcatalog.pig.TestE2EScenarios.java</file>
			<file type="D">org.apache.hcatalog.messaging.json.JSONDropTableMessage.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">4871</link>
			<link type="Blocker" description="blocks">4896</link>
			<link type="Duplicate" description="is duplicated by">4266</link>
		</links>
	</bug>
	<bug id="5228" opendate="2013-09-05 18:37:15" fixdate="2013-09-06 01:10:52" resolution="Duplicate">
		<buginformation>
			<summary>exceptions regarding NonExistentDatabaseUsedForHealthCheck are extremely annoying</summary>
			<description>There are bunch of exceptions in the logs of many tests, with long call stacks, regarding NonExistentDatabaseUsedForHealthCheck. This is coming from CacheableHiveMetaStoreClient. Perhaps it could do something better for health check, like remember last database that was used and use that instead. I am not familiar with the code so I would assume this health check is actually useful.</description>
			<version>0.11.0</version>
			<fixedVersion></fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.hcatalog.common.HiveClientCache.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">5225</link>
		</links>
	</bug>
	<bug id="5204" opendate="2013-09-04 00:19:53" fixdate="2013-09-10 04:04:41" resolution="Fixed">
		<buginformation>
			<summary>Change type compatibility methods to use PrimitiveCategory rather than TypeInfo</summary>
			<description>The type compatibility methods in the FunctionRegistry (getCommonClass, implicitConvertable) compare TypeInfo objects directly when its doing its type compatibility logic. This won&amp;amp;apos;t work as well with qualified types (varchar, char, decimal), because we will need different TypeInfo objects to represent varchar(5) and varchar(10), and the equality comparisons won&amp;amp;apos;t work anymore. We can change this logic to look at the PrimitiveCategory for the TypeInfo instead.</description>
			<version>0.12.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseCompare.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">6693</link>
			<link type="Incorporates" description="is part of">4844</link>
			<link type="dependent" description="depends upon">5203</link>
			<link type="dependent" description="is depended upon by">5206</link>
		</links>
	</bug>
	<bug id="5056" opendate="2013-08-11 13:25:03" fixdate="2013-09-10 17:03:41" resolution="Fixed">
		<buginformation>
			<summary>MapJoinProcessor ignores order of values in removing RS</summary>
			<description>http://www.mail-archive.com/user@hive.apache.org/msg09073.html</description>
			<version>0.11.0</version>
			<fixedVersion>0.12.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">5256</link>
		</links>
	</bug>
	<bug id="5256" opendate="2013-09-10 12:50:26" fixdate="2013-09-10 17:09:01" resolution="Duplicate">
		<buginformation>
			<summary>A map join operator may have in-consistent output row schema with the common join operator which it will replace</summary>
			<description>When generating a common join operator, Semantic Analyzer gets the input RowResolver of each parent operators. It then uses the following piece of code to interate the tables from the RowResolver (refer to genJoinOperatorChildren()):
        RowResolver inputRS = opParseCtx.get(input).getRowResolver();
        Iterator&amp;lt;String&amp;gt; keysIter = inputRS.getTableNames().iterator();
		...
        while (keysIter.hasNext()) {
          String key = keysIter.next();
Note that the interation order is not deterministic because of the current RowResolver implementation:
  private  HashMap&amp;lt;String, LinkedHashMap&amp;lt;String, ColumnInfo&amp;gt;&amp;gt; rslvMap;
  ...
  public Set&amp;lt;String&amp;gt; getTableNames() 
{
    return rslvMap.keySet();
  }

Generally, the interation order has no problem. However, it may be problematic when a common join operator is being converted to a map join operator.
MapJoinProcessor.convertMapJoin():
      RowResolver inputRS = opParseCtxMap.get(newParentOps.get(pos)).getRowResolver();
      List&amp;lt;ExprNodeDesc&amp;gt; values = new ArrayList&amp;lt;ExprNodeDesc&amp;gt;();
      Iterator&amp;lt;String&amp;gt; keysIter = inputRS.getTableNames().iterator();
      while (keysIter.hasNext()) {
The problem is that the table iteration order for a input RowResolver may be different from that in the generation of the common join operator, which result in an in-consistent output row schema. Thus wrong row schema may be input to child operators and will cause problems.
I found this issue when running a TPC-DS query. And this issue happens to be exposed due to HIVE-4078.
The proposed fix is to change RowResolver to define rslvMap as LinkedHashMap instead of HashMap. Thus the table iteration order of a RowResolver is fixed.</description>
			<version>0.11.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">5056</link>
		</links>
	</bug>
	<bug id="4619" opendate="2013-05-28 07:54:01" fixdate="2013-09-10 19:23:27" resolution="Fixed">
		<buginformation>
			<summary>Hive 0.11.0 is not working with pre-cdh3u6 and hadoop-0.23</summary>
			<description>path uris in input split are missing scheme (it&amp;amp;apos;s fixed on cdh3u6 and hadoop 1.0)

2013-05-28 14:34:28,857 INFO org.apache.hadoop.hive.ql.exec.MapOperator: Adding alias data_type to work list for file hdfs://qa14:9000/user/hive/warehouse/data_type
2013-05-28 14:34:28,858 ERROR org.apache.hadoop.hive.ql.exec.MapOperator: Configuration does not have any alias for path: /user/hive/warehouse/data_type/000000_0
2013-05-28 14:34:28,875 INFO org.apache.hadoop.mapred.TaskLogsTruncater: Initializing logs&amp;amp;apos; truncater with mapRetainSize=-1 and reduceRetainSize=-1
2013-05-28 14:34:28,877 WARN org.apache.hadoop.mapred.Child: Error running child
java.lang.RuntimeException: Error in configuring object
        at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:93)
        at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:64)
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)
        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:387)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:325)
        at org.apache.hadoop.mapred.Child$4.run(Child.java:266)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1278)
        at org.apache.hadoop.mapred.Child.main(Child.java:260)
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:88)
        ... 9 more
Caused by: java.lang.RuntimeException: Error in configuring object
        at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:93)
        at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:64)
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)
        at org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:34)
        ... 14 more
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:88)
        ... 17 more
Caused by: java.lang.RuntimeException: Map operator initialization failed
        at org.apache.hadoop.hive.ql.exec.ExecMapper.configure(ExecMapper.java:121)
        ... 22 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.ql.metadata.HiveException: Configuration and input path are inconsistent
        at org.apache.hadoop.hive.ql.exec.MapOperator.setChildren(MapOperator.java:522)
        at org.apache.hadoop.hive.ql.exec.ExecMapper.configure(ExecMapper.java:90)
        ... 22 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Configuration and input path are inconsistent
        at org.apache.hadoop.hive.ql.exec.MapOperator.setChildren(MapOperator.java:516)
        ... 23 more
2013-05-28 14:34:28,881 INFO org.apache.hadoop.mapred.Task: Runnning cleanup for the task

</description>
			<version>0.11.0</version>
			<fixedVersion>0.12.0, 0.13.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">4633</link>
		</links>
	</bug>
	<bug id="5225" opendate="2013-09-05 16:47:04" fixdate="2013-09-11 00:50:04" resolution="Fixed">
		<buginformation>
			<summary>There is no database named nonexistentdatabaseusedforhealthcheck</summary>
			<description>HiveClientCache.CacheableHiveMetaStoreClient.isOpen() tries to do a health status check of the connection.  This causes the following exception to be written to the log file.  It needlessly pollutes the log file and cause alarm for customers.
The exception itself is produced by the metastore so the client can&amp;amp;apos;t suppress it.
Metastore should not log this since this is clearly a user error.  It should instead throw an exception to the client (or return an error some other way).
13/09/05 12:04:38 ERROR metastore.RetryingHMSHandler: NoSuchObjectException(message:There is no database named nonexistentdatabaseusedforhealthcheck)
	at org.apache.hadoop.hive.metastore.ObjectStore.getMDatabase(ObjectStore.java:429)
	at org.apache.hadoop.hive.metastore.ObjectStore.getDatabase(ObjectStore.java:439)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hive.metastore.RetryingRawStore.invoke(RetryingRawStore.java:124)
	at $Proxy9.getDatabase(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_database(HiveMetaStore.java:627)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:103)
	at $Proxy10.get_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDatabase(HiveMetaStoreClient.java:810)
	at org.apache.hive.hcatalog.common.HiveClientCache$CacheableHiveMetaStoreClient.isOpen(HiveClientCache.java:276)
	at org.apache.hive.hcatalog.common.HiveClientCache.get(HiveClientCache.java:146)
	at org.apache.hive.hcatalog.common.HCatUtil.getHiveClient(HCatUtil.java:544)
	at org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.cancelDelegationTokens(FileOutputCommitterContainer.java:728)
	at org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.commitJob(FileOutputCommitterContainer.java:228)
	at org.apache.hive.hcatalog.mapreduce.HCatMapReduceTest.runMRCreate(HCatMapReduceTest.java:306)
	at org.apache.hive.hcatalog.mapreduce.TestHCatDynamicPartitioned.runHCatDynamicPartitionedTable(TestHCatDynamicPartitioned.java:118)
	at org.apache.hive.hcatalog.mapreduce.TestHCatDynamicPartitioned.testHCatDynamicPartitionedTable(TestHCatDynamicPartitioned.java:104)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:523)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1063)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:914)</description>
			<version>0.11.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.hcatalog.common.HiveClientCache.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">5228</link>
			<link type="Reference" description="is related to">5266</link>
		</links>
	</bug>
	<bug id="4868" opendate="2013-07-16 18:30:06" fixdate="2013-09-11 17:11:22" resolution="Duplicate">
		<buginformation>
			<summary>When reading an ORC file by an MR job, some Mappers may not be able to process data in some cases</summary>
			<description>Let&amp;amp;apos;s say a stripe of an ORC file is 256 MB and we set the split size for an MR job to 64 MB. Right now, splits are created based on byte ranges. 
Here is an example:


|&amp;lt;-The start of a stripe                |&amp;lt;-The end of a stripe
v                                       v
|---------------------------------------|
   ^                        ^ 
   |&amp;lt;- The start of a split |&amp;lt;- The end of a split


So, for some Mappers, it is possible that there is no start of a stripe within the byte range of a split. Those Mappers will process 0 record. We can improve how splits are created for ORC.
</description>
			<version>0.11.0</version>
			<fixedVersion></fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.shims.HadoopShims.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.TestOrcFile.java</file>
			<file type="M">org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
			<file type="M">org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.StripeInformation.java</file>
			<file type="M">org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">5102</link>
		</links>
	</bug>
	<bug id="5102" opendate="2013-08-15 21:45:59" fixdate="2013-09-12 12:09:14" resolution="Fixed">
		<buginformation>
			<summary>ORC getSplits should create splits based the stripes </summary>
			<description>Currently ORC inherits getSplits from FileFormat, which basically makes a split per an HDFS block. This can create too little parallelism and would be better done by having getSplits look at the file footer and create splits based on the stripes.</description>
			<version>0.11.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.shims.HadoopShims.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.TestOrcFile.java</file>
			<file type="M">org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
			<file type="M">org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.StripeInformation.java</file>
			<file type="M">org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">4868</link>
			<link type="Reference" description="relates to">6016</link>
		</links>
	</bug>
	<bug id="5237" opendate="2013-09-06 14:34:09" fixdate="2013-09-13 09:25:50" resolution="Duplicate">
		<buginformation>
			<summary>Incorrect group-by aggregation in 0.11.0</summary>
			<description>
group by with sub queries does not correctly aggregate results in Hive 0.11.0.
To reproduce:
Put the file


1,b
2,c
2,b
3,a
3,c
4,a


in HDFS, and run


create external table abc (x int, y string) row format delimited fields terminated by &amp;amp;apos;,&amp;amp;apos; location &amp;amp;apos;/data/&amp;amp;apos;;


The query


select
        x,
        count(*)
from
(select
        x,
        y
from
        abc
group by
      x,
      y
) a
group by
        x;


will then give the result


2	1
3	1
2	1
4	1
3	1
1	1


instead of the correct


1	1
2	2
3	2
4	1


In 0.9.0 and 0.10.0 this is all working correctly.</description>
			<version>0.11.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">5149</link>
		</links>
	</bug>
	<bug id="4173" opendate="2013-03-14 08:16:45" fixdate="2013-09-17 07:37:37" resolution="Duplicate">
		<buginformation>
			<summary>Hive Ignoring where clause for multitable insert</summary>
			<description>Hive is ignoring Filter conditions given at Multi Insert select statement when  Filtering given on Source Query..
To highlight this issue, please see below example with where clause (status!=&amp;amp;apos;C&amp;amp;apos;) from employee12 table causing issue and due to which insert filters (batch_id=&amp;amp;apos;12 and batch_id!=&amp;amp;apos;12&amp;amp;apos; )not working and dumping all the data coming from source to both the tables.
I have checked the hive execution plan, and didn&amp;amp;apos;t find Filter predicates under for filtering record per insert statements
from 
(from employee12
select * 
where status!=&amp;amp;apos;C&amp;amp;apos;) t
insert into table employee1
select 
status,
field1,
&amp;amp;apos;T&amp;amp;apos; as field2,
&amp;amp;apos;P&amp;amp;apos; as field3,
&amp;amp;apos;C&amp;amp;apos; as field4
where batch_id=&amp;amp;apos;12&amp;amp;apos;
insert into table employee2
select
status,
field1,
&amp;amp;apos;D&amp;amp;apos; as field2, 
&amp;amp;apos;P&amp;amp;apos; as field3,
&amp;amp;apos;C&amp;amp;apos; as field4
where batch_id!=&amp;amp;apos;12&amp;amp;apos;;
It is working fine with single insert. Hive generating plan properly.. 
I am able to reproduce this issue with 8.1 and 9.0 version of Hive.
</description>
			<version>0.8.1</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">3699</link>
		</links>
	</bug>
	<bug id="3420" opendate="2012-08-31 07:10:55" fixdate="2013-09-22 08:17:06" resolution="Fixed">
		<buginformation>
			<summary>Inefficiency in hbase handler when process query including rowkey range scan</summary>
			<description>When query hive with hbase rowkey range, hive map tasks do not leverage startrow, endrow information in tablesplit. For example, if the rowkeys fit into 5 hbase files, then where will be 5 map tasks. Ideally, each task will process 1 file. But in current implementation, each task processes 5 files repeatedly. The behavior not only waste network bandwidth, but also worse the lock contention in HBase block cache as each task have to access the same block. The problem code is in HiveHBaseTableInputFormat.convertFilte as below:

    if (tableSplit != null) 
{
      tableSplit = new TableSplit(
        tableSplit.getTableName(),
        startRow,
        stopRow,
        tableSplit.getRegionLocation());
    }
    scan.setStartRow(startRow);
    scan.setStopRow(stopRow);

As tableSplit already include startRow, endRow information of file, the better implementation will be:
        
        byte[] splitStart = startRow;
        byte[] splitStop = stopRow;
    if (tableSplit != null) {
           if(tableSplit.getStartRow() != null)
{
                        splitStart = startRow.length == 0 ||
          Bytes.compareTo(tableSplit.getStartRow(), startRow) &amp;gt;= 0 ?
            tableSplit.getStartRow() : startRow;
                }
                if(tableSplit.getEndRow() != null)
{
                        splitStop = (stopRow.length == 0 ||
          Bytes.compareTo(tableSplit.getEndRow(), stopRow) &amp;lt;= 0) &amp;amp;&amp;amp;
          tableSplit.getEndRow().length &amp;gt; 0 ?
            tableSplit.getEndRow() : stopRow;
                }
 
      tableSplit = new TableSplit(
        tableSplit.getTableName(),
        splitStart,
        splitStop,
        tableSplit.getRegionLocation());
    }
    scan.setStartRow(splitStart);
    scan.setStopRow(splitStop);
        
In my test, the changed code will improve performance more than 30%.</description>
			<version>0.9.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">4247</link>
			<link type="Reference" description="is related to">11609</link>
		</links>
	</bug>
	<bug id="4247" opendate="2013-03-28 20:06:28" fixdate="2013-09-22 08:23:39" resolution="Duplicate">
		<buginformation>
			<summary>Filtering on a hbase row key duplicates results across multiple mappers</summary>
			<description>Steps to reproduce
1. Create a Hive external table with HiveHbaseHandler with enough data in the hbase table to spawn multiple mappers for the hive query.
2. Write a query which has a filter (in the where clause) based on the hbase row key. 
3. Running the map reduce job leads to each mapper querying the entire data set.  duplicating the data for each mapper. Each mapper processes the entire filtered range and the results get multiplied as the number of mappers run.
Expected behavior:
Each mapper should process a different part of the data and should not duplicate.
Cause:
The cause seems to be the convertFilter method in HiveHBaseTableInputFormat. convertFilter has this piece of code which rewrites the start and the stop row for each split which leads each mapper to process the entire range
 if (tableSplit != null) 
{
      tableSplit = new TableSplit(
        tableSplit.getTableName(),
        startRow,
        stopRow,
        tableSplit.getRegionLocation());
    }

The scan already has the start and stop row set when the splits are created. So this piece of code is probably redundant.
</description>
			<version>0.9.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">3420</link>
		</links>
	</bug>
	<bug id="5329" opendate="2013-09-20 18:37:32" fixdate="2013-09-25 04:25:53" resolution="Fixed">
		<buginformation>
			<summary>Date and timestamp type converts invalid strings to &amp;apos;1970-01-01&amp;apos;</summary>
			<description>

select
  cast(&amp;amp;apos;abcd&amp;amp;apos; as date),
  cast(&amp;amp;apos;abcd&amp;amp;apos; as timestamp)
from src limit 1;



returns &amp;amp;apos;1970-01-01&amp;amp;apos;</description>
			<version>0.12.0</version>
			<fixedVersion>0.12.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableTimestampObjectInspector.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFToDate.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaDateObjectInspector.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaTimestampObjectInspector.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.primitive.TestPrimitiveObjectInspectorUtils.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableDateObjectInspector.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">5328</link>
		</links>
	</bug>
	<bug id="5427" opendate="2013-10-02 19:15:04" fixdate="2013-10-02 19:54:33" resolution="Duplicate">
		<buginformation>
			<summary>TestMetastoreVersion.testVersionRestriction fails on hive 0.12</summary>
			<description>TestMetastoreVersion.testVersionRestriction failed on hive 0.12 . See https://builds.apache.org/job/Hive-branch-0.12-hadoop1/lastCompletedBuild/testReport/org.apache.hadoop.hive.metastore/TestMetastoreVersion/testVersionRestriction/
It also failed in a test run on another machine I ran tests on.
The error - 
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:Hive Schema version 0.12.0 does not match metastore&amp;amp;apos;s schema version fooVersion Metastore is not upgraded or corrupt)
It looks like the fooVersion set by one test is getting used by this failing test.</description>
			<version>0.12.0</version>
			<fixedVersion>0.12.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.TestMetastoreVersion.java</file>
			<file type="M">org.apache.hive.beeline.HiveSchemaTool.java</file>
			<file type="M">org.apache.hive.beeline.HiveSchemaHelper.java</file>
			<file type="M">org.apache.hive.beeline.src.test.TestSchemaTool.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">5419</link>
			<link type="Reference" description="is related to">3764</link>
		</links>
	</bug>
	<bug id="5419" opendate="2013-10-02 05:49:46" fixdate="2013-10-02 22:25:18" resolution="Fixed">
		<buginformation>
			<summary>Fix schema tool issues with Oracle metastore </summary>
			<description>Address oracle schema upgrade script issue in 0.12 and trunk (0.13)</description>
			<version>0.12.0</version>
			<fixedVersion>0.12.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.TestMetastoreVersion.java</file>
			<file type="M">org.apache.hive.beeline.HiveSchemaTool.java</file>
			<file type="M">org.apache.hive.beeline.HiveSchemaHelper.java</file>
			<file type="M">org.apache.hive.beeline.src.test.TestSchemaTool.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">5427</link>
			<link type="Reference" description="is related to">5301</link>
		</links>
	</bug>
	<bug id="5364" opendate="2013-09-25 20:54:11" fixdate="2013-10-04 07:56:38" resolution="Fixed">
		<buginformation>
			<summary>NPE on some queries from partitioned orc table</summary>
			<description>If you create a partitioned ORC table with:


create table A
...
PARTITIONED BY (
year int,
month int,
day int)


This query will fail:
select count from A where where year=2013 and month=9 and day=15;</description>
			<version>0.12.0</version>
			<fixedVersion>0.12.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">5568</link>
			<link type="Supercedes" description="supercedes">5401</link>
		</links>
	</bug>
	<bug id="5546" opendate="2013-10-15 15:06:59" fixdate="2013-10-16 15:33:09" resolution="Fixed">
		<buginformation>
			<summary>A change in ORCInputFormat made by HIVE-4113 was reverted by HIVE-5391</summary>
			<description>

2013-10-15 10:49:49,386 INFO org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: included column ids = 
2013-10-15 10:49:49,386 INFO org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: included columns names = 
2013-10-15 10:49:49,386 INFO org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: No ORC pushdown predicate
2013-10-15 10:49:49,834 INFO org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader: Processing file hdfs://localhost:54310/user/hive/warehouse/web_sales_orc/000000_0
2013-10-15 10:49:49,834 INFO org.apache.hadoop.mapred.MapTask: numReduceTasks: 1
2013-10-15 10:49:49,840 INFO org.apache.hadoop.mapred.MapTask: io.sort.mb = 100
2013-10-15 10:49:49,968 INFO org.apache.hadoop.mapred.TaskLogsTruncater: Initializing logs&amp;amp;apos; truncater with mapRetainSize=-1 and reduceRetainSize=-1
2013-10-15 10:49:49,994 INFO org.apache.hadoop.io.nativeio.NativeIO: Initialized cache for UID to User mapping with a cache timeout of 14400 seconds.
2013-10-15 10:49:49,994 INFO org.apache.hadoop.io.nativeio.NativeIO: Got UserName yhuai for UID 1000 from the native implementation
2013-10-15 10:49:49,996 FATAL org.apache.hadoop.mapred.Child: Error running child : java.lang.OutOfMemoryError: Java heap space
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.&amp;lt;init&amp;gt;(MapTask.java:949)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:428)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1136)
	at org.apache.hadoop.mapred.Child.main(Child.java:249)


If includedColumnIds is an empty list, we do not need to read any column. But, right now, in OrcInputFormat.findIncludedColumns, we have ...


if (ColumnProjectionUtils.isReadAllColumns(conf) ||
      includedStr == null || includedStr.trim().length() == 0) {
      return null;
    } 


If includedStr is an empty string, the code assumes that we need all columns, which is not correct.</description>
			<version>0.13.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">5563</link>
			<link type="Regression" description="is broken by">5391</link>
		</links>
	</bug>
	<bug id="5563" opendate="2013-10-16 17:10:08" fixdate="2013-10-16 17:51:51" resolution="Duplicate">
		<buginformation>
			<summary>Skip reading columns in ORC for count(*)</summary>
			<description>With HIVE-4113, the semantics of ColumnProjectionUtils.getReadColumnIds was fixed so that an empty list means no columns instead of all columns. (Except the caveat of the override of ColumnProjectionUtils.isReadAllColumns.)
However, ORC&amp;amp;apos;s reader wasn&amp;amp;apos;t updated so it still reads all columns.</description>
			<version>0.13.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">5546</link>
		</links>
	</bug>
	<bug id="5511" opendate="2013-10-10 01:16:50" fixdate="2013-10-25 17:42:44" resolution="Fixed">
		<buginformation>
			<summary>percentComplete returned by job status from WebHCat is null</summary>
			<description>In hadoop1 the logging from MR is sent to stderr.  In H2, by default, to syslog.  templeton.tool.LaunchMapper expects to see the output on stderr to produce &amp;amp;apos;percentComplete&amp;amp;apos; in job status.</description>
			<version>0.12.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.shims.HadoopShims.java</file>
			<file type="M">org.apache.hadoop.mapred.WebHCatJTShim20S.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.PigJobIDParser.java</file>
			<file type="M">org.apache.hadoop.mapred.WebHCatJTShim23.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.TempletonUtils.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.JarJobIDParser.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.HiveJobIDParser.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.HDFSStorage.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.TestTrivialExecService.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.TempletonControllerJob.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.TrivialExecService.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.CompleteDelegator.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.AppConfig.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">5547</link>
			<link type="Blocker" description="is blocked by">5133</link>
			<link type="Duplicate" description="duplicates">4978</link>
			<link type="Reference" description="relates to">6768</link>
			<link type="Reference" description="relates to">5806</link>
			<link type="Reference" description="is related to">6035</link>
		</links>
	</bug>
	<bug id="5528" opendate="2013-10-12 20:06:28" fixdate="2013-10-29 15:14:12" resolution="Duplicate">
		<buginformation>
			<summary>hive log file name in local is ".log"</summary>
			<description>In local mode the log is getting written to /tmp/
{user.name}/.log instead of /tmp/{user.name}
/hive.log</description>
			<version>0.11.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TestExecDriver.java</file>
			<file type="M">org.apache.hadoop.hive.conf.TestHiveLogging.java</file>
			<file type="M">org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
			<file type="M">org.apache.hadoop.hive.ql.TestLocationQueries.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.formatting.MetaDataPrettyFormatUtils.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hive.service.auth.TestCustomAuthentication.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
			<file type="M">org.apache.hadoop.hive.conf.TestHiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.mr.MapRedTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.TestMemoryManager.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.TestPassProperties.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.TestMarkPartitionRemote.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
			<file type="M">org.apache.hadoop.hive.cli.TestCliDriverMethods.java</file>
			<file type="M">org.apache.hcatalog.mapreduce.FileOutputCommitterContainer.java</file>
			<file type="M">org.apache.hcatalog.mapreduce.TestPassProperties.java</file>
			<file type="M">org.apache.hadoop.hive.common.LogUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.TestMTQueries.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">5676</link>
		</links>
	</bug>
	<bug id="5676" opendate="2013-10-28 21:06:56" fixdate="2013-10-29 15:25:50" resolution="Fixed">
		<buginformation>
			<summary>Cleanup test cases as done during mavenization</summary>
			<description>A number of issues where found in HIVE-5107 and we plan on committing them directly to trunk.</description>
			<version>0.11.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TestExecDriver.java</file>
			<file type="M">org.apache.hadoop.hive.conf.TestHiveLogging.java</file>
			<file type="M">org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
			<file type="M">org.apache.hadoop.hive.ql.TestLocationQueries.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.formatting.MetaDataPrettyFormatUtils.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hive.service.auth.TestCustomAuthentication.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
			<file type="M">org.apache.hadoop.hive.conf.TestHiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.mr.MapRedTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.TestMemoryManager.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.TestPassProperties.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.TestMarkPartitionRemote.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
			<file type="M">org.apache.hadoop.hive.cli.TestCliDriverMethods.java</file>
			<file type="M">org.apache.hcatalog.mapreduce.FileOutputCommitterContainer.java</file>
			<file type="M">org.apache.hcatalog.mapreduce.TestPassProperties.java</file>
			<file type="M">org.apache.hadoop.hive.common.LogUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.TestMTQueries.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">5528</link>
			<link type="Reference" description="relates to">5610</link>
		</links>
	</bug>
	<bug id="5845" opendate="2013-11-18 23:07:41" fixdate="2013-11-21 02:21:19" resolution="Fixed">
		<buginformation>
			<summary>CTAS failed on vectorized code path</summary>
			<description>Following query fails:
 create table store_sales_2 stored as orc as select * from alltypesorc;</description>
			<version>0.13.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcSerde.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.VectorizedOrcSerde.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriter.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">5863</link>
		</links>
	</bug>
	<bug id="5863" opendate="2013-11-21 00:44:53" fixdate="2013-11-21 18:02:47" resolution="Duplicate">
		<buginformation>
			<summary>INSERT OVERWRITE TABLE fails in vectorized mode for ORC format target table</summary>
			<description>create table store(s_store_key int, s_city string)
stored as orc;
set hive.vectorized.execution.enabled = true;
insert overwrite table store
select cint, cstring1
from alltypesorc;
Alltypesorc is a test table that is checked in to the Hive source.
Expected result: data is added to store table.
Actual result:
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there&amp;amp;apos;s no reduce operator
Starting Job = job_201311191600_0007, Tracking URL = http://localhost:50030/jobdetails.jsp?jobid=job_201311191600_0007
Kill Command = c:\Hadoop\hadoop-1.1.0-SNAPSHOT\bin\hadoop.cmd job  -kill job_201311191600_0007
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2013-11-20 16:39:53,271 Stage-1 map = 0%,  reduce = 0%
2013-11-20 16:40:20,375 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201311191600_0007 with errors
Error during job, obtaining debugging information...
Job Tracking URL: http://localhost:50030/jobdetails.jsp?jobid=job_201311191600_0007
Examining task ID: task_201311191600_0007_m_000002 (and more) from job job_201311191600_0007
Task with the most failures(4):

Task ID:
  task_201311191600_0007_m_000000
URL:
http://localhost:50030/taskdetails.jsp?jobid=job_201311191600_0007&amp;amp;tipid=task_201311191600_0007_m_000000

Diagnostic Messages for this Task:
java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row
        at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:181)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)
        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:366)
        at org.apache.hadoop.mapred.Child$4.run(Child.java:266)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1136)
        at org.apache.hadoop.mapred.Child.main(Child.java:260)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row
        at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:45)
        at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:163)
        ... 8 more
Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.io.orc.OrcStruct cannot be cast to [Ljava.lang.Object;
        at org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getStructFieldData(StandardStructObjectInspec
tor.java:173)
        at org.apache.hadoop.hive.ql.io.orc.WriterImpl$StructTreeWriter.write(WriterImpl.java:1349)
        at org.apache.hadoop.hive.ql.io.orc.WriterImpl.addRow(WriterImpl.java:1962)
        at org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat$OrcRecordWriter.write(OrcOutputFormat.java:78)
        at org.apache.hadoop.hive.ql.exec.vector.VectorFileSinkOperator.processOp(VectorFileSinkOperator.java:159)
        at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:489)
        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:827)
        at org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.processOp(VectorSelectOperator.java:129)
        at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:489)
        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:827)
        at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:91)
        at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:489)
        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:827)
        at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:43)
        ... 9 more</description>
			<version>0.13.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcSerde.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.VectorizedOrcSerde.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriter.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">5845</link>
		</links>
	</bug>
	<bug id="5986" opendate="2013-12-09 05:59:28" fixdate="2013-12-10 21:49:43" resolution="Duplicate">
		<buginformation>
			<summary>ORC SARG evaluation fails with NPE for UDFs or expressions in predicate condition</summary>
			<description>
select s from orctable where length(substr(s, 1, 2)) &amp;lt;= 2 and s like &amp;amp;apos;%&amp;amp;apos;;

 kind of queries generate empty child expressions for the operator (AND in this case). When child expressions are empty evaluate(TruthValue[] leaves) functions returns null which results in NPE during orc split elimination or row group elimination. </description>
			<version>0.13.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.sarg.TestSearchArgumentImpl.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">5580</link>
		</links>
	</bug>
	<bug id="5580" opendate="2013-10-17 21:08:15" fixdate="2013-12-11 00:40:12" resolution="Fixed">
		<buginformation>
			<summary>push down predicates with an and-operator between non-SARGable predicates will get NPE</summary>
			<description>When all of the predicates in an AND-operator in a SARG expression get removed by the SARG builder, evaluation can end up with a NPE. Sub-expressions are typically removed from AND-operators because they aren&amp;amp;apos;t SARGable.</description>
			<version>0.13.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.sarg.TestSearchArgumentImpl.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">5986</link>
		</links>
	</bug>
	<bug id="4977" opendate="2013-08-01 20:08:55" fixdate="2013-12-16 05:17:05" resolution="Duplicate">
		<buginformation>
			<summary>HS2: support an alternate resultset serialization format between client and server</summary>
			<description>Current serialization protocol between client and server as defined in cli_service.thrift results in 2x (or more) throughput degradation compared to HS1.
Initial proposal is to introduce HS1 serialization protocol as a negotiable alternative.</description>
			<version>0.10.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="D">org.apache.hive.service.cli.Row.java</file>
			<file type="M">org.apache.hive.jdbc.HiveConnection.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ListSinkOperator.java</file>
			<file type="M">org.apache.hive.service.cli.session.SessionManager.java</file>
			<file type="M">org.apache.hive.service.cli.operation.GetTablesOperation.java</file>
			<file type="M">org.apache.hive.service.cli.operation.SQLOperation.java</file>
			<file type="M">org.apache.hive.jdbc.miniHS2.TestHiveServer2.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.TProtocolVersion.java</file>
			<file type="M">org.apache.hive.service.cli.TableSchema.java</file>
			<file type="M">org.apache.hive.service.cli.session.HiveSession.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.TOpenSessionResp.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.SerDeUtils.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
			<file type="M">org.apache.hive.jdbc.HiveBaseResultSet.java</file>
			<file type="M">org.apache.hive.service.cli.operation.HiveCommandOperation.java</file>
			<file type="M">org.apache.hive.service.cli.operation.GetSchemasOperation.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.TStatus.java</file>
			<file type="M">org.apache.hive.service.cli.SessionHandle.java</file>
			<file type="M">org.apache.hive.jdbc.HiveQueryResultSet.java</file>
			<file type="M">org.apache.hive.service.cli.operation.GetCatalogsOperation.java</file>
			<file type="M">org.apache.hive.service.cli.session.HiveSessionImplwithUGI.java</file>
			<file type="M">org.apache.hive.service.cli.operation.GetFunctionsOperation.java</file>
			<file type="M">org.apache.hive.service.cli.OperationHandle.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.ThriftCLIServiceTest.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.TExecuteStatementReq.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.TColumn.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FetchFormatter.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.TTypeQualifiers.java</file>
			<file type="M">org.apache.hive.service.cli.operation.GetColumnsOperation.java</file>
			<file type="M">org.apache.hive.service.cli.operation.GetTableTypesOperation.java</file>
			<file type="M">org.apache.hive.service.cli.operation.GetTypeInfoOperation.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.TGetTablesReq.java</file>
			<file type="M">org.apache.hive.jdbc.HiveDatabaseMetaData.java</file>
			<file type="M">org.apache.hive.service.cli.RowSet.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.ThriftCLIServiceClient.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.EmbeddedThriftBinaryCLIService.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.TRowSet.java</file>
			<file type="M">org.apache.hive.service.cli.ColumnValue.java</file>
			<file type="M">org.apache.hive.service.cli.CLIService.java</file>
			<file type="M">org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.TOpenSessionReq.java</file>
			<file type="M">org.apache.hive.service.cli.operation.Operation.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">3746</link>
			<link type="Reference" description="is related to">5276</link>
		</links>
	</bug>
	<bug id="5972" opendate="2013-12-06 03:19:42" fixdate="2013-12-16 05:17:38" resolution="Duplicate">
		<buginformation>
			<summary>Hiveserver2 is much slower than hiveserver1</summary>
			<description>we are building ms sql cube by linkedserver connectiong hiveserver with Cloudera&amp;amp;apos;s ODBC driver.
There are two test results:
1. hiveserver1 running on 2CPUs, 8G mem, took about 8 hours
2. hiveserver2 running on 4CPUs, 16 mem, took about 13 hours and 27min (never successful on machine with 2CPUs, 8G mem)
 Although on both cases, almost all CPUs are busy when building cube.
But I cannot understand why hiveserver2 is much slower than hiveserver1, because from doc, hs2 support concurrency, it should be faster than hs1, isn&amp;amp;apos;t it?
Thanks.
CDH4.3 on CentOS6.</description>
			<version>0.10.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="D">org.apache.hive.service.cli.Row.java</file>
			<file type="M">org.apache.hive.jdbc.HiveConnection.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ListSinkOperator.java</file>
			<file type="M">org.apache.hive.service.cli.session.SessionManager.java</file>
			<file type="M">org.apache.hive.service.cli.operation.GetTablesOperation.java</file>
			<file type="M">org.apache.hive.service.cli.operation.SQLOperation.java</file>
			<file type="M">org.apache.hive.jdbc.miniHS2.TestHiveServer2.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.TProtocolVersion.java</file>
			<file type="M">org.apache.hive.service.cli.TableSchema.java</file>
			<file type="M">org.apache.hive.service.cli.session.HiveSession.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.TOpenSessionResp.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.SerDeUtils.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
			<file type="M">org.apache.hive.jdbc.HiveBaseResultSet.java</file>
			<file type="M">org.apache.hive.service.cli.operation.HiveCommandOperation.java</file>
			<file type="M">org.apache.hive.service.cli.operation.GetSchemasOperation.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.TStatus.java</file>
			<file type="M">org.apache.hive.service.cli.SessionHandle.java</file>
			<file type="M">org.apache.hive.jdbc.HiveQueryResultSet.java</file>
			<file type="M">org.apache.hive.service.cli.operation.GetCatalogsOperation.java</file>
			<file type="M">org.apache.hive.service.cli.session.HiveSessionImplwithUGI.java</file>
			<file type="M">org.apache.hive.service.cli.operation.GetFunctionsOperation.java</file>
			<file type="M">org.apache.hive.service.cli.OperationHandle.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.ThriftCLIServiceTest.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.TExecuteStatementReq.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.TColumn.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FetchFormatter.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.TTypeQualifiers.java</file>
			<file type="M">org.apache.hive.service.cli.operation.GetColumnsOperation.java</file>
			<file type="M">org.apache.hive.service.cli.operation.GetTableTypesOperation.java</file>
			<file type="M">org.apache.hive.service.cli.operation.GetTypeInfoOperation.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.TGetTablesReq.java</file>
			<file type="M">org.apache.hive.jdbc.HiveDatabaseMetaData.java</file>
			<file type="M">org.apache.hive.service.cli.RowSet.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.ThriftCLIServiceClient.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.EmbeddedThriftBinaryCLIService.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.TRowSet.java</file>
			<file type="M">org.apache.hive.service.cli.ColumnValue.java</file>
			<file type="M">org.apache.hive.service.cli.CLIService.java</file>
			<file type="M">org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.TOpenSessionReq.java</file>
			<file type="M">org.apache.hive.service.cli.operation.Operation.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">3746</link>
			<link type="Reference" description="is related to">5276</link>
		</links>
	</bug>
	<bug id="4256" opendate="2013-03-29 07:29:05" fixdate="2013-12-19 06:34:31" resolution="Fixed">
		<buginformation>
			<summary>JDBC2 HiveConnection does not use the specified database</summary>
			<description>HiveConnection ignores the database specified in the connection string when configuring the connection.</description>
			<version>0.11.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.jdbc.HiveConnection.java</file>
			<file type="M">org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">5904</link>
			<link type="Duplicate" description="is duplicated by">2564</link>
			<link type="Reference" description="relates to">6180</link>
		</links>
	</bug>
	<bug id="3746" opendate="2012-11-26 20:59:43" fixdate="2014-01-03 01:34:35" resolution="Fixed">
		<buginformation>
			<summary>Fix HS2 ResultSet Serialization Performance Regression</summary>
			<description></description>
			<version>0.10.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Sub-task</type>
		</buginformation>
		<fixedFiles>
			<file type="D">org.apache.hive.service.cli.Row.java</file>
			<file type="M">org.apache.hive.jdbc.HiveConnection.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ListSinkOperator.java</file>
			<file type="M">org.apache.hive.service.cli.session.SessionManager.java</file>
			<file type="M">org.apache.hive.service.cli.operation.GetTablesOperation.java</file>
			<file type="M">org.apache.hive.service.cli.operation.SQLOperation.java</file>
			<file type="M">org.apache.hive.jdbc.miniHS2.TestHiveServer2.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.TProtocolVersion.java</file>
			<file type="M">org.apache.hive.service.cli.TableSchema.java</file>
			<file type="M">org.apache.hive.service.cli.session.HiveSession.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.TOpenSessionResp.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.SerDeUtils.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
			<file type="M">org.apache.hive.jdbc.HiveBaseResultSet.java</file>
			<file type="M">org.apache.hive.service.cli.operation.HiveCommandOperation.java</file>
			<file type="M">org.apache.hive.service.cli.operation.GetSchemasOperation.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.TStatus.java</file>
			<file type="M">org.apache.hive.service.cli.SessionHandle.java</file>
			<file type="M">org.apache.hive.jdbc.HiveQueryResultSet.java</file>
			<file type="M">org.apache.hive.service.cli.operation.GetCatalogsOperation.java</file>
			<file type="M">org.apache.hive.service.cli.session.HiveSessionImplwithUGI.java</file>
			<file type="M">org.apache.hive.service.cli.operation.GetFunctionsOperation.java</file>
			<file type="M">org.apache.hive.service.cli.OperationHandle.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.ThriftCLIServiceTest.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.TExecuteStatementReq.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.TColumn.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FetchFormatter.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.TTypeQualifiers.java</file>
			<file type="M">org.apache.hive.service.cli.operation.GetColumnsOperation.java</file>
			<file type="M">org.apache.hive.service.cli.operation.GetTableTypesOperation.java</file>
			<file type="M">org.apache.hive.service.cli.operation.GetTypeInfoOperation.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.TGetTablesReq.java</file>
			<file type="M">org.apache.hive.jdbc.HiveDatabaseMetaData.java</file>
			<file type="M">org.apache.hive.service.cli.RowSet.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.ThriftCLIServiceClient.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.EmbeddedThriftBinaryCLIService.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.TRowSet.java</file>
			<file type="M">org.apache.hive.service.cli.ColumnValue.java</file>
			<file type="M">org.apache.hive.service.cli.CLIService.java</file>
			<file type="M">org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.TOpenSessionReq.java</file>
			<file type="M">org.apache.hive.service.cli.operation.Operation.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">5972</link>
			<link type="Duplicate" description="is duplicated by">4977</link>
			<link type="Reference" description="relates to">6160</link>
		</links>
	</bug>
	<bug id="5904" opendate="2013-11-27 17:11:44" fixdate="2014-01-04 00:58:07" resolution="Duplicate">
		<buginformation>
			<summary>HiveServer2 JDBC connect to non-default database</summary>
			<description>When connecting to HiveServer to via the following URLs, the session uses the &amp;amp;apos;default&amp;amp;apos; database, instead of the intended database.
jdbc://localhost:10000/customDb
jdbc:///customDb</description>
			<version>0.12.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.jdbc.HiveConnection.java</file>
			<file type="M">org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">4256</link>
			<link type="Reference" description="relates to">2320</link>
		</links>
	</bug>
	<bug id="4386" opendate="2013-04-19 19:43:01" fixdate="2014-01-05 13:01:32" resolution="Duplicate">
		<buginformation>
			<summary>max() and min() return NULL on partition column; distinct() returns nothing</summary>
			<description>partitioned_table is partitioned on year, month, day.
&amp;gt; select max(day) from partitioned_table where year=2013 and month=4;
spins up zero mappers, one reducer, and returns NULL.  Same for
&amp;gt; select min(day) from ...
&amp;gt; select distinct(day) from... returns nothing at all.
Using an explicit intermediate table does work:
&amp;gt; create table foo_max as select day from partitioned_table where year=2013 and month=4;  
&amp;gt; select max(day) from foo_max; drop table foo_max;
Several map-reduce jobs later, the correct answer is given.</description>
			<version>0.8.1</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.MetadataOnlyOptimizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">2955</link>
			<link type="Duplicate" description="is duplicated by">2955</link>
		</links>
	</bug>
	<bug id="3739" opendate="2012-11-22 07:53:55" fixdate="2014-01-05 13:10:47" resolution="Duplicate">
		<buginformation>
			<summary>Hive auto convert join result error: java.lang.InstantiationException: org.antlr.runtime.CommonToken</summary>
			<description>After I set hive.auto.convert.join=true. Any HiveQL with a join executed in hive result a error as this:
-------------
java.lang.InstantiationException: org.antlr.runtime.CommonToken
 Continuing ...
 java.lang.RuntimeException: failed to evaluate: &amp;lt;unbound&amp;gt;=Class.new();
 Continuing ...
 java.lang.InstantiationException: org.antlr.runtime.CommonToken
 Continuing ...
 java.lang.RuntimeException: failed to evaluate: &amp;lt;unbound&amp;gt;=Class.new();
 Continuing ...
 java.lang.InstantiationException: org.antlr.runtime.CommonToken
 Continuing ...
 java.lang.RuntimeException: failed to evaluate: &amp;lt;unbound&amp;gt;=Class.new();
 Continuing ...
 java.lang.InstantiationException: org.antlr.runtime.CommonToken
 Continuing ...
 java.lang.RuntimeException: failed to evaluate: &amp;lt;unbound&amp;gt;=Class.new();
 Continuing ...
-----------------------
can anyone tell why?</description>
			<version>0.9.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.TestUtilities.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">4222</link>
		</links>
	</bug>
	<bug id="5680" opendate="2013-10-29 02:59:10" fixdate="2014-01-06 01:23:18" resolution="Duplicate">
		<buginformation>
			<summary>Hive writes to HBase table throws NullPointerException</summary>
			<description>When I create hive external table for hbase, and insert data into it under hive shell environment. It can run successfully. but after I quit it and reconnect with bin/hive command, the insert operation throws NullPointerException.
[hadoop@dn01 hive]$ bin/hive
hive&amp;gt; CREATE EXTERNAL TABLE `test`(`key` string, `uid` int) STORED BY &amp;amp;apos;org.apache.hadoop.hive.hbase.HBaseStorageHandler&amp;amp;apos; WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,d:uid#b")TBLPROPERTIES("hbase.table.name" = "test");
......
hive&amp;gt; insert into table test select key, uid from reg;
......successfully
hive&amp;gt; quit;
[hadoop@dn01 hive]$ bin/hive
hive&amp;gt; insert into table test select key, uid from reg;
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there&amp;amp;apos;s no reduce operator
java.lang.NullPointerException
	at java.util.Hashtable.put(Hashtable.java:394)
	at java.util.Properties.setProperty(Properties.java:143)
	at org.apache.hadoop.conf.Configuration.set(Configuration.java:438)
	at org.apache.hadoop.hive.ql.exec.Utilities.copyTableJobPropertiesToConf(Utilities.java:1840)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.checkOutputSpecs(FileSinkOperator.java:947)
	at org.apache.hadoop.hive.ql.io.HiveOutputFormatImpl.checkOutputSpecs(HiveOutputFormatImpl.java:67)
	at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:889)
	at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:850)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:850)
	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:824)
	at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:425)
	at org.apache.hadoop.hive.ql.exec.mr.MapRedTask.execute(MapRedTask.java:144)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:151)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:65)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1414)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1192)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1020)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:888)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:259)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:413)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:781)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
Job Submission failed with exception &amp;amp;apos;java.lang.NullPointerException(null)&amp;amp;apos;
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask
</description>
			<version>0.12.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Table.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">5515</link>
		</links>
	</bug>
	<bug id="5515" opendate="2013-10-10 21:54:16" fixdate="2014-01-14 04:45:06" resolution="Fixed">
		<buginformation>
			<summary>Writing to an HBase table throws IllegalArgumentException, failing job submission</summary>
			<description>Inserting data into HBase table via hive query fails with the following message:

$ hive -e "FROM pgc INSERT OVERWRITE TABLE pagecounts_hbase SELECT pgc.* WHERE rowkey LIKE &amp;amp;apos;en/q%&amp;amp;apos; LIMIT 10;"
...
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=&amp;lt;number&amp;gt;
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=&amp;lt;number&amp;gt;
In order to set a constant number of reducers:
  set mapred.reduce.tasks=&amp;lt;number&amp;gt;
java.lang.IllegalArgumentException: Property value must not be null
        at com.google.common.base.Preconditions.checkArgument(Preconditions.java:88)
        at org.apache.hadoop.conf.Configuration.set(Configuration.java:810)
        at org.apache.hadoop.conf.Configuration.set(Configuration.java:792)
        at org.apache.hadoop.hive.ql.exec.Utilities.copyTableJobPropertiesToConf(Utilities.java:2002)
        at org.apache.hadoop.hive.ql.exec.FileSinkOperator.checkOutputSpecs(FileSinkOperator.java:947)
        at org.apache.hadoop.hive.ql.io.HiveOutputFormatImpl.checkOutputSpecs(HiveOutputFormatImpl.java:67)
        at org.apache.hadoop.mapreduce.JobSubmitter.checkSpecs(JobSubmitter.java:458)
        at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:342)
        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1268)
        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1265)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)
        at org.apache.hadoop.mapreduce.Job.submit(Job.java:1265)
        at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)
        at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)
        at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)
        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)
        at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:425)
        at org.apache.hadoop.hive.ql.exec.mr.MapRedTask.execute(MapRedTask.java:136)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:151)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:65)
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1414)
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1192)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1020)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:888)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:259)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:413)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:348)
        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:731)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:601)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:212)
Job Submission failed with exception &amp;amp;apos;java.lang.IllegalArgumentException(Property value must not be null)&amp;amp;apos;
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask

</description>
			<version>0.12.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Table.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">5680</link>
			<link type="Reference" description="relates to">5431</link>
		</links>
	</bug>
	<bug id="2939" opendate="2012-04-10 05:51:04" fixdate="2014-01-14 07:43:25" resolution="Duplicate">
		<buginformation>
			<summary>LazyArray.getList changes array it previously returned </summary>
			<description>Simple query like:
SELECT a, e
FROM ikabiljo_test_string_array
LATERAL VIEW EXPLODE(a) x1 AS e
(table contains one column - ARRAY&amp;lt;STRING&amp;gt; - and has one row - [b,c,a] )
fails with ConcurrentModificationException, since LazyArray.getList changes the cached array it returns.
LazyArray.getList can easily:

return cached array if present, without clearing and refiling. Hive is already not going to work properly if you change input parameters in an UDF. If that doesn&amp;amp;apos;t sound good - it can return Collections.unmodifiableList
or just not cache anything

Same is true for LazyMap.getMap</description>
			<version>0.8.1</version>
			<fixedVersion>0.10.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyMap.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyArray.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">2540</link>
		</links>
	</bug>
	<bug id="6159" opendate="2014-01-07 21:05:42" fixdate="2014-01-17 03:20:01" resolution="Fixed">
		<buginformation>
			<summary>Hive uses deprecated hadoop configuration in Hadoop 2.0</summary>
			<description>Build hive against hadoop 2.0. Then run hive CLI, you&amp;amp;apos;ll see deprecated configurations warnings like this:
13/12/14 01:00:51 INFO Configuration.deprecation: mapred.input.dir.recursive is
 deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
 13/12/14 01:00:52 INFO Configuration.deprecation: mapred.max.split.size is depre
 cated. Instead, use mapreduce.input.fileinputformat.split.maxsize
 13/12/14 01:00:52 INFO Configuration.deprecation: mapred.min.split.size is depre
 cated. Instead, use mapreduce.input.fileinputformat.split.minsize
 13/12/14 01:00:52 INFO Configuration.deprecation: mapred.min.split.size.per.rack
 is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.r
 ack
 13/12/14 01:00:52 INFO Configuration.deprecation: mapred.min.split.size.per.node
 is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.n
 ode
 13/12/14 01:00:52 INFO Configuration.deprecation: mapred.reduce.tasks is depreca
 ted. Instead, use mapreduce.job.reduces
 13/12/14 01:00:52 INFO Configuration.deprecation: mapred.reduce.tasks.speculativ
 e.execution is deprecated. Instead, use mapreduce.reduce.speculative</description>
			<version>0.12.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
			<file type="M">org.apache.hadoop.hive.shims.HadoopShimsSecure.java</file>
			<file type="M">org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
			<file type="M">org.apache.hadoop.hive.shims.HadoopShims.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">6049</link>
		</links>
	</bug>
	<bug id="6295" opendate="2014-01-23 22:42:05" fixdate="2014-01-29 06:53:30" resolution="Duplicate">
		<buginformation>
			<summary>metadata_only test on Tez generates unoptimized plan</summary>
			<description>One of the queries in the test should be a 0 stage metadata only query, but on tez it still produces a table scan.</description>
			<version>0.13.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.TaskCompilerFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.stats.CounterStatsAggregatorTez.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.StatsTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.GenTezWork.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.TestGenTezWork.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.GenMRTableScan1.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanMapper.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">6218</link>
		</links>
	</bug>
	<bug id="6209" opendate="2014-01-16 01:02:58" fixdate="2014-01-31 23:16:00" resolution="Fixed">
		<buginformation>
			<summary>&amp;apos;LOAD DATA INPATH ... OVERWRITE ..&amp;apos; doesn&amp;apos;t overwrite current data</summary>
			<description>In case where user loads data into table using overwrite, using a different file, it is not being overwritten.


$ hdfs dfs -cat /tmp/data
aaa
bbb
ccc
$ hdfs dfs -cat /tmp/data2
ddd
eee
fff
$ hive
hive&amp;gt; create table test (id string); 
hive&amp;gt; load data inpath &amp;amp;apos;/tmp/data&amp;amp;apos; overwrite into table test;
hive&amp;gt; select * from test;
aaa
bbb
ccc
hive&amp;gt; load data inpath &amp;amp;apos;/tmp/data2&amp;amp;apos; overwrite into table test;
hive&amp;gt; select * from test;
aaa
bbb
ccc
ddd
eee
fff


It seems it is broken by HIVE-3756 which added another condition to whether "rmr" should be run on old directory, and skips in this case.
There is a workaround of set fs.hdfs.impl.disable.cache=true; 
which sabotages this condition, but this condition should be removed in long-term.</description>
			<version>0.12.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">5926</link>
			<link type="Regression" description="is broken by">3756</link>
		</links>
	</bug>
	<bug id="6218" opendate="2014-01-16 22:47:35" fixdate="2014-02-12 05:21:18" resolution="Fixed">
		<buginformation>
			<summary>Stats for row-count not getting updated with Tez insert + dbclass=counter</summary>
			<description>Inserting data into hive with Tez,  the stats on row-count is not getting updated when using the counter dbclass.
To reproduce, run "ANALYZE TABLE store_sales COMPUTE STATISTICS;" with tez as the execution engine.</description>
			<version>0.13.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.TaskCompilerFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.stats.CounterStatsAggregatorTez.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.StatsTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.GenTezWork.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.TestGenTezWork.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.GenMRTableScan1.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanMapper.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">6295</link>
		</links>
	</bug>
	<bug id="4996" opendate="2013-08-05 03:53:51" fixdate="2014-02-12 19:20:14" resolution="Fixed">
		<buginformation>
			<summary>unbalanced calls to openTransaction/commitTransaction</summary>
			<description>when we used hiveserver1 based on hive-0.10.0, we found the Exception thrown.It was:
FAILED: Error in metadata: MetaException(message:java.lang.RuntimeException: commitTransaction was called but openTransactionCalls = 0. This probably indicates that the
re are unbalanced calls to openTransaction/commitTransaction)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
help</description>
			<version>0.10.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
			<file type="D">org.apache.hadoop.hive.metastore.TestRawStoreTxn.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			<file type="D">org.apache.hadoop.hive.metastore.RetryingRawStore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.RetryingHMSHandler.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">3272</link>
			<link type="Reference" description="is related to">8850</link>
			<link type="Reference" description="is related to">5181</link>
			<link type="Reference" description="is related to">1760</link>
			<link type="Regression" description="is broken by">4807</link>
		</links>
	</bug>
	<bug id="6420" opendate="2014-02-12 23:08:04" fixdate="2014-02-18 18:01:30" resolution="Duplicate">
		<buginformation>
			<summary>upgrade script for Hive 13 is missing for Derby</summary>
			<description>There&amp;amp;apos;s an upgrade script for all DSes but not for Derby. Nothing needs to be done in that script but I&amp;amp;apos;m being told that some tools might break if there&amp;amp;apos;s no matching file.</description>
			<version>0.12.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.api.StorageDescriptor.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.java</file>
			<file type="D">org.apache.hadoop.hive.ql.TestDriver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.hooks.WriteEntity.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.TableStatsRequest.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.AddPartitionsResult.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.java</file>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ColumnStatistics.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.Schema.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.RequestPartsSpec.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.TableStatsResult.java</file>
			<file type="M">org.apache.hadoop.hive.ql.ErrorMsg.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.PartitionsStatsResult.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.HiveObjectRef.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.Type.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.HiveLockManager.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.Partition.java</file>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.Table.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.Function.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.SkewedInfo.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.PrivilegeBag.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Context.java</file>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.HiveLockMode.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.DropPartitionsResult.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">5843</link>
		</links>
	</bug>
	<bug id="6506" opendate="2014-02-26 00:52:45" fixdate="2014-02-26 01:07:32" resolution="Duplicate">
		<buginformation>
			<summary>hcatalog should automatically work with new tableproperties in ORC</summary>
			<description>HIVE-5504 has changes to handle existing table properties for ORC file format. But it does not automatically pick newly added table properties. We should refactor ORC so that its table property list can be automatically determined.</description>
			<version>0.13.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcFile.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.SpecialCases.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">6507</link>
			<link type="Reference" description="is related to">5504</link>
		</links>
	</bug>
	<bug id="6484" opendate="2014-02-22 00:17:04" fixdate="2014-03-01 02:17:32" resolution="Duplicate">
		<buginformation>
			<summary>HiveServer2 doAs should be session aware both for secured and unsecured session implementation.</summary>
			<description>Currently in unsecured case, the doAs is performed by decorating TProcessor.process method. This has been causing cleanup issues as we end up creating a new clientUgi for each request rather than for each session. This also cleans up the code.
Thejas M Nair Probably you can add more if you&amp;amp;apos;ve seen other issues related to this.</description>
			<version>0.13.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.service.cli.session.HiveSessionProxy.java</file>
			<file type="D">org.apache.hive.service.auth.TUGIContainingProcessor.java</file>
			<file type="M">org.apache.hive.service.cli.session.SessionManager.java</file>
			<file type="M">org.apache.hive.service.cli.session.HiveSession.java</file>
			<file type="M">org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
			<file type="M">org.apache.hive.service.auth.TSetIpAddressProcessor.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">6312</link>
			<link type="Reference" description="is related to">4501</link>
		</links>
	</bug>
	<bug id="5926" opendate="2013-12-03 09:33:38" fixdate="2014-03-03 08:55:19" resolution="Duplicate">
		<buginformation>
			<summary>Load Data OverWrite Into Table Throw org.apache.hadoop.hive.ql.metadata.HiveException</summary>
			<description>step1: create table 
step2: load data 
load data inpath &amp;amp;apos;/tianyi/usys_etl_map_total.del&amp;amp;apos; overwrite into table tianyi_test3
step3: copy file back
hadoop fs -cp /user/hive/warehouse/tianyi_test3/usys_etl_map_total.del /tianyi
step4: load data again
load data inpath &amp;amp;apos;/tianyi/usys_etl_map_total.del&amp;amp;apos; overwrite into table tianyi_test3
here we can see the error in console:
Failed with exception Error moving: hdfs://ocdccluster/tianyi/usys_etl_map_total.del into: /user/hive/warehouse/tianyi_test3/usys_etl_map_total.del
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask
we can find error detail in hive.log:
2013-12-03 17:26:41,717 ERROR exec.Task (SessionState.java:printError(419)) - Failed with exception Error moving: hdfs://ocdccluster/tianyi/usys_etl_map_total.del into: /user/hive/warehouse/tianyi_test3/usys_etl_map_total.del
org.apache.hadoop.hive.ql.metadata.HiveException: Error moving: hdfs://ocdccluster/tianyi/usys_etl_map_total.del into: /user/hive/warehouse/tianyi_test3/usys_etl_map_total.del
	at org.apache.hadoop.hive.ql.metadata.Hive.replaceFiles(Hive.java:2323)
	at org.apache.hadoop.hive.ql.metadata.Table.replaceFiles(Table.java:639)
	at org.apache.hadoop.hive.ql.metadata.Hive.loadTable(Hive.java:1441)
	at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:283)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:151)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:65)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1414)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1192)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1020)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:888)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:259)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:413)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:781)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:208)
Caused by: java.io.IOException: Error moving: hdfs://ocdccluster/tianyi/usys_etl_map_total.del into: /user/hive/warehouse/tianyi_test3/usys_etl_map_total.del
	at org.apache.hadoop.hive.ql.metadata.Hive.replaceFiles(Hive.java:2317)
	... 20 more
2013-12-03 17:26:41,718 ERROR ql.Driver (SessionState.java:printError(419)) - FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask</description>
			<version>0.12.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">6209</link>
		</links>
	</bug>
	<bug id="6542" opendate="2014-03-04 00:09:30" fixdate="2014-03-04 01:31:47" resolution="Duplicate">
		<buginformation>
			<summary>build error with jdk 7</summary>
			<description></description>
			<version>0.13.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.MapJoinEagerRowContainer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.TestPTFRowContainer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.PTFPartition.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.TestMapJoinEqualityTableContainer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.MapJoinRowContainer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.RowContainer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.TestMapJoinRowContainer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.PTFRowContainer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.AbstractRowContainer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.TestMapJoinTableContainer.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">6530</link>
		</links>
	</bug>
	<bug id="6530" opendate="2014-03-01 05:22:52" fixdate="2014-03-04 17:56:33" resolution="Fixed">
		<buginformation>
			<summary>JDK 7 trunk build fails after HIVE-6418 patch</summary>
			<description>JDK7 build fails with following error 

[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project hive-exec: Compilation failure
[ERROR] /home/prasadm/repos/apache/hive-trunk/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/LazyFlatRowContainer.java:[118,15] name clash: add(java.util.List&amp;lt;java.lang.Object&amp;gt;) in org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer overrides a method whose erasure is the same as another method, yet neither overrides the other
[ERROR] first method:  add(E) in java.util.AbstractCollection
[ERROR] second method: add(ROW) in org.apache.hadoop.hive.ql.exec.persistence.AbstractRowContainer
[ERROR] -&amp;gt; [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn &amp;lt;goals&amp;gt; -rf :hive-exec


This LazyFlatRowContainer.java is  a new file added as part of  HIVE-6418 patch. It&amp;amp;apos;s extending AbstractCollection and implements AbstractRowContainer. Looks like the both these have a add() method that&amp;amp;apos;s conflicting.</description>
			<version>0.13.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.MapJoinEagerRowContainer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.TestPTFRowContainer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.PTFPartition.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.TestMapJoinEqualityTableContainer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.MapJoinRowContainer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.RowContainer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.TestMapJoinRowContainer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.PTFRowContainer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.AbstractRowContainer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.TestMapJoinTableContainer.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">6542</link>
			<link type="Reference" description="is related to">6418</link>
		</links>
	</bug>
	<bug id="5843" opendate="2013-11-18 18:15:34" fixdate="2014-03-05 00:21:38" resolution="Fixed">
		<buginformation>
			<summary>Transaction manager for Hive</summary>
			<description>As part of the ACID work proposed in HIVE-5317 a transaction manager is required.</description>
			<version>0.12.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Sub-task</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.api.StorageDescriptor.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.java</file>
			<file type="D">org.apache.hadoop.hive.ql.TestDriver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.hooks.WriteEntity.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.TableStatsRequest.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.AddPartitionsResult.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.java</file>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ColumnStatistics.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.Schema.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.RequestPartsSpec.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.TableStatsResult.java</file>
			<file type="M">org.apache.hadoop.hive.ql.ErrorMsg.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.PartitionsStatsResult.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.HiveObjectRef.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.Type.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.HiveLockManager.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.Partition.java</file>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.Table.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.Function.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.SkewedInfo.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.PrivilegeBag.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Context.java</file>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.HiveLockMode.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.DropPartitionsResult.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">6420</link>
			<link type="Reference" description="is related to">6606</link>
			<link type="dependent" description="is depended upon by">5687</link>
		</links>
	</bug>
	<bug id="5218" opendate="2013-09-05 06:51:53" fixdate="2014-03-06 15:04:46" resolution="Duplicate">
		<buginformation>
			<summary>datanucleus does not work with MS SQLServer in Hive metastore</summary>
			<description>HIVE-3632 upgraded datanucleus version to 3.2.x, however, this version of datanucleus doesn&amp;amp;apos;t work with SQLServer as the metastore. The problem is that datanucleus tries to use fully qualified object name to find a table in the database but couldn&amp;amp;apos;t find it.
If I downgrade the version to HIVE-2084, SQLServer works fine.
It could be a bug in datanucleus.
This is the detailed exception I&amp;amp;apos;m getting when using datanucleus 3.2.x with SQL Server:

FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTa
sk. MetaException(message:javax.jdo.JDOException: Exception thrown calling table
.exists() for a2ee36af45e9f46c19e995bfd2d9b5fd1hivemetastore..SEQUENCE_TABLE
        at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusExc
eption(NucleusJDOHelper.java:596)
        at org.datanucleus.api.jdo.JDOPersistenceManager.jdoMakePersistent(JDOPe
rsistenceManager.java:732)

        at org.apache.hadoop.hive.metastore.RetryingRawStore.invoke(RetryingRawS
tore.java:111)
        at $Proxy0.createTable(Unknown Source)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_tabl
e_core(HiveMetaStore.java:1071)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_tabl
e_with_environment_context(HiveMetaStore.java:1104)

        at $Proxy11.create_table_with_environment_context(Unknown Source)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$cr
eate_table_with_environment_context.getResult(ThriftHiveMetastore.java:6417)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$cr
eate_table_with_environment_context.getResult(ThriftHiveMetastore.java:6401)

NestedThrowablesStackTrace:
com.microsoft.sqlserver.jdbc.SQLServerException: There is already an object name
d &amp;amp;apos;SEQUENCE_TABLE&amp;amp;apos; in the database.
        at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError
(SQLServerException.java:197)
        at com.microsoft.sqlserver.jdbc.SQLServerStatement.getNextResult(SQLServ
erStatement.java:1493)
        at com.microsoft.sqlserver.jdbc.SQLServerStatement.doExecuteStatement(SQ
LServerStatement.java:775)
        at com.microsoft.sqlserver.jdbc.SQLServerStatement$StmtExecCmd.doExecute
(SQLServerStatement.java:676)
        at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:4615)
        at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLSe
rverConnection.java:1400)
        at com.microsoft.sqlserver.jdbc.SQLServerStatement.executeCommand(SQLSer
verStatement.java:179)
        at com.microsoft.sqlserver.jdbc.SQLServerStatement.executeStatement(SQLS
erverStatement.java:154)
        at com.microsoft.sqlserver.jdbc.SQLServerStatement.execute(SQLServerStat
ement.java:649)
        at com.jolbox.bonecp.StatementHandle.execute(StatementHandle.java:300)
        at org.datanucleus.store.rdbms.table.AbstractTable.executeDdlStatement(A
bstractTable.java:760)
        at org.datanucleus.store.rdbms.table.AbstractTable.executeDdlStatementLi
st(AbstractTable.java:711)
        at org.datanucleus.store.rdbms.table.AbstractTable.create(AbstractTable.
java:425)
        at org.datanucleus.store.rdbms.table.AbstractTable.exists(AbstractTable.
java:488)
        at org.datanucleus.store.rdbms.valuegenerator.TableGenerator.repositoryE
xists(TableGenerator.java:242)
        at org.datanucleus.store.rdbms.valuegenerator.AbstractRDBMSGenerator.obt
ainGenerationBlock(AbstractRDBMSGenerator.java:86)
        at org.datanucleus.store.valuegenerator.AbstractGenerator.obtainGenerati
onBlock(AbstractGenerator.java:197)
        at org.datanucleus.store.valuegenerator.AbstractGenerator.next(AbstractG
enerator.java:105)
        at org.datanucleus.store.rdbms.RDBMSStoreManager.getStrategyValueForGene
rator(RDBMSStoreManager.java:2019)
        at org.datanucleus.store.AbstractStoreManager.getStrategyValue(AbstractS
toreManager.java:1385)
        at org.datanucleus.ExecutionContextImpl.newObjectId(ExecutionContextImpl
.java:3727)
        at org.datanucleus.state.JDOStateManager.setIdentity(JDOStateManager.jav
a:2574)
        at org.datanucleus.state.JDOStateManager.initialiseForPersistentNew(JDOS
tateManager.java:526)
        at org.datanucleus.state.ObjectProviderFactoryImpl.newForPersistentNew(O
bjectProviderFactoryImpl.java:202)
        at org.datanucleus.ExecutionContextImpl.newObjectProviderForPersistentNe
w(ExecutionContextImpl.java:1326)
        at org.datanucleus.ExecutionContextImpl.persistObjectInternal(ExecutionC
ontextImpl.java:2123)
        at org.datanucleus.ExecutionContextImpl.persistObjectWork(ExecutionConte
xtImpl.java:1972)
        at org.datanucleus.ExecutionContextImpl.persistObject(ExecutionContextIm
pl.java:1820)
        at org.datanucleus.ExecutionContextThreadedImpl.persistObject(ExecutionC
ontextThreadedImpl.java:217)
        at org.datanucleus.api.jdo.JDOPersistenceManager.jdoMakePersistent(JDOPe
rsistenceManager.java:727)
        at org.datanucleus.api.jdo.JDOPersistenceManager.makePersistent(JDOPersi
stenceManager.java:752)
        at org.apache.hadoop.hive.metastore.ObjectStore.createTable(ObjectStore.
java:646)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.
java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAcces
sorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:601)
        at org.apache.hadoop.hive.metastore.RetryingRawStore.invoke(RetryingRawS
tore.java:111)
        at $Proxy0.createTable(Unknown Source)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_tabl
e_core(HiveMetaStore.java:1071)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_tabl
e_with_environment_context(HiveMetaStore.java:1104)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.
java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAcces
sorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:601)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHM
SHandler.java:103)
        at $Proxy11.create_table_with_environment_context(Unknown Source)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$cr
eate_table_with_environment_context.getResult(ThriftHiveMetastore.java:6417)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$cr
eate_table_with_environment_context.getResult(ThriftHiveMetastore.java:6401)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
        at org.apache.hadoop.hive.metastore.TSetIpAddressProcessor.process(TSetI
pAddressProcessor.java:48)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadP
oolServer.java:206)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.
java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor
.java:603)
        at java.lang.Thread.run(Thread.java:722)

</description>
			<version>0.12.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.parser.ExpressionTree.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">5099</link>
			<link type="Reference" description="relates to">5099</link>
			<link type="Regression" description="is broken by">3632</link>
		</links>
	</bug>
	<bug id="6477" opendate="2014-02-20 23:11:40" fixdate="2014-03-07 19:41:38" resolution="Duplicate">
		<buginformation>
			<summary>Aggregation functions for tiny/smallint broken with parquet</summary>
			<description>Given the following table:

CREATE TABLE IF NOT EXISTS commontypesagg (
id int,
bool_col boolean,
tinyint_col tinyint,
smallint_col smallint,
int_col int,
bigint_col bigint,
float_col float,
double_col double,
date_string_col string,
string_col string)
PARTITIONED BY (year int, month int, day int)
STORED AS PARQUET;


The following queries throws ClassCastException:

select count(tinyint_col), min(tinyint_col), max(tinyint_col), sum(tinyint_col) from commontypesagg;

select count(smallint_col), min(smallint_col), max(smallint_col), sum(smallint_col) from commontypesagg;


Exception is the following:

2014-01-29 14:02:11,381 INFO org.apache.hadoop.mapred.TaskStatus: task-diagnostic-info for task attempt_201401290934_0006_m_000001_1 : java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {"id":null,"bool_col":null,"tinyint_col":1,"smallint_col":null,"int_col":null,"bigint_col":null,"float_col":null,"double_col":null,"date_string_col":null,"string_col":null,"year":"2009","month":"1"}
at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:175)
at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)
at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:417)
at org.apache.hadoop.mapred.MapTask.run(MapTask.java:332)
at org.apache.hadoop.mapred.Child$4.run(Child.java:268)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:415)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
at org.apache.hadoop.mapred.Child.main(Child.java:262)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {"id":null,"bool_col":null,"tinyint_col":1,"smallint_col":null,"int_col":null,"bigint_col":null,"float_col":null,"double_col":null,"date_string_col":null,"string_col":null,"year":"2009","month":"1"}
at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:529)
at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:157)
... 8 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: org.apache.hadoop.io.IntWritable cannot be cast to java.lang.Byte
at org.apache.hadoop.hive.ql.exec.GroupByOperator.processOp(GroupByOperator.java:796)
at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:504)
at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:844)
at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:87)
at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:504)
at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:844)
at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:91)
at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:504)
at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:844)
at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:519)
... 9 more
Caused by: java.lang.ClassCastException: org.apache.hadoop.io.IntWritable cannot be cast to java.lang.Byte
at org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaByteObjectInspector.get(JavaByteObjectInspector.java:40)
at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.compare(ObjectInspectorUtils.java:666)
at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.compare(ObjectInspectorUtils.java:631)
at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin$GenericUDAFMinEvaluator.merge(GenericUDAFMin.java:109)
at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin$GenericUDAFMinEvaluator.iterate(GenericUDAFMin.java:96)
at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.aggregate(GenericUDAFEvaluator.java:183)
at org.apache.hadoop.hive.ql.exec.GroupByOperator.updateAggregations(GroupByOperator.java:629)
at org.apache.hadoop.hive.ql.exec.GroupByOperator.processHashAggr(GroupByOperator.java:826)
at org.apache.hadoop.hive.ql.exec.GroupByOperator.processKey(GroupByOperator.java:723)
at org.apache.hadoop.hive.ql.exec.GroupByOperator.processOp(GroupByOperator.java:791)
... 18 more

</description>
			<version>0.13.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.parquet.serde.primitive.ParquetByteInspector.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.parquet.serde.primitive.ParquetShortInspector.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">6414</link>
		</links>
	</bug>
	<bug id="6414" opendate="2014-02-12 16:50:56" fixdate="2014-03-08 00:57:25" resolution="Fixed">
		<buginformation>
			<summary>ParquetInputFormat provides data values that do not match the object inspectors</summary>
			<description>While working on HIVE-5998 I noticed that the ParquetRecordReader returns IntWritable for all &amp;amp;apos;int like&amp;amp;apos; types, in disaccord with the row object inspectors. I though fine, and I worked my way around it. But I see now that the issue trigger failuers in other places, eg. in aggregates:

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {"cint":528534767,"ctinyint":31,"csmallint":4963,"cfloat":31.0,"cdouble":4963.0,"cstring1":"cvLH6Eat2yFsyy7p"}
        at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:534)
        at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:177)
        ... 8 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: org.apache.hadoop.io.IntWritable cannot be cast to java.lang.Short
        at org.apache.hadoop.hive.ql.exec.GroupByOperator.processOp(GroupByOperator.java:808)
        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790)
        at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:87)
        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790)
        at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:92)
        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790)
        at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:524)
        ... 9 more
Caused by: java.lang.ClassCastException: org.apache.hadoop.io.IntWritable cannot be cast to java.lang.Short
        at org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaShortObjectInspector.get(JavaShortObjectInspector.java:41)
        at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.compare(ObjectInspectorUtils.java:671)
        at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.compare(ObjectInspectorUtils.java:631)
        at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin$GenericUDAFMinEvaluator.merge(GenericUDAFMin.java:109)
        at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin$GenericUDAFMinEvaluator.iterate(GenericUDAFMin.java:96)
        at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.aggregate(GenericUDAFEvaluator.java:183)
        at org.apache.hadoop.hive.ql.exec.GroupByOperator.updateAggregations(GroupByOperator.java:641)
        at org.apache.hadoop.hive.ql.exec.GroupByOperator.processHashAggr(GroupByOperator.java:838)
        at org.apache.hadoop.hive.ql.exec.GroupByOperator.processKey(GroupByOperator.java:735)
        at org.apache.hadoop.hive.ql.exec.GroupByOperator.processOp(GroupByOperator.java:803)
        ... 15 more


My test is (I&amp;amp;apos;m writing a test .q from HIVE-5998, but the repro does not involve vectorization):

create table if not exists alltypes_parquet (
  cint int,
  ctinyint tinyint,
  csmallint smallint,
  cfloat float,
  cdouble double,
  cstring1 string) stored as parquet;

insert overwrite table alltypes_parquet
  select cint,
    ctinyint,
    csmallint,
    cfloat,
    cdouble,
    cstring1
  from alltypesorc;

explain select * from alltypes_parquet limit 10; select * from alltypes_parquet limit 10;

explain select ctinyint,
  max(cint),
  min(csmallint),
  count(cstring1),
  avg(cfloat),
  stddev_pop(cdouble)
  from alltypes_parquet
  group by ctinyint;
select ctinyint,
  max(cint),
  min(csmallint),
  count(cstring1),
  avg(cfloat),
  stddev_pop(cdouble)
  from alltypes_parquet
  group by ctinyint;

</description>
			<version>0.13.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.parquet.serde.primitive.ParquetByteInspector.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.parquet.serde.primitive.ParquetShortInspector.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">6477</link>
		</links>
	</bug>
	<bug id="5901" opendate="2013-11-27 08:12:23" fixdate="2014-03-08 11:52:45" resolution="Fixed">
		<buginformation>
			<summary>Query cancel should stop running MR tasks</summary>
			<description>Currently, query canceling does not stop running MR job immediately.</description>
			<version>0.12.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.DriverContext.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TaskRunner.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ConditionalTask.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">7198</link>
			<link type="Reference" description="relates to">4017</link>
		</links>
	</bug>
	<bug id="6491" opendate="2014-02-24 11:30:33" fixdate="2014-03-08 13:13:40" resolution="Duplicate">
		<buginformation>
			<summary>ClassCastException in AbstractParquetMapInspector</summary>
			<description>AbstractParquetMapInspector uses wrong class cast https://github.com/apache/hive/blob/trunk/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/AbstractParquetMapInspector.java#L144
It should be AbstractParquetMapInspector


final StandardParquetHiveMapInspector other = (StandardParquetHiveMapInspector) obj;


Such conversion leads to class cast exception in case of DeepParquetHiveMapInspector.


Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.io.parquet.serde.DeepParquetHiveMapInspector cannot be cast to org.apache.hadoop.hive.ql.io.parquet.serde.StandardParquetHiveMapInspector
        at org.apache.hadoop.hive.ql.io.parquet.serde.AbstractParquetMapInspector.equals(AbstractParquetMapInspector.java:131)
        at java.util.AbstractList.equals(AbstractList.java:523)
        at java.util.AbstractList.equals(AbstractList.java:523)
        at java.util.concurrent.ConcurrentHashMap.get(ConcurrentHashMap.java:996)
        at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector(ObjectInspectorFactory.java:281)
        at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector(ObjectInspectorFactory.java:268)
        at org.apache.hadoop.hive.ql.exec.Operator.initEvaluatorsAndReturnStruct(Operator.java:1022)
        at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:65)
        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:377)
        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:453)
        at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:409)
        at org.apache.hadoop.hive.ql.exec.TableScanOperator.initializeOp(TableScanOperator.java:188)
        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:377)
        at org.apache.hadoop.hive.ql.exec.FetchTask.initialize(FetchTask.java:80)
        ... 31 more

</description>
			<version>0.13.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.parquet.serde.AbstractParquetMapInspector.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">6575</link>
		</links>
	</bug>
	<bug id="5099" opendate="2013-08-15 01:00:07" fixdate="2014-03-12 12:32:42" resolution="Fixed">
		<buginformation>
			<summary>Some partition publish operation cause OOM in metastore backed by SQL Server</summary>
			<description>For certain metastore operation combination, metastore operation hangs and metastore server eventually fail due to OOM. This happens when metastore is backed by SQL Server. Here is a testcase to reproduce:


CREATE TABLE tbl_repro_oom1 (a STRING, b INT) PARTITIONED BY (c STRING, d STRING);
CREATE TABLE tbl_repro_oom_2 (a STRING ) PARTITIONED BY (e STRING);
ALTER TABLE tbl_repro_oom1 ADD PARTITION (c=&amp;amp;apos;France&amp;amp;apos;, d=4);
ALTER TABLE tbl_repro_oom1 ADD PARTITION (c=&amp;amp;apos;Russia&amp;amp;apos;, d=3);
ALTER TABLE tbl_repro_oom_2 ADD PARTITION (e=&amp;amp;apos;Russia&amp;amp;apos;);
ALTER TABLE tbl_repro_oom1 DROP PARTITION (c &amp;gt;= &amp;amp;apos;India&amp;amp;apos;); --failure


The code cause the issue is in ExpressionTree.java:


valString = "partitionName.substring(partitionName.indexOf(\"" + keyEqual + "\")+" + keyEqualLength + ").substring(0, partitionName.substring(partitionName.indexOf(\"" + keyEqual + "\")+" + keyEqualLength + ").indexOf(\"/\"))";


The snapshot of table partition before the "drop partition" statement is:


PART_ID  CREATE_TIME    LAST_ACCESS_TIME      PART_NAME                SD_ID   TBL_ID 
93	1376526718	0	             c=France/d=4	127	33
94	1376526718	0	             c=Russia/d=3	128	33
95	1376526718	0	             e=Russia	        129	34


Datanucleus query try to find the value of a particular key by locating "$key=" as the start, "/" as the end. For example, value of c in "c=France/d=4" by locating "c=" as the start, "/" following as the end. However, this query fail if we try to find value "e" in "e=Russia" since there is no tailing "/". 
Other database works since the query plan first filter out the partition not belonging to tbl_repro_oom1. Whether this error surface or not depends on the query optimizer.
When this exception happens, metastore keep trying and throw exception. The memory image of metastore contains a large number of exception objects:


com.microsoft.sqlserver.jdbc.SQLServerException: Invalid length parameter passed to the LEFT or SUBSTRING function.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:197)
	at com.microsoft.sqlserver.jdbc.SQLServerResultSet$FetchBuffer.nextRow(SQLServerResultSet.java:4762)
	at com.microsoft.sqlserver.jdbc.SQLServerResultSet.fetchBufferNext(SQLServerResultSet.java:1682)
	at com.microsoft.sqlserver.jdbc.SQLServerResultSet.next(SQLServerResultSet.java:955)
	at org.apache.commons.dbcp.DelegatingResultSet.next(DelegatingResultSet.java:207)
	at org.apache.commons.dbcp.DelegatingResultSet.next(DelegatingResultSet.java:207)
	at org.datanucleus.store.rdbms.query.ForwardQueryResult.&amp;lt;init&amp;gt;(ForwardQueryResult.java:90)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.performExecute(JDOQLQuery.java:686)
	at org.datanucleus.store.query.Query.executeQuery(Query.java:1791)
	at org.datanucleus.store.query.Query.executeWithMap(Query.java:1694)
	at org.datanucleus.api.jdo.JDOQuery.executeWithMap(JDOQuery.java:334)
	at org.apache.hadoop.hive.metastore.ObjectStore.listMPartitionsByFilter(ObjectStore.java:1715)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByFilter(ObjectStore.java:1590)
	at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.hadoop.hive.metastore.RetryingRawStore.invoke(RetryingRawStore.java:111)
	at $Proxy4.getPartitionsByFilter(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_partitions_by_filter(HiveMetaStore.java:2163)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:105)
	at $Proxy5.get_partitions_by_filter(Unknown Source)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_partitions_by_filter.getResult(ThriftHiveMetastore.java:5449)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_partitions_by_filter.getResult(ThriftHiveMetastore.java:5437)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.hadoop.hive.metastore.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:48)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:176)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
)


This eventually cause the OOM of metastore server. 
This Jira only try to fix the SQL exception, which prevent OOM from happening in the above scenario. We might also need to look at how to prevent metastore OOM when an error happen, but that is out of the scope of this Jira.</description>
			<version>0.12.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.parser.ExpressionTree.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">5218</link>
			<link type="Incorporates" description="incorporates">5303</link>
			<link type="Reference" description="is related to">5218</link>
		</links>
	</bug>
	<bug id="6507" opendate="2014-02-26 01:00:07" fixdate="2014-03-12 12:50:43" resolution="Fixed">
		<buginformation>
			<summary>OrcFile table property names are specified as strings</summary>
			<description>In HIVE-5504, we had to do some special casing in HCatalog to add a particular set of orc table properties from table properties to job properties.
In doing so, it&amp;amp;apos;s obvious that that is a bit cumbersome, and ideally, the list of all orc file table properties should really be an enum, rather than individual loosely tied constant strings. If we were to clean this up, we can clean up other code that references this to reference the entire enum, and avoid future errors when new table properties are introduced, but other referencing code is not updated.</description>
			<version>0.13.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcFile.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.SpecialCases.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">6506</link>
			<link type="Reference" description="is related to">5504</link>
		</links>
	</bug>
	<bug id="6575" opendate="2014-03-07 01:23:53" fixdate="2014-03-12 17:48:52" resolution="Fixed">
		<buginformation>
			<summary>select * fails on parquet table with map datatype</summary>
			<description>Create parquet table with map and run select * from parquet_table, returns following exception:

 FAILED: RuntimeException java.lang.ClassCastException: org.apache.hadoop.hive.ql.io.parquet.serde.DeepParquetHiveMapInspector cannot be cast to org.apache.hadoop.hive.ql.io.parquet.serde.StandardParquetHiveMapInspector


However select &amp;lt;mapcol&amp;gt; from parquet_table seems to work, and thus joins will work.</description>
			<version>0.13.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.parquet.serde.AbstractParquetMapInspector.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">6491</link>
			<link type="Duplicate" description="is duplicated by">6794</link>
			<link type="Incorporates" description="incorporates">6634</link>
		</links>
	</bug>
	<bug id="5568" opendate="2013-10-16 20:08:04" fixdate="2014-03-13 22:04:08" resolution="Duplicate">
		<buginformation>
			<summary>count(*) on ORC tables with predicate pushdown on partition columns fail</summary>
			<description>If the query is:


select count(*) from orc_table where x = 10;


where x is a partition column and predicate pushdown is enabled, you&amp;amp;apos;ll get an array out of bounds exception.</description>
			<version>0.12.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">5364</link>
		</links>
	</bug>
	<bug id="6312" opendate="2014-01-27 02:08:03" fixdate="2014-03-14 18:08:26" resolution="Fixed">
		<buginformation>
			<summary>doAs with plain sasl auth should be session aware</summary>
			<description>TUGIContainingProcessor creates new Subject for each invocation which induces FileSystem leakage when cache is enable(true by default).</description>
			<version>0.13.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.service.cli.session.HiveSessionProxy.java</file>
			<file type="D">org.apache.hive.service.auth.TUGIContainingProcessor.java</file>
			<file type="M">org.apache.hive.service.cli.session.SessionManager.java</file>
			<file type="M">org.apache.hive.service.cli.session.HiveSession.java</file>
			<file type="M">org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
			<file type="M">org.apache.hive.service.auth.TSetIpAddressProcessor.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">6663</link>
			<link type="Duplicate" description="is duplicated by">4501</link>
			<link type="Duplicate" description="is duplicated by">6484</link>
			<link type="Duplicate" description="is duplicated by">5447</link>
			<link type="Reference" description="relates to">6245</link>
			<link type="Reference" description="is related to">9234</link>
		</links>
	</bug>
	<bug id="4501" opendate="2013-05-06 04:23:51" fixdate="2014-03-14 18:17:31" resolution="Duplicate">
		<buginformation>
			<summary>HS2 memory leak - FileSystem objects in FileSystem.CACHE</summary>
			<description>org.apache.hadoop.fs.FileSystem objects are getting accumulated in FileSystem.CACHE, with HS2 in unsecure mode.
As a workaround, it is possible to set fs.hdfs.impl.disable.cache and fs.file.impl.disable.cache to true.
Users should not have to bother with this extra configuration. 
As a workaround disable impersonation by setting hive.server2.enable.doAs to false.</description>
			<version>0.11.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.service.cli.session.HiveSessionProxy.java</file>
			<file type="D">org.apache.hive.service.auth.TUGIContainingProcessor.java</file>
			<file type="M">org.apache.hive.service.cli.session.SessionManager.java</file>
			<file type="M">org.apache.hive.service.cli.session.HiveSession.java</file>
			<file type="M">org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
			<file type="M">org.apache.hive.service.auth.TSetIpAddressProcessor.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">6312</link>
			<link type="Duplicate" description="is duplicated by">8369</link>
			<link type="Reference" description="relates to">3545</link>
			<link type="Reference" description="relates to">6484</link>
			<link type="Reference" description="is related to">5268</link>
			<link type="Reference" description="is related to">3098</link>
			<link type="Reference" description="is related to">5296</link>
			<link type="Reference" description="is related to">9234</link>
		</links>
	</bug>
	<bug id="6176" opendate="2014-01-09 03:43:33" fixdate="2014-03-17 21:08:40" resolution="Duplicate">
		<buginformation>
			<summary>Beeline gives bogus error message if an unaccepted command line option is given</summary>
			<description>

$ beeline -o
-o (No such file or directory)
Beeline version 0.13.0-SNAPSHOT by Apache Hive
beeline&amp;gt; 


The message suggests that beeline accepts a file (without -f option) while it enters interactive mode any way.</description>
			<version>0.12.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.beeline.BeeLine.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">6652</link>
		</links>
	</bug>
	<bug id="6652" opendate="2014-03-13 18:09:35" fixdate="2014-03-20 22:52:54" resolution="Fixed">
		<buginformation>
			<summary>Beeline gives evasive error message for any unrecognized command line arguement</summary>
			<description>For any unrecognized command line argument, Beeline emits a warning message that&amp;amp;apos;s evasive and meaningless. For instance:


beeline abc
abc (No such file or directory)
Beeline version 0.14.0-SNAPSHOT by Apache Hive
...
beeline -hh
-hh (No such file or directory)


The error seeming suggests that Beeline accepts an argument as a file name. However, neither Beeline doc nor command line help indicates there is such an option. 


beeline --help
Usage: java org.apache.hive.cli.beeline.BeeLine 
   -u &amp;lt;database url&amp;gt;               the JDBC URL to connect to
   -n &amp;lt;username&amp;gt;                   the username to connect as
   -p &amp;lt;password&amp;gt;                   the password to connect as
   -d &amp;lt;driver class&amp;gt;               the driver class to use
   -e &amp;lt;query&amp;gt;                      query that should be executed
   -f &amp;lt;file&amp;gt;                       script file that should be executed
   --hiveconf property=value       Use value for given property
   --hivevar name=value            hive variable name and value
                                   This is Hive specific settings in which variables
                                   can be set at session level and referenced in Hive
                                   commands or queries.
   --color=[true/false]            control whether color is used for display
   --showHeader=[true/false]       show column names in query results
   --headerInterval=ROWS;          the interval between which heades are displayed
   --fastConnect=[true/false]      skip building table/column list for tab-completion
   --autoCommit=[true/false]       enable/disable automatic transaction commit
   --verbose=[true/false]          show verbose error messages and debug info
   --showWarnings=[true/false]     display connection warnings
   --showNestedErrs=[true/false]   display nested errors
   --numberFormat=[pattern]        format numbers using DecimalFormat pattern
   --force=[true/false]            continue running script even after errors
   --maxWidth=MAXWIDTH             the maximum width of the terminal
   --maxColumnWidth=MAXCOLWIDTH    the maximum width to use when displaying columns
   --silent=[true/false]           be more silent
   --autosave=[true/false]         automatically save preferences
   --outputformat=[table/vertical/csv/tsv]   format mode for result display
   --isolation=LEVEL               set the transaction isolation level
   --nullemptystring=[true/false]  set to true to get historic behavior of printing null as empty string
   --help                          display this message


Further research shows that this is a residual from SQLLine from which Beeline is derived, which allows user to specify a property file based on which SQLLine can make a DB connection.
While this might be useful, this isn&amp;amp;apos;t documented and has caused a lot of confusions. And it&amp;amp;apos;s the root cause for quite a few problems such as those described in HIVE-5677. HIVE-6173 had the same symptom, which uncovered another problem.
Thus, I&amp;amp;apos;d suggest we disable this option. If it&amp;amp;apos;s desirable to have this option, then we need at least corresponding documentation plus better error message.</description>
			<version>0.11.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.beeline.BeeLine.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">6176</link>
			<link type="Reference" description="relates to">5677</link>
			<link type="Reference" description="relates to">6173</link>
		</links>
	</bug>
	<bug id="6631" opendate="2014-03-12 16:27:27" fixdate="2014-03-22 02:14:54" resolution="Duplicate">
		<buginformation>
			<summary>NPE when select a field of a struct from a table stored by ORC</summary>
			<description>I have a table like this ...


create table lineitem_orc_cg
(
CG1 STRUCT&amp;lt;L_PARTKEY:INT,
           L_SUPPKEY:INT,
           L_COMMITDATE:STRING,
           L_RECEIPTDATE:STRING,
           L_SHIPINSTRUCT:STRING,
           L_SHIPMODE:STRING,
           L_COMMENT:STRING,
           L_TAX:float,
           L_RETURNFLAG:STRING,
           L_LINESTATUS:STRING,
           L_LINENUMBER:INT,
           L_ORDERKEY:INT&amp;gt;,
CG2 STRUCT&amp;lt;L_QUANTITY:float,
           L_EXTENDEDPRICE:float,
           L_DISCOUNT:float,
           L_SHIPDATE:STRING&amp;gt;
)
row format serde &amp;amp;apos;org.apache.hadoop.hive.ql.io.orc.OrcSerde&amp;amp;apos;
stored as orc tblproperties ("orc.compress"="NONE");


When I want to select a field from a struct by using


select cg1.l_comment from lineitem_orc_cg limit 1;


I got 


Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.ExprNodeFieldEvaluator.initialize(ExprNodeFieldEvaluator.java:61)
	at org.apache.hadoop.hive.ql.exec.Operator.initEvaluators(Operator.java:928)
	at org.apache.hadoop.hive.ql.exec.Operator.initEvaluatorsAndReturnStruct(Operator.java:954)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:65)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:375)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:459)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:415)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.initializeOp(TableScanOperator.java:189)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:375)
	at org.apache.hadoop.hive.ql.exec.MapOperator.initializeOp(MapOperator.java:409)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:375)
	at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.configure(ExecMapper.java:133)
	... 22 more

</description>
			<version>0.13.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcStruct.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">6716</link>
		</links>
	</bug>
	<bug id="2615" opendate="2011-11-30 00:44:25" fixdate="2014-03-23 02:25:33" resolution="Duplicate">
		<buginformation>
			<summary>CTAS with literal NULL creates VOID type</summary>
			<description>Create the table with a column that always contains NULL:

hive&amp;gt; create table bad as select 1 x, null z from dual;     
Because there&amp;amp;apos;s no type, Hive gives it the VOID type:

hive&amp;gt; describe bad;
OK
x	int	
z	void	
This seems weird, because AFAIK, there is no normal way to create a column of type VOID.  The problem is that the table can&amp;amp;apos;t be queried:

hive&amp;gt; select * from bad;
OK
Failed with exception java.io.IOException:java.lang.RuntimeException: Internal error: no LazyObject for VOID
Worse, even if you don&amp;amp;apos;t select that field, the query fails at runtime:

hive&amp;gt; select x from bad;
...
FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.MapRedTask</description>
			<version>0.6.0</version>
			<fixedVersion>0.12.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.api.SkewedValueList.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.SkewedInfo.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
			<file type="M">org.apache.hive.service.cli.Row.java</file>
			<file type="M">org.apache.hive.jdbc.HiveBaseResultSet.java</file>
			<file type="M">org.apache.hive.service.cli.Type.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyFactory.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.TTypeId.java</file>
			<file type="M">org.apache.hive.service.cli.ColumnValue.java</file>
			<file type="M">org.apache.hive.jdbc.HiveResultSetMetaData.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.TCLIServiceConstants.java</file>
			<file type="M">org.apache.hive.jdbc.TestJdbcDriver2.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">4172</link>
		</links>
	</bug>
	<bug id="6687" opendate="2014-03-17 21:26:56" fixdate="2014-03-24 05:28:52" resolution="Fixed">
		<buginformation>
			<summary>JDBC ResultSet fails to get value by qualified projection name</summary>
			<description>Getting value from result set using fully qualified name would throw exception. Only solution today is to use position of the column as opposed to column label.


String sql = "select r1.x, r2.x from r1 join r2 on r1.y=r2.y";
ResultSet res = stmt.executeQuery(sql);
res.getInt("r1.x");


res.getInt("r1.x"); would throw exception unknown column even though sql specifies it.
Fix is to fix resultsetschema in semantic analyzer.
</description>
			<version>0.12.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hive.jdbc.TestJdbcDriver2.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">14387</link>
			<link type="Reference" description="relates to">11614</link>
		</links>
	</bug>
	<bug id="6716" opendate="2014-03-21 02:00:36" fixdate="2014-03-24 19:18:08" resolution="Fixed">
		<buginformation>
			<summary>ORC struct throws NPE for tables with inner structs having null values </summary>
			<description>ORCStruct should return null when object passed to getStructFieldsDataAsList(Object obj) is null.


public List&amp;lt;Object&amp;gt; getStructFieldsDataAsList(Object object) {
      OrcStruct struct = (OrcStruct) object;
      List&amp;lt;Object&amp;gt; result = new ArrayList&amp;lt;Object&amp;gt;(struct.fields.length);


In the above code struct.fields will throw NPE if struct is NULL.</description>
			<version>0.13.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcStruct.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">6631</link>
		</links>
	</bug>
	<bug id="6492" opendate="2014-02-24 19:55:39" fixdate="2014-03-26 23:58:22" resolution="Fixed">
		<buginformation>
			<summary>limit partition number involved in a table scan</summary>
			<description>To protect the cluster, a new configure variable "hive.limit.query.max.table.partition" is added to hive configuration to
limit the table partitions involved in a table scan. 
The default value will be set to -1 which means there is no limit by default. 
This variable will not affect "metadata only" query.</description>
			<version>0.12.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>New Feature</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.MetadataOnlyOptimizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.ErrorMsg.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.TableScanDesc.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">12603</link>
		</links>
	</bug>
	<bug id="2752" opendate="2012-01-26 11:29:45" fixdate="2014-03-27 15:31:25" resolution="Fixed">
		<buginformation>
			<summary>Index names are case sensitive</summary>
			<description>The following script:
DROP TABLE IF EXISTS TestTable;
CREATE TABLE TestTable (a INT);
DROP INDEX IF EXISTS TestTableA_IDX ON TestTable;
CREATE INDEX TestTableA_IDX ON TABLE TestTable (a) AS &amp;amp;apos;org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler&amp;amp;apos; WITH DEFERRED REBUILD;
ALTER INDEX TestTableA_IDX ON TestTable REBUILD;
results in the following exception:
MetaException(message:index testtablea_idx doesn&amp;amp;apos;t exist)
	at org.apache.hadoop.hive.metastore.ObjectStore.alterIndex(ObjectStore.java:1880)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler$30.run(HiveMetaStore.java:1930)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler$30.run(HiveMetaStore.java:1927)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.executeWithRetry(HiveMetaStore.java:356)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_index(HiveMetaStore.java:1927)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_index(HiveMetaStoreClient.java:868)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterIndex(Hive.java:398)
	at org.apache.hadoop.hive.ql.exec.DDLTask.alterIndex(DDLTask.java:902)
	at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:236)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:134)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:57)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1332)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1123)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:931)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:255)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:212)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:338)
	at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:436)
	at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:446)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:642)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:554)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
When you execute: "SHOW INDEXES ON TestTable;", you get:
TestTableA_IDX      	testtable           	a                   	default_testtable_testtablea_idx_	compact
so it looks like things don&amp;amp;apos;t get lower cased when they go into the metastore, but they do when the rebuild op is trying to execute.</description>
			<version>0.9.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
		</fixedFiles>
	</bug>
	<bug id="3272" opendate="2012-07-18 19:38:46" fixdate="2014-03-27 18:51:19" resolution="Duplicate">
		<buginformation>
			<summary>RetryingRawStore will perform partial transaction on retry</summary>
			<description>By the time the RetryingRawStore retries a command the transaction encompassing it has already been rolled back.  This means that it will perform the remainder of the raw store commands outside of a transaction, unless there is another one encapsulating it which is definitely not always the case, and then fail when it tries to commit the transaction as there is none open.</description>
			<version>0.10.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
			<file type="D">org.apache.hadoop.hive.metastore.TestRawStoreTxn.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			<file type="D">org.apache.hadoop.hive.metastore.RetryingRawStore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.RetryingHMSHandler.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">4996</link>
		</links>
	</bug>
	<bug id="6877" opendate="2014-04-09 23:13:41" fixdate="2014-04-10 22:37:31" resolution="Fixed">
		<buginformation>
			<summary>TestOrcRawRecordMerger is deleting test.tmp.dir</summary>
			<description>TestOrcRawRecordMerger seems to be deleting the directory pointed to by test.tmp.dir.  This can cause some failures in any tests that run after this test if they need to use any files in the tmp dir such as conf files or creating Hive tables.</description>
			<version>0.13.0</version>
			<fixedVersion>0.14.0, 0.13.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.TestOrcRawRecordMerger.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">6953</link>
		</links>
	</bug>
	<bug id="6935" opendate="2014-04-18 19:44:46" fixdate="2014-04-18 20:33:41" resolution="Duplicate">
		<buginformation>
			<summary>skipTrash option in hive command line</summary>
			<description>hive drop table command deletes the data from HDFS warehouse and puts it into Trash.
Currently there is no way to provide flag to tell warehouse to skip trash while deleting table data.
This ticket is to add skipTrash feature in hive command-line, that looks as following. 
hive -e "drop table skipTrash testTable"
This would be good feature to add, so that user can specify when not to put data into trash directory and thus not to fill hdfs space instead of relying on trash interval and policy configuration to take care of disk filling issue.</description>
			<version>0.12.0</version>
			<fixedVersion></fixedVersion>
			<type>New Feature</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStoreFsImpl.java</file>
			<file type="M">org.apache.hadoop.hive.common.FileUtils.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">6469</link>
		</links>
	</bug>
	<bug id="6469" opendate="2014-02-20 00:40:12" fixdate="2014-04-28 04:05:02" resolution="Fixed">
		<buginformation>
			<summary>skipTrash option in hive command line</summary>
			<description>Th current behavior of hive metastore during a "drop table &amp;lt;table_name&amp;gt;" command is to delete the data from HDFS warehouse and put it into Trash.
Currently there is no way to provide a flag to tell the warehouse to skip trash while deleting table data.
This ticket is to add skipTrash configuration "hive.warehouse.data.skipTrash" , which when set to true, will skipTrash while dropping table data from hdfs warehouse. This will be set to false by default to keep current behavior.
This would be good feature to add, so that an admin of the cluster can specify when not to put data into the trash directory (eg. in a dev environment) and thus not to fill hdfs space instead of relying on trash interval and policy configuration to take care of disk filling issue.</description>
			<version>0.12.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>New Feature</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStoreFsImpl.java</file>
			<file type="M">org.apache.hadoop.hive.common.FileUtils.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">6935</link>
			<link type="Reference" description="is related to">7100</link>
			<link type="Reference" description="is related to">7289</link>
		</links>
	</bug>
	<bug id="6953" opendate="2014-04-22 12:37:33" fixdate="2014-04-28 07:10:46" resolution="Duplicate">
		<buginformation>
			<summary>All CompactorTest failing with Table/View &amp;apos;NEXT_TXN_ID&amp;apos; does not exist</summary>
			<description>When I&amp;amp;apos;m running all tests through the command &amp;amp;apos;mvn clean install -Phadoop-1&amp;amp;apos;, all CompactorTest classes TestInitiator, TestWorker, TestCleaner fail with following exception :

org.apache.hadoop.hive.metastore.api.MetaException: Unable to select from transaction database java.sql.SQLSyntaxErrorException: Table/View &amp;amp;apos;NEXT_TXN_ID&amp;amp;apos; does not exist.
        at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)
        at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
        at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
        at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)

....
Caused by: java.sql.SQLException: Table/View &amp;amp;apos;NEXT_TXN_ID&amp;amp;apos; does not exist.
        at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
        at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)



This is happening on branch-0.13. Has anyone faced this problem?
Owen O&amp;amp;apos;Malley or someone else help me solve this. Do i have to set anything?
</description>
			<version>0.13.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.TestOrcRawRecordMerger.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">6877</link>
		</links>
	</bug>
	<bug id="6741" opendate="2014-03-25 04:56:15" fixdate="2014-05-01 01:20:11" resolution="Fixed">
		<buginformation>
			<summary>HiveServer2 startup fails in secure (kerberos) mode due to backward incompatible hadoop change</summary>
			<description> HADOOP-10211 made a backward incompatible change due to which the following hive call returns a null map (HiveAuthFactory-old):


Map&amp;lt;String, String&amp;gt; hadoopSaslProps =  ShimLoader.getHadoopThriftAuthBridge().
        getHadoopSaslProperties(conf); 
SaslQOP hadoopSaslQOP = SaslQOP.fromString(hadoopSaslProps.get(Sasl.QOP));
if(hadoopSaslQOP.ordinal() &amp;gt; saslQOP.ordinal()) {
LOG.warn(MessageFormat.format("\"hadoop.rpc.protection\" is set to higher security level " +
          "{0} then {1} which is set to {2}", hadoopSaslQOP.toString(),
          ConfVars.HIVE_SERVER2_THRIFT_SASL_QOP.varname, saslQOP.toString()));
}


Since this code path is only used for logging hadoop sasl qop values in case hadoop&amp;amp;apos;s qop &amp;gt; hive&amp;amp;apos;s qop, we can do away with this and add a general log message.</description>
			<version>0.14.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.service.auth.HiveAuthFactory.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">6960</link>
			<link type="Duplicate" description="is duplicated by">8154</link>
			<link type="Reference" description="relates to">6987</link>
			<link type="Reference" description="is related to">7620</link>
			<link type="Reference" description="is related to">10451</link>
		</links>
	</bug>
	<bug id="6693" opendate="2014-03-18 17:27:26" fixdate="2014-05-08 07:39:43" resolution="Duplicate">
		<buginformation>
			<summary>CASE with INT and BIGINT fail</summary>
			<description>CREATE TABLE testCase (n BIGINT)
select case when (n &amp;gt;3) then n else 0 end from testCase
fail with error : 
[Error 10016]: Line 1:36 Argument type mismatch &amp;amp;apos;0&amp;amp;apos;: The expression after ELSE should have the same type as those after THEN: "bigint" is expected but "int" is found&amp;amp;apos;.
bigint and int should be more compatible, at least int should implictly cast to bigint. </description>
			<version>0.12.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseCompare.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">5204</link>
		</links>
	</bug>
	<bug id="7011" opendate="2014-05-03 01:33:29" fixdate="2014-05-12 22:08:01" resolution="Fixed">
		<buginformation>
			<summary>HiveInputFormat&amp;apos;s split generation isn&amp;apos;t thread safe</summary>
			<description>Tez will do split generation in parallel. Need to protect the inputformat cache against concurrent access.</description>
			<version>0.13.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">7393</link>
		</links>
	</bug>
	<bug id="6908" opendate="2014-04-14 23:19:10" fixdate="2014-05-13 16:31:23" resolution="Fixed">
		<buginformation>
			<summary>TestThriftBinaryCLIService.testExecuteStatementAsync has intermittent failures</summary>
			<description>This has failed sometimes in the pre-commit tests.
ThriftCLIServiceTest.testExecuteStatementAsync runs two statements.  They are given 100 second timeout total, not sure if its by intention.  As the first is a select query, it will take a majority of the time.  The second statement (create table) should be quicker, but it fails sometimes because timeout is already mostly used up.
The timeout should probably be reset after the first statement.  If the operation finishes before the timeout, it wont have any effect as it&amp;amp;apos;ll break out.</description>
			<version>0.13.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.service.cli.thrift.ThriftCLIServiceTest.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">6747</link>
		</links>
	</bug>
	<bug id="6626" opendate="2014-03-12 09:25:25" fixdate="2014-05-16 00:50:02" resolution="Duplicate">
		<buginformation>
			<summary>Hive does not expand the DOWNLOADED_RESOURCES_DIR path</summary>
			<description>The downloaded scratch dir is specified in HiveConf as:


DOWNLOADED_RESOURCES_DIR("hive.downloaded.resources.dir", System.getProperty("java.io.tmpdir") + File.separator  + "${hive.session.id}_resources"),



However, hive.session.id  does not get expanded.</description>
			<version>0.13.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Sub-task</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcFile.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
			<file type="M">org.apache.hadoop.hive.conf.TestHiveLogging.java</file>
			<file type="M">org.apache.hive.jdbc.TestJdbcDriver2.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.processors.SetProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.conf.TestHiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType.java</file>
			<file type="M">org.apache.hadoop.hive.conf.TestHiveConfRestrictList.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.VariableSubstitution.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
			<file type="M">org.apache.hive.service.cli.CLIService.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
			<file type="D">org.apache.hive.common.util.SystemVariables.java</file>
			<file type="D">org.apache.hadoop.hive.ant.GenHiveTemplate.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.HiveUtils.java</file>
			<file type="D">org.apache.hadoop.hive.conf.Validator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			<file type="D">org.apache.hadoop.hive.ql.plan.ShowConfDesc.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">6037</link>
		</links>
	</bug>
	<bug id="4561" opendate="2013-05-15 05:57:56" fixdate="2014-05-31 00:01:27" resolution="Fixed">
		<buginformation>
			<summary>Column stats :  LOW_VALUE (or HIGH_VALUE) will always be 0.0000 ,if all the column values larger than 0.0 (or if all column values smaller than 0.0)</summary>
			<description>if all column values larger than 0.0  DOUBLE_LOW_VALUE always will be 0.0 
or  if all column values less than 0.0,  DOUBLE_HIGH_VALUE will always be 
hive (default)&amp;gt; create table src_test (price double);
hive (default)&amp;gt; load data local inpath &amp;amp;apos;./test.txt&amp;amp;apos; into table src_test;
hive (default)&amp;gt; select * from src_test;
OK
1.0
2.0
3.0
Time taken: 0.313 seconds, Fetched: 3 row(s)
hive (default)&amp;gt; analyze table src_test compute statistics for columns price;
mysql&amp;gt; select * from TAB_COL_STATS \G;
                 CS_ID: 16
               DB_NAME: default
            TABLE_NAME: src_test
           COLUMN_NAME: price
           COLUMN_TYPE: double
                TBL_ID: 2586
        LONG_LOW_VALUE: 0
       LONG_HIGH_VALUE: 0
      DOUBLE_LOW_VALUE: 0.0000   # Wrong Result ! Expected is 1.0000
     DOUBLE_HIGH_VALUE: 3.0000
 BIG_DECIMAL_LOW_VALUE: NULL
BIG_DECIMAL_HIGH_VALUE: NULL
             NUM_NULLS: 0
         NUM_DISTINCTS: 1
           AVG_COL_LEN: 0.0000
           MAX_COL_LEN: 0
             NUM_TRUES: 0
            NUM_FALSES: 0
         LAST_ANALYZED: 1368596151
2 rows in set (0.00 sec)</description>
			<version>0.12.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.StatObjectConverter.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.LongColumnStatsData.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.NumDistinctValueEstimator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ColumnStatsTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.StatsOptimizer.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">7060</link>
			<link type="Reference" description="relates to">7060</link>
			<link type="Reference" description="is related to">1362</link>
		</links>
	</bug>
	<bug id="7060" opendate="2014-05-14 17:15:18" fixdate="2014-06-03 05:28:27" resolution="Duplicate">
		<buginformation>
			<summary>Column stats give incorrect min and distinct_count</summary>
			<description>It seems that the result from column statistics isn&amp;amp;apos;t correct on two measures for numeric columns: min (which is always 0) and distinct count. Here is an example:


select count(distinct avgTimeOnSite), min(avgTimeOnSite) from UserVisits_web_text_none;
...
OK
9	1
Time taken: 9.747 seconds, Fetched: 1 row(s)


The statisitics for the column:


desc formatted UserVisits_web_text_none avgTimeOnSite
...
# col_name              data_type               min                     max                     num_nulls               distinct_count          avg_col_len             max_col_len             num_trues               num_falses              comment

avgTimeOnSite           int                     0                       9                       0                       11                      null                    null                    null               

</description>
			<version>0.13.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.StatObjectConverter.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.LongColumnStatsData.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.NumDistinctValueEstimator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ColumnStatsTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.StatsOptimizer.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">4561</link>
			<link type="Reference" description="is related to">4561</link>
		</links>
	</bug>
	<bug id="2564" opendate="2011-11-09 02:01:39" fixdate="2014-06-11 19:00:22" resolution="Duplicate">
		<buginformation>
			<summary>Set dbname at JDBC URL or properties</summary>
			<description>The current Hive implementation ignores a database name at JDBC URL, 
though we can set it by executing "use &amp;lt;DBNAME&amp;gt;" statement.
I think it is better to also specify a database name at JDBC URL or database properties.
Therefore, I&amp;amp;apos;ll attach the patch.</description>
			<version>0.7.1</version>
			<fixedVersion></fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.jdbc.HiveConnection.java</file>
			<file type="M">org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">2320</link>
			<link type="Duplicate" description="duplicates">4256</link>
		</links>
	</bug>
	<bug id="6824" opendate="2014-04-03 01:13:59" fixdate="2014-06-13 00:51:55" resolution="Duplicate">
		<buginformation>
			<summary>Hive HBase query fails on Tez due to missing jars - part 2</summary>
			<description>Follow-up from HIVE-6739. We cannot wait for Tez 0.4 (or even be sure that it will have TEZ-1004 and TEZ-1005), so I will split the patch into two. Original jira will have the straightforward (but less efficient) fix. This jira will use new relocalize APIs. Depending on relative timing of Tez 0.4 release and Hive 0.13 release, this will go into 0.13 or 0.14 blocked on Tez 0.5</description>
			<version>0.14.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
			<file type="M">org.apache.hadoop.hive.ql.session.SessionState.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TestTezTask.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="is blocked by">6739</link>
			<link type="Duplicate" description="is duplicated by">7212</link>
		</links>
	</bug>
	<bug id="7198" opendate="2014-06-09 16:18:56" fixdate="2014-06-16 01:39:47" resolution="Duplicate">
		<buginformation>
			<summary>HiveServer2 CancelOperation does not work for long running queries</summary>
			<description>Sending the CancelOperation() call does not always stop the query and its related MapReduce jobs.
e.g. from https://issues.cloudera.org/browse/HUE-2144


I guess you&amp;amp;apos;re right. But the strange thing is that the canceled query shows in job browser as &amp;amp;apos;Running&amp;amp;apos; and the percents go up - 0%, 50%, then the job is failed.
How does the cancelling actually work? Is it like the hadoop kill command? It seems to me like it works until certain phase of map reduce is done.
And another thing - after cancelling the job in Hue I can kill it with hadoop job -kill job_&amp;lt;id&amp;gt;. If it was killed already, it would show "no such job".

</description>
			<version>0.12.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.DriverContext.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TaskRunner.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ConditionalTask.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">5901</link>
		</links>
	</bug>
	<bug id="7200" opendate="2014-06-09 19:46:35" fixdate="2014-06-16 17:44:24" resolution="Fixed">
		<buginformation>
			<summary>Beeline output displays column heading even if --showHeader=false is set</summary>
			<description>A few minor/cosmetic issues with the beeline CLI.
1) Tool prints the column headers despite setting the --showHeader to false. This property only seems to affect the subsequent header information that gets printed based on the value of property "headerInterval" (default value is 100).
2) When "showHeader" is true &amp;amp; "headerInterval &amp;gt; 0", the header after the first interval gets printed after &amp;lt;headerInterval - 1&amp;gt; rows. The code seems to count the initial header as a row, if you will.
3) The table footer(the line that closes the table) does not get printed if the "showHeader" is false. I think the table should get closed irrespective of whether it prints the header or not.


0: jdbc:hive2://localhost:10000&amp;gt; select * from stringvals;
+------+
| val  |
+------+
| t    |
| f    |
| T    |
| F    |
| 0    |
| 1    |
+------+
6 rows selected (3.998 seconds)
0: jdbc:hive2://localhost:10000&amp;gt; !set headerInterval 2
0: jdbc:hive2://localhost:10000&amp;gt; select * from stringvals;
+------+
| val  |
+------+
| t    |
+------+
| val  |
+------+
| f    |
| T    |
+------+
| val  |
+------+
| F    |
| 0    |
+------+
| val  |
+------+
| 1    |
+------+
6 rows selected (0.691 seconds)
0: jdbc:hive2://localhost:10000&amp;gt; !set showHeader false
0: jdbc:hive2://localhost:10000&amp;gt; select * from stringvals;
+------+
| val  |
+------+
| t    |
| f    |
| T    |
| F    |
| 0    |
| 1    |
6 rows selected (1.728 seconds)

</description>
			<version>0.13.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.beeline.TableOutputFormat.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">11798</link>
		</links>
	</bug>
	<bug id="7212" opendate="2014-06-11 04:06:19" fixdate="2014-06-16 18:30:39" resolution="Fixed">
		<buginformation>
			<summary>Use resource re-localization instead of restarting sessions in Tez</summary>
			<description>scriptfile1.q is failing on Tez because of a recent breakage in localization. On top of that we&amp;amp;apos;re currently restarting sessions if the resources have changed. (add file/add jar/etc). Instead of doing this we should just have tez relocalize these new resources. This way no session/AM restart is required.</description>
			<version>0.14.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
			<file type="M">org.apache.hadoop.hive.ql.session.SessionState.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TestTezTask.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">6824</link>
		</links>
	</bug>
	<bug id="6561" opendate="2014-03-06 01:39:29" fixdate="2014-06-18 23:32:29" resolution="Fixed">
		<buginformation>
			<summary>Beeline should accept -i option to Initializing a SQL file</summary>
			<description>Hive CLI has -i option. From Hive CLI help:


...
 -i &amp;lt;filename&amp;gt;                    Initialization SQL file
...


However, Beeline has no such option:


xzhang@xzlt:~/apa/hive3$ ./packaging/target/apache-hive-0.14.0-SNAPSHOT-bin/apache-hive-0.14.0-SNAPSHOT-bin/bin/beeline -u jdbc:hive2:// -i hive.rc
...
Connected to: Apache Hive (version 0.14.0-SNAPSHOT)
Driver: Hive JDBC (version 0.14.0-SNAPSHOT)
Transaction isolation: TRANSACTION_REPEATABLE_READ
-i (No such file or directory)
Property "url" is required
Beeline version 0.14.0-SNAPSHOT by Apache Hive
...

</description>
			<version>0.10.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.beeline.BeeLine.java</file>
			<file type="M">org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
			<file type="M">org.apache.hive.beeline.Commands.java</file>
			<file type="M">org.apache.hive.beeline.BeeLineOpts.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">14677</link>
			<link type="Reference" description="relates to">5867</link>
			<link type="Reference" description="relates to">5160</link>
		</links>
	</bug>
	<bug id="7127" opendate="2014-05-27 07:01:57" fixdate="2014-07-02 01:12:45" resolution="Fixed">
		<buginformation>
			<summary>Handover more details on exception in hiveserver2</summary>
			<description>Currently, JDBC hands over exception message and error codes. But it&amp;amp;apos;s not helpful for debugging.

org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: ParseException line 1:0 cannot recognize input near &amp;amp;apos;createa&amp;amp;apos; &amp;amp;apos;asd&amp;amp;apos; &amp;amp;apos;&amp;lt;EOF&amp;gt;&amp;amp;apos;
	at org.apache.hive.jdbc.Utils.verifySuccess(Utils.java:121)
	at org.apache.hive.jdbc.Utils.verifySuccessWithInfo(Utils.java:109)
	at org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:231)
	at org.apache.hive.beeline.Commands.execute(Commands.java:736)
	at org.apache.hive.beeline.Commands.sql(Commands.java:657)
	at org.apache.hive.beeline.BeeLine.dispatch(BeeLine.java:889)
	at org.apache.hive.beeline.BeeLine.begin(BeeLine.java:744)
	at org.apache.hive.beeline.BeeLine.mainWithInputRedirection(BeeLine.java:459)
	at org.apache.hive.beeline.BeeLine.main(BeeLine.java:442)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:160)


With this patch, JDBC client can get more details on hiveserver2. 

Caused by: org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: ParseException line 1:0 cannot recognize input near &amp;amp;apos;createa&amp;amp;apos; &amp;amp;apos;asd&amp;amp;apos; &amp;amp;apos;&amp;lt;EOF&amp;gt;&amp;amp;apos;
	at org.apache.hive.service.cli.operation.SQLOperation.prepare(Unknown Source)
	at org.apache.hive.service.cli.operation.SQLOperation.run(Unknown Source)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(Unknown Source)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(Unknown Source)
	at org.apache.hive.service.cli.CLIService.executeStatementAsync(Unknown Source)
	at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(Unknown Source)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(Unknown Source)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(Unknown Source)
	at org.apache.thrift.ProcessFunction.process(Unknown Source)
	at org.apache.thrift.TBaseProcessor.process(Unknown Source)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(Unknown Source)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)

</description>
			<version>0.12.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hive.service.cli.HiveSQLException.java</file>
			<file type="M">org.apache.hive.service.cli.operation.HiveCommandOperation.java</file>
			<file type="M">org.apache.hive.service.cli.operation.SQLOperation.java</file>
			<file type="M">org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.java</file>
			<file type="M">org.apache.hive.service.cli.operation.Operation.java</file>
			<file type="M">org.apache.hive.jdbc.Utils.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">7379</link>
		</links>
	</bug>
	<bug id="7379" opendate="2014-07-10 14:16:01" fixdate="2014-07-11 01:43:51" resolution="Duplicate">
		<buginformation>
			<summary>Beeline to fetch full stack trace for job (task) failures </summary>
			<description>When a query submitted via Beeline fails, Beeline displays a generic error message as below:
FAILED: Execution Error, return code 1 from 
This is expected, as Beeline is basically a regular JDBC client and is hence limited by JDBC&amp;amp;apos;s capabilities today. But it would be useful if Beeline can return the full remote stack trace and task diagnostics or job ID.</description>
			<version>0.12.0</version>
			<fixedVersion></fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hive.service.cli.HiveSQLException.java</file>
			<file type="M">org.apache.hive.service.cli.operation.HiveCommandOperation.java</file>
			<file type="M">org.apache.hive.service.cli.operation.SQLOperation.java</file>
			<file type="M">org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.java</file>
			<file type="M">org.apache.hive.service.cli.operation.Operation.java</file>
			<file type="M">org.apache.hive.jdbc.Utils.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">7127</link>
		</links>
	</bug>
	<bug id="7393" opendate="2014-07-11 23:02:50" fixdate="2014-07-12 05:04:07" resolution="Duplicate">
		<buginformation>
			<summary>Tez jobs sometimes fail with NPE processing input splits</summary>
			<description>Input files are either ORC or RC format.  Only occurs on occasion - if the query is repeated it is likely to complete successfully.

2014-07-11 15:31:45,367 INFO [InputInitializer [Map 3] #0] org.apache.hadoop.mapred.split.TezMapredSplitsGrouper: Grouping splits in Tez
2014-07-11 15:31:45,367 INFO [InputInitializer [Map 3] #0] org.apache.hadoop.mapred.split.TezMapredSplitsGrouper: Desired splits: 408 too large.  Desired splitLength: 614866 Min splitLength: 16777216 New desired splits: 15 Total length: 250865685 Original splits: 13
2014-07-11 15:31:45,367 INFO [InputInitializer [Map 3] #0] org.apache.hadoop.mapred.split.TezMapredSplitsGrouper: Using original number of splits: 13 desired splits: 15
2014-07-11 15:31:45,381 INFO [AsyncDispatcher event handler] org.apache.tez.dag.history.HistoryEventHandler: [HISTORY][DAG:dag_1405114778353_0004_1][Event:VERTEX_INITIALIZED]: vertexName=Reducer 4, vertexId=vertex_1405114778353_0004_1_09, initRequestedTime=1405117905313, initedTime=1405117905381, numTasks=999, processorName=org.apache.hadoop.hive.ql.exec.tez.ReduceTezProcessor, additionalInputsCount=0
2014-07-11 15:31:45,381 INFO [AsyncDispatcher event handler] org.apache.tez.dag.app.dag.impl.VertexImpl: vertex_1405114778353_0004_1_09 [Reducer 4] transitioned from NEW to INITED due to event V_INIT
2014-07-11 15:31:45,383 ERROR [AsyncDispatcher event handler] org.apache.tez.dag.app.dag.impl.VertexImpl: Vertex Input: csb initializer failed
java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:275)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:372)
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat.getSplits(TezGroupedSplitsInputFormat.java:68)
	at org.apache.tez.mapreduce.hadoop.MRHelpers.generateOldSplits(MRHelpers.java:263)
	at org.apache.tez.mapreduce.common.MRInputAMSplitGenerator.initialize(MRInputAMSplitGenerator.java:139)
	at org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable$1.run(RootInputInitializerRunner.java:154)
	at org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable$1.run(RootInputInitializerRunner.java:146)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1557)
	at org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable.call(RootInputInitializerRunner.java:146)
	at org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable.call(RootInputInitializerRunner.java:114)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

</description>
			<version>0.13.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">7011</link>
		</links>
	</bug>
	<bug id="6037" opendate="2013-12-16 07:02:13" fixdate="2014-07-13 19:00:28" resolution="Fixed">
		<buginformation>
			<summary>Synchronize HiveConf with hive-default.xml.template and support show conf</summary>
			<description>see HIVE-5879</description>
			<version>0.13.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcFile.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
			<file type="M">org.apache.hadoop.hive.conf.TestHiveLogging.java</file>
			<file type="M">org.apache.hive.jdbc.TestJdbcDriver2.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.processors.SetProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.conf.TestHiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType.java</file>
			<file type="M">org.apache.hadoop.hive.conf.TestHiveConfRestrictList.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.VariableSubstitution.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
			<file type="M">org.apache.hive.service.cli.CLIService.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
			<file type="D">org.apache.hive.common.util.SystemVariables.java</file>
			<file type="D">org.apache.hadoop.hive.ant.GenHiveTemplate.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.HiveUtils.java</file>
			<file type="D">org.apache.hadoop.hive.conf.Validator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			<file type="D">org.apache.hadoop.hive.ql.plan.ShowConfDesc.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">6626</link>
			<link type="Reference" description="relates to">7097</link>
			<link type="Reference" description="relates to">7917</link>
			<link type="Reference" description="relates to">5879</link>
			<link type="Reference" description="is related to">7227</link>
			<link type="Reference" description="is related to">6887</link>
			<link type="Reference" description="is related to">6586</link>
			<link type="Reference" description="is related to">8698</link>
			<link type="Reference" description="is related to">2196</link>
			<link type="Reference" description="is related to">7840</link>
			<link type="Reference" description="is related to">7496</link>
		</links>
	</bug>
	<bug id="7303" opendate="2014-06-27 02:25:16" fixdate="2014-07-22 04:44:04" resolution="Fixed">
		<buginformation>
			<summary>IllegalMonitorStateException when stmtHandle is null in HiveStatement</summary>
			<description>From http://www.mail-archive.com/dev@hive.apache.org/msg75617.html
Unlock can be called even it&amp;amp;apos;s not locked in some situation.</description>
			<version>0.13.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.jdbc.HiveStatement.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">9598</link>
		</links>
	</bug>
	<bug id="6049" opendate="2013-12-17 22:04:18" fixdate="2014-07-23 07:59:33" resolution="Duplicate">
		<buginformation>
			<summary>Hive uses deprecated hadoop configuration in Hadoop 2.0</summary>
			<description>Running hive CLI on hadoop 2.0, you&amp;amp;apos;ll see deprecated configurations warnings like this:
13/12/14 01:00:51 INFO Configuration.deprecation: mapred.input.dir.recursive is
 deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
 13/12/14 01:00:52 INFO Configuration.deprecation: mapred.max.split.size is depre
 cated. Instead, use mapreduce.input.fileinputformat.split.maxsize
 13/12/14 01:00:52 INFO Configuration.deprecation: mapred.min.split.size is depre
 cated. Instead, use mapreduce.input.fileinputformat.split.minsize
 13/12/14 01:00:52 INFO Configuration.deprecation: mapred.min.split.size.per.rack
 is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.r
 ack
 13/12/14 01:00:52 INFO Configuration.deprecation: mapred.min.split.size.per.node
 is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.n
 ode
 13/12/14 01:00:52 INFO Configuration.deprecation: mapred.reduce.tasks is depreca
 ted. Instead, use mapreduce.job.reduces
 13/12/14 01:00:52 INFO Configuration.deprecation: mapred.reduce.tasks.speculativ
 e.execution is deprecated. Instead, use mapreduce.reduce.speculative</description>
			<version>0.12.0</version>
			<fixedVersion>0.13.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
			<file type="M">org.apache.hadoop.hive.shims.HadoopShimsSecure.java</file>
			<file type="M">org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
			<file type="M">org.apache.hadoop.hive.shims.HadoopShims.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">6159</link>
		</links>
	</bug>
	<bug id="7475" opendate="2014-07-22 22:06:33" fixdate="2014-07-24 01:18:30" resolution="Duplicate">
		<buginformation>
			<summary>Beeline requires newline at the end of each query in a file</summary>
			<description>When using the -f option on beeline its required to have a newline at the end of each query otherwise the connection is closed before the query is run.


$ cat test.hql
show databases;%
$ beeline -u jdbc:hive2://localhost:10000 --incremental=true -f test.hql
scan complete in 3ms
Connecting to jdbc:hive2://localhost:10000
Connected to: Apache Hive (version 0.13.1)
Driver: Hive JDBC (version 0.13.1)
Transaction isolation: TRANSACTION_REPEATABLE_READ
Beeline version 0.13.1 by Apache Hive
0: jdbc:hive2://localhost:10000&amp;gt; show databases;Closing: 0: jdbc:hive2://localhost:10000

</description>
			<version>0.13.1</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.beeline.BeeLine.java</file>
			<file type="M">org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">10541</link>
		</links>
	</bug>
	<bug id="6806" opendate="2014-04-01 17:56:05" fixdate="2014-07-25 18:02:53" resolution="Fixed">
		<buginformation>
			<summary>CREATE TABLE should support STORED AS AVRO</summary>
			<description>Avro is well established and widely used within Hive, however creating Avro-backed tables requires the messy listing of the SerDe, InputFormat and OutputFormat classes.
Similarly to HIVE-5783 for Parquet, Hive would be easier to use if it had native Avro support.</description>
			<version>0.12.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>New Feature</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.serde2.avro.AvroDeserializer.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.IOConstants.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.TestStorageFormatDescriptor.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.avro.AvroSerDe.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">3159</link>
			<link type="Reference" description="relates to">5976</link>
			<link type="Reference" description="is related to">7446</link>
			<link type="Reference" description="is related to">8591</link>
			<link type="dependent" description="is depended upon by">7440</link>
		</links>
	</bug>
	<bug id="1986" opendate="2011-02-14 04:44:51" fixdate="2014-07-28 08:07:00" resolution="Duplicate">
		<buginformation>
			<summary>partition pruner do not take effect for non-deterministic UDF</summary>
			<description>hive udf can be deterministic or non-deterministic,but for non-deterministic udf such as rand and unix_timestamp,ppr do not take effect.
and for unix_timestamp with para, for example unix_timestamp(&amp;amp;apos;2010-01-01&amp;amp;apos;),I think it is deterministic.
case :
hive -hiveconf hive.root.logger=DEBUG,console
create kv_part(key int,value string) partitioned by(ds string);
alter table kv_part add partition (ds=2010) partition (ds=2011) partition (ds=2012);
create kv2(key int,value string) partitioned by(ds string);
alter table kv2 add partition (ds=2013) partition (ds=2014) partition (ds=2015);
explain select * from kv_part join kv2 on(kv_part.key=kv2.key) where kv_part.ds=2011 and rand() &amp;gt; 0.5
rand() is non-deterministic ,so kv_part.ds=2011 no not filter the partition ds=2010,ds=2012
.....
11/02/14 12:22:32 DEBUG lazy.LazySimpleSerDe: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[key, value] columnTypes=[int, string] separator=[[B@1ac9683] nullstring=\N lastColumnTakesRest=false
11/02/14 12:22:32 INFO hive.log: DDL: struct kv_part 
{ i32 key, string value}
11/02/14 12:22:32 DEBUG optimizer.GenMapRedUtils: Information added for path hdfs://172.25.38.253:54310/user/hive/warehouse/kv_part/ds=2010
11/02/14 12:22:32 DEBUG optimizer.GenMapRedUtils: Information added for path hdfs://172.25.38.253:54310/user/hive/warehouse/kv_part/ds=2011
11/02/14 12:22:32 DEBUG optimizer.GenMapRedUtils: Information added for path hdfs://172.25.38.253:54310/user/hive/warehouse/kv_part/ds=2012
11/02/14 12:22:32 INFO parse.SemanticAnalyzer: Completed plan generation
.....
explain select * from kv_part join kv2 on(kv_part.key=kv2.key) where kv_part.ds=2011 and sin(kv2.key) &amp;lt; 0.5;
sin() is deterministic,so ppr work ok
.....
11/02/14 12:25:22 DEBUG optimizer.GenMapRedUtils: Information added for path hdfs://172.25.38.253:54310/user/hive/warehouse/kv_part/ds=2011
....
And user should get the deterministic info for UDF from wiki,or we shoud add this info to describe function</description>
			<version>0.4.1</version>
			<fixedVersion>0.11.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">3853</link>
			<link type="Reference" description="relates to">746</link>
			<link type="Reference" description="is related to">2124</link>
		</links>
	</bug>
	<bug id="7434" opendate="2014-07-17 06:47:54" fixdate="2014-07-29 19:08:59" resolution="Duplicate">
		<buginformation>
			<summary>beeline should not always enclose the output by default in CSV/TSV mode</summary>
			<description>When using beeline in CSV/TSV mode (via command !outputformat csv) , the output is always enclosed in single quotes. This is however not the case for Hive CLI, so we need to make this enclose optional.</description>
			<version>0.13.1</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.beeline.BeeLine.java</file>
			<file type="M">org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
			<file type="M">org.apache.hive.beeline.SeparatedValuesOutputFormat.java</file>
			<file type="M">org.apache.hive.beeline.BeeLineOpts.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">7390</link>
		</links>
	</bug>
	<bug id="7583" opendate="2014-08-01 00:17:48" fixdate="2014-08-06 22:16:38" resolution="Fixed">
		<buginformation>
			<summary>Use FileSystem.access() if available to check file access for user</summary>
			<description>Hive currently implements its own file access checks to determine if a user is allowed to perform an specified action on a file path (in StorageBasedAuthorizationProvider, also FileUtils). This can be prone to errors or inconsistencies with how file access is actually checked in Hadoop.
HDFS-6570 adds a new FileSystem.access() API, so that we can perform the check using the actual HDFS logic rather than having to imitate that behavior in Hive. For versions of Hadoop that have this API available, we should use this API.</description>
			<version>0.13.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.security.TestMetastoreAuthorizationProvider.java</file>
			<file type="M">org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
			<file type="M">org.apache.hadoop.hive.common.FileUtils.java</file>
			<file type="M">org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils.java</file>
			<file type="M">org.apache.hadoop.hive.shims.HadoopShimsSecure.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProvider.java</file>
			<file type="M">org.apache.hadoop.hive.shims.HadoopShims.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">7714</link>
			<link type="Reference" description="is related to">6570</link>
		</links>
	</bug>
	<bug id="7539" opendate="2014-07-29 02:03:34" fixdate="2014-08-10 04:31:10" resolution="Fixed">
		<buginformation>
			<summary>streaming windowing UDAF seems to be broken without Partition Spec</summary>
			<description>

select  avg(c_int) over(rows between 1 PRECEDING and current row) from t1


results in 

:1}}
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:271)
	... 9 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage$GenericUDAFAverageEvaluatorDouble$1.getNextResult(GenericUDAFAverage.java:180)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage$GenericUDAFAverageEvaluatorDouble$1.getNextResult(GenericUDAFAverage.java:166)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStreamingEvaluator$SumAvgEnhancer.iterate(GenericUDAFStreamingEvaluator.java:166)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.aggregate(GenericUDAFEvaluator.java:185)
	at org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.processRow(WindowingTableFunction.java:348)
	at org.apache.hadoop.hive.ql.exec.PTFOperator$PTFInvocation.processRow(PTFOperator.java:318)
	at org.apache.hadoop.hive.ql.exec.PTFOperator.processOp(PTFOperator.java:131)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:800)
	at org.apache.hadoop.hive.ql.exec.ExtractOperator.processOp(ExtractOperator.java:45)
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:262)
	... 9 more

</description>
			<version>0.14.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">7306</link>
		</links>
	</bug>
	<bug id="4064" opendate="2013-02-22 18:41:18" fixdate="2014-08-11 17:23:09" resolution="Fixed">
		<buginformation>
			<summary>Handle db qualified names consistently across all HiveQL statements</summary>
			<description>Hive doesn&amp;amp;apos;t consistently handle db qualified names across all HiveQL statements. While some HiveQL statements such as SELECT support DB qualified names, other such as CREATE INDEX doesn&amp;amp;apos;t. </description>
			<version>0.10.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.AuthorizationUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.AlterTableAlterPartDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.ShowColumnsDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.authorization.TestHiveAuthorizationTaskFactory.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.ShowGrantDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.authorization.TestPrivilegesV2.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.RenamePartitionDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.TestHive.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.TestQBCompact.java</file>
			<file type="M">org.apache.hadoop.hive.ql.hooks.CheckColumnAccessHook.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.Warehouse.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.index.RewriteGBUsingIndex.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.PrivilegeObjectDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.AlterIndexDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.IndexUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.authorization.TestPrivilegesV1.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.HiveV1Authorizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.authorization.PrivilegesTestBase.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.ColumnAccessInfo.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.IndexUpdater.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
		<links>
			<link type="Container" description="contains">5912</link>
			<link type="Container" description="contains">3589</link>
			<link type="Container" description="contains">7238</link>
			<link type="Container" description="contains">2584</link>
			<link type="Container" description="contains">4086</link>
			<link type="Container" description="contains">5956</link>
			<link type="Duplicate" description="is duplicated by">3589</link>
			<link type="Duplicate" description="is duplicated by">7238</link>
			<link type="Duplicate" description="is duplicated by">8712</link>
			<link type="Duplicate" description="is duplicated by">8538</link>
			<link type="Duplicate" description="is duplicated by">5956</link>
			<link type="Duplicate" description="is duplicated by">6933</link>
			<link type="Reference" description="relates to">1977</link>
			<link type="Reference" description="relates to">7681</link>
			<link type="Reference" description="is related to">7678</link>
		</links>
	</bug>
	<bug id="7390" opendate="2014-07-11 12:42:21" fixdate="2014-08-12 18:08:52" resolution="Fixed">
		<buginformation>
			<summary>Make single quote character optional and configurable in BeeLine CSV/TSV output</summary>
			<description>Currently when either the CSV or TSV output formats are used in beeline each column is wrapped in single quotes. Quote wrapping of columns should be optional and the user should be able to choose the character used to wrap the columns.</description>
			<version>0.13.1</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>New Feature</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.beeline.BeeLine.java</file>
			<file type="M">org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
			<file type="M">org.apache.hive.beeline.SeparatedValuesOutputFormat.java</file>
			<file type="M">org.apache.hive.beeline.BeeLineOpts.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">7434</link>
			<link type="Reference" description="is related to">9788</link>
			<link type="Regression" description="breaks">8544</link>
			<link type="Regression" description="breaks">8615</link>
		</links>
	</bug>
	<bug id="2584" opendate="2011-11-16 18:19:03" fixdate="2014-08-13 07:56:49" resolution="Duplicate">
		<buginformation>
			<summary>Alter table should accept database name</summary>
			<description>It would be nice if alter table accepts database name.
For example:
This would be more useful in certain usecases:
alter table DB.Tbl set location &amp;lt;location&amp;gt;;
rather than 2 statements.
use DB;
alter table Tbl set location &amp;lt;location&amp;gt;;
</description>
			<version>0.7.1</version>
			<fixedVersion></fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.hooks.UpdateInputAccessTimeHook.java</file>
			<file type="M">org.apache.hive.hcatalog.cli.TestSemanticAnalysis.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.AlterTableDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.IndexUpdater.java</file>
			<file type="M">org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler.java</file>
			<file type="M">org.apache.hadoop.hive.ql.index.IndexMetadataChangeTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.index.bitmap.BitmapIndexHandler.java</file>
			<file type="M">org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
			<file type="M">org.apache.hive.hcatalog.cli.SemanticAnalysis.CreateTableHook.java</file>
		</fixedFiles>
		<links>
			<link type="Container" description="Is contained by">4064</link>
			<link type="Duplicate" description="is duplicated by">7681</link>
		</links>
	</bug>
	<bug id="7714" opendate="2014-08-13 15:30:45" fixdate="2014-08-13 17:24:50" resolution="Duplicate">
		<buginformation>
			<summary>Implement ACL support for Storage-System Based Authorization</summary>
			<description>In the Hive wiki it is stated that:
Note: Support for HDFS ACL (introduced in Apache Hadoop 2.4) is yet to be added to this model. Which means, that it checks only the traditional rwx style permissions to determine if a user can write to the file system.
(https://cwiki.apache.org/confluence/display/Hive/HCatalog+Authorization#HCatalogAuthorization-Storage-SystemBasedAuthorizationModel)
but there is no JIRA associated (not in the wiki and I didn&amp;amp;apos;t find any on the JIRA too).</description>
			<version>0.13.0</version>
			<fixedVersion></fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.security.TestMetastoreAuthorizationProvider.java</file>
			<file type="M">org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
			<file type="M">org.apache.hadoop.hive.common.FileUtils.java</file>
			<file type="M">org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils.java</file>
			<file type="M">org.apache.hadoop.hive.shims.HadoopShimsSecure.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProvider.java</file>
			<file type="M">org.apache.hadoop.hive.shims.HadoopShims.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">7583</link>
		</links>
	</bug>
	<bug id="7620" opendate="2014-08-05 20:10:22" fixdate="2014-08-15 18:15:58" resolution="Fixed">
		<buginformation>
			<summary>Hive metastore fails to start in secure mode due to "java.lang.NoSuchFieldError: SASL_PROPS" error</summary>
			<description>When Hive metastore is started in a Hadoop 2.5 cluster, it fails to start with following error


14/07/31 17:45:58 [main]: ERROR metastore.HiveMetaStore: Metastore Thrift Server threw an exception...
java.lang.NoSuchFieldError: SASL_PROPS
	at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.getHadoopSaslProperties(HadoopThriftAuthBridge20S.java:126)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.getMetaStoreSaslProperties(MetaStoreUtils.java:1483)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.startMetaStore(HiveMetaStore.java:5225)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.main(HiveMetaStore.java:5152)


Changes in HADOOP-10451 to remove SaslRpcServer.SASL_PROPS are causing this error.</description>
			<version>0.13.1</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.shims.ShimLoader.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">6987</link>
			<link type="Duplicate" description="is duplicated by">8154</link>
			<link type="Reference" description="relates to">6741</link>
			<link type="Reference" description="is related to">10451</link>
		</links>
	</bug>
	<bug id="7341" opendate="2014-07-02 22:52:09" fixdate="2014-08-18 11:02:21" resolution="Fixed">
		<buginformation>
			<summary>Support for Table replication across HCatalog instances</summary>
			<description>The HCatClient currently doesn&amp;amp;apos;t provide very much support for replicating HCatTable definitions between 2 HCatalog Server (i.e. Hive metastore) instances. 
Systems similar to Apache Falcon might find the need to replicate partition data between 2 clusters, and keep the HCatalog metadata in sync between the two. This poses a couple of problems:

The definition of the source table might change (in column schema, I/O formats, record-formats, serde-parameters, etc.) The system will need a way to diff 2 tables and update the target-metastore with the changes. E.g.


targetTable.resolve( sourceTable, targetTable.diff(sourceTable) );
hcatClient.updateTableSchema(dbName, tableName, targetTable);


The current HCatClient.addPartitions() API requires that the partition&amp;amp;apos;s schema be derived from the table&amp;amp;apos;s schema, thereby requiring that the table-schema be resolved before partitions with the new schema are added to the table. This is problematic, because it introduces race conditions when 2 partitions with differing column-schemas (e.g. right after a schema change) are copied in parallel. This can be avoided if each HCatAddPartitionDesc kept track of the partition&amp;amp;apos;s schema, in flight.
The source and target metastores might be running different/incompatible versions of Hive.

The impending patch attempts to address these concerns (with some caveats).

HCatTable now has
	
a diff() method, to compare against another HCatTable instance
a resolve(diff) method to copy over specified table-attributes from another HCatTable
a serialize/deserialize mechanism (via HCatClient.serializeTable() and HCatClient.deserializeTable()), so that HCatTable instances constructed in other class-loaders may be used for comparison


HCatPartition now provides finer-grained control over a Partition&amp;amp;apos;s column-schema, StorageDescriptor settings, etc. This allows partitions to be copied completely from source, with the ability to override specific properties if required (e.g. location).
HCatClient.updateTableSchema() can now update the entire table-definition, not just the column schema.
I&amp;amp;apos;ve cleaned up and removed most of the redundancy between the HCatTable, HCatCreateTableDesc and HCatCreateTableDesc.Builder. The prior API failed to separate the table-attributes from the add-table-operation&amp;amp;apos;s attributes. By providing fluent-interfaces in HCatTable, and composing an HCatTable instance in HCatCreateTableDesc, the interfaces are cleaner(ish). The old setters are deprecated, in favour of those in HCatTable. Likewise, HCatPartition and HCatAddPartitionDesc.

I&amp;amp;apos;ll post a patch for trunk shortly.</description>
			<version>0.13.1</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>New Feature</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.hcatalog.api.HCatPartition.java</file>
			<file type="M">org.apache.hive.hcatalog.api.TestHCatClient.java</file>
			<file type="M">org.apache.hive.hcatalog.api.HCatClientHMSImpl.java</file>
			<file type="M">org.apache.hive.hcatalog.api.HCatAddPartitionDesc.java</file>
			<file type="M">org.apache.hive.hcatalog.api.HCatCreateTableDesc.java</file>
			<file type="M">org.apache.hive.hcatalog.api.HCatTable.java</file>
			<file type="M">org.apache.hive.hcatalog.api.HCatClient.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">4274</link>
			<link type="Reference" description="is related to">7770</link>
			<link type="Reference" description="is related to">6378</link>
			<link type="dependent" description="is depended upon by">7576</link>
		</links>
	</bug>
	<bug id="6987" opendate="2014-04-29 19:46:41" fixdate="2014-08-22 08:59:37" resolution="Duplicate">
		<buginformation>
			<summary>Metastore qop settings won&amp;apos;t work with Hadoop-2.4</summary>
			<description> HADOOP-10211 made a backward incompatible change due to which the following hive call returns a null map:


Map&amp;lt;String, String&amp;gt; hadoopSaslProps =  ShimLoader.getHadoopThriftAuthBridge().
        getHadoopSaslProperties(conf); 


Metastore uses the underlying hadoop.rpc.protection values to set the qop between metastore client/server. </description>
			<version>0.14.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.shims.ShimLoader.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">7620</link>
			<link type="Reference" description="is related to">6741</link>
			<link type="Regression" description="is broken by">10211</link>
		</links>
	</bug>
	<bug id="7681" opendate="2014-08-11 20:03:44" fixdate="2014-08-25 21:56:49" resolution="Fixed">
		<buginformation>
			<summary>qualified tablenames usage does not work with several alter-table commands</summary>
			<description>Changes were made in HIVE-4064 for use of qualified table names in more types of queries. But several alter table commands don&amp;amp;apos;t work with qualified 

alter table default.tmpfoo set tblproperties ("bar" = "bar value")
ALTER TABLE default.kv_rename_test CHANGE a a STRING
add,drop partition
alter index rebuild

</description>
			<version>0.7.1</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.hooks.UpdateInputAccessTimeHook.java</file>
			<file type="M">org.apache.hive.hcatalog.cli.TestSemanticAnalysis.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.AlterTableDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.IndexUpdater.java</file>
			<file type="M">org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler.java</file>
			<file type="M">org.apache.hadoop.hive.ql.index.IndexMetadataChangeTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.index.bitmap.BitmapIndexHandler.java</file>
			<file type="M">org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
			<file type="M">org.apache.hive.hcatalog.cli.SemanticAnalysis.CreateTableHook.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">2584</link>
			<link type="Duplicate" description="is duplicated by">9180</link>
			<link type="Reference" description="relates to">7678</link>
			<link type="Reference" description="is related to">4064</link>
		</links>
	</bug>
	<bug id="7764" opendate="2014-08-18 10:00:47" fixdate="2014-08-26 18:46:34" resolution="Fixed">
		<buginformation>
			<summary>Support all JDBC-HiveServer2 authentication modes on a secure cluster</summary>
			<description>Currently, HiveServer2 logs in with its keytab only if hive.server2.authentication is set to KERBEROS. However, hive.server2.authentication is config that determines the auth type an end user will use while authenticating with HiveServer2. There is a valid use case of user authenticating with HiveServer2 using LDAP for example, while HiveServer2 runs the query on a kerberized cluster.</description>
			<version>0.14.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.service.cli.CLIService.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">5447</link>
		</links>
	</bug>
	<bug id="5447" opendate="2013-10-04 18:48:11" fixdate="2014-08-29 21:46:19" resolution="Duplicate">
		<buginformation>
			<summary>HiveServer2 should allow secure impersonation over LDAP or other non-kerberos connection</summary>
			<description>Currently the impersonation on a secure hadoop cluster only works when HS2 connection itself is kerberos. This forces clients to configure kerberos which can be a deployment nightmare.
We should allow other authentications mechanism to perform secure impersonation.</description>
			<version>0.11.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.service.cli.CLIService.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">6312</link>
			<link type="Duplicate" description="duplicates">7764</link>
			<link type="Duplicate" description="is duplicated by">6973</link>
		</links>
	</bug>
	<bug id="7399" opendate="2014-07-14 02:30:18" fixdate="2014-09-01 01:53:07" resolution="Fixed">
		<buginformation>
			<summary>Timestamp type is not copied by ObjectInspectorUtils.copyToStandardObject</summary>
			<description>Most of primitive types are non-mutable, so copyToStandardObject retuns input object as-is. But for Timestamp objects, it&amp;amp;apos;s used something like wrapper and inside value changed by hive. copyToStandardObject should return real copy of them.</description>
			<version>0.13.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaTimestampObjectInspector.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaBinaryObjectInspector.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">8297</link>
		</links>
	</bug>
	<bug id="5956" opendate="2013-12-05 01:22:00" fixdate="2014-09-02 04:49:12" resolution="Duplicate">
		<buginformation>
			<summary>SHOW TBLPROPERTIES doesn&amp;apos;t take db name</summary>
			<description>Identified in HIVE-5912.</description>
			<version>0.12.0</version>
			<fixedVersion></fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.AuthorizationUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.AlterTableAlterPartDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.ShowColumnsDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.authorization.TestHiveAuthorizationTaskFactory.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.ShowGrantDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.authorization.TestPrivilegesV2.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.RenamePartitionDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.TestHive.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.TestQBCompact.java</file>
			<file type="M">org.apache.hadoop.hive.ql.hooks.CheckColumnAccessHook.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.Warehouse.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.index.RewriteGBUsingIndex.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.PrivilegeObjectDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.AlterIndexDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.IndexUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.authorization.TestPrivilegesV1.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.HiveV1Authorizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.authorization.PrivilegesTestBase.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.ColumnAccessInfo.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.IndexUpdater.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
		<links>
			<link type="Container" description="Is contained by">4064</link>
			<link type="Duplicate" description="duplicates">4064</link>
		</links>
	</bug>
	<bug id="3589" opendate="2012-10-17 03:18:07" fixdate="2014-09-02 04:49:31" resolution="Duplicate">
		<buginformation>
			<summary>describe/show partition/show tblproperties command should accept database name</summary>
			<description>describe command not giving the details when called as describe dbname.tablename.
Throwing the error "Table dbname not found".
Ex: hive -e "describe masterdb.table1" will throw error
"Table masterdb not found"</description>
			<version>0.8.1</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.AuthorizationUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.AlterTableAlterPartDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.ShowColumnsDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.authorization.TestHiveAuthorizationTaskFactory.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.ShowGrantDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.authorization.TestPrivilegesV2.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.RenamePartitionDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.TestHive.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.TestQBCompact.java</file>
			<file type="M">org.apache.hadoop.hive.ql.hooks.CheckColumnAccessHook.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.Warehouse.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.index.RewriteGBUsingIndex.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.PrivilegeObjectDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.AlterIndexDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.IndexUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.authorization.TestPrivilegesV1.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.HiveV1Authorizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.authorization.PrivilegesTestBase.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.ColumnAccessInfo.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.IndexUpdater.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
		<links>
			<link type="Container" description="Is contained by">4064</link>
			<link type="Duplicate" description="duplicates">4064</link>
			<link type="Reference" description="relates to">1977</link>
			<link type="Reference" description="relates to">7678</link>
		</links>
	</bug>
	<bug id="7238" opendate="2014-06-16 18:23:23" fixdate="2014-09-02 04:49:42" resolution="Duplicate">
		<buginformation>
			<summary>show partitions when using schema does not work</summary>
			<description>SHOW PARTITIONS command when used with the schema option throws an error "ParseException line 1:24 missing EOF". User would need to every time go into the schema to see the partitions. 
Details of the issue/hive commands:
----------------------------------------------
hive (kh_work)&amp;gt; show partitions closeout.pos_retaileritem;
FAILED: ParseException line 1:24 missing EOF at &amp;amp;apos;.&amp;amp;apos; near &amp;amp;apos;closeout&amp;amp;apos;
hive (kh_work)&amp;gt; use closeout;
OK
Time taken: 0.0060 seconds
hive (closeout)&amp;gt; show partitions pos_retaileritem;
OK
partition
pk_business=tec/pk_data_source=pos/pk_frequency=cat/pk_data_state=c07132
Time taken: 0.183 seconds, Fetched: 1 row(s)
hive (closeout)&amp;gt;</description>
			<version>0.12.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.AuthorizationUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.AlterTableAlterPartDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.ShowColumnsDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.authorization.TestHiveAuthorizationTaskFactory.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.ShowGrantDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.authorization.TestPrivilegesV2.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.RenamePartitionDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.TestHive.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.TestQBCompact.java</file>
			<file type="M">org.apache.hadoop.hive.ql.hooks.CheckColumnAccessHook.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.Warehouse.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.index.RewriteGBUsingIndex.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.PrivilegeObjectDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.AlterIndexDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.IndexUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.authorization.TestPrivilegesV1.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.HiveV1Authorizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.authorization.PrivilegesTestBase.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.ColumnAccessInfo.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.IndexUpdater.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
		<links>
			<link type="Container" description="Is contained by">4064</link>
			<link type="Duplicate" description="duplicates">4064</link>
		</links>
	</bug>
	<bug id="7306" opendate="2014-06-27 18:03:14" fixdate="2014-09-02 06:21:21" resolution="Duplicate">
		<buginformation>
			<summary>Ineffective null check in GenericUDAFAverage#GenericUDAFAverageEvaluatorDouble#getNextResult()</summary>
			<description>

            Object[] o = ss.intermediateVals.remove(0);
            Double d = o == null ? 0.0 : (Double) o[0];
            r = r == null ? null : r - d;
            cnt = cnt - ((Long) o[1]);


Array o is accessed without null check in the last line above.</description>
			<version>0.13.1</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">7539</link>
		</links>
	</bug>
	<bug id="6933" opendate="2014-04-18 17:36:05" fixdate="2014-09-02 07:04:23" resolution="Duplicate">
		<buginformation>
			<summary>"alter table add partition" should support db_name.table_name as table name.</summary>
			<description>Currently, 
"alter table table_name add partition ...." works.
but
"alter table db_name.table_name add partition ..." throws an error message (different error for different versions).
For consistency, I suggest with support both ways of referring to a table.</description>
			<version>0.12.0</version>
			<fixedVersion></fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.AuthorizationUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.AlterTableAlterPartDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.ShowColumnsDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.authorization.TestHiveAuthorizationTaskFactory.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.ShowGrantDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.authorization.TestPrivilegesV2.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.RenamePartitionDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.TestHive.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.TestQBCompact.java</file>
			<file type="M">org.apache.hadoop.hive.ql.hooks.CheckColumnAccessHook.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.Warehouse.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.index.RewriteGBUsingIndex.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.PrivilegeObjectDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.AlterIndexDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.IndexUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.authorization.TestPrivilegesV1.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.HiveV1Authorizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.authorization.PrivilegesTestBase.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.ColumnAccessInfo.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.IndexUpdater.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">4064</link>
		</links>
	</bug>
	<bug id="7937" opendate="2014-09-02 15:32:34" fixdate="2014-09-02 16:15:22" resolution="Duplicate">
		<buginformation>
			<summary>Compilation error on hadoop-1 profile</summary>
			<description>Currently while building with the hadoop-1 profile, it seems like we have a compilation error.

[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project hive-common: Compilation failure
[ERROR] /Users/sk018283/git-repo/hive/common/src/java/org/apache/hadoop/hive/common/FileUtils.java:[664,35] cannot find symbol
[ERROR] symbol  : method getStickyBit()
[ERROR] location: class org.apache.hadoop.fs.permission.FsPermission
[ERROR] -&amp;gt; [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.


Build works fine with hadoop-2.</description>
			<version>0.14.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.shims.HadoopShims.java</file>
			<file type="M">org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
			<file type="M">org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
			<file type="M">org.apache.hadoop.hive.common.FileUtils.java</file>
			<file type="M">org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">7927</link>
		</links>
	</bug>
	<bug id="7944" opendate="2014-09-02 22:30:42" fixdate="2014-09-03 20:52:47" resolution="Fixed">
		<buginformation>
			<summary>current update stats for columns of a partition of a table is not correct</summary>
			<description>We worked hard towards faster update stats for columns of a partition of a table previously 
https://issues.apache.org/jira/browse/HIVE-7736
and
https://issues.apache.org/jira/browse/HIVE-7876
Although there is some improvement, it is only correct in the first run. There will be duplicate column stats later. Thanks to Eugene Koifman &amp;amp;apos;s comments</description>
			<version>0.14.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.RawStore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">7811</link>
			<link type="Duplicate" description="is duplicated by">7982</link>
		</links>
	</bug>
	<bug id="4274" opendate="2013-02-13 21:01:06" fixdate="2014-09-04 18:23:16" resolution="Duplicate">
		<buginformation>
			<summary>Table created using HCatalog java client doesn&amp;apos;t set the owner</summary>
			<description>HCatalog client doesn&amp;amp;apos;t seem to set the owner field. The owner field of the table remains null instead of being populated with the user who created the table. Creating table with hive cli seems to work fine.</description>
			<version>0.10.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.hcatalog.api.HCatPartition.java</file>
			<file type="M">org.apache.hive.hcatalog.api.TestHCatClient.java</file>
			<file type="M">org.apache.hive.hcatalog.api.HCatClientHMSImpl.java</file>
			<file type="M">org.apache.hive.hcatalog.api.HCatAddPartitionDesc.java</file>
			<file type="M">org.apache.hive.hcatalog.api.HCatCreateTableDesc.java</file>
			<file type="M">org.apache.hive.hcatalog.api.HCatTable.java</file>
			<file type="M">org.apache.hive.hcatalog.api.HCatClient.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">7341</link>
		</links>
	</bug>
	<bug id="6847" opendate="2014-04-04 23:56:34" fixdate="2014-09-04 20:27:14" resolution="Fixed">
		<buginformation>
			<summary>Improve / fix bugs in Hive scratch dir setup</summary>
			<description>Currently, the hive server creates scratch directory and changes permission to 777 however, this is not great with respect to security. We need to create user specific scratch directories instead. Also refer to HIVE-6782 1st iteration of the patch for approach.</description>
			<version>0.14.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="D">org.apache.hadoop.hive.ql.TestUtilitiesDfs.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TestUtilities.java</file>
			<file type="M">org.apache.hive.service.cli.session.HiveSessionBase.java</file>
			<file type="M">org.apache.hadoop.hive.common.ServerUtils.java</file>
			<file type="M">org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			<file type="D">org.apache.hive.service.cli.TestScratchDir.java</file>
			<file type="M">org.apache.hadoop.hive.ql.session.SessionState.java</file>
			<file type="M">org.apache.hive.service.cli.CLIService.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
			<file type="M">org.apache.hive.service.cli.session.SessionManager.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Context.java</file>
			<file type="M">org.apache.hive.jdbc.TestJdbcWithMiniMr.java</file>
			<file type="M">org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
			<file type="M">org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">8606</link>
			<link type="Reference" description="relates to">4487</link>
			<link type="Reference" description="relates to">6602</link>
			<link type="Reference" description="relates to">8143</link>
			<link type="Reference" description="is related to">8643</link>
		</links>
	</bug>
	<bug id="7982" opendate="2014-09-04 14:01:22" fixdate="2014-09-04 22:05:41" resolution="Duplicate">
		<buginformation>
			<summary>Regression in explain with CBO enabled due to issuing query per K,V for the stats</summary>
			<description>
Now explain for Q17 is back in the 12 second range, I checked the queries issues to MySQL and they are very different than before 
on August 15 explain was completing in under 5 seconds and we issued the following queries : 


select "COLUMN_NAME", "COLUMN_TYPE", min("LONG_LOW_VALUE"), max("LONG_HIGH_VALUE"), min("DOUBLE_LOW_VALUE"), max("DOUBLE_HIGH_VALUE"), min("BIG_DECIMAL_LOW_VALUE"), max("BIG_DECIMAL_HIGH_VALUE"), sum("NUM_NULLS"), max("NUM_DISTINCTS"), max("AVG_COL_LEN"), max("MAX_COL_LEN"), sum("NUM_TRUES"), sum("NUM_FALSES") from "PART_COL_STATS" where "DB_NAME" = &amp;amp;apos;tpcds_bin_partitioned_orc_30000&amp;amp;apos; and "TABLE_NAME" = &amp;amp;apos;store_returns&amp;amp;apos; and "COLUMN_NAME" in (&amp;amp;apos;sr_item_sk&amp;amp;apos;,&amp;amp;apos;sr_customer_sk&amp;amp;apos;,&amp;amp;apos;sr_ticket_number&amp;amp;apos;) AND "PARTITION_NAME" in (&amp;amp;apos;sr_returned_date=1998-01-06&amp;amp;apos;,&amp;amp;apos;sr_returned_date=1998-01-07&amp;amp;apos;,..&amp;amp;apos;sr_returned_date=2003-07-01&amp;amp;apos;) group by "COLUMN_NAME", "COLUMN_TYPE";

select "COLUMN_NAME", "COLUMN_TYPE", min("LONG_LOW_VALUE"), max("LONG_HIGH_VALUE"), min("DOUBLE_LOW_VALUE"), max("DOUBLE_HIGH_VALUE"), min("BIG_DECIMAL_LOW_VALUE"), max("BIG_DECIMAL_HIGH_VALUE"), sum("NUM_NULLS"), max("NUM_DISTINCTS"), max("AVG_COL_LEN"), max("MAX_COL_LEN"), sum("NUM_TRUES"), sum("NUM_FALSES") from "PART_COL_STATS" where "DB_NAME" = &amp;amp;apos;tpcds_bin_partitioned_orc_30000&amp;amp;apos; and "TABLE_NAME" = &amp;amp;apos;store_returns&amp;amp;apos; and "COLUMN_NAME" in (&amp;amp;apos;sr_returned_date_sk&amp;amp;apos;) AND "PARTITION_NAME" in (&amp;amp;apos;sr_returned_date=1998-01-06&amp;amp;apos;..&amp;amp;apos;sr_returned_date=2003-07-01&amp;amp;apos;) group by "COLUMN_NAME", "COLUMN_TYPE"


Currently explain Q17 takes 11 seconds and the queries sent to MySQL are very inefficient because 
1) They no longer do the aggregation on MySQL and get a row per partition 
2) There is a query per stats K,V pair so the number of queries is up by 9x


		select COLUMN_NAME, COLUMN_TYPE, count(PARTITION_NAME)  from PART_COL_STATS where DB_NAME = &amp;amp;apos;tpcds_bin_partitioned_orc_200&amp;amp;apos; and TABLE_NAME = &amp;amp;apos;store_returns&amp;amp;apos;  and COLUMN_NAME in (&amp;amp;apos;sr_item_sk&amp;amp;apos;,&amp;amp;apos;sr_customer_sk&amp;amp;apos;,&amp;amp;apos;sr_ticket_number&amp;amp;apos;) and PARTITION_NAME in (&amp;amp;apos;sr_returned_date=1998-01-06&amp;amp;apos;,&amp;amp;apos;sr_returned_date=2003-07-01&amp;amp;apos;) group by COLUMN_NAME, COLUMN_TYPE
		select COLUMN_NAME, sum(NUM_NULLS), sum(NUM_TRUES), sum(NUM_FALSES) from PART_COL_STATS where DB_NAME = &amp;amp;apos;tpcds_bin_partitioned_orc_200&amp;amp;apos; and TABLE_NAME = &amp;amp;apos;store_returns&amp;amp;apos;  and COLUMN_NAME in (&amp;amp;apos;sr_customer_sk&amp;amp;apos;,&amp;amp;apos;sr_item_sk&amp;amp;apos;,&amp;amp;apos;sr_ticket_number&amp;amp;apos;) and PARTITION_NAME in (&amp;amp;apos;sr_returned_date=1998-01-06&amp;amp;apos;,&amp;amp;apos;sr_returned_date=2003-07-01&amp;amp;apos;) group by COLUMN_NAME
		select LONG_LOW_VALUE,PARTITION_NAME from PART_COL_STATS where DB_NAME = &amp;amp;apos;tpcds_bin_partitioned_orc_200&amp;amp;apos; and TABLE_NAME = &amp;amp;apos;store_returns&amp;amp;apos; and COLUMN_NAME in (&amp;amp;apos;sr_customer_sk&amp;amp;apos;) and PARTITION_NAME in (&amp;amp;apos;sr_returned_date=1998-01-06&amp;amp;apos;,&amp;amp;apos;sr_returned_date=2003-07-01&amp;amp;apos;) order by &amp;amp;apos;LONG_LOW_VALUE&amp;amp;apos;
		select LONG_HIGH_VALUE,PARTITION_NAME from PART_COL_STATS where DB_NAME = &amp;amp;apos;tpcds_bin_partitioned_orc_200&amp;amp;apos; and TABLE_NAME = &amp;amp;apos;store_returns&amp;amp;apos; and COLUMN_NAME in (&amp;amp;apos;sr_customer_sk&amp;amp;apos;) and PARTITION_NAME in (&amp;amp;apos;sr_returned_date=1998-01-06&amp;amp;apos;,&amp;amp;apos;sr_returned_date=2003-07-01&amp;amp;apos;) order by &amp;amp;apos;LONG_HIGH_VALUE&amp;amp;apos;
		select DOUBLE_LOW_VALUE,PARTITION_NAME from PART_COL_STATS where DB_NAME = &amp;amp;apos;tpcds_bin_partitioned_orc_200&amp;amp;apos; and TABLE_NAME = &amp;amp;apos;store_returns&amp;amp;apos; and COLUMN_NAME in (&amp;amp;apos;sr_customer_sk&amp;amp;apos;) and PARTITION_NAME in (&amp;amp;apos;sr_returned_date=1998-01-06&amp;amp;apos;,&amp;amp;apos;sr_returned_date=2003-07-01&amp;amp;apos;) order by &amp;amp;apos;DOUBLE_LOW_VALUE&amp;amp;apos;
		select DOUBLE_HIGH_VALUE,PARTITION_NAME from PART_COL_STATS where DB_NAME = &amp;amp;apos;tpcds_bin_partitioned_orc_200&amp;amp;apos; and TABLE_NAME = &amp;amp;apos;store_returns&amp;amp;apos; and COLUMN_NAME in (&amp;amp;apos;sr_customer_sk&amp;amp;apos;) and PARTITION_NAME in (&amp;amp;apos;sr_returned_date=1998-01-06&amp;amp;apos;,&amp;amp;apos;sr_returned_date=2003-07-01&amp;amp;apos;) order by &amp;amp;apos;DOUBLE_HIGH_VALUE&amp;amp;apos;
		select BIG_DECIMAL_LOW_VALUE,PARTITION_NAME from PART_COL_STATS where DB_NAME = &amp;amp;apos;tpcds_bin_partitioned_orc_200&amp;amp;apos; and TABLE_NAME = &amp;amp;apos;store_returns&amp;amp;apos; and COLUMN_NAME in (&amp;amp;apos;sr_customer_sk&amp;amp;apos;) and PARTITION_NAME in (&amp;amp;apos;sr_returned_date=1998-01-06&amp;amp;apos;,&amp;amp;apos;sr_returned_date=2003-07-01&amp;amp;apos;) order by &amp;amp;apos;BIG_DECIMAL_LOW_VALUE&amp;amp;apos;
		select BIG_DECIMAL_HIGH_VALUE,PARTITION_NAME from PART_COL_STATS where DB_NAME = &amp;amp;apos;tpcds_bin_partitioned_orc_200&amp;amp;apos; and TABLE_NAME = &amp;amp;apos;store_returns&amp;amp;apos; and COLUMN_NAME in (&amp;amp;apos;sr_customer_sk&amp;amp;apos;) and PARTITION_NAME in (&amp;amp;apos;sr_returned_date=1998-01-06&amp;amp;apos;,&amp;amp;apos;sr_returned_date=2003-07-01&amp;amp;apos;) order by &amp;amp;apos;BIG_DECIMAL_HIGH_VALUE&amp;amp;apos;
		select NUM_DISTINCTS,PARTITION_NAME from PART_COL_STATS where DB_NAME = &amp;amp;apos;tpcds_bin_partitioned_orc_200&amp;amp;apos; and TABLE_NAME = &amp;amp;apos;store_returns&amp;amp;apos; and COLUMN_NAME in (&amp;amp;apos;sr_customer_sk&amp;amp;apos;) and PARTITION_NAME in (&amp;amp;apos;sr_returned_date=1998-01-06&amp;amp;apos;,&amp;amp;apos;sr_returned_date=2003-07-01&amp;amp;apos;) order by &amp;amp;apos;NUM_DISTINCTS&amp;amp;apos;
		select AVG_COL_LEN,PARTITION_NAME from PART_COL_STATS where DB_NAME = &amp;amp;apos;tpcds_bin_partitioned_orc_200&amp;amp;apos; and TABLE_NAME = &amp;amp;apos;store_returns&amp;amp;apos; and COLUMN_NAME in (&amp;amp;apos;sr_customer_sk&amp;amp;apos;) and PARTITION_NAME in (&amp;amp;apos;sr_returned_date=1998-01-06&amp;amp;apos;,&amp;amp;apos;sr_returned_date=2003-07-01&amp;amp;apos;) order by &amp;amp;apos;AVG_COL_LEN&amp;amp;apos;
		select MAX_COL_LEN,PARTITION_NAME from PART_COL_STATS where DB_NAME = &amp;amp;apos;tpcds_bin_partitioned_orc_200&amp;amp;apos; and TABLE_NAME = &amp;amp;apos;store_returns&amp;amp;apos; and COLUMN_NAME in (&amp;amp;apos;sr_customer_sk&amp;amp;apos;) and PARTITION_NAME in (&amp;amp;apos;sr_returned_date=1998-01-06&amp;amp;apos;,&amp;amp;apos;sr_returned_date=2003-07-01&amp;amp;apos;) order by &amp;amp;apos;MAX_COL_LEN&amp;amp;apos;
		select LONG_LOW_VALUE,PARTITION_NAME from PART_COL_STATS where DB_NAME = &amp;amp;apos;tpcds_bin_partitioned_orc_200&amp;amp;apos; and TABLE_NAME = &amp;amp;apos;store_returns&amp;amp;apos; and COLUMN_NAME in (&amp;amp;apos;sr_item_sk&amp;amp;apos;) and PARTITION_NAME in (&amp;amp;apos;sr_returned_date=1998-01-06&amp;amp;apos;,&amp;amp;apos;sr_returned_date=2003-07-01&amp;amp;apos;) order by &amp;amp;apos;LONG_LOW_VALUE&amp;amp;apos;
		select LONG_HIGH_VALUE,PARTITION_NAME from PART_COL_STATS where DB_NAME = &amp;amp;apos;tpcds_bin_partitioned_orc_200&amp;amp;apos; and TABLE_NAME = &amp;amp;apos;store_returns&amp;amp;apos; and COLUMN_NAME in (&amp;amp;apos;sr_item_sk&amp;amp;apos;) and PARTITION_NAME in (&amp;amp;apos;sr_returned_date=1998-01-06&amp;amp;apos;,&amp;amp;apos;sr_returned_date=2003-07-01&amp;amp;apos;) order by &amp;amp;apos;LONG_HIGH_VALUE&amp;amp;apos;
 
</description>
			<version>0.14.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.RawStore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">7944</link>
		</links>
	</bug>
	<bug id="5871" opendate="2013-11-22 13:24:29" fixdate="2014-09-05 21:45:54" resolution="Fixed">
		<buginformation>
			<summary>Use multiple-characters as field delimiter</summary>
			<description>By default, hive only allows user to use single character as field delimiter. Although there&amp;amp;apos;s RegexSerDe to specify multiple-character delimiter, it can be daunting to use, especially for amateurs.
The patch adds a new SerDe named MultiDelimitSerDe. With MultiDelimitSerDe, users can specify a multiple-character field delimiter when creating tables, in a way most similar to typical table creations. For example:


create table test (id string,hivearray array&amp;lt;binary&amp;gt;,hivemap map&amp;lt;string,int&amp;gt;) ROW FORMAT SERDE &amp;amp;apos;org.apache.hadoop.hive.contrib.serde2.MultiDelimitSerDe&amp;amp;apos; WITH SERDEPROPERTIES ("field.delim"="[,]","collection.delim"=":","mapkey.delim"="@");


where field.delim is the field delimiter, collection.delim and mapkey.delim is the delimiter for collection items and key value pairs, respectively. Among these delimiters, field.delim is mandatory and can be of multiple characters, while collection.delim and mapkey.delim is optional and only support single character.
To use MultiDelimitSerDe, you have to add the hive-contrib jar to the class path, e.g. with the add jar command.</description>
			<version>0.12.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyStruct.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyBinary.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.TestLazyPrimitive.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">14989</link>
			<link type="Reference" description="is related to">14989</link>
			<link type="Reference" description="is related to">9172</link>
			<link type="Supercedes" description="supercedes">1762</link>
		</links>
	</bug>
	<bug id="7927" opendate="2014-09-01 06:04:53" fixdate="2014-09-05 21:47:08" resolution="Fixed">
		<buginformation>
			<summary>Checking sticky bit needs shim</summary>
			<description>Hive cannot be built on hadoop-1 after HIVE-7895, which checks sticky bit in FsPermission.</description>
			<version>0.14.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.shims.HadoopShims.java</file>
			<file type="M">org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
			<file type="M">org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
			<file type="M">org.apache.hadoop.hive.common.FileUtils.java</file>
			<file type="M">org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">7937</link>
		</links>
	</bug>
	<bug id="1363" opendate="2010-05-23 03:10:17" fixdate="2014-09-08 03:42:10" resolution="Fixed">
		<buginformation>
			<summary>&amp;apos;SHOW TABLE EXTENDED LIKE&amp;apos; command does not strip single/double quotes</summary>
			<description>

hive&amp;gt; SHOW TABLE EXTENDED LIKE pokes;
OK
tableName:pokes
owner:carl
location:hdfs://localhost/user/hive/warehouse/pokes
inputformat:org.apache.hadoop.mapred.TextInputFormat
outputformat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
columns:struct columns { i32 num}
partitioned:false
partitionColumns:
totalNumberFiles:0
totalFileSize:0
maxFileSize:0
minFileSize:0
lastAccessTime:0
lastUpdateTime:1274517075221

hive&amp;gt; SHOW TABLE EXTENDED LIKE "p*";
FAILED: Error in metadata: MetaException(message:Got exception: javax.jdo.JDOUserException &amp;amp;apos;)&amp;amp;apos; expected at character 54 in "database.name == dbName &amp;amp;&amp;amp; ( tableName.matches("(?i)"p.*""))")
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask

hive&amp;gt; SHOW TABLE EXTENDED LIKE &amp;amp;apos;p*&amp;amp;apos;;
OK

hive&amp;gt; SHOW TABLE EXTENDED LIKE `p*`;
OK
tableName:pokes
owner:carl
location:hdfs://localhost/user/hive/warehouse/pokes
inputformat:org.apache.hadoop.mapred.TextInputFormat
outputformat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
columns:struct columns { i32 num}
partitioned:false
partitionColumns:
totalNumberFiles:0
totalFileSize:0
maxFileSize:0
minFileSize:0
lastAccessTime:0
lastUpdateTime:1274517075221


</description>
			<version>0.5.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">1676</link>
			<link type="Required" description="is required by">423</link>
		</links>
	</bug>
	<bug id="2390" opendate="2011-08-18 00:50:39" fixdate="2014-09-09 09:07:13" resolution="Fixed">
		<buginformation>
			<summary>Add UNIONTYPE serialization support to LazyBinarySerDe</summary>
			<description>When the union type was introduced, full support for it wasn&amp;amp;apos;t provided.  For instance, when working with a union that gets passed to LazyBinarySerde: 

Caused by: java.lang.RuntimeException: Unrecognized type: UNION
	at org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe.serialize(LazyBinarySerDe.java:468)
	at org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe.serializeStruct(LazyBinarySerDe.java:230)
	at org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe.serialize(LazyBinarySerDe.java:184)

</description>
			<version>0.13.1</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryObjectInspectorFactory.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryFactory.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">7936</link>
			<link type="Reference" description="is related to">2508</link>
			<link type="Reference" description="is related to">7936</link>
			<link type="Reference" description="is related to">537</link>
			<link type="Reference" description="is related to">2517</link>
		</links>
	</bug>
	<bug id="6747" opendate="2014-03-25 22:24:48" fixdate="2014-09-10 06:55:01" resolution="Duplicate">
		<buginformation>
			<summary>TestEmbeddedThriftBinaryCLIService.testExecuteStatementAsync is failing</summary>
			<description></description>
			<version>0.13.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.service.cli.thrift.ThriftCLIServiceTest.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">6908</link>
		</links>
	</bug>
	<bug id="8030" opendate="2014-09-09 11:47:53" fixdate="2014-09-16 08:54:37" resolution="Duplicate">
		<buginformation>
			<summary>NullPointerException on getSchemas</summary>
			<description>java.lang.NullPointerException
	at java.util.ArrayList.&amp;lt;init&amp;gt;(ArrayList.java:164)
	at org.apache.hadoop.hive.jdbc.HiveMetaDataResultSet.&amp;lt;init&amp;gt;(HiveMetaDataResultSet.java:32)
	at org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData$3.&amp;lt;init&amp;gt;(HiveDatabaseMetaData.java:482)
	at org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getSchemas(HiveDatabaseMetaData.java:481)
	at org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getSchemas(HiveDatabaseMetaData.java:476)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.pentaho.hadoop.shim.common.DriverProxyInvocationChain$DatabaseMetaDataInvocationHandler.invoke(DriverProxyInvocationChain.java:368)
	at com.sun.proxy.$Proxy20.getSchemas(Unknown Source)
	at org.pentaho.di.core.database.Database.getSchemas(Database.java:3857)
	at org.pentaho.di.ui.trans.steps.tableoutput.TableOutputDialog.getSchemaNames(TableOutputDialog.java:1036)
	at org.pentaho.di.ui.trans.steps.tableoutput.TableOutputDialog.access$2400(TableOutputDialog.java:94)
	at org.pentaho.di.ui.trans.steps.tableoutput.TableOutputDialog$24.widgetSelected(TableOutputDialog.java:863)
	at org.eclipse.swt.widgets.TypedListener.handleEvent(Unknown Source)
	at org.eclipse.swt.widgets.EventTable.sendEvent(Unknown Source)
	at org.eclipse.swt.widgets.Widget.sendEvent(Unknown Source)
	at org.eclipse.swt.widgets.Display.runDeferredEvents(Unknown Source)
	at org.eclipse.swt.widgets.Display.readAndDispatch(Unknown Source)
	at org.pentaho.di.ui.trans.steps.tableoutput.TableOutputDialog.open(TableOutputDialog.java:884)
	at org.pentaho.di.ui.spoon.delegates.SpoonStepsDelegate.editStep(SpoonStepsDelegate.java:124)
	at org.pentaho.di.ui.spoon.Spoon.editStep(Spoon.java:8648)
	at org.pentaho.di.ui.spoon.trans.TransGraph.editStep(TransGraph.java:3020)
	at org.pentaho.di.ui.spoon.trans.TransGraph.mouseDoubleClick(TransGraph.java:737)
	at org.eclipse.swt.widgets.TypedListener.handleEvent(Unknown Source)
	at org.eclipse.swt.widgets.EventTable.sendEvent(Unknown Source)
	at org.eclipse.swt.widgets.Widget.sendEvent(Unknown Source)
	at org.eclipse.swt.widgets.Display.runDeferredEvents(Unknown Source)
	at org.eclipse.swt.widgets.Display.readAndDispatch(Unknown Source)
	at org.pentaho.di.ui.spoon.Spoon.readAndDispatch(Spoon.java:1297)
	at org.pentaho.di.ui.spoon.Spoon.waitForDispose(Spoon.java:7801)
	at org.pentaho.di.ui.spoon.Spoon.start(Spoon.java:9130)
	at org.pentaho.di.ui.spoon.Spoon.main(Spoon.java:638)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.pentaho.commons.launcher.Launcher.main(Launcher.java:151)</description>
			<version>0.13.1</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.HiveMetaDataResultSet.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">2069</link>
		</links>
	</bug>
	<bug id="7935" opendate="2014-09-02 07:20:49" fixdate="2014-09-16 19:41:34" resolution="Fixed">
		<buginformation>
			<summary>Support dynamic service discovery for HiveServer2</summary>
			<description>To support Rolling Upgrade / HA, we need a mechanism by which a JDBC client can dynamically resolve an HiveServer2 to connect to.
High Level Design: 
Whether, dynamic service discovery is supported or not, can be configured by setting HIVE_SERVER2_SUPPORT_DYNAMIC_SERVICE_DISCOVERY. ZooKeeper is used to support this.

When an instance of HiveServer2 comes up, it adds itself as a znode to ZooKeeper under a configurable namespace (HIVE_SERVER2_ZOOKEEPER_NAMESPACE).
A JDBC/ODBC client now specifies the ZooKeeper ensemble in its connection string, instead of pointing to a specific HiveServer2 instance. The JDBC driver, uses the ZooKeeper ensemble to pick an instance of HiveServer2 to connect for the entire session.
When an instance is removed from ZooKeeper, the existing client sessions continue till completion. When the last client session completes, the instance shuts down.
All new client connection pick one of the available HiveServer2 uris from ZooKeeper.

</description>
			<version>0.14.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Sub-task</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.jdbc.HiveConnection.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.ThriftHttpCLIService.java</file>
			<file type="M">org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.zookeeper.TestZookeeperLockManager.java</file>
			<file type="M">org.apache.hive.service.cli.session.TestSessionGlobalInitFile.java</file>
			<file type="M">org.apache.hive.jdbc.TestJdbcDriver2.java</file>
			<file type="M">org.apache.hive.service.cli.operation.OperationManager.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.java</file>
			<file type="M">org.apache.hive.service.server.HiveServer2.java</file>
			<file type="M">org.apache.hive.service.cli.session.SessionManager.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hive.service.cli.CLIService.java</file>
			<file type="M">org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
			<file type="M">org.apache.hive.jdbc.HiveDriver.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.java</file>
			<file type="M">org.apache.hive.jdbc.Utils.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">6363</link>
		</links>
	</bug>
	<bug id="8154" opendate="2014-09-17 01:23:31" fixdate="2014-09-18 03:19:59" resolution="Duplicate">
		<buginformation>
			<summary>HadoopThriftAuthBridge20S.getHadoopSaslProperties is incompatible with Hadoop 2.4.1 and later</summary>
			<description>Enabled Kerberos in Hadoop 2.4.1 and Hive 0.13.1, with all kerberos properties and principals/keytabs configured correctly. Hadoop cluster is healthy but Hive Server2 is not able to start, due to following error in hive.log:
2014-09-16 13:52:32,964 ERROR thrift.ThriftCLIService (ThriftBinaryCLIService.java:run(93)) - Error: 
java.lang.IllegalArgumentException: Unknown auth type: null Allowed values are: [auth-int, auth-conf, auth]
	at org.apache.hive.service.auth.SaslQOP.fromString(SaslQOP.java:56)
	at org.apache.hive.service.auth.HiveAuthFactory.getSaslProperties(HiveAuthFactory.java:118)
	at org.apache.hive.service.auth.HiveAuthFactory.getAuthTransFactory(HiveAuthFactory.java:133)
	at org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.run(ThriftBinaryCLIService.java:43)
	at java.lang.Thread.run(Thread.java:853)</description>
			<version>0.13.1</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.shims.ShimLoader.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">6741</link>
			<link type="Duplicate" description="duplicates">7620</link>
		</links>
	</bug>
	<bug id="8264" opendate="2014-09-26 00:23:16" fixdate="2014-09-26 09:50:35" resolution="Duplicate">
		<buginformation>
			<summary>Math UDFs in Reducer-with-vectorization fail with ArrayIndexOutOfBoundsException</summary>
			<description>Following queries are representative of the exceptions we are seeing with trunk. These queries pass if vectorization is disabled (or if limit is removed, which means no reducer).
select name, log2(0) from (select name from mytable limit 1) t;
select name, rand() from (select name from mytable limit 1) t;
.. similar patterns with other Math UDFs&amp;amp;apos;.
Exception:
], TaskAttempt 3 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:177)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:142)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:180)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:172)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1637)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:172)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:167)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:254)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:167)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:154)
	... 14 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:360)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:242)
	... 16 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Error evaluating null
	at org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.processOp(VectorSelectOperator.java:127)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:801)
	at org.apache.hadoop.hive.ql.exec.vector.VectorLimitOperator.processOp(VectorLimitOperator.java:47)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:801)
	at org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.processOp(VectorSelectOperator.java:139)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:347)
	... 17 more
Caused by: java.lang.ArrayIndexOutOfBoundsException: 0
	at org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.evaluateLong(ConstantVectorExpression.java:102)
	at org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.evaluate(ConstantVectorExpression.java:150)
	at org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.processOp(VectorSelectOperator.java:125)
	... 22 more</description>
			<version>0.14.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">8171</link>
		</links>
	</bug>
	<bug id="8171" opendate="2014-09-18 00:35:00" fixdate="2014-09-26 22:31:21" resolution="Fixed">
		<buginformation>
			<summary>Tez and Vectorized Reduce doesn&amp;apos;t create scratch columns</summary>
			<description>This query fails with ArrayIndexOutofBound exception in the reducer.


create table varchar_3 (
  field varchar(25)
) stored as orc;

insert into table varchar_3 select cint from alltypesorc limit 10;

</description>
			<version>0.14.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">8264</link>
		</links>
	</bug>
	<bug id="8246" opendate="2014-09-24 20:39:50" fixdate="2014-09-28 08:51:15" resolution="Fixed">
		<buginformation>
			<summary>HiveServer2 in http-kerberos mode is restrictive on client usernames</summary>
			<description>Unable to use client usernames of the format:


username/host@REALM
username@FOREIGN_REALM


The following works fine:


username@REALM 

</description>
			<version>0.13.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.service.auth.HttpAuthUtils.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.ThriftHttpServlet.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">6801</link>
			<link type="Regression" description="breaks">8324</link>
		</links>
	</bug>
	<bug id="8298" opendate="2014-09-29 23:51:54" fixdate="2014-09-30 15:56:16" resolution="Fixed">
		<buginformation>
			<summary>Incorrect results for n-way join when join expressions are not in same order across joins</summary>
			<description>select *  from srcpart a join srcpart b on a.key = b.key and a.hr = b.hr join srcpart c on a.hr = c.hr and a.key = c.key;
is minimal query which reproduces it</description>
			<version>0.13.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">9146</link>
			<link type="Duplicate" description="is duplicated by">8856</link>
		</links>
	</bug>
	<bug id="8290" opendate="2014-09-29 17:02:41" fixdate="2014-10-01 21:20:16" resolution="Fixed">
		<buginformation>
			<summary>With DbTxnManager configured, all ORC tables forced to be transactional</summary>
			<description>Currently, once a user configures DbTxnManager to the be transaction manager, all tables that use ORC are expected to be transactional.  This means they all have to have buckets.  This most likely won&amp;amp;apos;t be what users want.
We need to add a specific mark to a table so that users can indicate it should be treated in a transactional way.</description>
			<version>0.14.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.QBMetaData.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.TestUpdateDeleteSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Table.java</file>
			<file type="M">org.apache.hadoop.hive.ql.ErrorMsg.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">8231</link>
			<link type="Reference" description="is related to">8323</link>
		</links>
	</bug>
	<bug id="8231" opendate="2014-09-23 10:08:57" fixdate="2014-10-02 08:26:47" resolution="Duplicate">
		<buginformation>
			<summary>Error when insert into empty table with ACID</summary>
			<description>Steps to show the bug :
1. create table 


create table encaissement_1b_64m like encaissement_1b;


2. check table 


desc encaissement_1b_64m;
dfs -ls hdfs://nc-h04/user/hive/warehouse/casino.db/encaissement_1b_64m;


everything is ok:

0: jdbc:hive2://nc-h04:10000/casino&amp;gt; desc encaissement_1b_64m;                                                                                                              +------------+------------+----------+--+
|  col_name  | data_type  | comment  |
+------------+------------+----------+--+
| id         | int        |          |
| idmagasin  | int        |          |
| zibzin     | string     |          |
| cheque     | int        |          |
| montant    | double     |          |
| date       | timestamp  |          |
| col_6      | string     |          |
| col_7      | string     |          |
| col_8      | string     |          |
+------------+------------+----------+--+
9 rows selected (0.158 seconds)
0: jdbc:hive2://nc-h04:10000/casino&amp;gt; dfs -ls hdfs://nc-h04/user/hive/warehouse/casino.db/encaissement_1b_64m/;
+-------------+--+
| DFS Output  |
+-------------+--+
+-------------+--+
No rows selected (0.01 seconds)



3. Insert values into the new table

insert into table encaissement_1b_64m VALUES (1, 1, &amp;amp;apos;800000000909000000000000&amp;amp;apos;, 1, 12.5, &amp;amp;apos;12/05/2014&amp;amp;apos;, &amp;amp;apos;&amp;amp;apos;,&amp;amp;apos;&amp;amp;apos;,&amp;amp;apos;&amp;amp;apos;);


4. Check

0: jdbc:hive2://nc-h04:10000/casino&amp;gt; select id from encaissement_1b_64m;
+-----+--+
| id  |
+-----+--+
+-----+--+
No rows selected (0.091 seconds)


There are already a pb. I don&amp;amp;apos;t see the inserted row.
5. When I&amp;amp;apos;m checking HDFS directory, I see delta_0000421_0000421 folder

0: jdbc:hive2://nc-h04:10000/casino&amp;gt; dfs -ls hdfs://nc-h04/user/hive/warehouse/casino.db/encaissement_1b_64m/;
+-----------------------------------------------------------------------------------------------------------------------------------------------------+--+
|                                                                     DFS Output                                                                      |
+-----------------------------------------------------------------------------------------------------------------------------------------------------+--+
| Found 1 items                                                                                                                                       |
| drwxr-xr-x   - hduser supergroup          0 2014-09-23 12:17 hdfs://nc-h04/user/hive/warehouse/casino.db/encaissement_1b_64m/delta_0000421_0000421  |
+-----------------------------------------------------------------------------------------------------------------------------------------------------+--+
2 rows selected (0.014 seconds)


6. Doing a major compaction solves the bug

0: jdbc:hive2://nc-h04:10000/casino&amp;gt; alter table encaissement_1b_64m compact &amp;amp;apos;major&amp;amp;apos;;
No rows affected (0.046 seconds)
0: jdbc:hive2://nc-h04:10000/casino&amp;gt; dfs -ls hdfs://nc-h04/user/hive/warehouse/casino.db/encaissement_1b_64m/;
+--------------------------------------------------------------------------------------------------------------------------------------------+--+
|                                                                 DFS Output                                                                 |
+--------------------------------------------------------------------------------------------------------------------------------------------+--+
| Found 1 items                                                                                                                              |
| drwxr-xr-x   - hduser supergroup          0 2014-09-23 12:21 hdfs://nc-h04/user/hive/warehouse/casino.db/encaissement_1b_64m/base_0000421  |
+--------------------------------------------------------------------------------------------------------------------------------------------+--+
2 rows selected (0.02 seconds)


</description>
			<version>0.14.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.QBMetaData.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.TestUpdateDeleteSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Table.java</file>
			<file type="M">org.apache.hadoop.hive.ql.ErrorMsg.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">8290</link>
		</links>
	</bug>
	<bug id="8316" opendate="2014-09-30 21:56:32" fixdate="2014-10-08 05:15:16" resolution="Duplicate">
		<buginformation>
			<summary>CBO : cardinality estimation for filters is much lower than actual row count</summary>
			<description>CBO underestimates selectivity from filter which consequently results in under estimation throughout the plan.


8 rows, 0.0 cpu, 0.0 io}, id = 7808
                                            HiveJoinRel(condition=[=($0, $12)], joinType=[inner]): rowcount = 11459.928208333333, cumulative cost = {5.50076555E8 rows, 0.0 cpu, 0.0 io}, id = 7426
                                              HiveProjectRel(ss_item_sk=[$1], ss_customer_sk=[$2], ss_cdemo_sk=[$3], ss_hdemo_sk=[$4], ss_addr_sk=[$5], ss_store_sk=[$6], ss_promo_sk=[$7], ss_ticket_number=[$8], ss_wholesale_cost=[$10], ss_list_price=[$11], ss_coupon_amt=[$18], ss_sold_date_sk=[$22]): rowcount = 5.50076554E8, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 893
                                                HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_200_orig.store_sales]]): rowcount = 5.50076554E8, cumulative cost = {0}, id = 55
                                              HiveProjectRel(i_item_sk=[$0], i_current_price=[$5], i_color=[$17], i_product_name=[$21]): rowcount = 1.0, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 1163
                                                HiveFilterRel(condition=[AND(in($17, &amp;amp;apos;maroon&amp;amp;apos;, &amp;amp;apos;burnished&amp;amp;apos;, &amp;amp;apos;dim&amp;amp;apos;, &amp;amp;apos;steel&amp;amp;apos;, &amp;amp;apos;navajo&amp;amp;apos;, &amp;amp;apos;chocolate&amp;amp;apos;), between(false, $5, 35, +(35, 10)), between(false, $5, +(35, 1), +(35, 15)))]): rowcount = 1.0, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 1161
                                                  HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_200_orig.item]]): rowcount = 48000.0, cumulative cost = {0}, id = 68




select count(*) from item where  i_color in (&amp;amp;apos;maroon&amp;amp;apos;,&amp;amp;apos;burnished&amp;amp;apos;,&amp;amp;apos;dim&amp;amp;apos;,&amp;amp;apos;steel&amp;amp;apos;,&amp;amp;apos;navajo&amp;amp;apos;,&amp;amp;apos;chocolate&amp;amp;apos;) and
         i_current_price between 35 and 35 + 10 and
         i_current_price between 35 + 1 and 35 + 15;

</description>
			<version>0.14.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.optiq.stats.HiveRelMdSelectivity.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">8315</link>
		</links>
	</bug>
	<bug id="8315" opendate="2014-09-30 21:47:39" fixdate="2014-10-08 15:29:00" resolution="Fixed">
		<buginformation>
			<summary>CBO : Negate condition underestimates selectivity which results in an in-efficient plan</summary>
			<description>For TPC-DS Q64 the predicate cd1.cd_marital_status &amp;lt;&amp;gt; cd2.cd_marital_status under estimate the join selectivity by a huge margin and results in in-efficient join order.
This is a subset of the logical plan showing that item was joined very last


                                HiveJoinRel(condition=[=($0, $37)], joinType=[inner]): rowcount = 1.0, cumulative cost = {6.386017602518958E8 rows, 0.0 cpu, 0.0 io}, id = 3790
                                  HiveJoinRel(condition=[=($0, $33)], joinType=[inner]): rowcount = 1.0, cumulative cost = {6.386017582518958E8 rows, 0.0 cpu, 0.0 io}, id = 3067
                                    HiveFilterRel(condition=[&amp;lt;&amp;gt;($30, $32)]): rowcount = 1.8252236387887635, cumulative cost = {6.386017554266721E8 rows, 0.0 cpu, 0.0 io}, id = 1153
                                      HiveProjectRel(ss_item_sk=[$2], ss_customer_sk=[$3], ss_cdemo_sk=[$4], ss_hdemo_sk=[$5], ss_addr_sk=[$6], ss_store_sk=[$7], ss_promo_sk=[$8], ss_ticket_number=[$9], ss_wholesale_cost=[$10], ss_list_price=[$11], ss_coupon_amt=[$12], ss_sold_date_sk=[$13], sr_item_sk=[$0], sr_ticket_number=[$1], c_customer_sk=[$23], c_current_cdemo_sk=[$24], c_current_hdemo_sk=[$25], c_current_addr_sk=[$26], c_first_shipto_date_sk=[$27], c_first_sales_date_sk=[$28], d_date_sk=[$14], d_year=[$15], d_date_sk0=[$29], d_year0=[$30], d_date_sk1=[$31], d_year1=[$32], s_store_sk=[$18], s_store_name=[$19], s_zip=[$20], cd_demo_sk=[$16], cd_marital_status=[$17], cd_demo_sk0=[$21], cd_marital_status0=[$22]): rowcount = 3.6246005783468924E7, cumulative cost = {6.386017554266721E8 rows, 0.0 cpu, 0.0 io}, id = 2312
                                        HiveJoinRel(condition=[AND(=($2, $0), =($9, $1))], joinType=[inner]): rowcount = 3.6246005783468924E7, cumulative cost = {6.386017554266721E8 rows, 0.0 cpu, 0.0 io}, id = 2310
                                          HiveProjectRel(sr_item_sk=[$1], sr_ticket_number=[$8]): rowcount = 5.5578005E7, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 912
                                            HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_200_orig.store_returns]]): rowcount = 5.5578005E7, cumulative cost = {0}, id = 62
                                          HiveJoinRel(condition=[=($1, $21)], joinType=[inner]): rowcount = 1.2950939439433252E7, cumulative cost = {5.700728109872389E8 rows, 0.0 cpu, 0.0 io}, id = 2308
                                            HiveJoinRel(condition=[=($5, $16)], joinType=[inner]): rowcount = 5491530.921341597, cumulative cost = {5.629812800658973E8 rows, 0.0 cpu, 0.0 io}, id = 2301
                                              HiveJoinRel(condition=[=($2, $14)], joinType=[inner]): rowcount = 5491530.921341597, cumulative cost = {5.574895371445558E8 rows, 0.0 cpu, 0.0 io}, id = 2299
                                                HiveJoinRel(condition=[=($11, $12)], joinType=[inner]): rowcount = 5491530.921341597, cumulative cost = {5.500772062232143E8 rows, 0.0 cpu, 0.0 io}, id = 1898
                                                  HiveProjectRel(ss_item_sk=[$1], ss_customer_sk=[$2], ss_cdemo_sk=[$3], ss_hdemo_sk=[$4], ss_addr_sk=[$5], ss_store_sk=[$6], ss_promo_sk=[$7], ss_ticket_number=[$8], ss_wholesale_cost=[$10], ss_list_price=[$11], ss_coupon_amt=[$18], ss_sold_date_sk=[$22]): rowcount = 5.50076554E8, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 909
                                                    HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_200_orig.store_sales]]): rowcount = 5.50076554E8, cumulative cost = {0}, id = 55

Query 


select cs1.product_name ,cs1.store_name ,cs1.store_zip ,cs1.b_street_number ,cs1.b_streen_name ,cs1.b_city
     ,cs1.b_zip ,cs1.c_street_number ,cs1.c_street_name ,cs1.c_city ,cs1.c_zip ,cs1.syear ,cs1.cnt
     ,cs1.s1 ,cs1.s2 ,cs1.s3
     ,cs2.s1 ,cs2.s2 ,cs2.s3 ,cs2.syear ,cs2.cnt
from
(select i_product_name as product_name ,i_item_sk as item_sk ,s_store_name as store_name
     ,s_zip as store_zip ,ad1.ca_street_number as b_street_number ,ad1.ca_street_name as b_streen_name
     ,ad1.ca_city as b_city ,ad1.ca_zip as b_zip ,ad2.ca_street_number as c_street_number
     ,ad2.ca_street_name as c_street_name ,ad2.ca_city as c_city ,ad2.ca_zip as c_zip
     ,d1.d_year as syear ,d2.d_year as fsyear ,d3.d_year as s2year ,count(*) as cnt
     ,sum(ss_wholesale_cost) as s1 ,sum(ss_list_price) as s2 ,sum(ss_coupon_amt) as s3
  FROM   store_sales
        JOIN store_returns ON store_sales.ss_item_sk = store_returns.sr_item_sk and store_sales.ss_ticket_number = store_returns.sr_ticket_number
        JOIN customer ON store_sales.ss_customer_sk = customer.c_customer_sk
        JOIN date_dim d1 ON store_sales.ss_sold_date_sk = d1.d_date_sk
        JOIN date_dim d2 ON customer.c_first_sales_date_sk = d2.d_date_sk 
        JOIN date_dim d3 ON customer.c_first_shipto_date_sk = d3.d_date_sk
        JOIN store ON store_sales.ss_store_sk = store.s_store_sk
        JOIN customer_demographics cd1 ON store_sales.ss_cdemo_sk= cd1.cd_demo_sk
        JOIN customer_demographics cd2 ON customer.c_current_cdemo_sk = cd2.cd_demo_sk
        JOIN promotion ON store_sales.ss_promo_sk = promotion.p_promo_sk
        JOIN household_demographics hd1 ON store_sales.ss_hdemo_sk = hd1.hd_demo_sk
        JOIN household_demographics hd2 ON customer.c_current_hdemo_sk = hd2.hd_demo_sk
        JOIN customer_address ad1 ON store_sales.ss_addr_sk = ad1.ca_address_sk
        JOIN customer_address ad2 ON customer.c_current_addr_sk = ad2.ca_address_sk
        JOIN income_band ib1 ON hd1.hd_income_band_sk = ib1.ib_income_band_sk
        JOIN income_band ib2 ON hd2.hd_income_band_sk = ib2.ib_income_band_sk
        JOIN item ON store_sales.ss_item_sk = item.i_item_sk
        JOIN
 (select cs_item_sk
        ,sum(cs_ext_list_price) as sale,sum(cr_refunded_cash+cr_reversed_charge+cr_store_credit) as refund
  from catalog_sales JOIN catalog_returns
  ON catalog_sales.cs_item_sk = catalog_returns.cr_item_sk
    and catalog_sales.cs_order_number = catalog_returns.cr_order_number
  group by cs_item_sk
  having sum(cs_ext_list_price)&amp;gt;2*sum(cr_refunded_cash+cr_reversed_charge+cr_store_credit)) cs_ui
ON store_sales.ss_item_sk = cs_ui.cs_item_sk
  WHERE  
         cd1.cd_marital_status &amp;lt;&amp;gt; cd2.cd_marital_status and
         i_color in (&amp;amp;apos;maroon&amp;amp;apos;,&amp;amp;apos;burnished&amp;amp;apos;,&amp;amp;apos;dim&amp;amp;apos;,&amp;amp;apos;steel&amp;amp;apos;,&amp;amp;apos;navajo&amp;amp;apos;,&amp;amp;apos;chocolate&amp;amp;apos;) and
         i_current_price between 35 and 35 + 10 and
         i_current_price between 35 + 1 and 35 + 15
group by i_product_name ,i_item_sk ,s_store_name ,s_zip ,ad1.ca_street_number
       ,ad1.ca_street_name ,ad1.ca_city ,ad1.ca_zip ,ad2.ca_street_number
       ,ad2.ca_street_name ,ad2.ca_city ,ad2.ca_zip ,d1.d_year ,d2.d_year ,d3.d_year
) cs1
JOIN
(select i_product_name as product_name ,i_item_sk as item_sk ,s_store_name as store_name
     ,s_zip as store_zip ,ad1.ca_street_number as b_street_number ,ad1.ca_street_name as b_streen_name
     ,ad1.ca_city as b_city ,ad1.ca_zip as b_zip ,ad2.ca_street_number as c_street_number
     ,ad2.ca_street_name as c_street_name ,ad2.ca_city as c_city ,ad2.ca_zip as c_zip
     ,d1.d_year as syear ,d2.d_year as fsyear ,d3.d_year as s2year ,count(*) as cnt
     ,sum(ss_wholesale_cost) as s1 ,sum(ss_list_price) as s2 ,sum(ss_coupon_amt) as s3
  FROM   store_sales
        JOIN store_returns ON store_sales.ss_item_sk = store_returns.sr_item_sk and store_sales.ss_ticket_number = store_returns.sr_ticket_number
        JOIN customer ON store_sales.ss_customer_sk = customer.c_customer_sk
        JOIN date_dim d1 ON store_sales.ss_sold_date_sk = d1.d_date_sk
        JOIN date_dim d2 ON customer.c_first_sales_date_sk = d2.d_date_sk 
        JOIN date_dim d3 ON customer.c_first_shipto_date_sk = d3.d_date_sk
        JOIN store ON store_sales.ss_store_sk = store.s_store_sk
        JOIN customer_demographics cd1 ON store_sales.ss_cdemo_sk= cd1.cd_demo_sk
        JOIN customer_demographics cd2 ON customer.c_current_cdemo_sk = cd2.cd_demo_sk
        JOIN promotion ON store_sales.ss_promo_sk = promotion.p_promo_sk
        JOIN household_demographics hd1 ON store_sales.ss_hdemo_sk = hd1.hd_demo_sk
        JOIN household_demographics hd2 ON customer.c_current_hdemo_sk = hd2.hd_demo_sk
        JOIN customer_address ad1 ON store_sales.ss_addr_sk = ad1.ca_address_sk
        JOIN customer_address ad2 ON customer.c_current_addr_sk = ad2.ca_address_sk
        JOIN income_band ib1 ON hd1.hd_income_band_sk = ib1.ib_income_band_sk
        JOIN income_band ib2 ON hd2.hd_income_band_sk = ib2.ib_income_band_sk
        JOIN item ON store_sales.ss_item_sk = item.i_item_sk
        JOIN
 (select cs_item_sk
        ,sum(cs_ext_list_price) as sale,sum(cr_refunded_cash+cr_reversed_charge+cr_store_credit) as refund
  from catalog_sales JOIN catalog_returns
  ON catalog_sales.cs_item_sk = catalog_returns.cr_item_sk
    and catalog_sales.cs_order_number = catalog_returns.cr_order_number
  group by cs_item_sk
  having sum(cs_ext_list_price)&amp;gt;2*sum(cr_refunded_cash+cr_reversed_charge+cr_store_credit)) cs_ui
ON store_sales.ss_item_sk = cs_ui.cs_item_sk
  WHERE  
         cd1.cd_marital_status &amp;lt;&amp;gt; cd2.cd_marital_status and
         i_color in (&amp;amp;apos;maroon&amp;amp;apos;,&amp;amp;apos;burnished&amp;amp;apos;,&amp;amp;apos;dim&amp;amp;apos;,&amp;amp;apos;steel&amp;amp;apos;,&amp;amp;apos;navajo&amp;amp;apos;,&amp;amp;apos;chocolate&amp;amp;apos;) and
         i_current_price between 35 and 35 + 10 and
         i_current_price between 35 + 1 and 35 + 15
group by i_product_name ,i_item_sk ,s_store_name ,s_zip ,ad1.ca_street_number
       ,ad1.ca_street_name ,ad1.ca_city ,ad1.ca_zip ,ad2.ca_street_number
       ,ad2.ca_street_name ,ad2.ca_city ,ad2.ca_zip ,d1.d_year ,d2.d_year ,d3.d_year
) cs2
ON cs1.item_sk=cs2.item_sk
where 
     cs1.syear = 2000 and
     cs2.syear = 2000 + 1 and
     cs2.cnt &amp;lt;= cs1.cnt and
     cs1.store_name = cs2.store_name and
     cs1.store_zip = cs2.store_zip
order by cs1.product_name ,cs1.store_name ,cs2.cnt

</description>
			<version>0.14.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.optiq.stats.HiveRelMdSelectivity.java</file>
		</fixedFiles>
		<links>
			<link type="Cloners" description="is a clone of">8263</link>
			<link type="Duplicate" description="is duplicated by">8316</link>
			<link type="Reference" description="is related to">8283</link>
		</links>
	</bug>
	<bug id="8369" opendate="2014-10-06 21:37:24" fixdate="2014-10-11 00:51:35" resolution="Duplicate">
		<buginformation>
			<summary>SimpleFetchOptimizer needs to re-enable FS caching before scanning dirs</summary>
			<description>SimpleFetchOptimizer spends a lot of CPU within itself because hive disables HDFS fs caching (fs.hdfs.impl.disable.cache).
SimpleFetchOptimizer needs a revisit for its optimization rules, along with a fix for this case.</description>
			<version>0.14.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.service.cli.session.HiveSessionProxy.java</file>
			<file type="D">org.apache.hive.service.auth.TUGIContainingProcessor.java</file>
			<file type="M">org.apache.hive.service.cli.session.SessionManager.java</file>
			<file type="M">org.apache.hive.service.cli.session.HiveSession.java</file>
			<file type="M">org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
			<file type="M">org.apache.hive.service.auth.TSetIpAddressProcessor.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">4501</link>
		</links>
	</bug>
	<bug id="3159" opendate="2012-06-19 14:14:51" fixdate="2014-10-13 23:25:24" resolution="Duplicate">
		<buginformation>
			<summary>Update AvroSerde to determine schema of new tables</summary>
			<description>Currently when writing tables to Avro one must manually provide an Avro schema that matches what is being delivered by Hive. It&amp;amp;apos;d be better to have the serde infer this schema by converting the table&amp;amp;apos;s TypeInfo into an appropriate AvroSchema.</description>
			<version>0.12.0</version>
			<fixedVersion></fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.serde2.avro.AvroDeserializer.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.IOConstants.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.TestStorageFormatDescriptor.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.avro.AvroSerDe.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">6806</link>
			<link type="dependent" description="depends upon">7049</link>
		</links>
	</bug>
	<bug id="6410" opendate="2014-02-12 04:59:09" fixdate="2014-10-14 13:35:35" resolution="Duplicate">
		<buginformation>
			<summary>Allow output serializations separators to be set for HDFS path as well.</summary>
			<description>HIVE-3682 adds functionality for users to set serialization constants for &amp;amp;apos;insert overwrite local directory&amp;amp;apos;. The same functionality should be available for hdfs path as well. The workaround suggested is to create a table with required format and insert into the table, which enforces the users to know the schema of the result and create the table ahead. Though that works, it is good to have the functionality for loading into directory as well.
I&amp;amp;apos;m planning to add the same functionality in &amp;amp;apos;insert overwrite directory&amp;amp;apos; in this jira.</description>
			<version>0.12.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.QB.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">518</link>
			<link type="Duplicate" description="duplicates">5672</link>
			<link type="Duplicate" description="is duplicated by">6833</link>
			<link type="Reference" description="relates to">3682</link>
		</links>
	</bug>
	<bug id="6833" opendate="2014-04-03 14:47:42" fixdate="2014-10-14 13:36:57" resolution="Duplicate">
		<buginformation>
			<summary>when output hive table query to HDFS file,users should have a separator of their own choice</summary>
			<description>HIVE-3682 allows user to store output of Hive query to a local file along with delimiters and separators of their choice.
e.g. insert overwrite local directory &amp;amp;apos;/users/home/XYZ&amp;amp;apos; row format delimited FIELDS TERMINATED BY &amp;amp;apos;,&amp;amp;apos; SELECT * from table_name;
Storing query output with default separator(\001) to HDFS is possible.
e.g. insert overwrite directory &amp;amp;apos;/user/xYZ/security&amp;amp;apos; SELECT * from table_name;
But user can not store output of Hive query to a HDFS directory with delimiters and separators of their choice. 
e.g. insert overwrite directory &amp;amp;apos;/user/xYZ/security&amp;amp;apos;  row format delimited FIELDS TERMINATED BY &amp;amp;apos;,&amp;amp;apos; SELECT * from table_name; (Gives ERROR)</description>
			<version>0.11.0</version>
			<fixedVersion></fixedVersion>
			<type>New Feature</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.QB.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">5672</link>
			<link type="Duplicate" description="duplicates">6410</link>
		</links>
	</bug>
	<bug id="8541" opendate="2014-10-21 17:51:49" fixdate="2014-10-22 14:39:57" resolution="Duplicate">
		<buginformation>
			<summary>Decimal values contains extra trailing zeros when vectorization is on</summary>
			<description>The fix done on HIVE-7373 preserves the trailing zeroes from decimal values
as they are read from the table files, but when vectorization is on, these
values contain extra tralinig zeroes up to what the scale allows when aggregation
expressions are used.
Here&amp;amp;apos;s an example (data gotten from vector_decimal_aggregate.q):

hive&amp;gt; SET hive.vectorized.execution.enabled=false;
hive&amp;gt; SELECT cint, MAX(cdecimal2) max FROM decimal_vgby GROUP BY cint HAVING COUNT(*) &amp;gt; 1;
+------------+---------------------+--+
|    cint    |         max         |
+------------+---------------------+--+
| NULL       | 11160.715384615385  |
| -3728      | 6984454.211097692   |
| -563       | -617.5607769230769  |
| 762        | 6984454.211097692   |
| 6981       | 6984454.211097692   |
| 253665376  | 11697.969230769231  |
| 528534767  | 6984454.211097692   |
| 626923679  | 11645.746153846154  |
+------------+---------------------+--+

hive&amp;gt; SET hive.vectorized.execution.enabled=true;
hive&amp;gt; SELECT cint, MAX(cdecimal2) max FROM decimal_vgby GROUP BY cint HAVING COUNT(*) &amp;gt; 1;
+------------+-------------------------+--+
|    cint    |          max2           |
+------------+-------------------------+--+
| NULL       | 11160.71538461538500    |
| -3728      | 6984454.21109769200000  |
| -563       | -617.56077692307690     |
| 762        | 6984454.21109769200000  |
| 6981       | 6984454.211097692       |
| 253665376  | 11697.96923076923100    |
| 528534767  | 6984454.21109769200000  |
| 626923679  | 11645.74615384615400    |
+------------+-------------------------+--+


Hive should not add trailing zeroes when aggregation functions are used.</description>
			<version>0.13.1</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriter.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorRowBatchFromObjectIterables.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmeticExpressions.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToLong.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToDecimal.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalColumnInList.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.TestDecimalUtil.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToBoolean.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFAvgDecimal.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumDecimal.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.FuncDoubleToDecimal.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToString.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToTimestamp.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.FuncRoundWithNumDigitsDecimalToDecimal.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLongToDecimal.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToDouble.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalUtil.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFAdaptor.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.CastLongToDecimal.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorizedBatchUtil.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.io.HiveDecimalWritable.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.java</file>
			<file type="M">org.apache.hadoop.hive.common.type.HiveDecimal.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToDecimal.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.IDecimalInExpr.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.CastTimestampToDecimal.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.util.VectorizedRowGroupGenUtil.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.FuncDecimalToLong.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.CastDoubleToDecimal.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorColumnAssignFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.FilterDecimalColumnInList.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.TestConstantVectorExpression.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">8461</link>
			<link type="Regression" description="is broken by">7373</link>
		</links>
	</bug>
	<bug id="6801" opendate="2014-03-31 23:35:14" fixdate="2014-10-24 23:05:32" resolution="Duplicate">
		<buginformation>
			<summary>beeline kerberos authentication fails if the client principal name has hostname part</summary>
			<description>Kinited as guest@EXAMPLE.COM
Connected successfully with beeline using command
!connect jdbc:hive2://hdps.example.com:10001/default;principal=hive/hdps.example.com@EXAMPLE.COM;hive.server2.proxy.user=guest?hive.server2.transport.mode=http;hive.server2.thrift.http.path=cliservice  dummy dummy-pass org.apache.hive.jdbc.HiveDriver
Kinited as bob/hdps.exmaple.com@EXAMPLE.COM
!connect jdbc:hive2://hdps.example.com:10001/default;principal=hive/hdps.example.com@EXAMPLE.COM;hive.server2.proxy.user=guest?hive.server2.transport.mode=http;hive.server2.thrift.http.path=cliservice  dummy dummy-pass org.apache.hive.jdbc.HiveDriver
Failed with stack trace
Error: Could not establish connection to jdbc:hive2://hdps.example.com:10001/default;principal=hive/hdps.example.com@EXAMPLE.COM;hive.server2.proxy.user=guest?hive.server2.transport.mode=http;hive.server2.thrift.http.path=cliservice: org.apache.http.client.ClientProtocolException (state=08S01,code=0)
java.sql.SQLException: Could not establish connection to jdbc:hive2://hdps.example.com:10001/default;principal=hive/hdps.example.com@EXAMPLE.COM;hive.server2.proxy.user=guest?hive.server2.transport.mode=http;hive.server2.thrift.http.path=cliservice: org.apache.http.client.ClientProtocolException
	at org.apache.hive.jdbc.HiveConnection.openSession(HiveConnection.java:426)
	at org.apache.hive.jdbc.HiveConnection.&amp;lt;init&amp;gt;(HiveConnection.java:193)
	at org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:105)
	at java.sql.DriverManager.getConnection(DriverManager.java:582)
	at java.sql.DriverManager.getConnection(DriverManager.java:154)
	at org.apache.hive.beeline.DatabaseConnection.connect(DatabaseConnection.java:145)
	at org.apache.hive.beeline.DatabaseConnection.getConnection(DatabaseConnection.java:186)
	at org.apache.hive.beeline.Commands.connect(Commands.java:959)
	at org.apache.hive.beeline.Commands.connect(Commands.java:880)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hive.beeline.ReflectiveCommandHandler.execute(ReflectiveCommandHandler.java:44)
	at org.apache.hive.beeline.BeeLine.dispatch(BeeLine.java:792)
	at org.apache.hive.beeline.BeeLine.begin(BeeLine.java:659)
	at org.apache.hive.beeline.BeeLine.mainWithInputRedirection(BeeLine.java:368)
	at org.apache.hive.beeline.BeeLine.main(BeeLine.java:351)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:212)
Caused by: org.apache.thrift.transport.TTransportException: org.apache.http.client.ClientProtocolException
	at org.apache.thrift.transport.THttpClient.flushUsingHttpClient(THttpClient.java:281)
	at org.apache.thrift.transport.THttpClient.flush(THttpClient.java:297)
	at org.apache.thrift.TServiceClient.sendBase(TServiceClient.java:65)
	at org.apache.hive.service.cli.thrift.TCLIService$Client.send_OpenSession(TCLIService.java:150)
	at org.apache.hive.service.cli.thrift.TCLIService$Client.OpenSession(TCLIService.java:142)
	at org.apache.hive.jdbc.HiveConnection.openSession(HiveConnection.java:415)
	... 22 more
Caused by: org.apache.http.client.ClientProtocolException
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:909)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:827)
	at org.apache.thrift.transport.THttpClient.flushUsingHttpClient(THttpClient.java:235)
	... 27 more
Caused by: org.apache.http.HttpException
	at org.apache.hive.jdbc.HttpKerberosRequestInterceptor.process(HttpKerberosRequestInterceptor.java:67)
	at org.apache.http.protocol.ImmutableHttpProcessor.process(ImmutableHttpProcessor.java:109)
	at org.apache.http.protocol.HttpRequestExecutor.preProcess(HttpRequestExecutor.java:176)
	at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:518)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:906)
	... 29 more
Caused by: java.lang.reflect.UndeclaredThrowableException
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1563)
	at org.apache.hive.service.auth.HttpAuthUtils.getKerberosServiceTicket(HttpAuthUtils.java:94)
	at org.apache.hive.jdbc.HttpKerberosRequestInterceptor.process(HttpKerberosRequestInterceptor.java:61)
	... 33 more
Caused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)
	at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:130)
	at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:106)
	at sun.security.jgss.GSSManagerImpl.getCredentialElement(GSSManagerImpl.java:178)
	at sun.security.jgss.GSSCredentialImpl.add(GSSCredentialImpl.java:384)
	at sun.security.jgss.GSSCredentialImpl.&amp;lt;init&amp;gt;(GSSCredentialImpl.java:42)
	at sun.security.jgss.GSSManagerImpl.createCredential(GSSManagerImpl.java:139)
	at org.apache.hive.service.auth.HttpAuthUtils$HttpKerberosClientAction.run(HttpAuthUtils.java:161)
	at org.apache.hive.service.auth.HttpAuthUtils$HttpKerberosClientAction.run(HttpAuthUtils.java:126)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	... 35 more
</description>
			<version>0.13.0</version>
			<fixedVersion></fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.service.auth.HttpAuthUtils.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.ThriftHttpServlet.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">8246</link>
		</links>
	</bug>
	<bug id="8606" opendate="2014-10-26 10:45:52" fixdate="2014-10-26 12:30:25" resolution="Duplicate">
		<buginformation>
			<summary>[hs2] Do not unnecessarily call setPermission on staging directories</summary>
			<description>HS2 has made setPermission mandatory within its CLIService#setupStagingDir method as a result of HIVE-6602.
This causes HS2 to fail to start if the owner of the staging directory is not the same user as it, even though the directory is already 777. This is because only owners and superusers of a directory can change its permission, not group or others.
Failure appears as:


Caused by: org.apache.hive.service.ServiceException: Error setting stage directories 
at org.apache.hive.service.cli.CLIService.start(CLIService.java:132) 
at org.apache.hive.service.CompositeService.start(CompositeService.java:70) 
... 8 more 
Caused by: org.apache.hadoop.security.AccessControlException: Permission denied 


We should only call the setPermission if it is unsatisfactory.</description>
			<version>0.13.1</version>
			<fixedVersion></fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="D">org.apache.hadoop.hive.ql.TestUtilitiesDfs.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TestUtilities.java</file>
			<file type="M">org.apache.hive.service.cli.session.HiveSessionBase.java</file>
			<file type="M">org.apache.hadoop.hive.common.ServerUtils.java</file>
			<file type="M">org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			<file type="D">org.apache.hive.service.cli.TestScratchDir.java</file>
			<file type="M">org.apache.hadoop.hive.ql.session.SessionState.java</file>
			<file type="M">org.apache.hive.service.cli.CLIService.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
			<file type="M">org.apache.hive.service.cli.session.SessionManager.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Context.java</file>
			<file type="M">org.apache.hive.jdbc.TestJdbcWithMiniMr.java</file>
			<file type="M">org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
			<file type="M">org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">6847</link>
		</links>
	</bug>
	<bug id="8604" opendate="2014-10-25 07:42:09" fixdate="2014-10-28 04:22:00" resolution="Duplicate">
		<buginformation>
			<summary>Re-enable auto_sortmerge_join_5 on tez</summary>
			<description></description>
			<version>0.14.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">8614</link>
			<link type="Reference" description="is related to">8603</link>
		</links>
	</bug>
	<bug id="8614" opendate="2014-10-27 18:38:34" fixdate="2014-10-29 04:10:33" resolution="Fixed">
		<buginformation>
			<summary>Upgrade hive to use tez version 0.5.2-SNAPSHOT</summary>
			<description></description>
			<version>0.14.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">8604</link>
		</links>
	</bug>
	<bug id="8653" opendate="2014-10-29 20:11:02" fixdate="2014-10-30 19:48:19" resolution="Fixed">
		<buginformation>
			<summary>CBO: Push Semi Join through, Project/Filter/Join</summary>
			<description>CLEAR LIBRARY CACHE</description>
			<version>0.14.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Sub-task</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">8526</link>
		</links>
	</bug>
	<bug id="8526" opendate="2014-10-20 22:56:24" fixdate="2014-10-31 19:21:45" resolution="Duplicate">
		<buginformation>
			<summary>Hive : CBO incorrect join order in TPC-DS Q45 as self join selectivity has incorrect CE</summary>
			<description>The join order has Item joined last where it should be joined first
Query 


select  ca_zip, ca_county, sum(ws_sales_price)
 from
    web_sales
    JOIN customer ON web_sales.ws_bill_customer_sk = customer.c_customer_sk
    JOIN customer_address ON customer.c_current_addr_sk = customer_address.ca_address_sk 
    JOIN date_dim ON web_sales.ws_sold_date_sk = date_dim.d_date_sk
    JOIN item ON web_sales.ws_item_sk = item.i_item_sk 
 where
        ( item.i_item_id in (select i_item_id
                             from item i2
                             where i2.i_item_sk in (2, 3, 5, 7, 11, 13, 17, 19, 23, 29)
                             )
            )
        and d_qoy = 2 and d_year = 2000
 group by ca_zip, ca_county
 order by ca_zip, ca_county
 limit 100


Plan


2014-10-20 18:43:16,521 DEBUG [main]: parse.SemanticAnalyzer (SemanticAnalyzer.java:apply(12330)) - HiveSortRel(fetch=[100]): rowcount = 1.710158597922807E7, cumulative cost = {7.169080587598123E10 rows, 3.420317295845614E7 cpu, 0.0 io}, id = 579
  HiveSortRel(sort0=[$0], sort1=[$1], dir0=[ASC], dir1=[ASC]): rowcount = 1.710158597922807E7, cumulative cost = {6.827294821015483E10 rows, 1.710158697922807E7 cpu, 0.0 io}, id = 577
    HiveProjectRel(ca_zip=[$0], ca_county=[$1], _o__c2=[$2]): rowcount = 1.710158597922807E7, cumulative cost = {6.485509054432843E10 rows, 1.0 cpu, 0.0 io}, id = 575
      HiveAggregateRel(group=[{0, 1}], agg#0=[sum($2)]): rowcount = 1.710158597922807E7, cumulative cost = {6.485509054432843E10 rows, 1.0 cpu, 0.0 io}, id = 573
        HiveProjectRel($f0=[$2], $f1=[$1], $f2=[$0]): rowcount = 6.0197670310147226E7, cumulative cost = {6.485509054432843E10 rows, 1.0 cpu, 0.0 io}, id = 571
          HiveProjectRel(ws_sales_price=[$2], ca_county=[$7], ca_zip=[$8]): rowcount = 6.0197670310147226E7, cumulative cost = {6.485509054432843E10 rows, 1.0 cpu, 0.0 io}, id = 569
            HiveFilterRel(condition=[AND(=($11, 2), =($10, 2000))]): rowcount = 6.0197670310147226E7, cumulative cost = {6.485509054432843E10 rows, 1.0 cpu, 0.0 io}, id = 567
              SemiJoinRel(condition=[=($13, $14)], joinType=[inner]): rowcount = 3.371069537368245E10, cumulative cost = {6.485509054432843E10 rows, 1.0 cpu, 0.0 io}, id = 565
                HiveProjectRel(ws_item_sk=[$0], ws_bill_customer_sk=[$1], ws_sales_price=[$2], ws_sold_date_sk=[$3], c_customer_sk=[$9], c_current_addr_sk=[$10], ca_address_sk=[$11], ca_county=[$12], ca_zip=[$13], d_date_sk=[$6], d_year=[$7], d_qoy=[$8], i_item_sk=[$4], i_item_id=[$5]): rowcount = 3.371069537368245E10, cumulative cost = {6.485509054332843E10 rows, 0.0 cpu, 0.0 io}, id = 669
                  HiveJoinRel(condition=[=($1, $9)], joinType=[inner]): rowcount = 3.371069537368245E10, cumulative cost = {6.485509054332843E10 rows, 0.0 cpu, 0.0 io}, id = 667
                    HiveJoinRel(condition=[=($3, $6)], joinType=[inner]): rowcount = 2.1594638446E10, cumulative cost = {4.3189811941E10 rows, 0.0 cpu, 0.0 io}, id = 664
                      HiveJoinRel(condition=[=($0, $4)], joinType=[inner]): rowcount = 2.1594638446E10, cumulative cost = {2.1595100446E10 rows, 0.0 cpu, 0.0 io}, id = 601
                        HiveProjectRel(ws_item_sk=[$2], ws_bill_customer_sk=[$3], ws_sales_price=[$20], ws_sold_date_sk=[$33]): rowcount = 2.1594638446E10, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 497
                          HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_30000.web_sales]]): rowcount = 2.1594638446E10, cumulative cost = {0}, id = 341
                        HiveProjectRel(i_item_sk=[$0], i_item_id=[$1]): rowcount = 462000.0, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 555
                          HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_30000.item]]): rowcount = 462000.0, cumulative cost = {0}, id = 340
                      HiveProjectRel(d_date_sk=[$0], d_year=[$6], d_qoy=[$10]): rowcount = 73049.0, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 551
                        HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_30000.date_dim]]): rowcount = 73049.0, cumulative cost = {0}, id = 342
                    HiveJoinRel(condition=[=($1, $2)], joinType=[inner]): rowcount = 7.064015632843196E7, cumulative cost = {1.2E8 rows, 0.0 cpu, 0.0 io}, id = 598
                      HiveProjectRel(c_customer_sk=[$0], c_current_addr_sk=[$4]): rowcount = 8.0E7, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 500
                        HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_30000.customer]]): rowcount = 8.0E7, cumulative cost = {0}, id = 343
                      HiveProjectRel(ca_address_sk=[$0], ca_county=[$7], ca_zip=[$9]): rowcount = 4.0E7, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 547
                        HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_30000.customer_address]]): rowcount = 4.0E7, cumulative cost = {0}, id = 339
                HiveProjectRel(i_item_id=[$1]): rowcount = 1.05119214745814, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 563
                  HiveProjectRel(i_item_sk=[$0], i_item_id=[$1]): rowcount = 1.05119214745814, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 561
                    HiveFilterRel(condition=[in($0, 2, 3, 5, 7, 11, 13, 17, 19, 23, 29)]): rowcount = 1.05119214745814, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 559
                      HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_30000.item]]): rowcount = 462000.0, cumulative cost = {0}, id = 340


Then I rewrote the query trying to force CBO to generate the correct join order


with items as (select i_item_sk from 
item  where
        ( item.i_item_id in (select i_item_id
                             from item i2
                             where i2.i_item_sk in (2, 3, 5, 7, 11, 13, 17, 19, 23, 29)
                             )
            )
)

select  ca_zip, ca_county, sum(ws_sales_price)
 from
    web_sales
    JOIN items ON web_sales.ws_item_sk = items.i_item_sk 
    JOIN customer ON web_sales.ws_bill_customer_sk = customer.c_customer_sk
    JOIN customer_address ON customer.c_current_addr_sk = customer_address.ca_address_sk 
    JOIN date_dim ON web_sales.ws_sold_date_sk = date_dim.d_date_sk
 where
 d_qoy = 2 and d_year = 2000
 group by ca_zip, ca_county
 order by ca_zip, ca_county
 limit 100


But the correct join order wasn&amp;amp;apos;t generated because CE for item x item + filter has a selectivity of 1.


2014-10-20 18:46:27,120 DEBUG [main]: parse.SemanticAnalyzer (SemanticAnalyzer.java:apply(12330)) - HiveSortRel(fetch=[100]): rowcount = 1.6595391288544238E7, cumulative cost = {2.8364280421639153E10 rows, 3.3190782577088475E7 cpu, 0.0 io}, id = 1291
  HiveSortRel(sort0=[$0], sort1=[$1], dir0=[ASC], dir1=[ASC]): rowcount = 1.6595391288544238E7, cumulative cost = {2.505357243157397E10 rows, 1.6595391288544238E7 cpu, 0.0 io}, id = 1289
    HiveProjectRel(ca_zip=[$0], ca_county=[$1], _o__c2=[$2]): rowcount = 1.6595391288544238E7, cumulative cost = {2.174286444150879E10 rows, 0.0 cpu, 0.0 io}, id = 1287
      HiveAggregateRel(group=[{0, 1}], agg#0=[sum($2)]): rowcount = 1.6595391288544238E7, cumulative cost = {2.174286444150879E10 rows, 0.0 cpu, 0.0 io}, id = 1285
        HiveProjectRel($f0=[$9], $f1=[$8], $f2=[$2]): rowcount = 6.019767031014723E7, cumulative cost = {2.174286444150879E10 rows, 0.0 cpu, 0.0 io}, id = 1283
          HiveProjectRel(ws_item_sk=[$5], ws_bill_customer_sk=[$6], ws_sales_price=[$7], ws_sold_date_sk=[$8], i_item_sk=[$12], c_customer_sk=[$0], c_current_addr_sk=[$1], ca_address_sk=[$2], ca_county=[$3], ca_zip=[$4], d_date_sk=[$9], d_year=[$10], d_qoy=[$11]): rowcount = 6.019767031014723E7, cumulative cost = {2.174286444150879E10 rows, 0.0 cpu, 0.0 io}, id = 1380
            HiveJoinRel(condition=[=($6, $0)], joinType=[inner]): rowcount = 6.019767031014723E7, cumulative cost = {2.174286444150879E10 rows, 0.0 cpu, 0.0 io}, id = 1378
              HiveJoinRel(condition=[=($1, $2)], joinType=[inner]): rowcount = 7.064015632843196E7, cumulative cost = {1.2E8 rows, 0.0 cpu, 0.0 io}, id = 1309
                HiveProjectRel(c_customer_sk=[$0], c_current_addr_sk=[$4]): rowcount = 8.0E7, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 1269
                  HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_30000.customer]]): rowcount = 8.0E7, cumulative cost = {0}, id = 1035
                HiveProjectRel(ca_address_sk=[$0], ca_county=[$7], ca_zip=[$9]): rowcount = 4.0E7, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 1273
                  HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_30000.customer_address]]): rowcount = 4.0E7, cumulative cost = {0}, id = 1032
              HiveJoinRel(condition=[=($0, $7)], joinType=[inner]): rowcount = 3.856185436785714E7, cumulative cost = {2.16336624308125E10 rows, 0.0 cpu, 0.0 io}, id = 1376
                HiveJoinRel(condition=[=($3, $4)], joinType=[inner]): rowcount = 3.856185436785714E7, cumulative cost = {2.159463857644464E10 rows, 0.0 cpu, 0.0 io}, id = 1316
                  HiveProjectRel(ws_item_sk=[$2], ws_bill_customer_sk=[$3], ws_sales_price=[$20], ws_sold_date_sk=[$33]): rowcount = 2.1594638446E10, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 1205
                    HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_30000.web_sales]]): rowcount = 2.1594638446E10, cumulative cost = {0}, id = 1033
                  HiveProjectRel(d_date_sk=[$0], d_year=[$6], d_qoy=[$10]): rowcount = 130.44464285714287, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 1279
                    HiveFilterRel(condition=[AND(=($10, 2), =($6, 2000))]): rowcount = 130.44464285714287, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 1277
                      HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_30000.date_dim]]): rowcount = 73049.0, cumulative cost = {0}, id = 1034
                HiveProjectRel(i_item_sk=[$0]): rowcount = 462000.0, cumulative cost = {1.0 rows, 1.0 cpu, 0.0 io}, id = 1265
                  HiveFilterRel(condition=[=(1, 1)]): rowcount = 462000.0, cumulative cost = {1.0 rows, 1.0 cpu, 0.0 io}, id = 1263
                    SemiJoinRel(condition=[=($1, $2)], joinType=[inner]): rowcount = 462000.0, cumulative cost = {1.0 rows, 1.0 cpu, 0.0 io}, id = 1261
                      HiveProjectRel(i_item_sk=[$0], i_item_id=[$1]): rowcount = 462000.0, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 1253
                        HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_30000.item]]): rowcount = 462000.0, cumulative cost = {0}, id = 1024
                      HiveProjectRel(i_item_id=[$1]): rowcount = 1.05119214745814, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 1259
                        HiveProjectRel(i_item_sk=[$0], i_item_id=[$1]): rowcount = 1.05119214745814, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 1257
                          HiveFilterRel(condition=[in($0, 2, 3, 5, 7, 11, 13, 17, 19, 23, 29)]): rowcount = 1.05119214745814, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 1255
                            HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_30000.item]]): rowcount = 462000.0, cumulative cost = {0}, id = 1024


This query generates the correct join order 


 with items as (select i_item_sk from 
item  where
         item.i_item_id in (select i_item_id
                             from item i2
                             where i2.i_item_sk in (2, 3, 5, 7, 11, 13, 17, 19, 23, 29)
                             )
            
),
  ws as (
 select ws_bill_customer_sk,ws_sales_price,ws_sold_date_sk
from  web_sales
    JOIN items ON web_sales.ws_item_sk = items.i_item_sk 
 )
 select  ca_zip, ca_county, sum(ws_sales_price)
 from ws 
    JOIN customer ON ws.ws_bill_customer_sk = customer.c_customer_sk
    JOIN customer_address ON customer.c_current_addr_sk = customer_address.ca_address_sk 
    JOIN date_dim ON ws.ws_sold_date_sk = date_dim.d_date_sk
 where d_qoy = 2 and d_year = 2000
 group by ca_zip, ca_county
 order by ca_zip, ca_county
 limit 100


Plan 


2014-10-20 19:13:15,989 DEBUG [main]: parse.SemanticAnalyzer (SemanticAnalyzer.java:apply(12330)) - HiveSortRel(fetch=[100]): rowcount = 1.6595391288544238E7, cumulative cost = {4.99203570142713E10 rows, 3.3190783577088475E7 cpu, 0.0 io}, id = 4367
  HiveSortRel(sort0=[$0], sort1=[$1], dir0=[ASC], dir1=[ASC]): rowcount = 1.6595391288544238E7, cumulative cost = {4.6609649024206116E10 rows, 1.6595392288544238E7 cpu, 0.0 io}, id = 4365
    HiveProjectRel(ca_zip=[$0], ca_county=[$1], _o__c2=[$2]): rowcount = 1.6595391288544238E7, cumulative cost = {4.329894103414093E10 rows, 1.0 cpu, 0.0 io}, id = 4363
      HiveAggregateRel(group=[{0, 1}], agg#0=[sum($2)]): rowcount = 1.6595391288544238E7, cumulative cost = {4.329894103414093E10 rows, 1.0 cpu, 0.0 io}, id = 4361
        HiveProjectRel($f0=[$7], $f1=[$6], $f2=[$1]): rowcount = 6.019767031014723E7, cumulative cost = {4.329894103414093E10 rows, 1.0 cpu, 0.0 io}, id = 4359
          HiveProjectRel(ws_bill_customer_sk=[$5], ws_sales_price=[$6], ws_sold_date_sk=[$7], c_customer_sk=[$0], c_current_addr_sk=[$1], ca_address_sk=[$2], ca_county=[$3], ca_zip=[$4], d_date_sk=[$8], d_year=[$9], d_qoy=[$10]): rowcount = 6.019767031014723E7, cumulative cost = {4.329894103414093E10 rows, 1.0 cpu, 0.0 io}, id = 4426
            HiveJoinRel(condition=[=($5, $0)], joinType=[inner]): rowcount = 6.019767031014723E7, cumulative cost = {4.329894103414093E10 rows, 1.0 cpu, 0.0 io}, id = 4424
              HiveJoinRel(condition=[=($1, $2)], joinType=[inner]): rowcount = 7.064015632843196E7, cumulative cost = {1.2E8 rows, 0.0 cpu, 0.0 io}, id = 4392
                HiveProjectRel(c_customer_sk=[$0], c_current_addr_sk=[$4]): rowcount = 8.0E7, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 4345
                  HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_30000.customer]]): rowcount = 8.0E7, cumulative cost = {0}, id = 4101
                HiveProjectRel(ca_address_sk=[$0], ca_county=[$7], ca_zip=[$9]): rowcount = 4.0E7, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 4349
                  HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_30000.customer_address]]): rowcount = 4.0E7, cumulative cost = {0}, id = 4099
              HiveJoinRel(condition=[=($2, $3)], joinType=[inner]): rowcount = 3.856185436785714E7, cumulative cost = {4.318973902344464E10 rows, 1.0 cpu, 0.0 io}, id = 4395
                HiveProjectRel(ws_bill_customer_sk=[$1], ws_sales_price=[$2], ws_sold_date_sk=[$3]): rowcount = 2.1594638446E10, cumulative cost = {2.1595100447E10 rows, 1.0 cpu, 0.0 io}, id = 4343
                  HiveProjectRel(ws_item_sk=[$0], ws_bill_customer_sk=[$1], ws_sales_price=[$2], ws_sold_date_sk=[$3], i_item_sk=[$4]): rowcount = 2.1594638446E10, cumulative cost = {2.1595100447E10 rows, 1.0 cpu, 0.0 io}, id = 4388
                    HiveJoinRel(condition=[=($0, $4)], joinType=[inner]): rowcount = 2.1594638446E10, cumulative cost = {2.1595100447E10 rows, 1.0 cpu, 0.0 io}, id = 4383
                      HiveProjectRel(ws_item_sk=[$2], ws_bill_customer_sk=[$3], ws_sales_price=[$20], ws_sold_date_sk=[$33]): rowcount = 2.1594638446E10, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 4277
                        HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_30000.web_sales]]): rowcount = 2.1594638446E10, cumulative cost = {0}, id = 4096
                      HiveProjectRel(i_item_sk=[$0]): rowcount = 462000.0, cumulative cost = {1.0 rows, 1.0 cpu, 0.0 io}, id = 4339
                        HiveFilterRel(condition=[=(1, 1)]): rowcount = 462000.0, cumulative cost = {1.0 rows, 1.0 cpu, 0.0 io}, id = 4337
                          SemiJoinRel(condition=[=($1, $2)], joinType=[inner]): rowcount = 462000.0, cumulative cost = {1.0 rows, 1.0 cpu, 0.0 io}, id = 4335
                            HiveProjectRel(i_item_sk=[$0], i_item_id=[$1]): rowcount = 462000.0, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 4327
                              HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_30000.item]]): rowcount = 462000.0, cumulative cost = {0}, id = 4088
                            HiveProjectRel(i_item_id=[$1]): rowcount = 1.05119214745814, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 4333
                              HiveProjectRel(i_item_sk=[$0], i_item_id=[$1]): rowcount = 1.05119214745814, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 4331
                                HiveFilterRel(condition=[in($0, 2, 3, 5, 7, 11, 13, 17, 19, 23, 29)]): rowcount = 1.05119214745814, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 4329
                                  HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_30000.item]]): rowcount = 462000.0, cumulative cost = {0}, id = 4088
                HiveProjectRel(d_date_sk=[$0], d_year=[$6], d_qoy=[$10]): rowcount = 130.44464285714287, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 4355
                  HiveFilterRel(condition=[AND(=($10, 2), =($6, 2000))]): rowcount = 130.44464285714287, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 4353
                    HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_30000.date_dim]]): rowcount = 73049.0, cumulative cost = {0}, id = 4100


</description>
			<version>0.14.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">8653</link>
		</links>
	</bug>
	<bug id="8461" opendate="2014-10-14 21:20:30" fixdate="2014-11-01 21:10:47" resolution="Fixed">
		<buginformation>
			<summary>Make Vectorized Decimal query results match Non-Vectorized query results with respect to trailing zeroes... .0000</summary>
			<description></description>
			<version>0.14.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriter.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorRowBatchFromObjectIterables.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmeticExpressions.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToLong.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToDecimal.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalColumnInList.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.TestDecimalUtil.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToBoolean.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFAvgDecimal.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumDecimal.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.FuncDoubleToDecimal.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToString.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToTimestamp.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.FuncRoundWithNumDigitsDecimalToDecimal.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLongToDecimal.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToDouble.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalUtil.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFAdaptor.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.CastLongToDecimal.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorizedBatchUtil.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.io.HiveDecimalWritable.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.java</file>
			<file type="M">org.apache.hadoop.hive.common.type.HiveDecimal.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToDecimal.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.IDecimalInExpr.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.CastTimestampToDecimal.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.util.VectorizedRowGroupGenUtil.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.FuncDecimalToLong.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.CastDoubleToDecimal.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorColumnAssignFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.FilterDecimalColumnInList.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.TestConstantVectorExpression.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">8541</link>
		</links>
	</bug>
	<bug id="8712" opendate="2014-11-03 20:33:46" fixdate="2014-11-04 20:06:46" resolution="Duplicate">
		<buginformation>
			<summary>Cross schema query fails when table has index</summary>
			<description>I have two schemas, default and accesstesting. 
I create a table in the second schema with an index.
When I query the table using a WHERE clause from the first schema, the query fails:
use default;
drop table salary_hive;
use accesstesting;
drop table salary_hive;
use accesstesting;
create table salary_hive (idnum int, salary int, startdate timestamp, enddate timestamp, jobcode char(20));
create index salary_hive_idnum_index on table salary_hive(idnum) as &amp;amp;apos;compact&amp;amp;apos; with deferred rebuild;
select * from accesstesting.salary_hive where 0=1;
use default;
select * from accesstesting.salary_hive where 0=1;
FAILED: SemanticException org.apache.hadoop.hive.ql.metadata.InvalidTableException: Table not found accesstesting_salary_hive_salary_hive_idnum_index_</description>
			<version>0.13.0</version>
			<fixedVersion>0.13.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.AuthorizationUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.AlterTableAlterPartDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.ShowColumnsDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.authorization.TestHiveAuthorizationTaskFactory.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.ShowGrantDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.authorization.TestPrivilegesV2.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.RenamePartitionDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.TestHive.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.TestQBCompact.java</file>
			<file type="M">org.apache.hadoop.hive.ql.hooks.CheckColumnAccessHook.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.Warehouse.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.index.RewriteGBUsingIndex.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.PrivilegeObjectDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.AlterIndexDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.IndexUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.authorization.TestPrivilegesV1.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.HiveV1Authorizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.authorization.PrivilegesTestBase.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.ColumnAccessInfo.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.IndexUpdater.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">4064</link>
		</links>
	</bug>
	<bug id="8752" opendate="2014-11-06 00:32:08" fixdate="2014-11-08 02:29:15" resolution="Duplicate">
		<buginformation>
			<summary>Disjunction cardinality estimation has selectivity of 1</summary>
			<description>TPC-DS Q89 has the wrong join order.
Store_sales should be joining with item first then date_dim.
The issue is that the predicate on item shows a selectivity of 1 


((i_category in (&amp;amp;apos;Home&amp;amp;apos;,&amp;amp;apos;Books&amp;amp;apos;,&amp;amp;apos;Electronics&amp;amp;apos;) and
          i_class in (&amp;amp;apos;wallpaper&amp;amp;apos;,&amp;amp;apos;parenting&amp;amp;apos;,&amp;amp;apos;musical&amp;amp;apos;)
         )
      or (i_category in (&amp;amp;apos;Shoes&amp;amp;apos;,&amp;amp;apos;Jewelry&amp;amp;apos;,&amp;amp;apos;Men&amp;amp;apos;) and
          i_class in (&amp;amp;apos;womens&amp;amp;apos;,&amp;amp;apos;birdal&amp;amp;apos;,&amp;amp;apos;pants&amp;amp;apos;) 
        ))




                HiveProjectRel(i_item_sk=[$0], i_brand=[$8], i_class=[$10], i_category=[$12]): rowcount = 462000.0, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 4052
                      HiveFilterRel(condition=[OR(AND(in($12, &amp;amp;apos;Home&amp;amp;apos;, &amp;amp;apos;Books&amp;amp;apos;, &amp;amp;apos;Electronics&amp;amp;apos;), in($10, &amp;amp;apos;wallpaper&amp;amp;apos;, &amp;amp;apos;parenting&amp;amp;apos;, &amp;amp;apos;musical&amp;amp;apos;)), AND(in($12, &amp;amp;apos;Shoes&amp;amp;apos;, &amp;amp;apos;Jewelry&amp;amp;apos;, &amp;amp;apos;Men&amp;amp;apos;), in($10, &amp;amp;apos;womens&amp;amp;apos;, &amp;amp;apos;birdal&amp;amp;apos;, &amp;amp;apos;pants&amp;amp;apos;)))]): rowcount = 462000.0, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 4050
                        HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_30000.item]]): rowcount = 462000.0, cumulative cost = {0}, id = 3818


Query



select  *
from(
select i_category, i_class, i_brand,
       s_store_name, s_company_name,
       d_moy,
       sum(ss_sales_price) sum_sales,
       avg(sum(ss_sales_price)) over
         (partition by i_category, i_brand, s_store_name, s_company_name)
         avg_monthly_sales
from item, store_sales, date_dim, store
where store_sales.ss_item_sk = item.i_item_sk and
      store_sales.ss_sold_date_sk = date_dim.d_date_sk and
      store_sales.ss_store_sk = store.s_store_sk and
      d_year in (2000) and
        ((i_category in (&amp;amp;apos;Home&amp;amp;apos;,&amp;amp;apos;Books&amp;amp;apos;,&amp;amp;apos;Electronics&amp;amp;apos;) and
          i_class in (&amp;amp;apos;wallpaper&amp;amp;apos;,&amp;amp;apos;parenting&amp;amp;apos;,&amp;amp;apos;musical&amp;amp;apos;)
         )
      or (i_category in (&amp;amp;apos;Shoes&amp;amp;apos;,&amp;amp;apos;Jewelry&amp;amp;apos;,&amp;amp;apos;Men&amp;amp;apos;) and
          i_class in (&amp;amp;apos;womens&amp;amp;apos;,&amp;amp;apos;birdal&amp;amp;apos;,&amp;amp;apos;pants&amp;amp;apos;) 
        ))
group by i_category, i_class, i_brand,
         s_store_name, s_company_name, d_moy) tmp1
where case when (avg_monthly_sales &amp;lt;&amp;gt; 0) then (abs(sum_sales - avg_monthly_sales) / avg_monthly_sales) else null end &amp;gt; 0.1
order by sum_sales - avg_monthly_sales, s_store_name
limit 100


The result of the wrong join order is that the query runs in 335 seconds compared to 124 seconds with the correct join order.
Removing the disjunction in the item filter produces the correct plan


 i_category in (&amp;amp;apos;Home&amp;amp;apos;,&amp;amp;apos;Books&amp;amp;apos;,&amp;amp;apos;Electronics&amp;amp;apos;) and
          i_class in (&amp;amp;apos;wallpaper&amp;amp;apos;,&amp;amp;apos;parenting&amp;amp;apos;,&amp;amp;apos;musical&amp;amp;apos;)

</description>
			<version>0.14.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.optiq.translator.SqlFunctionConverter.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.optiq.rules.HivePushFilterPastJoinRule.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.optiq.stats.FilterSelectivityEstimator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">8768</link>
		</links>
	</bug>
	<bug id="8768" opendate="2014-11-06 22:35:57" fixdate="2014-11-08 22:40:44" resolution="Fixed">
		<buginformation>
			<summary>CBO: Fix filter selectivity for "in clause" &amp; "&lt;&gt;" </summary>
			<description></description>
			<version>0.14.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Sub-task</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.optiq.translator.SqlFunctionConverter.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.optiq.rules.HivePushFilterPastJoinRule.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.optiq.stats.FilterSelectivityEstimator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">8752</link>
		</links>
	</bug>
	<bug id="8856" opendate="2014-11-13 14:19:44" fixdate="2014-11-13 16:12:19" resolution="Duplicate">
		<buginformation>
			<summary>Multiple joins must have conditionals in same order </summary>
			<description>SELECT 
   COUNT 
FROM 
   TBL_A,TBL_B,TBL_C
WHERE 
   A.key1 = B.key1 AND A.key2 = B.key2
AND 
   A.key2 = C.key2 AND A.Key1 = B.key1 
Where key1 is a string and key2 is a double. 
Note: This effects explicit joins as well
A look at the query plan reveals the following:
Map Join Operator
              condition map:
                   Inner Join 0 to 1
                   Inner Join 0 to 2
              condition expressions:
                0 
{prdct_id} {bu_cd}
                1 {prdct_id}
 
{bu_cd}
                2 {prdct_id} {bu_cd}
              keys:
                0 UDFToDouble(prdct_id) (type: double), bu_cd (type: double)
                1 UDFToDouble(prdct_id) (type: double), bu_cd (type: double)
                2 bu_cd (type: double), UDFToDouble(prdct_id) (type: double)
The ordering of keys within a join should not dictate it&amp;amp;apos;s type. This is something the query optimizer should handle prior to making the plan. This way users do not have to worry about ordrering their conditionals. At the very least it should fail, instead it silently converts them to nulls and returns 0. </description>
			<version>0.13.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">8298</link>
		</links>
	</bug>
	<bug id="4978" opendate="2013-08-01 21:01:57" fixdate="2014-11-15 17:16:46" resolution="Duplicate">
		<buginformation>
			<summary>[WebHCat] Close the PrintWriter after writing data</summary>
			<description>We are not closing the PrintWriter after writing data into it. I haven&amp;amp;apos;t seen any problems so far, but it is good to close the PrintWriter so that resources are released properly.</description>
			<version>0.11.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.shims.HadoopShims.java</file>
			<file type="M">org.apache.hadoop.mapred.WebHCatJTShim20S.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.PigJobIDParser.java</file>
			<file type="M">org.apache.hadoop.mapred.WebHCatJTShim23.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.TempletonUtils.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.JarJobIDParser.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.HiveJobIDParser.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.HDFSStorage.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.TestTrivialExecService.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.TempletonControllerJob.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.TrivialExecService.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.CompleteDelegator.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.AppConfig.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">4773</link>
			<link type="Duplicate" description="is duplicated by">5511</link>
		</links>
	</bug>
	<bug id="7164" opendate="2014-06-02 07:35:53" fixdate="2014-11-28 18:55:23" resolution="Duplicate">
		<buginformation>
			<summary>Support non-string partition types in HCatalog</summary>
			<description>Currently querying hive tables with non-string partition columns using HCat  gives us the following error. 
Error: Filtering is supported only on partition keys of type string
Related discussion here : https://www.mail-archive.com/dev@hive.apache.org/msg18011.html</description>
			<version>0.8.1</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.parser.ExpressionTree.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">2702</link>
			<link type="Reference" description="is related to">5545</link>
			<link type="Supercedes" description="supercedes">23</link>
		</links>
	</bug>
	<bug id="5328" opendate="2013-09-20 18:30:23" fixdate="2014-12-02 23:04:08" resolution="Duplicate">
		<buginformation>
			<summary>Date and timestamp type converts invalid strings to &amp;apos;1970-01-01&amp;apos;</summary>
			<description>

select
  cast(&amp;amp;apos;abcd&amp;amp;apos; as date),
  cast(&amp;amp;apos;abcd&amp;amp;apos; as timestamp)
from src limit 1;


This prints out 1970-01-01.</description>
			<version>0.12.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableTimestampObjectInspector.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFToDate.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaDateObjectInspector.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaTimestampObjectInspector.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.primitive.TestPrimitiveObjectInspectorUtils.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableDateObjectInspector.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">5329</link>
		</links>
	</bug>
	<bug id="8870" opendate="2014-11-14 13:27:58" fixdate="2014-12-05 08:46:30" resolution="Fixed">
		<buginformation>
			<summary>errors when selecting a struct field within an array from ORC based tables</summary>
			<description>When using ORC as storage for a table, we get errors on selecting a struct field within an array. These errors do not appear with default format.


CREATE  TABLE `foobar_orc`(
  `uid` bigint,
  `elements` array&amp;lt;struct&amp;lt;elementid:bigint,foo:struct&amp;lt;bar:string&amp;gt;&amp;gt;&amp;gt;)
STORED AS ORC;


When selecting from this empty table, we get a direct NPE within the Hive CLI:


SELECT
  elements.elementId
FROM
  foobar_orc;
-- FAILED: RuntimeException java.lang.NullPointerException


A more real-world query produces a RuntimeException / NullPointerException in the mapper:


SELECT
  uid,
  element.elementId
FROM
  foobar_orc
LATERAL VIEW
  EXPLODE(elements) e AS element;


Error: java.lang.RuntimeException: Error in configuring object
	at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)
[...]
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.ExprNodeFieldEvaluator.initialize(ExprNodeFieldEvaluator.java:61)
[...]
FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask


Both queries run fine on a non-orc table:


CREATE  TABLE `foobar`(
  `uid` bigint,
  `elements` array&amp;lt;struct&amp;lt;elementid:bigint,foo:struct&amp;lt;bar:string&amp;gt;&amp;gt;&amp;gt;);  

SELECT
  elements.elementId
FROM
  foobar;
-- OK
-- Time taken: 0.225 seconds

SELECT
  uid,
  element.elementId
FROM
  foobar
LATERAL VIEW
  EXPLODE(elements) e AS element;
-- Total MapReduce CPU Time Spent: 1 seconds 920 msec
-- OK
-- Time taken: 25.905 seconds

</description>
			<version>0.13.0</version>
			<fixedVersion>1.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcStruct.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">6198</link>
			<link type="Duplicate" description="is duplicated by">9107</link>
		</links>
	</bug>
	<bug id="8440" opendate="2014-10-13 09:08:51" fixdate="2014-12-09 10:57:02" resolution="Duplicate">
		<buginformation>
			<summary>Hiveserver block query</summary>
			<description>In our  environment Hiveserver handle approximately 15000 jobs per daySometimes we found a large number of  ETL tasks wait in our ETL schedule  queue to runIn this situation only few Hadoop Job running in Hadoop cluster,many maps and reduces slots free.We view Hiveserver heap found Hiveserver block a large number of query:
"pool-1-thread-17903" prio=10 tid=0x00007f89589c8000 nid=0x13865 waiting for monitor entry [0x00007f893413b000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:977)

waiting to lock &amp;lt;0x00000006000ac458&amp;gt; (a java.lang.Object)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:888)
	at org.apache.hadoop.hive.service.HiveServer$HiveServerHandler.execute(HiveServer.java:198)
	at org.apache.hadoop.hive.service.ThriftHive$Processor$execute.getResult(ThriftHive.java:644)
	at org.apache.hadoop.hive.service.ThriftHive$Processor$execute.getResult(ThriftHive.java:628)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)

     It is a problem of HiveServer cannot handle concurrent requests from more than one client.?Why HiveServer cannot handle concurrent requests?
https://github.com/apache/hive/blob/branch-0.12/ql/src/java/org/apache/hadoop/hive/ql/Driver.java#L975
Under code in org.apache.hadoop.hive.ql cause Hiveserver cannot handle concurrent requests?
 int ret;
    synchronized (compileMonitor) 
{
      ret = compile(command);
    }
</description>
			<version>0.12.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.GenTezProcContext.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hive.service.cli.CLIServiceTest.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.RemoveDynamicPruningBySize.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.GenTezWork.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.ql.session.SessionState.java</file>
			<file type="M">org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
			<file type="M">org.apache.hive.service.cli.session.HiveSessionImplwithUGI.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">4239</link>
			<link type="Reference" description="relates to">4239</link>
		</links>
	</bug>
	<bug id="6198" opendate="2014-01-14 20:45:59" fixdate="2014-12-15 03:54:23" resolution="Duplicate">
		<buginformation>
			<summary>ORC file and struct column names are case sensitive</summary>
			<description>HiveQL document states that the "Table names and column names are case insensitive". But the struct behavior for ORC file is different. 
Consider a sample text file:


$ cat data.txt
line1|key11:value11,key12:value12,key13:value13|a,b,c|one,two
line2|key21:value21,key22:value22,key23:value23|d,e,f|three,four
line3|key31:value31,key32:value32,key33:value33|g,h,i|five,six


Creating a table stored as txt and then using this to create a table stored as orc 


CREATE TABLE orig (
  str STRING,
  mp  MAP&amp;lt;STRING,STRING&amp;gt;,
  lst ARRAY&amp;lt;STRING&amp;gt;,
  strct STRUCT&amp;lt;A:STRING,B:STRING&amp;gt;
) ROW FORMAT DELIMITED
    FIELDS TERMINATED BY &amp;amp;apos;|&amp;amp;apos;
    COLLECTION ITEMS TERMINATED BY &amp;amp;apos;,&amp;amp;apos;
    MAP KEYS TERMINATED BY &amp;amp;apos;:&amp;amp;apos;;
LOAD DATA LOCAL INPATH &amp;amp;apos;data.txt&amp;amp;apos; INTO TABLE orig;

CREATE TABLE tableorc (
  str STRING,
  mp  MAP&amp;lt;STRING,STRING&amp;gt;,
  lst ARRAY&amp;lt;STRING&amp;gt;,
  strct STRUCT&amp;lt;A:STRING,B:STRING&amp;gt;
) STORED AS ORC;
INSERT OVERWRITE TABLE tableorc SELECT * FROM orig;


Suppose we project columns or read the strct columns for both table types, here are the results. I have also tested the same with RC. The behavior is similar to txt files.


hive&amp;gt; SELECT * FROM orig;
line1   {"key11":"value11","key12":"value12","key13":"value13"} ["a","b","c"]  
{"a":"one","b":"two"}
line2   {"key21":"value21","key22":"value22","key23":"value23"} ["d","e","f"]  
{"a":"three","b":"four"}
line3   {"key31":"value31","key32":"value32","key33":"value33"} ["g","h","i"]  
{"a":"five","b":"six"}
Time taken: 0.126 seconds, Fetched: 3 row(s)

hive&amp;gt; SELECT * FROM tableorc;
line1   {"key12":"value12","key11":"value11","key13":"value13"} ["a","b","c"]  
{"A":"one","B":"two"}
line2   {"key21":"value21","key23":"value23","key22":"value22"} ["d","e","f"]  
{"A":"three","B":"four"}
line3   {"key33":"value33","key31":"value31","key32":"value32"} ["g","h","i"]  
{"A":"five","B":"six"}
Time taken: 0.178 seconds, Fetched: 3 row(s)

hive&amp;gt; SELECT strct FROM tableorc;
{"a":"one","b":"two"}
{"a":"three","b":"four"}
{"a":"five","b":"six"}

hive&amp;gt;SELECT strct.A FROM orig;
one
three
five

hive&amp;gt;SELECT strct.a FROM orig;
one
three
five

hive&amp;gt;SELECT strct.A FROM tableorc;
one
three
five

hive&amp;gt;SELECT strct.a FROM tableorc;
FAILED: Execution Error, return code 2 from
org.apache.hadoop.hive.ql.exec.mr.MapRedTask
MapReduce Jobs Launched: 
Job 0: Map: 1   HDFS Read: 0 HDFS Write: 0 FAIL


So it seems that ORC behaves differently for struct columns. Also why are we storing the column names for struct for the other types as CASE SENSITIVE? What is the standard for Hive QL with respect to structs?
Regards
Viraj
</description>
			<version>0.11.0</version>
			<fixedVersion>1.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcStruct.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">8870</link>
		</links>
	</bug>
	<bug id="9107" opendate="2014-12-15 21:17:12" fixdate="2014-12-17 06:34:28" resolution="Duplicate">
		<buginformation>
			<summary>Non-lowercase field names in structs causes NullPointerException</summary>
			<description>If an HQL query references a struct field with mixed or upper case, Hive throws a NullPointerException instead of giving a better error message or simply lower-casing the name.
For example, if I have a struct in column mystruct with a field named myfield, a query like
select mystruct.MyField from tablename;
passes the local initialize (it submits an M-R job) but the remote initialize jobs throw NullPointerExceptions.  The exception is on line 61 of org.apache.hadoop.hive.ql.exec.ExprNodeFieldEvaluator, which is right after the field name is extracted and not forced to be lower-case.</description>
			<version>0.13.1</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcStruct.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">8870</link>
		</links>
	</bug>
	<bug id="9199" opendate="2014-12-23 18:45:44" fixdate="2014-12-24 22:40:21" resolution="Fixed">
		<buginformation>
			<summary>Excessive exclusive lock used in some DDLs with DummyTxnManager</summary>
			<description>In DummyTxnManager, the lockMode for a WriteEntity (e.g. database, table) is determined by "complete" instead of its writeType. But since DDL output WriteEntity is usually complete, some DDL operations might be given exclusive locks which are actually not needed, which causes unnecessary locking contention. For example, in createTable, DummyTxnManager suggests an exclusive lock to table database writeentity since it is complete.</description>
			<version>0.13.1</version>
			<fixedVersion>1.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">9203</link>
			<link type="Duplicate" description="duplicates">9546</link>
			<link type="Duplicate" description="duplicates">8192</link>
		</links>
	</bug>
	<bug id="8192" opendate="2014-09-19 20:25:45" fixdate="2014-12-25 03:31:23" resolution="Duplicate">
		<buginformation>
			<summary>Check DDL&amp;apos;s writetype in DummyTxnManager</summary>
			<description>The patch of HIVE-6734 added some DDL writetypes and checked DDL writetype in DbTxnManager.java.
We use DummyTxnManager as the default value of hive.txn.manager in hive-site.xml. We noticed that the operation of CREATE TEMPORARY FUNCTION has a DLL_NO_LOCK writetype but it requires a EXCLUSIVE lock. If we try to create a temporary function while there&amp;amp;apos;s a SELECT is processing at the same database, then the console will print &amp;amp;apos;conflicting lock present for default mode EXCLUSIVE&amp;amp;apos; and the CREATE TEMPORARY FUNCTION operation won&amp;amp;apos;t get the lock until the SELECT is done. Maybe it&amp;amp;apos;s a good idea to check the DDL&amp;amp;apos;s writetype in DummyTxnManager too.</description>
			<version>0.13.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">9199</link>
		</links>
	</bug>
	<bug id="9278" opendate="2015-01-07 01:44:45" fixdate="2015-01-08 17:50:26" resolution="Fixed">
		<buginformation>
			<summary>Cached expression feature broken in one case</summary>
			<description>Different query result depending on whether hive.cache.expr.evaluation is true or false.  When true, no query results are produced (this is wrong).
The q file:

set hive.cache.expr.evaluation=true;

CREATE TABLE cache_expr_repro (date_str STRING);
LOAD DATA LOCAL INPATH &amp;amp;apos;../../data/files/cache_expr_repro.txt&amp;amp;apos; INTO TABLE cache_expr_repro;

SELECT MONTH(date_str) AS `mon`, CAST((MONTH(date_str) - 1) / 3 + 1 AS int) AS `quarter`,   YEAR(date_str) AS `year` FROM cache_expr_repro WHERE ((CAST((MONTH(date_str) - 1) / 3 + 1 AS int) = 1) AND (YEAR(date_str) = 2015)) GROUP BY MONTH(date_str), CAST((MONTH(date_str) - 1) / 3 + 1 AS int),   YEAR(date_str) ;


cache_expr_repro.txt

2015-01-01 00:00:00
2015-02-01 00:00:00
2015-01-01 00:00:00
2015-02-01 00:00:00
2015-01-01 00:00:00
2015-01-01 00:00:00
2015-02-01 00:00:00
2015-02-01 00:00:00
2015-01-01 00:00:00
2015-01-01 00:00:00

</description>
			<version>0.14.0</version>
			<fixedVersion>1.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">9459</link>
			<link type="Duplicate" description="is duplicated by">9632</link>
		</links>
	</bug>
	<bug id="9249" opendate="2015-01-04 09:06:42" fixdate="2015-01-09 09:24:00" resolution="Fixed">
		<buginformation>
			<summary>java.lang.ClassCastException: org.apache.hadoop.hive.serde2.io.HiveVarcharWritable cannot be cast to org.apache.hadoop.hive.common.type.HiveVarchar when joining tables</summary>
			<description>VectorColumnAssignFactory doesn&amp;amp;apos;t handle HiveCharWritable / HiveVarcharWritable objects.
Either:
HiveVarcharWritable cannot be cast to ... HiveVarchar
or
HiveCharWritable cannot be cast to ... HiveChar


Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.io.HiveVarcharWritable cannot be cast to org.apache.hadoop.hive.common.type.HiveVarchar
	at org.apache.hadoop.hive.ql.exec.vector.VectorColumnAssignFactory$17.assignObjectValue(VectorColumnAssignFactory.java:417)
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.internalForward(VectorMapJoinOperator.java:196)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genAllOneUniqueJoinObject(CommonJoinOperator.java:670)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject(CommonJoinOperator.java:748)
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:299)
	... 24 more

</description>
			<version>0.14.0</version>
			<fixedVersion>1.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorColumnAssignFactory.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">9739</link>
		</links>
	</bug>
	<bug id="9194" opendate="2014-12-22 23:43:31" fixdate="2015-01-15 22:16:23" resolution="Fixed">
		<buginformation>
			<summary>Support select distinct *</summary>
			<description>As per Laljo John Pullokkaran&amp;amp;apos;s review comments, implement select distinct *</description>
			<version>0.6.0</version>
			<fixedVersion>1.1.0</fixedVersion>
			<type>Sub-task</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.UnparseTranslator.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">1344</link>
			<link type="Duplicate" description="is duplicated by">1654</link>
			<link type="Reference" description="is related to">1344</link>
			<link type="Reference" description="is related to">1654</link>
			<link type="Reference" description="is related to">3167</link>
			<link type="Reference" description="is related to">3199</link>
			<link type="Reference" description="is related to">3288</link>
		</links>
	</bug>
	<bug id="1344" opendate="2010-05-14 00:03:06" fixdate="2015-01-19 18:11:01" resolution="Duplicate">
		<buginformation>
			<summary>error in select disinct</summary>
			<description> from T a select distinct a.* where a.ds=&amp;amp;apos;2010-05-01&amp;amp;apos;;
gets a error</description>
			<version>0.6.0</version>
			<fixedVersion>1.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.UnparseTranslator.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="is blocked by">919</link>
			<link type="Duplicate" description="duplicates">9194</link>
			<link type="Reference" description="relates to">9194</link>
		</links>
	</bug>
	<bug id="1654" opendate="2010-09-17 22:31:27" fixdate="2015-01-19 18:12:50" resolution="Duplicate">
		<buginformation>
			<summary>select distinct should allow column name regex</summary>
			<description>This works (matching column name foo):
select `fo.*` from pokes;
but this
select distinct `fo.*` from pokes;
gives
FAILED: Error in semantic analysis: line 1:16 Invalid Table Alias or Column Reference `fo.*`
It should work consistently.</description>
			<version>0.6.0</version>
			<fixedVersion>1.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.UnparseTranslator.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">9194</link>
			<link type="Reference" description="relates to">9194</link>
		</links>
	</bug>
	<bug id="9357" opendate="2015-01-13 03:41:04" fixdate="2015-01-19 19:44:50" resolution="Fixed">
		<buginformation>
			<summary>Create ADD_MONTHS UDF</summary>
			<description>ADD_MONTHS adds a number of months to startdate: 
add_months(&amp;amp;apos;2015-01-14&amp;amp;apos;, 1) = &amp;amp;apos;2015-02-14&amp;amp;apos;
add_months(&amp;amp;apos;2015-01-31&amp;amp;apos;, 1) = &amp;amp;apos;2015-02-28&amp;amp;apos;
add_months(&amp;amp;apos;2015-02-28&amp;amp;apos;, 2) = &amp;amp;apos;2015-04-30&amp;amp;apos;
add_months(&amp;amp;apos;2015-02-28&amp;amp;apos;, 12) = &amp;amp;apos;2016-02-29&amp;amp;apos;</description>
			<version>0.9.0</version>
			<fixedVersion>1.1.0</fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">3942</link>
		</links>
	</bug>
	<bug id="1869" opendate="2010-12-28 06:06:29" fixdate="2015-01-21 23:58:41" resolution="Fixed">
		<buginformation>
			<summary>TestMTQueries failing on jenkins</summary>
			<description>TestMTQueries has been failing intermittently on Hudson. The first failure I can find
a record of on Hudson is from svn rev 1052414 on December 24th, but it&amp;amp;apos;s 
likely that the failures actually started earlier.</description>
			<version>0.15.0</version>
			<fixedVersion>1.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.QTestUtil.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">9119</link>
		</links>
	</bug>
	<bug id="8485" opendate="2014-10-16 12:59:45" fixdate="2015-01-26 23:23:29" resolution="Fixed">
		<buginformation>
			<summary>HMS on Oracle incompatibility</summary>
			<description>Oracle does not distinguish between empty strings and NULL,which proves problematic for DataNucleus.
In the event a user creates a table with some property stored as an empty string the table will no longer be accessible.
i.e. TBLPROPERTIES (&amp;amp;apos;serialization.null.format&amp;amp;apos;=&amp;amp;apos;&amp;amp;apos;)
If they try to select, describe, drop, etc the client prints the following exception.
ERROR ql.Driver: FAILED: SemanticException [Error 10001]: Table not found &amp;lt;table name&amp;gt;
The work around for this was to go into the hive metastore on the Oracle database and replace NULL with some other string. Users could then drop the tables or alter their data to use the new null format they just set.</description>
			<version>1.1.0</version>
			<fixedVersion>1.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">14116</link>
			<link type="Reference" description="relates to">12476</link>
		</links>
	</bug>
	<bug id="9459" opendate="2015-01-26 00:03:31" fixdate="2015-01-28 02:00:18" resolution="Duplicate">
		<buginformation>
			<summary>Concat plus date functions appear to be broken in 0.14</summary>
			<description>In the below example I create year_month and month_year vars. These each should be yyyymm and mmyyyy integer strings but it appears as if hive is calling the first function twice such that it is returning mmmm and yyyyyyyy.
hive&amp;gt; select
    &amp;gt; month(a.joined) month,
    &amp;gt; year(a.joined) year,
    &amp;gt; concat(cast(year(a.joined) as string),cast(month(a.joined) as string)) year_month,
    &amp;gt; concat(cast(month(a.joined) as string),cast(year(a.joined) as string)) month_year
    &amp;gt; from a limit 20;
OK
month	year	year_month	month_year
7	2014	20142014	77
7	2014	20142014	77
7	2014	20142014	77
7	2014	20142014	77
7	2014	20142014	77
7	2014	20142014	77
7	2014	20142014	77
7	2014	20142014	77
7	2014	20142014	77
7	2014	20142014	77
7	2014	20142014	77
7	2014	20142014	77
7	2014	20142014	77
7	2014	20142014	77
7	2014	20142014	77
7	2014	20142014	77
7	2014	20142014	77
7	2014	20142014	77
7	2014	20142014	77
7	2014	20142014	77
Time taken: 0.109 seconds, Fetched: 20 row(s)
Other users appear to experience similar issues in this stack overflow: http://stackoverflow.com/questions/27740866/convert-date-to-decimal-format-in-hive .
I tested this in 0.13 and 0.14 and it does not appear to be an issue in 0.13.
I looked around and could not find a similar issue so hopefully this is not a duplicate.</description>
			<version>0.14.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">9278</link>
		</links>
	</bug>
	<bug id="9616" opendate="2015-02-09 05:21:49" fixdate="2015-02-09 09:59:25" resolution="Duplicate">
		<buginformation>
			<summary>Hive 0.14</summary>
			<description>Hi, 
I am using hive 0.14 version which will support all crud operation as said by support team
I am not able to select specific columns to insert, like 
insert into table table1 id,name,sal select id,name,sal from table2 where table1.id = table2.id</description>
			<version>0.14.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.QBMetaData.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.TestIUD.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">9481</link>
		</links>
	</bug>
	<bug id="9598" opendate="2015-02-06 18:52:50" fixdate="2015-02-12 07:33:14" resolution="Duplicate">
		<buginformation>
			<summary>java.lang.IllegalMonitorStateException/java.util.concurrent.locks.ReentrantLock$Sync.tryRelease if ResultSet.closed called after Statement.close called</summary>
			<description>http://docs.oracle.com/javase/7/docs/api/java/sql/ResultSet.html#close()
http://docs.oracle.com/javase/7/docs/api/java/sql/Statement.html#close()
		Statement stmt;
		try {
			stmt = dbConnection.createStatement();
			stmt.executeQuery("select* from t");
			ResultSet rs = stmt.getResultSet();
			stmt.close();
			if (rs != null) {
				System.out.println("IS NOT NULL");
// Hive does not implement isClosed()
//				if (!rs.isClosed()) 
{
//					System.out.println("IS NOT CLOSED");
//				}
				rs.close();
			}
		} catch (SQLException e) 
{
			// TODO Auto-generated catch block
			e.printStackTrace();
		}

Exception in thread "main" java.lang.IllegalMonitorStateException
	at java.util.concurrent.locks.ReentrantLock$Sync.tryRelease(ReentrantLock.java:166)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.release(AbstractQueuedSynchronizer.java:1271)
	at java.util.concurrent.locks.ReentrantLock.unlock(ReentrantLock.java:471)
	at org.apache.hive.jdbc.HiveStatement.closeClientOperation(HiveStatement.java:175)
	at org.apache.hive.jdbc.HiveQueryResultSet.close(HiveQueryResultSet.java:293)
/D:/JDBC/Hortonworks_Hive13/commons-configuration-1.6.jar
/D:/JDBC/Hortonworks_Hive13/commons-logging-1.1.3.jar
/D:/JDBC/Hortonworks_Hive13/hadoop-common-2.4.0.2.1.1.0-385.jar
/D:/JDBC/Hortonworks_Hive13/hive-exec-0.13.0.2.1.1.0-385.jar
/D:/JDBC/Hortonworks_Hive13/hive-jdbc-0.13.0.2.1.1.0-385.jar
/D:/JDBC/Hortonworks_Hive13/hive-service-0.13.0.2.1.1.0-385.jar
/D:/JDBC/Hortonworks_Hive13/httpclient-4.2.5.jar
/D:/JDBC/Hortonworks_Hive13/httpcore-4.2.5.jar
/D:/JDBC/Hortonworks_Hive13/libfb303-0.9.0.jar
/D:/JDBC/Hortonworks_Hive13/libthrift-0.9.0.jar
/D:/JDBC/Hortonworks_Hive13/log4j-1.2.16.jar
/D:/JDBC/Hortonworks_Hive13/slf4j-api-1.7.5.jar
/D:/JDBC/Hortonworks_Hive13/slf4j-log4j12-1.7.5.jar</description>
			<version>0.13.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.jdbc.HiveStatement.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">7303</link>
		</links>
	</bug>
	<bug id="9632" opendate="2015-02-10 01:00:45" fixdate="2015-02-12 08:01:51" resolution="Duplicate">
		<buginformation>
			<summary>inconsistent results between year(), month(), day(), and the actual values in formulas</summary>
			<description>In wanting to create a date dimension value which would match our existing database environment, I figured I would be able to do as I have done in the past and use the following formula:
(year(date)*10000)+(month(date)*100)+day(date)
Given the date of 2015-01-09, the above formula should result in a value of 20150109.  Instead, the resulting value is 20353515.
SELECT
                          &amp;gt; adjusted_activity_date_utc,
                          &amp;gt; year(adjusted_activity_date_utc),
                          &amp;gt; month(adjusted_activity_date_utc),
                          &amp;gt; day(adjusted_activity_date_utc),
                          &amp;gt; (year(adjusted_activity_date_utc)*10000)+(month(adjusted_activity_date_utc)*100)+day(adjusted_activity_date_utc),
                          &amp;gt; (year(adjusted_activity_date_utc)*10000),
                          &amp;gt; (month(adjusted_activity_date_utc)*100),
                          &amp;gt; day(adjusted_activity_date_utc)
                          &amp;gt; from event_histories limit 5;
OK
adjusted_activity_date_utc	_c1	_c2	_c3	_c4	_c5	_c6	_c7
2015-01-09	2015	1	9	20353515	20150000	100	9
2015-01-09	2015	1	9	20353515	20150000	100	9
2015-01-09	2015	1	9	20353515	20150000	100	9
2015-01-09	2015	1	9	20353515	20150000	100	9
2015-01-09	2015	1	9	20353515	20150000	100	9
Oddly enough, this works as expected when a specific date value is used for the column.
I have tried this with partition and non-partition columns and found the result to be the same.
SELECT
                          &amp;gt; adjusted_activity_date_utc,
                          &amp;gt; year(adjusted_activity_date_utc),
                          &amp;gt; month(adjusted_activity_date_utc),
                          &amp;gt; day(adjusted_activity_date_utc),
                          &amp;gt; (year(adjusted_activity_date_utc)*10000)+(month(adjusted_activity_date_utc)*100)+day(adjusted_activity_date_utc),
                          &amp;gt; (year(adjusted_activity_date_utc)*10000),
                          &amp;gt; (month(adjusted_activity_date_utc)*100),
                          &amp;gt; day(adjusted_activity_date_utc)
                          &amp;gt; from event_histories
                          &amp;gt; where adjusted_activity_date_utc = &amp;amp;apos;2015-01-09&amp;amp;apos;
                          &amp;gt; limit 5;
OK
adjusted_activity_date_utc	_c1	_c2	_c3	_c4	_c5	_c6	_c7
2015-01-09	2015	1	9	20150109	20150000	100	9
2015-01-09	2015	1	9	20150109	20150000	100	9
2015-01-09	2015	1	9	20150109	20150000	100	9
2015-01-09	2015	1	9	20150109	20150000	100	9
2015-01-09	2015	1	9	20150109	20150000	100	9
</description>
			<version>0.14.0</version>
			<fixedVersion>1.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">9278</link>
		</links>
	</bug>
	<bug id="9669" opendate="2015-02-12 12:33:18" fixdate="2015-02-12 22:08:21" resolution="Duplicate">
		<buginformation>
			<summary>selected columns</summary>
			<description>Hi Team,
In Hive 1.0, selected columns patch is updated?
Because i am not able to insert selected columns.
I am using Hive 1.0 bin, how can i apply patch directly in Hive instead of trunk or source.
Thanks in advance.</description>
			<version>0.14.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.QBMetaData.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.TestIUD.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">9481</link>
		</links>
	</bug>
	<bug id="9481" opendate="2015-01-27 22:48:33" fixdate="2015-02-13 18:32:23" resolution="Fixed">
		<buginformation>
			<summary>allow column list specification in INSERT statement</summary>
			<description>Given a table FOO(a int, b int, c int), ANSI SQL supports insert into FOO(c,b) select x,y from T.  The expectation is that &amp;amp;apos;x&amp;amp;apos; is written to column &amp;amp;apos;c&amp;amp;apos; and &amp;amp;apos;y&amp;amp;apos; is written column &amp;amp;apos;b&amp;amp;apos; and &amp;amp;apos;a&amp;amp;apos; is set to NULL, assuming column &amp;amp;apos;a&amp;amp;apos; is NULLABLE.
Hive does not support this.  In Hive one has to ensure that the data producing statement has a schema that matches target table schema.
Since Hive doesn&amp;amp;apos;t support DEFAULT value for columns in CREATE TABLE, when target schema is explicitly provided, missing columns will be set to NULL if they are NULLABLE, otherwise an error will be raised.
If/when DEFAULT clause is supported, this can be enhanced to set default value rather than NULL.
Thus, given 

create table source (a int, b int);
create table target (x int, y int, z int);
create table target2 (x int, y int, z int);



insert into target(y,z) select * from source;

will mean 

insert into target select null as x, a, b from source;

and 

insert into target(z,y) select * from source;

will meant 

insert into target select null as x, b, a from source;

Also,

from source 
  insert into target(y,z) select null as x, * 
  insert into target2(y,z) select null as x, source.*;


and for partitioned tables, given

Given:
CREATE TABLE pageviews (userid VARCHAR(64), link STRING, "from" STRING)
  PARTITIONED BY (datestamp STRING) CLUSTERED BY (userid) INTO 256 BUCKETS STORED AS ORC;

INSERT INTO TABLE pageviews PARTITION (datestamp = &amp;amp;apos;2014-09-23&amp;amp;apos;)(userid,link)  
   VALUES (&amp;amp;apos;jsmith&amp;amp;apos;, &amp;amp;apos;mail.com&amp;amp;apos;);


And dynamic partitioning

INSERT INTO TABLE pageviews PARTITION (datestamp)(userid,datestamp,link) 
    VALUES (&amp;amp;apos;jsmith&amp;amp;apos;, &amp;amp;apos;2014-09-23&amp;amp;apos;, &amp;amp;apos;mail.com&amp;amp;apos;);


In all cases, the schema specification contains columns of the target table which are matched by position to the values produced by VALUES clause/SELECT statement.  If the producer side provides values for a dynamic partition column, the column should be in the specified schema.  Static partition values are part of the partition spec and thus are not produced by the producer and should not be part of the schema specification.</description>
			<version>0.14.0</version>
			<fixedVersion>1.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.QBMetaData.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.TestIUD.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">9616</link>
			<link type="Duplicate" description="is duplicated by">9669</link>
			<link type="Reference" description="relates to">7646</link>
			<link type="Reference" description="is related to">10776</link>
			<link type="Reference" description="is related to">10828</link>
		</links>
	</bug>
	<bug id="9685" opendate="2015-02-13 16:05:11" fixdate="2015-02-15 01:20:03" resolution="Fixed">
		<buginformation>
			<summary>CLIService should create SessionState after logging into kerberos</summary>
			<description>
javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
        at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:211)
        at org.apache.thrift.transport.TSaslClientTransport.handleSaslStartMessage(TSaslClientTransport.java:94)
        at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:271)
        at org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37)
        at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:52)
        at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:49)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
        at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport.open(TUGIAssumingTransport.java:49)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:409)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&amp;lt;init&amp;gt;(HiveMetaStoreClient.java:230)
        at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&amp;lt;init&amp;gt;(SessionHiveMetaStoreClient.java:74)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
        at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1483)
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&amp;lt;init&amp;gt;(RetryingMetaStoreClient.java:64)
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:74)
        at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2841)
        at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2860)
        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:453)
        at org.apache.hive.service.cli.CLIService.applyAuthorizationConfigPolicy(CLIService.java:123)
        at org.apache.hive.service.cli.CLIService.init(CLIService.java:81)
        at org.apache.hive.service.CompositeService.init(CompositeService.java:59)
        at org.apache.hive.service.server.HiveServer2.init(HiveServer2.java:92)
        at org.apache.hive.service.server.HiveServer2.startHiveServer2(HiveServer2.java:309)
        at org.apache.hive.service.server.HiveServer2.access$400(HiveServer2.java:68)
        at org.apache.hive.service.server.HiveServer2$StartOptionExecutor.execute(HiveServer2.java:523)
        at org.apache.hive.service.server.HiveServer2.main(HiveServer2.java:396)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:483)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)

</description>
			<version>1.1.0</version>
			<fixedVersion>1.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.service.cli.CLIService.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">10240</link>
			<link type="Reference" description="is related to">1808</link>
		</links>
	</bug>
	<bug id="9546" opendate="2015-02-02 17:28:21" fixdate="2015-02-18 18:13:53" resolution="Duplicate">
		<buginformation>
			<summary>Create table taking substantially longer time when other select queries are run in parallel.</summary>
			<description>Create table taking substantially longer time when other select queries are run in parallel.
We were able to reproduce the issue using beeline in two sessions.
Beeline Shell 1: 
 a) create table with no other queries running on hive ( took approximately 0.313 seconds)
 b) Insert Data into the table
 c) Run a select count query on the above table
Beeline Shell 2: 
 a) create table while step c) is running in the Beeline Shell 1. (took approximately 60.431 seconds)</description>
			<version>0.13.1</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">9199</link>
		</links>
	</bug>
	<bug id="9655" opendate="2015-02-11 20:58:13" fixdate="2015-02-24 18:48:15" resolution="Fixed">
		<buginformation>
			<summary>Dynamic partition table insertion error</summary>
			<description>We have these two tables:


create table t1 (c1 bigint, c2 string);

CREATE TABLE t2 (c1 int, c2 string)
PARTITIONED BY (p1 string);

load data local inpath &amp;amp;apos;data&amp;amp;apos; into table t1;
load data local inpath &amp;amp;apos;data&amp;amp;apos; into table t1;
load data local inpath &amp;amp;apos;data&amp;amp;apos; into table t1;
load data local inpath &amp;amp;apos;data&amp;amp;apos; into table t1;
load data local inpath &amp;amp;apos;data&amp;amp;apos; into table t1;


But, when try to insert into table t2 from t1:


SET hive.exec.dynamic.partition.mode=nonstrict;
insert overwrite table t2 partition(p1) select *,c1 as p1 from t1 distribute by p1;


The query failed with the following exception:

2015-02-11 12:50:52,756 ERROR [LocalJobRunner Map Task Executor #0]: mr.ExecMapper (ExecMapper.java:map(178)) - org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {"c1":1,"c2":"one"}
  at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:503)
  at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:170)
  at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
  at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)
  at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)
  at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
  at java.util.concurrent.FutureTask.run(FutureTask.java:262)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
  at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: cannot find field _col2 from [0:_col0, 1:_col1]
  at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.processOp(ReduceSinkOperator.java:397)
  at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)
  at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)
  at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)
  at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95)
  at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:157)
  at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:493)
  ... 10 more
Caused by: java.lang.RuntimeException: cannot find field _col2 from [0:_col0, 1:_col1]
  at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.getStandardStructFieldRef(ObjectInspectorUtils.java:410)
  at org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getStructFieldRef(StandardStructObjectInspector.java:147)
  at org.apache.hadoop.hive.ql.exec.ExprNodeColumnEvaluator.initialize(ExprNodeColumnEvaluator.java:55)
  at org.apache.hadoop.hive.ql.exec.Operator.initEvaluators(Operator.java:954)
  at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.processOp(ReduceSinkOperator.java:325)
  ... 16 more

</description>
			<version>1.1.0</version>
			<fixedVersion>1.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">9718</link>
		</links>
	</bug>
	<bug id="9624" opendate="2015-02-09 19:21:04" fixdate="2015-03-03 02:17:00" resolution="Duplicate">
		<buginformation>
			<summary>NullPointerException in MapJoinOperator.processOp(MapJoinOperator.java:253) for TPC-DS Q75 against un-partitioned schema</summary>
			<description>Running TPC-DS Q75 against a non-partitioned schema fails with 


Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unexpected exception: null
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:314)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.internalForward(CommonJoinOperator.java:638)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.createForwardJoinObject(CommonJoinOperator.java:433)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genObject(CommonJoinOperator.java:525)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genObject(CommonJoinOperator.java:522)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genJoinObject(CommonJoinOperator.java:451)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject(CommonJoinOperator.java:752)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinObject(CommonMergeJoinOperator.java:248)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinOneGroup(CommonMergeJoinOperator.java:213)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.processOp(CommonMergeJoinOperator.java:196)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:328)
	... 16 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:253)
	... 27 more
]], Vertex failed as one or more tasks failed. failedTasks:1, Vertex vertex_1422895755428_0924_1_29 [Reducer 27] killed/failed due to:null]


This line maps to hashMapRowGetters = new ReusableGetAdaptor[mapJoinTables.length] in the code snippet below


     alias = (byte) tag;
      if (hashMapRowGetters == null) {
        hashMapRowGetters = new ReusableGetAdaptor[mapJoinTables.length];
        MapJoinKey refKey = getRefKey(alias);
        for (byte pos = 0; pos &amp;lt; order.length; pos++) {
          if (pos != alias) {
            hashMapRowGetters[pos] = mapJoinTables[pos].createGetter(refKey);
          }
        }
      }


Query 



WITH all_sales AS (
 SELECT d_year
       ,i_brand_id
       ,i_class_id
       ,i_category_id
       ,i_manufact_id
       ,SUM(sales_cnt) AS sales_cnt
       ,SUM(sales_amt) AS sales_amt
 FROM (SELECT d_year
             ,i_brand_id
             ,i_class_id
             ,i_category_id
             ,i_manufact_id
             ,cs_quantity - COALESCE(cr_return_quantity,0) AS sales_cnt
             ,cs_ext_sales_price - COALESCE(cr_return_amount,0.0) AS sales_amt
       FROM catalog_sales JOIN item ON i_item_sk=cs_item_sk
                          JOIN date_dim ON d_date_sk=cs_sold_date_sk
                          LEFT JOIN catalog_returns ON (cs_order_number=cr_order_number 
                                                    AND cs_item_sk=cr_item_sk)
       WHERE i_category=&amp;amp;apos;Sports&amp;amp;apos;
       UNION ALL
       SELECT d_year
             ,i_brand_id
             ,i_class_id
             ,i_category_id
             ,i_manufact_id
             ,ss_quantity - COALESCE(sr_return_quantity,0) AS sales_cnt
             ,ss_ext_sales_price - COALESCE(sr_return_amt,0.0) AS sales_amt
       FROM store_sales JOIN item ON i_item_sk=ss_item_sk
                        JOIN date_dim ON d_date_sk=ss_sold_date_sk
                        LEFT JOIN store_returns ON (ss_ticket_number=sr_ticket_number 
                                                AND ss_item_sk=sr_item_sk)
       WHERE i_category=&amp;amp;apos;Sports&amp;amp;apos;
       UNION ALL
       SELECT d_year
             ,i_brand_id
             ,i_class_id
             ,i_category_id
             ,i_manufact_id
             ,ws_quantity - COALESCE(wr_return_quantity,0) AS sales_cnt
             ,ws_ext_sales_price - COALESCE(wr_return_amt,0.0) AS sales_amt
       FROM web_sales JOIN item ON i_item_sk=ws_item_sk
                      JOIN date_dim ON d_date_sk=ws_sold_date_sk
                      LEFT JOIN web_returns ON (ws_order_number=wr_order_number 
                                            AND ws_item_sk=wr_item_sk)
       WHERE i_category=&amp;amp;apos;Sports&amp;amp;apos;) sales_detail
 GROUP BY d_year, i_brand_id, i_class_id, i_category_id, i_manufact_id)
 SELECT  prev_yr.d_year AS prev_year
                          ,curr_yr.d_year AS year
                          ,curr_yr.i_brand_id
                          ,curr_yr.i_class_id
                          ,curr_yr.i_category_id
                          ,curr_yr.i_manufact_id
                          ,prev_yr.sales_cnt AS prev_yr_cnt
                          ,curr_yr.sales_cnt AS curr_yr_cnt
                          ,curr_yr.sales_cnt-prev_yr.sales_cnt AS sales_cnt_diff
                          ,curr_yr.sales_amt-prev_yr.sales_amt AS sales_amt_diff
 FROM all_sales curr_yr, all_sales prev_yr
 WHERE curr_yr.i_brand_id=prev_yr.i_brand_id
   AND curr_yr.i_class_id=prev_yr.i_class_id
   AND curr_yr.i_category_id=prev_yr.i_category_id
   AND curr_yr.i_manufact_id=prev_yr.i_manufact_id
   AND curr_yr.d_year=2002
   AND prev_yr.d_year=2002-1
   AND CAST(curr_yr.sales_cnt AS DECIMAL(17,2))/CAST(prev_yr.sales_cnt AS DECIMAL(17,2))&amp;lt;0.9
 ORDER BY sales_cnt_diff
 limit 100


explain 


STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
      Edges:
        Map 1 &amp;lt;- Map 6 (BROADCAST_EDGE)
        Map 14 &amp;lt;- Map 16 (BROADCAST_EDGE)
        Map 18 &amp;lt;- Reducer 15 (BROADCAST_EDGE), Union 3 (CONTAINS)
        Map 19 &amp;lt;- Map 23 (BROADCAST_EDGE)
        Map 26 &amp;lt;- Map 28 (BROADCAST_EDGE)
        Map 31 &amp;lt;- Map 33 (BROADCAST_EDGE)
        Map 35 &amp;lt;- Reducer 32 (BROADCAST_EDGE), Union 21 (CONTAINS)
        Map 9 &amp;lt;- Map 11 (BROADCAST_EDGE)
        Reducer 10 &amp;lt;- Map 12 (SIMPLE_EDGE), Map 13 (BROADCAST_EDGE), Map 9 (SIMPLE_EDGE), Union 3 (CONTAINS)
        Reducer 15 &amp;lt;- Map 14 (SIMPLE_EDGE), Map 17 (SIMPLE_EDGE)
        Reducer 2 &amp;lt;- Map 1 (SIMPLE_EDGE), Map 7 (SIMPLE_EDGE), Map 8 (BROADCAST_EDGE), Union 3 (CONTAINS)
        Reducer 20 &amp;lt;- Map 19 (SIMPLE_EDGE), Map 24 (SIMPLE_EDGE), Map 25 (BROADCAST_EDGE), Union 21 (CONTAINS)
        Reducer 22 &amp;lt;- Union 21 (SIMPLE_EDGE)
        Reducer 27 &amp;lt;- Map 26 (SIMPLE_EDGE), Map 29 (SIMPLE_EDGE), Map 30 (BROADCAST_EDGE), Union 21 (CONTAINS)
        Reducer 32 &amp;lt;- Map 31 (SIMPLE_EDGE), Map 34 (SIMPLE_EDGE)
        Reducer 4 &amp;lt;- Reducer 22 (BROADCAST_EDGE), Union 3 (SIMPLE_EDGE)
        Reducer 5 &amp;lt;- Reducer 4 (SIMPLE_EDGE)
      DagName: mmokhtar_20150207174141_8f167b31-c893-4c6e-86d6-855d20744d92:1
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: catalog_sales
                  filterExpr: (cs_sold_date_sk is not null and cs_item_sk is not null) (type: boolean)
                  Statistics: Num rows: 817736652 Data size: 16354733056 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: (cs_sold_date_sk is not null and cs_item_sk is not null) (type: boolean)
                    Statistics: Num rows: 817736652 Data size: 16348976724 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: cs_sold_date_sk (type: int), cs_item_sk (type: int), cs_order_number (type: int), cs_quantity (type: int), cs_ext_sales_price (type: float)
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4
                      Statistics: Num rows: 817736652 Data size: 16348976724 Basic stats: COMPLETE Column stats: COMPLETE
                      Map Join Operator
                        condition map:
                             Inner Join 0 to 1
                        keys:
                          0 _col0 (type: int)
                          1 _col0 (type: int)
                        outputColumnNames: _col1, _col2, _col3, _col4, _col6
                        input vertices:
                          1 Map 6
                        Statistics: Num rows: 148779579 Data size: 2380473264 Basic stats: COMPLETE Column stats: COMPLETE
                        Reduce Output Operator
                          key expressions: _col2 (type: int), _col1 (type: int)
                          sort order: ++
                          Map-reduce partition columns: _col2 (type: int), _col1 (type: int)
                          Statistics: Num rows: 148779579 Data size: 2380473264 Basic stats: COMPLETE Column stats: COMPLETE
                          value expressions: _col3 (type: int), _col4 (type: float), _col6 (type: int)
            Execution mode: vectorized
        Map 11 
            Map Operator Tree:
                TableScan
                  alias: date_dim
                  filterExpr: ((d_year = 2002) and d_date_sk is not null) (type: boolean)
                  Statistics: Num rows: 45363 Data size: 362905 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: ((d_year = 2002) and d_date_sk is not null) (type: boolean)
                    Statistics: Num rows: 405 Data size: 3240 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: d_date_sk (type: int)
                      outputColumnNames: _col0
                      Statistics: Num rows: 405 Data size: 1620 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Statistics: Num rows: 405 Data size: 1620 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: 2002 (type: int)
            Execution mode: vectorized
        Map 12 
            Map Operator Tree:
                TableScan
                  alias: store_returns
                  filterExpr: sr_item_sk is not null (type: boolean)
                  Statistics: Num rows: 167243952 Data size: 2675903232 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: sr_item_sk is not null (type: boolean)
                    Statistics: Num rows: 167243952 Data size: 2667828428 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: sr_item_sk (type: int), sr_ticket_number (type: int), sr_return_quantity (type: int), sr_return_amt (type: float)
                      outputColumnNames: _col0, _col1, _col2, _col3
                      Statistics: Num rows: 167243952 Data size: 2667828428 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col1 (type: int), _col0 (type: int)
                        sort order: ++
                        Map-reduce partition columns: _col1 (type: int), _col0 (type: int)
                        Statistics: Num rows: 167243952 Data size: 2667828428 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col2 (type: int), _col3 (type: float)
            Execution mode: vectorized
        Map 13 
            Map Operator Tree:
                TableScan
                  alias: item
                  filterExpr: ((((((i_category = &amp;amp;apos;Sports&amp;amp;apos;) and i_item_sk is not null) and i_brand_id is not null) and i_class_id is not null) and i_category_id is not null) and i_manufact_id is not null) (type: boolean)
                  Statistics: Num rows: 118835 Data size: 3089722 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: ((((((i_category = &amp;amp;apos;Sports&amp;amp;apos;) and i_item_sk is not null) and i_brand_id is not null) and i_class_id is not null) and i_category_id is not null) and i_manufact_id is not null) (type: boolean)
                    Statistics: Num rows: 11836 Data size: 1301772 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: i_item_sk (type: int), i_brand_id (type: int), i_class_id (type: int), i_category_id (type: int), i_manufact_id (type: int)
                      outputColumnNames: _col0, _col1, _col2, _col3, _col5
                      Statistics: Num rows: 11836 Data size: 236532 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Statistics: Num rows: 11836 Data size: 236532 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col1 (type: int), _col2 (type: int), _col3 (type: int), _col5 (type: int)
            Execution mode: vectorized
        Map 14 
            Map Operator Tree:
                TableScan
                  alias: web_sales
                  filterExpr: (ws_sold_date_sk is not null and ws_item_sk is not null) (type: boolean)
                  Statistics: Num rows: 447759411 Data size: 8955188224 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: (ws_sold_date_sk is not null and ws_item_sk is not null) (type: boolean)
                    Statistics: Num rows: 447759411 Data size: 8955044072 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: ws_sold_date_sk (type: int), ws_item_sk (type: int), ws_order_number (type: int), ws_quantity (type: int), ws_ext_sales_price (type: float)
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4
                      Statistics: Num rows: 447759411 Data size: 8955044072 Basic stats: COMPLETE Column stats: COMPLETE
                      Map Join Operator
                        condition map:
                             Inner Join 0 to 1
                        keys:
                          0 _col0 (type: int)
                          1 _col0 (type: int)
                        outputColumnNames: _col1, _col2, _col3, _col4, _col6
                        input vertices:
                          1 Map 16
                        Statistics: Num rows: 81465661 Data size: 1303450576 Basic stats: COMPLETE Column stats: COMPLETE
                        Reduce Output Operator
                          key expressions: _col2 (type: int), _col1 (type: int)
                          sort order: ++
                          Map-reduce partition columns: _col2 (type: int), _col1 (type: int)
                          Statistics: Num rows: 81465661 Data size: 1303450576 Basic stats: COMPLETE Column stats: COMPLETE
                          value expressions: _col3 (type: int), _col4 (type: float), _col6 (type: int)
            Execution mode: vectorized
        Map 16 
            Map Operator Tree:
                TableScan
                  alias: date_dim
                  filterExpr: ((d_year = 2002) and d_date_sk is not null) (type: boolean)
                  Statistics: Num rows: 45363 Data size: 362905 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: ((d_year = 2002) and d_date_sk is not null) (type: boolean)
                    Statistics: Num rows: 405 Data size: 3240 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: d_date_sk (type: int)
                      outputColumnNames: _col0
                      Statistics: Num rows: 405 Data size: 1620 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Statistics: Num rows: 405 Data size: 1620 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: 2002 (type: int)
            Execution mode: vectorized
        Map 17 
            Map Operator Tree:
                TableScan
                  alias: web_returns
                  filterExpr: wr_item_sk is not null (type: boolean)
                  Statistics: Num rows: 50457044 Data size: 807312704 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: wr_item_sk is not null (type: boolean)
                    Statistics: Num rows: 50457044 Data size: 804725540 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: wr_item_sk (type: int), wr_order_number (type: int), wr_return_quantity (type: int), wr_return_amt (type: float)
                      outputColumnNames: _col0, _col1, _col2, _col3
                      Statistics: Num rows: 50457044 Data size: 804725540 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col1 (type: int), _col0 (type: int)
                        sort order: ++
                        Map-reduce partition columns: _col1 (type: int), _col0 (type: int)
                        Statistics: Num rows: 50457044 Data size: 804725540 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col2 (type: int), _col3 (type: float)
            Execution mode: vectorized
        Map 18 
            Map Operator Tree:
                TableScan
                  alias: item
                  filterExpr: ((((((i_category = &amp;amp;apos;Sports&amp;amp;apos;) and i_item_sk is not null) and i_brand_id is not null) and i_class_id is not null) and i_category_id is not null) and i_manufact_id is not null) (type: boolean)
                  Filter Operator
                    predicate: ((((((i_category = &amp;amp;apos;Sports&amp;amp;apos;) and i_item_sk is not null) and i_brand_id is not null) and i_class_id is not null) and i_category_id is not null) and i_manufact_id is not null) (type: boolean)
                    Select Operator
                      expressions: i_item_sk (type: int), i_brand_id (type: int), i_class_id (type: int), i_category_id (type: int), i_manufact_id (type: int)
                      outputColumnNames: _col0, _col1, _col2, _col3, _col5
                      Map Join Operator
                        condition map:
                             Inner Join 0 to 1
                        keys:
                          0 _col1 (type: int)
                          1 _col0 (type: int)
                        outputColumnNames: _col3, _col4, _col6, _col9, _col10, _col12, _col13, _col14, _col16
                        input vertices:
                          0 Reducer 15
                        Select Operator
                          expressions: _col6 (type: int), _col12 (type: int), _col13 (type: int), _col14 (type: int), _col16 (type: int), (_col3 - COALESCE(_col9,0)) (type: int), (UDFToDouble(_col4) - COALESCE(_col10,0.0)) (type: double)
                          outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                          Group By Operator
                            aggregations: sum(_col5), sum(_col6)
                            keys: _col0 (type: int), _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)
                            mode: hash
                            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                            Reduce Output Operator
                              key expressions: _col0 (type: int), _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)
                              sort order: +++++
                              Map-reduce partition columns: _col0 (type: int), _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)
                              value expressions: _col5 (type: bigint), _col6 (type: double)
            Execution mode: vectorized
        Map 19 
            Map Operator Tree:
                TableScan
                  alias: catalog_sales
                  filterExpr: (cs_sold_date_sk is not null and cs_item_sk is not null) (type: boolean)
                  Statistics: Num rows: 817736652 Data size: 16354733056 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: (cs_sold_date_sk is not null and cs_item_sk is not null) (type: boolean)
                    Statistics: Num rows: 817736652 Data size: 16348976724 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: cs_sold_date_sk (type: int), cs_item_sk (type: int), cs_order_number (type: int), cs_quantity (type: int), cs_ext_sales_price (type: float)
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4
                      Statistics: Num rows: 817736652 Data size: 16348976724 Basic stats: COMPLETE Column stats: COMPLETE
                      Map Join Operator
                        condition map:
                             Inner Join 0 to 1
                        keys:
                          0 _col0 (type: int)
                          1 _col0 (type: int)
                        outputColumnNames: _col1, _col2, _col3, _col4, _col6
                        input vertices:
                          1 Map 23
                        Statistics: Num rows: 148779579 Data size: 2380473264 Basic stats: COMPLETE Column stats: COMPLETE
                        Reduce Output Operator
                          key expressions: _col2 (type: int), _col1 (type: int)
                          sort order: ++
                          Map-reduce partition columns: _col2 (type: int), _col1 (type: int)
                          Statistics: Num rows: 148779579 Data size: 2380473264 Basic stats: COMPLETE Column stats: COMPLETE
                          value expressions: _col3 (type: int), _col4 (type: float), _col6 (type: int)
            Execution mode: vectorized
        Map 23 
            Map Operator Tree:
                TableScan
                  alias: date_dim
                  filterExpr: ((d_year = 2001) and d_date_sk is not null) (type: boolean)
                  Statistics: Num rows: 45363 Data size: 362905 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: ((d_year = 2001) and d_date_sk is not null) (type: boolean)
                    Statistics: Num rows: 405 Data size: 3240 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: d_date_sk (type: int)
                      outputColumnNames: _col0
                      Statistics: Num rows: 405 Data size: 1620 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Statistics: Num rows: 405 Data size: 1620 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: 2001 (type: int)
            Execution mode: vectorized
        Map 24 
            Map Operator Tree:
                TableScan
                  alias: catalog_returns
                  filterExpr: cr_item_sk is not null (type: boolean)
                  Statistics: Num rows: 108409176 Data size: 1734546816 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: cr_item_sk is not null (type: boolean)
                    Statistics: Num rows: 108409176 Data size: 1729935996 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: cr_item_sk (type: int), cr_order_number (type: int), cr_return_quantity (type: int), cr_return_amount (type: float)
                      outputColumnNames: _col0, _col1, _col2, _col3
                      Statistics: Num rows: 108409176 Data size: 1729935996 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col1 (type: int), _col0 (type: int)
                        sort order: ++
                        Map-reduce partition columns: _col1 (type: int), _col0 (type: int)
                        Statistics: Num rows: 108409176 Data size: 1729935996 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col2 (type: int), _col3 (type: float)
            Execution mode: vectorized
        Map 25 
            Map Operator Tree:
                TableScan
                  alias: item
                  filterExpr: ((((((i_category = &amp;amp;apos;Sports&amp;amp;apos;) and i_item_sk is not null) and i_brand_id is not null) and i_class_id is not null) and i_category_id is not null) and i_manufact_id is not null) (type: boolean)
                  Statistics: Num rows: 118835 Data size: 3089722 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: ((((((i_category = &amp;amp;apos;Sports&amp;amp;apos;) and i_item_sk is not null) and i_brand_id is not null) and i_class_id is not null) and i_category_id is not null) and i_manufact_id is not null) (type: boolean)
                    Statistics: Num rows: 11836 Data size: 1301772 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: i_item_sk (type: int), i_brand_id (type: int), i_class_id (type: int), i_category_id (type: int), i_manufact_id (type: int)
                      outputColumnNames: _col0, _col1, _col2, _col3, _col5
                      Statistics: Num rows: 11836 Data size: 236532 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Statistics: Num rows: 11836 Data size: 236532 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col1 (type: int), _col2 (type: int), _col3 (type: int), _col5 (type: int)
            Execution mode: vectorized
        Map 26 
            Map Operator Tree:
                TableScan
                  alias: store_sales
                  filterExpr: (ss_sold_date_sk is not null and ss_item_sk is not null) (type: boolean)
                  Statistics: Num rows: 1174353612 Data size: 23487072256 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: (ss_sold_date_sk is not null and ss_item_sk is not null) (type: boolean)
                    Statistics: Num rows: 1174353612 Data size: 23383406888 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: ss_sold_date_sk (type: int), ss_item_sk (type: int), ss_ticket_number (type: int), ss_quantity (type: int), ss_ext_sales_price (type: float)
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4
                      Statistics: Num rows: 1174353612 Data size: 23383406888 Basic stats: COMPLETE Column stats: COMPLETE
                      Map Join Operator
                        condition map:
                             Inner Join 0 to 1
                        keys:
                          0 _col0 (type: int)
                          1 _col0 (type: int)
                        outputColumnNames: _col1, _col2, _col3, _col4, _col6
                        input vertices:
                          1 Map 28
                        Statistics: Num rows: 213662719 Data size: 3418603504 Basic stats: COMPLETE Column stats: COMPLETE
                        Reduce Output Operator
                          key expressions: _col2 (type: int), _col1 (type: int)
                          sort order: ++
                          Map-reduce partition columns: _col2 (type: int), _col1 (type: int)
                          Statistics: Num rows: 213662719 Data size: 3418603504 Basic stats: COMPLETE Column stats: COMPLETE
                          value expressions: _col3 (type: int), _col4 (type: float), _col6 (type: int)
            Execution mode: vectorized
        Map 28 
            Map Operator Tree:
                TableScan
                  alias: date_dim
                  filterExpr: ((d_year = 2001) and d_date_sk is not null) (type: boolean)
                  Statistics: Num rows: 45363 Data size: 362905 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: ((d_year = 2001) and d_date_sk is not null) (type: boolean)
                    Statistics: Num rows: 405 Data size: 3240 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: d_date_sk (type: int)
                      outputColumnNames: _col0
                      Statistics: Num rows: 405 Data size: 1620 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Statistics: Num rows: 405 Data size: 1620 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: 2001 (type: int)
            Execution mode: vectorized
        Map 29 
            Map Operator Tree:
                TableScan
                  alias: store_returns
                  filterExpr: sr_item_sk is not null (type: boolean)
                  Statistics: Num rows: 167243952 Data size: 2675903232 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: sr_item_sk is not null (type: boolean)
                    Statistics: Num rows: 167243952 Data size: 2667828428 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: sr_item_sk (type: int), sr_ticket_number (type: int), sr_return_quantity (type: int), sr_return_amt (type: float)
                      outputColumnNames: _col0, _col1, _col2, _col3
                      Statistics: Num rows: 167243952 Data size: 2667828428 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col1 (type: int), _col0 (type: int)
                        sort order: ++
                        Map-reduce partition columns: _col1 (type: int), _col0 (type: int)
                        Statistics: Num rows: 167243952 Data size: 2667828428 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col2 (type: int), _col3 (type: float)
            Execution mode: vectorized
        Map 30 
            Map Operator Tree:
                TableScan
                  alias: item
                  filterExpr: ((((((i_category = &amp;amp;apos;Sports&amp;amp;apos;) and i_item_sk is not null) and i_brand_id is not null) and i_class_id is not null) and i_category_id is not null) and i_manufact_id is not null) (type: boolean)
                  Statistics: Num rows: 118835 Data size: 3089722 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: ((((((i_category = &amp;amp;apos;Sports&amp;amp;apos;) and i_item_sk is not null) and i_brand_id is not null) and i_class_id is not null) and i_category_id is not null) and i_manufact_id is not null) (type: boolean)
                    Statistics: Num rows: 11836 Data size: 1301772 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: i_item_sk (type: int), i_brand_id (type: int), i_class_id (type: int), i_category_id (type: int), i_manufact_id (type: int)
                      outputColumnNames: _col0, _col1, _col2, _col3, _col5
                      Statistics: Num rows: 11836 Data size: 236532 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Statistics: Num rows: 11836 Data size: 236532 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col1 (type: int), _col2 (type: int), _col3 (type: int), _col5 (type: int)
            Execution mode: vectorized
        Map 31 
            Map Operator Tree:
                TableScan
                  alias: web_sales
                  filterExpr: (ws_sold_date_sk is not null and ws_item_sk is not null) (type: boolean)
                  Statistics: Num rows: 447759411 Data size: 8955188224 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: (ws_sold_date_sk is not null and ws_item_sk is not null) (type: boolean)
                    Statistics: Num rows: 447759411 Data size: 8955044072 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: ws_sold_date_sk (type: int), ws_item_sk (type: int), ws_order_number (type: int), ws_quantity (type: int), ws_ext_sales_price (type: float)
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4
                      Statistics: Num rows: 447759411 Data size: 8955044072 Basic stats: COMPLETE Column stats: COMPLETE
                      Map Join Operator
                        condition map:
                             Inner Join 0 to 1
                        keys:
                          0 _col0 (type: int)
                          1 _col0 (type: int)
                        outputColumnNames: _col1, _col2, _col3, _col4, _col6
                        input vertices:
                          1 Map 33
                        Statistics: Num rows: 81465661 Data size: 1303450576 Basic stats: COMPLETE Column stats: COMPLETE
                        Reduce Output Operator
                          key expressions: _col2 (type: int), _col1 (type: int)
                          sort order: ++
                          Map-reduce partition columns: _col2 (type: int), _col1 (type: int)
                          Statistics: Num rows: 81465661 Data size: 1303450576 Basic stats: COMPLETE Column stats: COMPLETE
                          value expressions: _col3 (type: int), _col4 (type: float), _col6 (type: int)
            Execution mode: vectorized
        Map 33 
            Map Operator Tree:
                TableScan
                  alias: date_dim
                  filterExpr: ((d_year = 2001) and d_date_sk is not null) (type: boolean)
                  Statistics: Num rows: 45363 Data size: 362905 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: ((d_year = 2001) and d_date_sk is not null) (type: boolean)
                    Statistics: Num rows: 405 Data size: 3240 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: d_date_sk (type: int)
                      outputColumnNames: _col0
                      Statistics: Num rows: 405 Data size: 1620 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Statistics: Num rows: 405 Data size: 1620 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: 2001 (type: int)
            Execution mode: vectorized
        Map 34 
            Map Operator Tree:
                TableScan
                  alias: web_returns
                  filterExpr: wr_item_sk is not null (type: boolean)
                  Statistics: Num rows: 50457044 Data size: 807312704 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: wr_item_sk is not null (type: boolean)
                    Statistics: Num rows: 50457044 Data size: 804725540 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: wr_item_sk (type: int), wr_order_number (type: int), wr_return_quantity (type: int), wr_return_amt (type: float)
                      outputColumnNames: _col0, _col1, _col2, _col3
                      Statistics: Num rows: 50457044 Data size: 804725540 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col1 (type: int), _col0 (type: int)
                        sort order: ++
                        Map-reduce partition columns: _col1 (type: int), _col0 (type: int)
                        Statistics: Num rows: 50457044 Data size: 804725540 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col2 (type: int), _col3 (type: float)
            Execution mode: vectorized
        Map 35 
            Map Operator Tree:
                TableScan
                  alias: item
                  filterExpr: ((((((i_category = &amp;amp;apos;Sports&amp;amp;apos;) and i_item_sk is not null) and i_brand_id is not null) and i_class_id is not null) and i_category_id is not null) and i_manufact_id is not null) (type: boolean)
                  Filter Operator
                    predicate: ((((((i_category = &amp;amp;apos;Sports&amp;amp;apos;) and i_item_sk is not null) and i_brand_id is not null) and i_class_id is not null) and i_category_id is not null) and i_manufact_id is not null) (type: boolean)
                    Select Operator
                      expressions: i_item_sk (type: int), i_brand_id (type: int), i_class_id (type: int), i_category_id (type: int), i_manufact_id (type: int)
                      outputColumnNames: _col0, _col1, _col2, _col3, _col5
                      Map Join Operator
                        condition map:
                             Inner Join 0 to 1
                        keys:
                          0 _col1 (type: int)
                          1 _col0 (type: int)
                        outputColumnNames: _col3, _col4, _col6, _col9, _col10, _col12, _col13, _col14, _col16
                        input vertices:
                          0 Reducer 32
                        Select Operator
                          expressions: _col6 (type: int), _col12 (type: int), _col13 (type: int), _col14 (type: int), _col16 (type: int), (_col3 - COALESCE(_col9,0)) (type: int), (UDFToDouble(_col4) - COALESCE(_col10,0.0)) (type: double)
                          outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                          Group By Operator
                            aggregations: sum(_col5), sum(_col6)
                            keys: _col0 (type: int), _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)
                            mode: hash
                            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                            Reduce Output Operator
                              key expressions: _col0 (type: int), _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)
                              sort order: +++++
                              Map-reduce partition columns: _col0 (type: int), _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)
                              value expressions: _col5 (type: bigint), _col6 (type: double)
            Execution mode: vectorized
        Map 6 
            Map Operator Tree:
                TableScan
                  alias: date_dim
                  filterExpr: ((d_year = 2002) and d_date_sk is not null) (type: boolean)
                  Statistics: Num rows: 45363 Data size: 362905 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: ((d_year = 2002) and d_date_sk is not null) (type: boolean)
                    Statistics: Num rows: 405 Data size: 3240 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: d_date_sk (type: int)
                      outputColumnNames: _col0
                      Statistics: Num rows: 405 Data size: 1620 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Statistics: Num rows: 405 Data size: 1620 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: 2002 (type: int)
            Execution mode: vectorized
        Map 7 
            Map Operator Tree:
                TableScan
                  alias: catalog_returns
                  filterExpr: cr_item_sk is not null (type: boolean)
                  Statistics: Num rows: 108409176 Data size: 1734546816 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: cr_item_sk is not null (type: boolean)
                    Statistics: Num rows: 108409176 Data size: 1729935996 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: cr_item_sk (type: int), cr_order_number (type: int), cr_return_quantity (type: int), cr_return_amount (type: float)
                      outputColumnNames: _col0, _col1, _col2, _col3
                      Statistics: Num rows: 108409176 Data size: 1729935996 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col1 (type: int), _col0 (type: int)
                        sort order: ++
                        Map-reduce partition columns: _col1 (type: int), _col0 (type: int)
                        Statistics: Num rows: 108409176 Data size: 1729935996 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col2 (type: int), _col3 (type: float)
            Execution mode: vectorized
        Map 8 
            Map Operator Tree:
                TableScan
                  alias: item
                  filterExpr: ((((((i_category = &amp;amp;apos;Sports&amp;amp;apos;) and i_item_sk is not null) and i_brand_id is not null) and i_class_id is not null) and i_category_id is not null) and i_manufact_id is not null) (type: boolean)
                  Statistics: Num rows: 118835 Data size: 3089722 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: ((((((i_category = &amp;amp;apos;Sports&amp;amp;apos;) and i_item_sk is not null) and i_brand_id is not null) and i_class_id is not null) and i_category_id is not null) and i_manufact_id is not null) (type: boolean)
                    Statistics: Num rows: 11836 Data size: 1301772 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: i_item_sk (type: int), i_brand_id (type: int), i_class_id (type: int), i_category_id (type: int), i_manufact_id (type: int)
                      outputColumnNames: _col0, _col1, _col2, _col3, _col5
                      Statistics: Num rows: 11836 Data size: 236532 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Statistics: Num rows: 11836 Data size: 236532 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col1 (type: int), _col2 (type: int), _col3 (type: int), _col5 (type: int)
            Execution mode: vectorized
        Map 9 
            Map Operator Tree:
                TableScan
                  alias: store_sales
                  filterExpr: (ss_sold_date_sk is not null and ss_item_sk is not null) (type: boolean)
                  Statistics: Num rows: 1174353612 Data size: 23487072256 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: (ss_sold_date_sk is not null and ss_item_sk is not null) (type: boolean)
                    Statistics: Num rows: 1174353612 Data size: 23383406888 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: ss_sold_date_sk (type: int), ss_item_sk (type: int), ss_ticket_number (type: int), ss_quantity (type: int), ss_ext_sales_price (type: float)
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4
                      Statistics: Num rows: 1174353612 Data size: 23383406888 Basic stats: COMPLETE Column stats: COMPLETE
                      Map Join Operator
                        condition map:
                             Inner Join 0 to 1
                        keys:
                          0 _col0 (type: int)
                          1 _col0 (type: int)
                        outputColumnNames: _col1, _col2, _col3, _col4, _col6
                        input vertices:
                          1 Map 11
                        Statistics: Num rows: 213662719 Data size: 3418603504 Basic stats: COMPLETE Column stats: COMPLETE
                        Reduce Output Operator
                          key expressions: _col2 (type: int), _col1 (type: int)
                          sort order: ++
                          Map-reduce partition columns: _col2 (type: int), _col1 (type: int)
                          Statistics: Num rows: 213662719 Data size: 3418603504 Basic stats: COMPLETE Column stats: COMPLETE
                          value expressions: _col3 (type: int), _col4 (type: float), _col6 (type: int)
            Execution mode: vectorized
        Reducer 10 
            Reduce Operator Tree:
              Merge Join Operator
                condition map:
                     Left Outer Join0 to 1
                keys:
                  0 _col2 (type: int), _col1 (type: int)
                  1 _col1 (type: int), _col0 (type: int)
                outputColumnNames: _col1, _col3, _col4, _col6, _col9, _col10
                Map Join Operator
                  condition map:
                       Inner Join 0 to 1
                  keys:
                    0 _col1 (type: int)
                    1 _col0 (type: int)
                  outputColumnNames: _col3, _col4, _col6, _col9, _col10, _col12, _col13, _col14, _col16
                  input vertices:
                    1 Map 13
                  Select Operator
                    expressions: _col6 (type: int), _col12 (type: int), _col13 (type: int), _col14 (type: int), _col16 (type: int), (_col3 - COALESCE(_col9,0)) (type: int), (UDFToDouble(_col4) - COALESCE(_col10,0.0)) (type: double)
                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                    Group By Operator
                      aggregations: sum(_col5), sum(_col6)
                      keys: _col0 (type: int), _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                      Reduce Output Operator
                        key expressions: _col0 (type: int), _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)
                        sort order: +++++
                        Map-reduce partition columns: _col0 (type: int), _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)
                        value expressions: _col5 (type: bigint), _col6 (type: double)
        Reducer 15 
            Reduce Operator Tree:
              Merge Join Operator
                condition map:
                     Left Outer Join0 to 1
                keys:
                  0 _col2 (type: int), _col1 (type: int)
                  1 _col1 (type: int), _col0 (type: int)
                outputColumnNames: _col1, _col3, _col4, _col6, _col9, _col10
                Statistics: Num rows: 8204 Data size: 164080 Basic stats: COMPLETE Column stats: COMPLETE
                Reduce Output Operator
                  key expressions: _col1 (type: int)
                  sort order: +
                  Map-reduce partition columns: _col1 (type: int)
                  Statistics: Num rows: 8204 Data size: 164080 Basic stats: COMPLETE Column stats: COMPLETE
                  value expressions: _col3 (type: int), _col4 (type: float), _col6 (type: int), _col9 (type: int), _col10 (type: float)
        Reducer 2 
            Reduce Operator Tree:
              Merge Join Operator
                condition map:
                     Left Outer Join0 to 1
                keys:
                  0 _col2 (type: int), _col1 (type: int)
                  1 _col1 (type: int), _col0 (type: int)
                outputColumnNames: _col1, _col3, _col4, _col6, _col9, _col10
                Map Join Operator
                  condition map:
                       Inner Join 0 to 1
                  keys:
                    0 _col1 (type: int)
                    1 _col0 (type: int)
                  outputColumnNames: _col3, _col4, _col6, _col9, _col10, _col12, _col13, _col14, _col16
                  input vertices:
                    1 Map 8
                  Select Operator
                    expressions: _col6 (type: int), _col12 (type: int), _col13 (type: int), _col14 (type: int), _col16 (type: int), (_col3 - COALESCE(_col9,0)) (type: int), (UDFToDouble(_col4) - COALESCE(_col10,0.0)) (type: double)
                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                    Group By Operator
                      aggregations: sum(_col5), sum(_col6)
                      keys: _col0 (type: int), _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                      Reduce Output Operator
                        key expressions: _col0 (type: int), _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)
                        sort order: +++++
                        Map-reduce partition columns: _col0 (type: int), _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)
                        value expressions: _col5 (type: bigint), _col6 (type: double)
        Reducer 20 
            Reduce Operator Tree:
              Merge Join Operator
                condition map:
                     Left Outer Join0 to 1
                keys:
                  0 _col2 (type: int), _col1 (type: int)
                  1 _col1 (type: int), _col0 (type: int)
                outputColumnNames: _col1, _col3, _col4, _col6, _col9, _col10
                Map Join Operator
                  condition map:
                       Inner Join 0 to 1
                  keys:
                    0 _col1 (type: int)
                    1 _col0 (type: int)
                  outputColumnNames: _col3, _col4, _col6, _col9, _col10, _col12, _col13, _col14, _col16
                  input vertices:
                    1 Map 25
                  Select Operator
                    expressions: _col6 (type: int), _col12 (type: int), _col13 (type: int), _col14 (type: int), _col16 (type: int), (_col3 - COALESCE(_col9,0)) (type: int), (UDFToDouble(_col4) - COALESCE(_col10,0.0)) (type: double)
                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                    Group By Operator
                      aggregations: sum(_col5), sum(_col6)
                      keys: _col0 (type: int), _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                      Reduce Output Operator
                        key expressions: _col0 (type: int), _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)
                        sort order: +++++
                        Map-reduce partition columns: _col0 (type: int), _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)
                        value expressions: _col5 (type: bigint), _col6 (type: double)
        Reducer 22 
            Reduce Operator Tree:
              Group By Operator
                aggregations: sum(VALUE._col0), sum(VALUE._col1)
                keys: KEY._col0 (type: int), KEY._col1 (type: int), KEY._col2 (type: int), KEY._col3 (type: int), KEY._col4 (type: int)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                Statistics: Num rows: 1 Data size: 16 Basic stats: COMPLETE Column stats: COMPLETE
                Reduce Output Operator
                  key expressions: _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)
                  sort order: ++++
                  Map-reduce partition columns: _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)
                  Statistics: Num rows: 1 Data size: 16 Basic stats: COMPLETE Column stats: COMPLETE
                  value expressions: _col0 (type: int), _col5 (type: bigint), _col6 (type: double)
        Reducer 27 
            Reduce Operator Tree:
              Merge Join Operator
                condition map:
                     Left Outer Join0 to 1
                keys:
                  0 _col2 (type: int), _col1 (type: int)
                  1 _col1 (type: int), _col0 (type: int)
                outputColumnNames: _col1, _col3, _col4, _col6, _col9, _col10
                Map Join Operator
                  condition map:
                       Inner Join 0 to 1
                  keys:
                    0 _col1 (type: int)
                    1 _col0 (type: int)
                  outputColumnNames: _col3, _col4, _col6, _col9, _col10, _col12, _col13, _col14, _col16
                  input vertices:
                    1 Map 30
                  Select Operator
                    expressions: _col6 (type: int), _col12 (type: int), _col13 (type: int), _col14 (type: int), _col16 (type: int), (_col3 - COALESCE(_col9,0)) (type: int), (UDFToDouble(_col4) - COALESCE(_col10,0.0)) (type: double)
                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                    Group By Operator
                      aggregations: sum(_col5), sum(_col6)
                      keys: _col0 (type: int), _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                      Reduce Output Operator
                        key expressions: _col0 (type: int), _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)
                        sort order: +++++
                        Map-reduce partition columns: _col0 (type: int), _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)
                        value expressions: _col5 (type: bigint), _col6 (type: double)
        Reducer 32 
            Reduce Operator Tree:
              Merge Join Operator
                condition map:
                     Left Outer Join0 to 1
                keys:
                  0 _col2 (type: int), _col1 (type: int)
                  1 _col1 (type: int), _col0 (type: int)
                outputColumnNames: _col1, _col3, _col4, _col6, _col9, _col10
                Statistics: Num rows: 8204 Data size: 164080 Basic stats: COMPLETE Column stats: COMPLETE
                Reduce Output Operator
                  key expressions: _col1 (type: int)
                  sort order: +
                  Map-reduce partition columns: _col1 (type: int)
                  Statistics: Num rows: 8204 Data size: 164080 Basic stats: COMPLETE Column stats: COMPLETE
                  value expressions: _col3 (type: int), _col4 (type: float), _col6 (type: int), _col9 (type: int), _col10 (type: float)
        Reducer 4 
            Reduce Operator Tree:
              Group By Operator
                aggregations: sum(VALUE._col0), sum(VALUE._col1)
                keys: KEY._col0 (type: int), KEY._col1 (type: int), KEY._col2 (type: int), KEY._col3 (type: int), KEY._col4 (type: int)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                Statistics: Num rows: 1 Data size: 16 Basic stats: COMPLETE Column stats: COMPLETE
                Map Join Operator
                  condition map:
                       Inner Join 0 to 1
                  keys:
                    0 _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)
                    1 _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col12, _col13
                  input vertices:
                    1 Reducer 22
                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: COMPLETE
                  Filter Operator
                    predicate: ((CAST( _col5 AS decimal(17,2)) / CAST( _col12 AS decimal(17,2))) &amp;lt; CAST( 0.9 AS decimal(37,20))) (type: boolean)
                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: COMPLETE
                    Select Operator
                      expressions: _col7 (type: int), _col0 (type: int), _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int), _col12 (type: bigint), _col5 (type: bigint), (_col5 - _col12) (type: bigint), (_col6 - _col13) (type: double)
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9
                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col8 (type: bigint)
                        sort order: +
                        Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: COMPLETE
                        TopN Hash Memory Usage: 0.04
                        value expressions: _col0 (type: int), _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int), _col5 (type: int), _col6 (type: bigint), _col7 (type: bigint), _col9 (type: double)
        Reducer 5 
            Reduce Operator Tree:
              Select Operator
                expressions: VALUE._col0 (type: int), VALUE._col1 (type: int), VALUE._col2 (type: int), VALUE._col3 (type: int), VALUE._col4 (type: int), VALUE._col5 (type: int), VALUE._col6 (type: bigint), VALUE._col7 (type: bigint), KEY.reducesinkkey0 (type: bigint), VALUE._col8 (type: double)
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9
                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: COMPLETE
                Limit
                  Number of rows: 100
                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: COMPLETE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: COMPLETE
                    table:
                        input format: org.apache.hadoop.mapred.TextInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Union 21 
            Vertex: Union 21
        Union 3 
            Vertex: Union 3

  Stage: Stage-0
    Fetch Operator
      limit: 100
      Processor Tree:
        ListSink

Time taken: 12.351 seconds, Fetched: 821 row(s)



The full exception


Status: Failed
15/02/07 17:43:20 [main]: ERROR SessionState: Status: Failed
Vertex failed, vertexName=Reducer 27, vertexId=vertex_1422895755428_0924_1_29, diagnostics=[Task failed, taskId=task_1422895755428_0924_1_29_000020, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {"key":{"reducesinkkey0":76,"reducesinkkey1":19267},"value":{"_col1":50,"_col2":1629.5,"_col4":2001}}
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:186)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:138)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:328)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:171)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:166)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {"key":{"reducesinkkey0":76,"reducesinkkey1":19267},"value":{"_col1":50,"_col2":1629.5,"_col4":2001}}
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:269)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:168)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:163)
	... 13 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {"key":{"reducesinkkey0":76,"reducesinkkey1":19267},"value":{"_col1":50,"_col2":1629.5,"_col4":2001}}
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:337)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:258)
	... 15 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unexpected exception: null
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:314)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.internalForward(CommonJoinOperator.java:638)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.createForwardJoinObject(CommonJoinOperator.java:433)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genObject(CommonJoinOperator.java:525)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genObject(CommonJoinOperator.java:522)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genJoinObject(CommonJoinOperator.java:451)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject(CommonJoinOperator.java:752)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinObject(CommonMergeJoinOperator.java:248)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinOneGroup(CommonMergeJoinOperator.java:213)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.processOp(CommonMergeJoinOperator.java:196)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:328)
	... 16 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:253)
	... 27 more
], TaskAttempt 1 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {"key":{"reducesinkkey0":76,"reducesinkkey1":19267},"value":{"_col1":50,"_col2":1629.5,"_col4":2001}}
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:186)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:138)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:328)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:171)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:166)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {"key":{"reducesinkkey0":76,"reducesinkkey1":19267},"value":{"_col1":50,"_col2":1629.5,"_col4":2001}}
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:269)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:168)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:163)
	... 13 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {"key":{"reducesinkkey0":76,"reducesinkkey1":19267},"value":{"_col1":50,"_col2":1629.5,"_col4":2001}}
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:337)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:258)
	... 15 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unexpected exception: null
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:314)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.internalForward(CommonJoinOperator.java:638)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.createForwardJoinObject(CommonJoinOperator.java:433)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genObject(CommonJoinOperator.java:525)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genObject(CommonJoinOperator.java:522)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genJoinObject(CommonJoinOperator.java:451)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject(CommonJoinOperator.java:752)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinObject(CommonMergeJoinOperator.java:248)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinOneGroup(CommonMergeJoinOperator.java:213)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.processOp(CommonMergeJoinOperator.java:196)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:328)
	... 16 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:253)
	... 27 more
], TaskAttempt 2 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {"key":{"reducesinkkey0":76,"reducesinkkey1":19267},"value":{"_col1":50,"_col2":1629.5,"_col4":2001}}
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:186)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:138)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:328)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:171)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:166)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {"key":{"reducesinkkey0":76,"reducesinkkey1":19267},"value":{"_col1":50,"_col2":1629.5,"_col4":2001}}
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:269)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:168)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:163)
	... 13 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {"key":{"reducesinkkey0":76,"reducesinkkey1":19267},"value":{"_col1":50,"_col2":1629.5,"_col4":2001}}
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:337)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:258)
	... 15 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unexpected exception: null
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:314)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.internalForward(CommonJoinOperator.java:638)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.createForwardJoinObject(CommonJoinOperator.java:433)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genObject(CommonJoinOperator.java:525)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genObject(CommonJoinOperator.java:522)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genJoinObject(CommonJoinOperator.java:451)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject(CommonJoinOperator.java:752)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinObject(CommonMergeJoinOperator.java:248)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinOneGroup(CommonMergeJoinOperator.java:213)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.processOp(CommonMergeJoinOperator.java:196)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:328)
	... 16 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:253)
	... 27 more
], TaskAttempt 3 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {"key":{"reducesinkkey0":76,"reducesinkkey1":19267},"value":{"_col1":50,"_col2":1629.5,"_col4":2001}}
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:186)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:138)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:328)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:171)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:166)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {"key":{"reducesinkkey0":76,"reducesinkkey1":19267},"value":{"_col1":50,"_col2":1629.5,"_col4":2001}}
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:269)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:168)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:163)
	... 13 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {"key":{"reducesinkkey0":76,"reducesinkkey1":19267},"value":{"_col1":50,"_col2":1629.5,"_col4":2001}}
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:337)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:258)
	... 15 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unexpected exception: null
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:314)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.internalForward(CommonJoinOperator.java:638)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.createForwardJoinObject(CommonJoinOperator.java:433)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genObject(CommonJoinOperator.java:525)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genObject(CommonJoinOperator.java:522)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genJoinObject(CommonJoinOperator.java:451)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject(CommonJoinOperator.java:752)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinObject(CommonMergeJoinOperator.java:248)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinOneGroup(CommonMergeJoinOperator.java:213)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.processOp(CommonMergeJoinOperator.java:196)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:328)
	... 16 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:253)
	... 27 more
]], Vertex failed as one or more tasks failed. failedTasks:1, Vertex vertex_1422895755428_0924_1_29 [Reducer 27] killed/failed due to:null]
15/02/07 17:43:20 [main]: ERROR SessionState: Vertex failed, vertexName=Reducer 27, vertexId=vertex_1422895755428_0924_1_29, diagnostics=[Task failed, taskId=task_1422895755428_0924_1_29_000020, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {"key":{"reducesinkkey0":76,"reducesinkkey1":19267},"value":{"_col1":50,"_col2":1629.5,"_col4":2001}}
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:186)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:138)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:328)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:171)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:166)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {"key":{"reducesinkkey0":76,"reducesinkkey1":19267},"value":{"_col1":50,"_col2":1629.5,"_col4":2001}}
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:269)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:168)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:163)
	... 13 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {"key":{"reducesinkkey0":76,"reducesinkkey1":19267},"value":{"_col1":50,"_col2":1629.5,"_col4":2001}}
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:337)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:258)
	... 15 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unexpected exception: null
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:314)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.internalForward(CommonJoinOperator.java:638)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.createForwardJoinObject(CommonJoinOperator.java:433)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genObject(CommonJoinOperator.java:525)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genObject(CommonJoinOperator.java:522)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genJoinObject(CommonJoinOperator.java:451)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject(CommonJoinOperator.java:752)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinObject(CommonMergeJoinOperator.java:248)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinOneGroup(CommonMergeJoinOperator.java:213)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.processOp(CommonMergeJoinOperator.java:196)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:328)
	... 16 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:253)
	... 27 more
], TaskAttempt 1 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {"key":{"reducesinkkey0":76,"reducesinkkey1":19267},"value":{"_col1":50,"_col2":1629.5,"_col4":2001}}
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:186)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:138)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:328)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:171)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:166)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {"key":{"reducesinkkey0":76,"reducesinkkey1":19267},"value":{"_col1":50,"_col2":1629.5,"_col4":2001}}
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:269)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:168)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:163)
	... 13 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {"key":{"reducesinkkey0":76,"reducesinkkey1":19267},"value":{"_col1":50,"_col2":1629.5,"_col4":2001}}
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:337)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:258)
	... 15 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unexpected exception: null
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:314)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.internalForward(CommonJoinOperator.java:638)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.createForwardJoinObject(CommonJoinOperator.java:433)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genObject(CommonJoinOperator.java:525)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genObject(CommonJoinOperator.java:522)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genJoinObject(CommonJoinOperator.java:451)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject(CommonJoinOperator.java:752)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinObject(CommonMergeJoinOperator.java:248)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinOneGroup(CommonMergeJoinOperator.java:213)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.processOp(CommonMergeJoinOperator.java:196)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:328)
	... 16 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:253)
	... 27 more
], TaskAttempt 2 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {"key":{"reducesinkkey0":76,"reducesinkkey1":19267},"value":{"_col1":50,"_col2":1629.5,"_col4":2001}}
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:186)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:138)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:328)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:171)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:166)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {"key":{"reducesinkkey0":76,"reducesinkkey1":19267},"value":{"_col1":50,"_col2":1629.5,"_col4":2001}}
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:269)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:168)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:163)
	... 13 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {"key":{"reducesinkkey0":76,"reducesinkkey1":19267},"value":{"_col1":50,"_col2":1629.5,"_col4":2001}}
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:337)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:258)
	... 15 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unexpected exception: null
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:314)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.internalForward(CommonJoinOperator.java:638)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.createForwardJoinObject(CommonJoinOperator.java:433)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genObject(CommonJoinOperator.java:525)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genObject(CommonJoinOperator.java:522)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genJoinObject(CommonJoinOperator.java:451)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject(CommonJoinOperator.java:752)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinObject(CommonMergeJoinOperator.java:248)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinOneGroup(CommonMergeJoinOperator.java:213)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.processOp(CommonMergeJoinOperator.java:196)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:328)
	... 16 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:253)
	... 27 more
], TaskAttempt 3 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {"key":{"reducesinkkey0":76,"reducesinkkey1":19267},"value":{"_col1":50,"_col2":1629.5,"_col4":2001}}
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:186)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:138)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:328)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:171)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:166)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {"key":{"reducesinkkey0":76,"reducesinkkey1":19267},"value":{"_col1":50,"_col2":1629.5,"_col4":2001}}
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:269)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:168)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:163)
	... 13 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {"key":{"reducesinkkey0":76,"reducesinkkey1":19267},"value":{"_col1":50,"_col2":1629.5,"_col4":2001}}
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:337)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:258)
	... 15 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unexpected exception: null
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:314)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.internalForward(CommonJoinOperator.java:638)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.createForwardJoinObject(CommonJoinOperator.java:433)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genObject(CommonJoinOperator.java:525)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genObject(CommonJoinOperator.java:522)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genJoinObject(CommonJoinOperator.java:451)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject(CommonJoinOperator.java:752)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinObject(CommonMergeJoinOperator.java:248)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinOneGroup(CommonMergeJoinOperator.java:213)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.processOp(CommonMergeJoinOperator.java:196)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:328)
	... 16 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:253)
	... 27 more

</description>
			<version>0.14.0</version>
			<fixedVersion>1.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.plan.MergeJoinWork.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">9832</link>
		</links>
	</bug>
	<bug id="9739" opendate="2015-02-20 14:28:49" fixdate="2015-03-12 20:05:55" resolution="Duplicate">
		<buginformation>
			<summary>Various queries fails with Tez/ORC file org.apache.hadoop.hive.ql.exec.tez.TezTask due to Caused by: java.lang.ClassCastException</summary>
			<description>This fails when using Tez and ORC. 
It will run when text files are used or text/ORC and MapReduce and not Tez used.
Is this another example of a type issue per https://issues.apache.org/jira/browse/HIVE-9735
select rnum, c1, c2 from tset1 as t1 where exists ( select c1 from tset2 where c1 = t1.c1 )
This will run in both Tez and MapReduce using a text file
select rnum, c1, c2 from t_tset1 as t1 where exists ( select c1 from t_tset2 where c1 = t1.c1 )
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:91)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:68)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:294)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:163)
	... 13 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:52)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:83)
	... 16 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unexpected exception: org.apache.hadoop.hive.serde2.io.HiveCharWritable cannot be cast to org.apache.hadoop.hive.common.type.HiveChar
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:311)
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.processOp(VectorMapJoinOperator.java:249)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)
	at org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.processOp(VectorFilterOperator.java:111)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95)
	at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:157)
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:45)
	... 17 more
Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.io.HiveCharWritable cannot be cast to org.apache.hadoop.hive.common.type.HiveChar
	at org.apache.hadoop.hive.ql.exec.vector.VectorColumnAssignFactory$18.assignObjectValue(VectorColumnAssignFactory.java:432)
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.internalForward(VectorMapJoinOperator.java:196)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genAllOneUniqueJoinObject(CommonJoinOperator.java:670)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject(CommonJoinOperator.java:748)
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:299)
	... 24 more
create table  if not exists T_TSET1 (RNUM int , C1 int, C2 char(3))
 ROW FORMAT DELIMITED FIELDS TERMINATED BY &amp;amp;apos;|&amp;amp;apos; LINES TERMINATED BY &amp;amp;apos;\n&amp;amp;apos; 
 STORED AS textfile ;
create table  if not exists T_TSET1 (RNUM int , C1 int, C2 char(3))
 STORED AS ORC ;
create table  if not exists T_TSET2 (RNUM int , C1 int, C2 char(3))
 ROW FORMAT DELIMITED FIELDS TERMINATED BY &amp;amp;apos;|&amp;amp;apos; LINES TERMINATED BY &amp;amp;apos;\n&amp;amp;apos; 
 STORED AS textfile ;
TSET1 data
0|10|AAA
1|10|AAA
2|10|AAA
3|20|BBB
4|30|CCC
5|40|DDD
6|50|\N
7|60|\N
8|\N|AAA
9|\N|AAA
10|\N|\N
11|\N|\N
TSET2 DATA
0|10|AAA
1|10|AAA
2|40|DDD
3|50|EEE
4|60|FFF</description>
			<version>0.14.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorColumnAssignFactory.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">9249</link>
		</links>
	</bug>
	<bug id="6363" opendate="2014-02-04 07:09:12" fixdate="2015-03-23 21:22:18" resolution="Duplicate">
		<buginformation>
			<summary>IllegalArgumentException is thrown instead of SQLException</summary>
			<description>parseURL in the following code is throwing IllegalArgumentException 
http://svn.apache.org/viewvc/hive/trunk/jdbc/src/java/org/apache/hive/jdbc/Utils.java?view=markup 
This is going to break other JDBC based connectors because java.sql.DriverManager doesnt catch IllegalArgumentException while probing for correct Driver for a given URL. 
A simple test case can have class.forName(org.apache.hive.jdbc.HiveDriver) (Loading hiveserver2 JDBC driver) followed by class.forName(org.apache.hadoop.hive.jdbc.HiveDriver)(Loading hiveserver JDBC driver).
In this case hiveserver connection will fail with BAD URL format for hiveserver. If you reverse the driver loading to hiveserver followed by hiveserver2, both the connections will be successful.
Following code in java.sql.DriverManager is causing the issue 
[[ 
// Worker method called by the public getConnection() methods. 
private static Connection getConnection( 
// Walk through the loaded registeredDrivers attempting to make a connection. 
// Remember the first exception that gets raised so we can reraise it. 
for(DriverInfo aDriver : registeredDrivers) { 
// If the caller does not have permission to load the driver then 
// skip it. 
if(isDriverAllowed(aDriver.driver, callerCL)) { 
try { 
Connection con = aDriver.driver.connect(url, info); 
if (con != null) 
{ 
// Success! 
println("getConnection returning " + aDriver.driver.getClass().getName()); 
return (con); 
}
 
} catch (SQLException ex) { 
if (reason == null) 
{ 
reason = ex; 
}
 
} 
} else 
{ 
println(" skipping: " + aDriver.getClass().getName()); 
}
 
} 
} 
]] 
Marking it as critical because this is going to restrict consuming JDBC driver in production environment where many drivers are loaded on requirement rather than statically loading all drivers.</description>
			<version>0.10.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.jdbc.HiveConnection.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.ThriftHttpCLIService.java</file>
			<file type="M">org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.zookeeper.TestZookeeperLockManager.java</file>
			<file type="M">org.apache.hive.service.cli.session.TestSessionGlobalInitFile.java</file>
			<file type="M">org.apache.hive.jdbc.TestJdbcDriver2.java</file>
			<file type="M">org.apache.hive.service.cli.operation.OperationManager.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.java</file>
			<file type="M">org.apache.hive.service.server.HiveServer2.java</file>
			<file type="M">org.apache.hive.service.cli.session.SessionManager.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hive.service.cli.CLIService.java</file>
			<file type="M">org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
			<file type="M">org.apache.hive.jdbc.HiveDriver.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.java</file>
			<file type="M">org.apache.hive.jdbc.Utils.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">7935</link>
		</links>
	</bug>
	<bug id="9832" opendate="2015-03-03 00:47:31" fixdate="2015-04-07 00:10:55" resolution="Fixed">
		<buginformation>
			<summary>Merge join followed by union and a map join in hive on tez fails.</summary>
			<description>

select a.key, b.value from (select x.key as key, y.value as value from
srcpart x join srcpart y on (x.key = y.key)
union all
select key, value from srcpart z) a join src b on (a.value = b.value);




TaskAttempt 3 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: Hive Runtime Error while closing operators: null
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:186)
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:138)
        at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)
        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)
        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)
        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: Hive Runtime Error while closing operators: null
        at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.close(ReduceRecordProcessor.java:214)
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:177)
        ... 13 more
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.exec.MapJoinOperator.closeOp(MapJoinOperator.java:317)
        at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:598)
        at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:610)
        at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:610)
        at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:610)
        at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:610)
        at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.close(ReduceRecordProcessor.java:196)
        ... 14 more
]], Vertex failed as one or more tasks failed. failedTasks:1, Vertex vertex_1425055721029_0048_4_09 [Reducer 5] killed/failed due to:null]
Vertex killed, vertexName=Reducer 7, vertexId=vertex_1425055721029_0048_4_11, diagnostics=[Vertex received Kill while in RUNNING state., Vertex killed as other vertex failed. failedTasks:0, Vertex vertex_1425055721029_0048_4_11 [Reducer 7] killed/failed due to:null]
Vertex killed, vertexName=Reducer 4, vertexId=vertex_1425055721029_0048_4_07, diagnostics=[Vertex received Kill while in RUNNING state., Vertex killed as other vertex failed. failedTasks:0, Vertex vertex_1425055721029_0048_4_07 [Reducer 4] killed/failed due to:null]
DAG failed due to vertex failure. failedVertices:1 killedVertices:2
FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask

</description>
			<version>1.0.0</version>
			<fixedVersion>1.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.plan.MergeJoinWork.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">9624</link>
			<link type="Reference" description="is related to">12530</link>
		</links>
	</bug>
	<bug id="8297" opendate="2014-09-29 22:19:30" fixdate="2015-04-09 18:46:49" resolution="Duplicate">
		<buginformation>
			<summary>Wrong results with JDBC direct read of TIMESTAMP column in RCFile and ORC format</summary>
			<description>For the case:
SELECT * FROM [table]
JDBC direct reads the table backing data, versus cranking up a MR and creating a result set.  Where table format is RCFile or ORC, incorrect results are delivered by JDBC direct read for TIMESTAMP columns.  If you force a result set, correct data is returned.
To reproduce using beeline:
1) Create this file as follows in HDFS.
$ cat &amp;gt; /tmp/ts.txt
2014-09-28 00:00:00
2014-09-29 00:00:00
2014-09-30 00:00:00
&amp;lt;ctrl-D&amp;gt;
$ hadoop fs -copyFromLocal /tmp/ts.txt /tmp/ts.txt
2) In beeline load above HDFS data to a TEXTFILE table, and verify ok:
$ beeline
&amp;gt; !connect jdbc:hive2://&amp;lt;host&amp;gt;:&amp;lt;port&amp;gt;/&amp;lt;db&amp;gt; hive pass org.apache.hive.jdbc.HiveDriver
&amp;gt; drop table `TIMESTAMP_TEXT`;
&amp;gt; CREATE TABLE `TIMESTAMP_TEXT` (`ts` TIMESTAMP) ROW FORMAT DELIMITED FIELDS TERMINATED BY &amp;amp;apos;\001&amp;amp;apos;
LINES TERMINATED BY &amp;amp;apos;\012&amp;amp;apos; STORED AS TEXTFILE;
&amp;gt; LOAD DATA INPATH &amp;amp;apos;/tmp/ts.txt&amp;amp;apos; OVERWRITE INTO TABLE
`TIMESTAMP_TEXT`;
&amp;gt; select * from `TIMESTAMP_TEXT`;
3) In beeline create and load an RCFile from the TEXTFILE:
&amp;gt; drop table `TIMESTAMP_RCFILE`;
&amp;gt; CREATE TABLE `TIMESTAMP_RCFILE` (`ts` TIMESTAMP) stored as rcfile;
&amp;gt; INSERT INTO TABLE `TIMESTAMP_RCFILE` SELECT * FROM `TIMESTAMP_TEXT`;
4) Demonstrate incorrect direct JDBC read versus good read by inducing result set creation:
&amp;gt; SELECT * FROM `TIMESTAMP_RCFILE`;
------------------------


  timestamp_rcfile.ts   


------------------------


 2014-09-30 00:00:00.0  


 2014-09-30 00:00:00.0  


 2014-09-30 00:00:00.0  


------------------------
&amp;gt;  SELECT * FROM `TIMESTAMP_RCFILE` where ts is not NULL;
------------------------


  timestamp_rcfile.ts   


------------------------


 2014-09-28 00:00:00.0  


 2014-09-29 00:00:00.0  


 2014-09-30 00:00:00.0  


------------------------
Note 1: The incorrect conduct demonstrated above replicates with a standalone Java/JDBC program.
Note 2: Don&amp;amp;apos;t know if this is an issue with any other data types, also don&amp;amp;apos;t know what releases affected, however this occurs in Hive 13.  Direct JDBC read of TEXTFILE and SEQUENCEFILE work fine.  As above for RCFile and ORC wrong results are delivered, did not test any other file types.</description>
			<version>0.13.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaTimestampObjectInspector.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaBinaryObjectInspector.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">7399</link>
		</links>
	</bug>
	<bug id="9647" opendate="2015-02-10 23:54:42" fixdate="2015-04-10 00:08:13" resolution="Fixed">
		<buginformation>
			<summary>Discrepancy in cardinality estimates between partitioned and un-partitioned tables </summary>
			<description>High-level summary
HiveRelMdSelectivity.computeInnerJoinSelectivity relies on per column number of distinct value to estimate join selectivity.
The way statistics are aggregated for partitioned tables results in discrepancy in number of distinct values which results in different plans between partitioned and un-partitioned schemas.
The table below summarizes the NDVs in computeInnerJoinSelectivity which are used to estimate selectivity of joins.


Column	
Partitioned count distincts
 	Un-Partitioned count distincts


sr_customer_sk	
71,245	
1,415,625


sr_item_sk	
38,846
	62,562


sr_ticket_number	
71,245	
34,931,085


ss_customer_sk	
88,476
	1,415,625


ss_item_sk	
38,846
	62,562


ss_ticket_number
	100,756	
56,256,175


The discrepancy is because NDV calculation for a partitioned table assumes that the NDV range is contained within each partition and is calculates as "select max(NUM_DISTINCTS) from PART_COL_STATS .
This is problematic for columns like ticket number which are naturally increasing with the partitioned date column ss_sold_date_sk.
Suggestions
Use Hyper Log Log as suggested by Gopal, there is an HLL implementation for HBASE co-porccessors which we can use as a reference here 
Using the global stats from TAB_COL_STATS and the per partition stats from PART_COL_STATS extrapolate the NDV for the qualified partitions as in :
Max ( (NUM_DISTINCTS from TAB_COL_STATS) x (Number of qualified partitions) / (Number of Partitions), max(NUM_DISTINCTS) from PART_COL_STATS))
More details
While doing TPC-DS Partitioned vs. Un-Partitioned runs I noticed that many of the plans are different, then I dumped the CBO logical plan and I found that join estimates are drastically different
Unpartitioned schema :


2015-02-10 11:33:27,624 DEBUG [main]: parse.SemanticAnalyzer (SemanticAnalyzer.java:apply(12624)) - Plan After Join Reordering:
HiveProjectRel(store_sales_quantitycount=[$0], store_sales_quantityave=[$1], store_sales_quantitystdev=[$2], store_sales_quantitycov=[/($2, $1)], as_store_returns_quantitycount=[$3], as_store_returns_quantityave=[$4], as_store_returns_quantitystdev=[$5], store_returns_quantitycov=[/($5, $4)]): rowcount = 1.0, cumulative cost = {6.056835407771381E8 rows, 0.0 cpu, 0.0 io}, id = 2956
  HiveAggregateRel(group=[{}], agg#0=[count($0)], agg#1=[avg($0)], agg#2=[stddev_samp($0)], agg#3=[count($1)], agg#4=[avg($1)], agg#5=[stddev_samp($1)]): rowcount = 1.0, cumulative cost = {6.056835407771381E8 rows, 0.0 cpu, 0.0 io}, id = 2954
    HiveProjectRel($f0=[$4], $f1=[$8]): rowcount = 40.05611776795562, cumulative cost = {6.056835407771381E8 rows, 0.0 cpu, 0.0 io}, id = 2952
      HiveProjectRel(ss_sold_date_sk=[$0], ss_item_sk=[$1], ss_customer_sk=[$2], ss_ticket_number=[$3], ss_quantity=[$4], sr_item_sk=[$5], sr_customer_sk=[$6], sr_ticket_number=[$7], sr_return_quantity=[$8], d_date_sk=[$9], d_quarter_name=[$10]): rowcount = 40.05611776795562, cumulative cost = {6.056835407771381E8 rows, 0.0 cpu, 0.0 io}, id = 2982
        HiveJoinRel(condition=[=($9, $0)], joinType=[inner]): rowcount = 40.05611776795562, cumulative cost = {6.056835407771381E8 rows, 0.0 cpu, 0.0 io}, id = 2980
          HiveJoinRel(condition=[AND(AND(=($2, $6), =($1, $5)), =($3, $7))], joinType=[inner]): rowcount = 28880.460910696, cumulative cost = {6.05654559E8 rows, 0.0 cpu, 0.0 io}, id = 2964
            HiveProjectRel(ss_sold_date_sk=[$0], ss_item_sk=[$2], ss_customer_sk=[$3], ss_ticket_number=[$9], ss_quantity=[$10]): rowcount = 5.50076554E8, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 2920
              HiveTableScanRel(table=[[tpcds_bin_orc_200.store_sales]]): rowcount = 5.50076554E8, cumulative cost = {0}, id = 2822
            HiveProjectRel(sr_item_sk=[$2], sr_customer_sk=[$3], sr_ticket_number=[$9], sr_return_quantity=[$10]): rowcount = 5.5578005E7, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 2923
              HiveTableScanRel(table=[[tpcds_bin_orc_200.store_returns]]): rowcount = 5.5578005E7, cumulative cost = {0}, id = 2823
          HiveProjectRel(d_date_sk=[$0], d_quarter_name=[$15]): rowcount = 101.31622746185853, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 2948
            HiveFilterRel(condition=[=($15, &amp;amp;apos;2000Q1&amp;amp;apos;)]): rowcount = 101.31622746185853, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 2946
              HiveTableScanRel(table=[[tpcds_bin_orc_200.date_dim]]): rowcount = 73049.0, cumulative cost = {0}, id = 2821


Partitioned schema :


2015-02-10 11:32:16,880 DEBUG [main]: parse.SemanticAnalyzer (SemanticAnalyzer.java:apply(12624)) - Plan After Join Reordering:
HiveProjectRel(store_sales_quantitycount=[$0], store_sales_quantityave=[$1], store_sales_quantitystdev=[$2], store_sales_quantitycov=[/($2, $1)], as_store_returns_quantitycount=[$3], as_store_returns_quantityave=[$4], as_store_returns_quantitystdev=[$5], store_returns_quantitycov=[/($5, $4)]): rowcount = 1.0, cumulative cost = {6.064175958973647E8 rows, 0.0 cpu, 0.0 io}, id = 2791
  HiveAggregateRel(group=[{}], agg#0=[count($0)], agg#1=[avg($0)], agg#2=[stddev_samp($0)], agg#3=[count($1)], agg#4=[avg($1)], agg#5=[stddev_samp($1)]): rowcount = 1.0, cumulative cost = {6.064175958973647E8 rows, 0.0 cpu, 0.0 io}, id = 2789
    HiveProjectRel($f0=[$3], $f1=[$8]): rowcount = 100840.08570910375, cumulative cost = {6.064175958973647E8 rows, 0.0 cpu, 0.0 io}, id = 2787
      HiveProjectRel(ss_item_sk=[$4], ss_customer_sk=[$5], ss_ticket_number=[$6], ss_quantity=[$7], ss_sold_date_sk=[$8], sr_item_sk=[$0], sr_customer_sk=[$1], sr_ticket_number=[$2], sr_return_quantity=[$3], d_date_sk=[$9], d_quarter_name=[$10]): rowcount = 100840.08570910375, cumulative cost = {6.064175958973647E8 rows, 0.0 cpu, 0.0 io}, id = 2817
        HiveJoinRel(condition=[AND(AND(=($5, $1), =($4, $0)), =($6, $2))], joinType=[inner]): rowcount = 100840.08570910375, cumulative cost = {6.064175958973647E8 rows, 0.0 cpu, 0.0 io}, id = 2815
          HiveProjectRel(sr_item_sk=[$1], sr_customer_sk=[$2], sr_ticket_number=[$8], sr_return_quantity=[$9]): rowcount = 5.5578005E7, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 2758
            HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_200_orig.store_returns]]): rowcount = 5.5578005E7, cumulative cost = {0}, id = 2658
          HiveJoinRel(condition=[=($5, $4)], joinType=[inner]): rowcount = 762935.5811373093, cumulative cost = {5.500766553162274E8 rows, 0.0 cpu, 0.0 io}, id = 2801
            HiveProjectRel(ss_item_sk=[$1], ss_customer_sk=[$2], ss_ticket_number=[$8], ss_quantity=[$9], ss_sold_date_sk=[$22]): rowcount = 5.50076554E8, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 2755
              HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_200_orig.store_sales]]): rowcount = 5.50076554E8, cumulative cost = {0}, id = 2657
            HiveProjectRel(d_date_sk=[$0], d_quarter_name=[$15]): rowcount = 101.31622746185853, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 2783
              HiveFilterRel(condition=[=($15, &amp;amp;apos;2000Q1&amp;amp;apos;)]): rowcount = 101.31622746185853, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 2781
                HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_200_orig.date_dim]]): rowcount = 73049.0, cumulative cost = {0}, id = 2656


This was puzzling knowing that the stats for both tables are identical in TAB_COL_STATS.
Column statistics from TAB_COL_STATS, notice how the column statistics are identical in both cases.


DB_NAME	
COLUMN_NAME
	COLUMN_TYPE
	NUM_NULLS
	LONG_HIGH_VALUE
	LONG_LOW_VALUE
	MAX_COL_LEN
	NUM_DISTINCTS


tpcds_bin_orc_200
	d_date_sk
	int
	0
	2,488,070
	2,415,022
	NULL
	65,332


tpcds_bin_partitioned_orc_200
	d_date_sk
	int
	0
	2,488,070
	2,415,022
	NULL
	65,332


tpcds_bin_orc_200	
d_quarter_name
	string
	0
	NULL
	NULL
	6
	721


tpcds_bin_partitioned_orc_200
	d_quarter_name
	string
	0
	NULL
	NULL
	6
	721


tpcds_bin_orc_200
	sr_customer_sk
	int
	1,009,571
	1,600,000
	1
	NULL
	1,415,625


tpcds_bin_partitioned_orc_200
	sr_customer_sk
	int
	1,009,571
	1,600,000
	1
	NULL
	1,415,625


tpcds_bin_orc_200	
sr_item_sk
	int
	0
	48,000
	1
	NULL
	62,562


tpcds_bin_partitioned_orc_200
	sr_item_sk
	int
	0
	48,000
	1
	NULL
	62,562


tpcds_bin_orc_200	
sr_ticket_number
	int
	0
	48,000,000
	1
	NULL
	34,931,085


tpcds_bin_partitioned_orc_200
	sr_ticket_number
	int
	0
	48,000,000
	1
	NULL
	34,931,085


tpcds_bin_orc_200	
ss_customer_sk
	int
	12,960,424
	1,600,000
	1
	NULL
	1,415,625


tpcds_bin_partitioned_orc_200
	ss_customer_sk
	int
	12,960,424
	1,600,000
	1
	NULL
	1,415,625


tpcds_bin_orc_200	
ss_item_sk
	int
	0
	48,000
	1
	NULL
	62,562


tpcds_bin_partitioned_orc_200
	ss_item_sk
	int
	0
	48,000
	1	
NULL
	62,562


tpcds_bin_orc_200
	ss_sold_date_sk
	int
	0
	2,452,642
	2,450,816
	NULL
	2,226


tpcds_bin_partitioned_orc_200
	ss_sold_date_sk	
int
	0
	2,452,642
	2,450,816
	NULL
	2,226


tpcds_bin_orc_200	
ss_ticket_number
	int
	0
	48,000,000
	1
	NULL
	56,256,175


tpcds_bin_partitioned_orc_200
	ss_ticket_number
	int
	0
	48,000,000
	1
	NULL
	56,256,175


For partitioned tables we get the statistics using get_aggr_stats_for which eventually issues the query below


select 
    COLUMN_NAME,
    COLUMN_TYPE,
    
    max(NUM_DISTINCTS),
    
from
    PART_COL_STATS
Where
where
    DB_NAME = 
        and TABLE_NAME = 
        and COLUMN_NAME in 
        and PARTITION_NAME in (1  N)
group by COLUMN_NAME , COLUMN_TYPE;


 </description>
			<version>0.14.0</version>
			<fixedVersion>1.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.StatObjectConverter.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.IExtrapolatePartStatus.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.LinearExtrapolatePartStatus.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">9717</link>
			<link type="Reference" description="relates to">10612</link>
			<link type="Reference" description="is related to">9905</link>
			<link type="Reference" description="is related to">9717</link>
			<link type="Reference" description="is related to">9689</link>
		</links>
	</bug>
	<bug id="9717" opendate="2015-02-18 19:16:52" fixdate="2015-04-10 00:10:34" resolution="Duplicate">
		<buginformation>
			<summary>The max/min function used by AggrStats for decimal type is not what we expected</summary>
			<description>In current version hive-schema-1.2.0, in TABLE PART_COL_STATS, we store the "BIG_DECIMAL_LOW_VALUE" and "BIG_DECIMAL_HIGH_VALUE" as varchar. For example,
derby
"BIG_DECIMAL_LOW_VALUE" VARCHAR(4000), "BIG_DECIMAL_HIGH_VALUE" VARCHAR(4000)
mssql
BIG_DECIMAL_HIGH_VALUE varchar(255) NULL,
    BIG_DECIMAL_LOW_VALUE varchar(255) NULL,
mysql
`BIG_DECIMAL_LOW_VALUE` varchar(4000) CHARACTER SET latin1 COLLATE latin1_bin,
 `BIG_DECIMAL_HIGH_VALUE` varchar(4000) CHARACTER SET latin1 COLLATE latin1_bin,
oracle
BIG_DECIMAL_LOW_VALUE VARCHAR2(4000),
 BIG_DECIMAL_HIGH_VALUE VARCHAR2(4000),
postgres
"BIG_DECIMAL_LOW_VALUE" character varying(4000) DEFAULT NULL::character varying,
 "BIG_DECIMAL_HIGH_VALUE" character varying(4000) DEFAULT NULL::character varying,
And, when we do the aggrstats, we do a MAX/MIN of all the BIG_DECIMAL_HIGH_VALU/BIG_DECIMAL_LOW_VALUEE of partitions. We are expecting a max/min of a decimal (a number). However, it is actually a max/min of a varchar (a string). As a result, &amp;amp;apos;900&amp;amp;apos; is more than &amp;amp;apos;1000&amp;amp;apos;. This also affects the extrapolation of the status. The proposed solution is to use a CAST function to cast it to decimal. </description>
			<version>0.14.0</version>
			<fixedVersion>1.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.StatObjectConverter.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.IExtrapolatePartStatus.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.LinearExtrapolatePartStatus.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">9647</link>
			<link type="Reference" description="relates to">9647</link>
		</links>
	</bug>
	<bug id="10240" opendate="2015-04-07 18:39:23" fixdate="2015-04-10 00:23:03" resolution="Duplicate">
		<buginformation>
			<summary>Patch HIVE-9473 breaks KERBEROS</summary>
			<description>The patch from HIVE-9473 introduces a regression. Hive-Server2 does not start properly any more for our config (more or less the bigtop environment)
sql std auth enabled, enableDoAs disabled, tez enabled, kerberos enabled.
Problem seems to be that the kerberos ticket is not present when hive-server2 tries first to access HDFS. When HIVE-9473 is reverted getting the ticket is one of the first things hive-server2 does.
Posting startup of vanilla hive-1.0.0 and startup of a hive-1.0.0 with this commit revoked, where hive-server2 correctly starts.


commit 35582c2065a6b90b003a656bdb3b0ff08b0c35b9
Author: Thejas Nair &amp;lt;thejas@apache.org&amp;gt;
Date:   Fri Jan 30 00:05:50 2015 +0000

    HIVE-9473 : sql std auth should disallow built-in udfs that allow any java methods to be called (Thejas Nair, reviewed by Jason Dere)
    
    git-svn-id: https://svn.apache.org/repos/asf/hive/branches/branch-1.0@1655891 13f79535-47bb-0310-9956-ffa450edef68


revoked.
Startup of vanilla hive-1.0.0 hive-server2 


STARTUP_MSG:   build = git://os2-debian80/net/os2-debian80/fs1/olaf/bigtop/output/hive/hive-1.0.0 -r 813996292c9f966109f990127ddd5673cf813125; compiled by &amp;amp;apos;olaf&amp;amp;apos; on Tue Apr 7 09:33:01 CEST 2015
************************************************************/
2015-04-07 10:23:52,579 INFO  [main]: server.HiveServer2 (HiveServer2.java:startHiveServer2(292)) - Starting HiveServer2
2015-04-07 10:23:53,104 INFO  [main]: metastore.HiveMetaStore (HiveMetaStore.java:newRawStore(556)) - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2015-04-07 10:23:53,135 INFO  [main]: metastore.ObjectStore (ObjectStore.java:initialize(264)) - ObjectStore, initialize called
2015-04-07 10:23:54,775 INFO  [main]: metastore.ObjectStore (ObjectStore.java:getPMF(345)) - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Pa
rtition,Database,Type,FieldSchema,Order"
2015-04-07 10:23:56,953 INFO  [main]: metastore.MetaStoreDirectSql (MetaStoreDirectSql.java:&amp;lt;init&amp;gt;(132)) - Using direct SQL, underlying DB is DERBY
2015-04-07 10:23:56,954 INFO  [main]: metastore.ObjectStore (ObjectStore.java:setConf(247)) - Initialized ObjectStore
2015-04-07 10:23:57,275 INFO  [main]: metastore.HiveMetaStore (HiveMetaStore.java:createDefaultRoles_core(630)) - Added admin role in metastore
2015-04-07 10:23:57,276 INFO  [main]: metastore.HiveMetaStore (HiveMetaStore.java:createDefaultRoles_core(639)) - Added public role in metastore
2015-04-07 10:23:58,241 WARN  [main]: ipc.Client (Client.java:run(675)) - Exception encountered while connecting to the server : javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
2015-04-07 10:23:58,248 WARN  [main]: ipc.Client (Client.java:run(675)) - Exception encountered while connecting to the server : javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
2015-04-07 10:23:58,249 INFO  [main]: retry.RetryInvocationHandler (RetryInvocationHandler.java:invoke(140)) - Exception while invoking getFileInfo of class ClientNamenodeProtocolTranslatorPB over node2.proto.bsi.de/192.168.100.22:8020 after 1 fail over attempts. Trying to fail over immediately.
java.io.IOException: Failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]; Host Details : local host is: "node2.proto.bsi.de/192.168.100.22"; destination host is: "node2.proto.bsi.de":8020; 
        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:772)
        at org.apache.hadoop.ipc.Client.call(Client.java:1472)
        at org.apache.hadoop.ipc.Client.call(Client.java:1399)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
        at com.sun.proxy.$Proxy14.getFileInfo(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:752)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
        at com.sun.proxy.$Proxy15.getFileInfo(Unknown Source)
        at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1988)
        at org.apache.hadoop.hdfs.DistributedFileSystem$18.doCall(DistributedFileSystem.java:1118)
        at org.apache.hadoop.hdfs.DistributedFileSystem$18.doCall(DistributedFileSystem.java:1114)
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
        at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1114)
        at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1400)
        at org.apache.hadoop.hive.ql.session.SessionState.createRootHDFSDir(SessionState.java:520)
        at org.apache.hadoop.hive.ql.session.SessionState.createSessionDirs(SessionState.java:478)
        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:430)
        at org.apache.hive.service.cli.CLIService.applyAuthorizationConfigPolicy(CLIService.java:123)
        at org.apache.hive.service.cli.CLIService.init(CLIService.java:81)
        at org.apache.hive.service.CompositeService.init(CompositeService.java:59)
        at org.apache.hive.service.server.HiveServer2.init(HiveServer2.java:89)
        at org.apache.hive.service.server.HiveServer2.startHiveServer2(HiveServer2.java:298)
        at org.apache.hive.service.server.HiveServer2.access$400(HiveServer2.java:65)
        at org.apache.hive.service.server.HiveServer2$StartOptionExecutor.execute(HiveServer2.java:508)
        at org.apache.hive.service.server.HiveServer2.main(HiveServer2.java:381)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
        at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:680)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)


And now startup log of reverted patch


2015-04-07 16:38:40,862 INFO  [main]: server.HiveServer2 (HiveServer2.java:startHiveServer2(292)) - Starting HiveServer2
2015-04-07 16:38:41,384 INFO  [main]: security.UserGroupInformation (UserGroupInformation.java:loginUserFromKeytab(938)) - Login successful for user hive/node2.proto.bsi.de@PROTO.BSI.DE using keytab file /etc/hive.keytab
2015-04-07 16:38:41,384 INFO  [main]: cli.CLIService (CLIService.java:init(100)) - SPNego httpUGI not created, spNegoPrincipal: , ketabFile: 
2015-04-07 16:38:42,044 INFO  [main]: metastore.HiveMetaStore (HiveMetaStore.java:newRawStore(556)) - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2015-04-07 16:38:42,074 INFO  [main]: metastore.ObjectStore (ObjectStore.java:initialize(264)) - ObjectStore, initialize called
2015-04-07 16:38:44,682 INFO  [main]: metastore.ObjectStore (ObjectStore.java:getPMF(345)) - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2015-04-07 16:38:46,762 INFO  [main]: metastore.MetaStoreDirectSql (MetaStoreDirectSql.java:&amp;lt;init&amp;gt;(132)) - Using direct SQL, underlying DB is DERBY
2015-04-07 16:38:46,762 INFO  [main]: metastore.ObjectStore (ObjectStore.java:setConf(247)) - Initialized ObjectStore
2015-04-07 16:38:47,067 INFO  [main]: metastore.HiveMetaStore (HiveMetaStore.java:createDefaultRoles_core(630)) - Added admin role in metastore
2015-04-07 16:38:47,068 INFO  [main]: metastore.HiveMetaStore (HiveMetaStore.java:createDefaultRoles_core(639)) - Added public role in metastore
2015-04-07 16:38:48,101 INFO  [main]: session.SessionState (SessionState.java:createPath(558)) - Created local directory: /tmp/7a90155d-fbd9-4991-bce6-f583a9ef259f_resources
2015-04-07 16:38:48,111 INFO  [main]: session.SessionState (SessionState.java:createPath(558)) - Created HDFS directory: /tmp/hive/hive/7a90155d-fbd9-4991-bce6-f583a9ef259f
2015-04-07 16:38:48,113 INFO  [main]: session.SessionState (SessionState.java:createPath(558)) - Created local directory: /tmp/hive/7a90155d-fbd9-4991-bce6-f583a9ef259f
2015-04-07 16:38:48,118 INFO  [main]: session.SessionState (SessionState.java:createPath(558)) - Created HDFS directory: /tmp/hive/hive/7a90155d-fbd9-4991-bce6-f583a9ef259f/_tmp_space.db
2015-04-07 16:38:48,120 INFO  [main]: session.SessionState (SessionState.java:start(460)) - No Tez session required at this point. hive.execution.engine=mr.
2015-04-07 16:38:48,134 INFO  [main]: sqlstd.SQLStdHiveAccessController (SQLStdHiveAccessController.java:&amp;lt;init&amp;gt;(95)) - Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=7a90155d-fbd9-4991-bce6-f583a9ef259f, clientType=HIVESERVER2]
2015-04-07 16:38:48,148 INFO  [main]: service.CompositeService (SessionManager.java:initOperationLogRootDir(148)) - Operation log root directory is created: /tmp/hive/operation_logs
2015-04-07 16:38:48,153 INFO  [main]: service.CompositeService (SessionManager.java:createBackgroundOperationPool(96)) - HiveServer2: Background operation thread pool size: 100
2015-04-07 16:38:48,153 INFO  [main]: service.CompositeService (SessionManager.java:createBackgroundOperationPool(98)) - HiveServer2: Background operation thread wait queue size: 100
2015-04-07 16:38:48,153 INFO  [main]: service.CompositeService (SessionManager.java:createBackgroundOperationPool(101)) - HiveServer2: Background operation thread keepalive time: 10 seconds
2015-04-07 16:38:48,157 INFO  [main]: service.AbstractService (AbstractService.java:init(89)) - Service:OperationManager is inited.
2015-04-07 16:38:48,157 INFO  [main]: service.AbstractService (AbstractService.java:init(89)) - Service:SessionManager is inited.
2015-04-07 16:38:48,157 INFO  [main]: service.AbstractService (AbstractService.java:init(89)) - Service:CLIService is inited.
2015-04-07 16:38:48,158 INFO  [main]: service.AbstractService (AbstractService.java:init(89)) - Service:ThriftBinaryCLIService is inited.
2015-04-07 16:38:48,158 INFO  [main]: service.AbstractService (AbstractService.java:init(89)) - Service:HiveServer2 is inited.
2015-04-07 16:38:48,158 INFO  [main]: service.AbstractService (AbstractService.java:start(104)) - Service:OperationManager is started.
2015-04-07 16:38:48,158 INFO  [main]: service.AbstractService (AbstractService.java:start(104)) - Service:SessionManager is started.
2015-04-07 16:38:48,158 INFO  [main]: service.AbstractService (AbstractService.java:start(104)) - Service:CLIService is started.
2015-04-07 16:38:48,161 INFO  [main]: metastore.ObjectStore (ObjectStore.java:initialize(264)) - ObjectStore, initialize called
2015-04-07 16:38:48,167 INFO  [main]: metastore.MetaStoreDirectSql (MetaStoreDirectSql.java:&amp;lt;init&amp;gt;(132)) - Using direct SQL, underlying DB is DERBY
2015-04-07 16:38:48,167 INFO  [main]: metastore.ObjectStore (ObjectStore.java:setConf(247)) - Initialized ObjectStore
2015-04-07 16:38:48,168 INFO  [main]: metastore.HiveMetaStore (HiveMetaStore.java:logInfo(713)) - 0: get_databases: default
2015-04-07 16:38:48,168 INFO  [main]: HiveMetaStore.audit (HiveMetaStore.java:logAuditEvent(339)) - ugi=hive/node2.proto.bsi.de@PROTO.BSI.DE    ip=unknown-ip-addr      cmd=get_databases: default      
2015-04-07 16:38:48,193 INFO  [main]: metastore.HiveMetaStore (HiveMetaStore.java:logInfo(713)) - 0: Shutting down the object store...
2015-04-07 16:38:48,193 INFO  [main]: HiveMetaStore.audit (HiveMetaStore.java:logAuditEvent(339)) - ugi=hive/node2.proto.bsi.de@PROTO.BSI.DE    ip=unknown-ip-addr      cmd=Shutting down the object store...   
2015-04-07 16:38:48,193 INFO  [main]: metastore.HiveMetaStore (HiveMetaStore.java:logInfo(713)) - 0: Metastore shutdown complete.
2015-04-07 16:38:48,194 INFO  [main]: HiveMetaStore.audit (HiveMetaStore.java:logAuditEvent(339)) - ugi=hive/node2.proto.bsi.de@PROTO.BSI.DE    ip=unknown-ip-addr      cmd=Metastore shutdown complete.
        
2015-04-07 16:38:48,194 INFO  [main]: service.AbstractService (AbstractService.java:start(104)) - Service:ThriftBinaryCLIService is started.
2015-04-07 16:38:48,194 INFO  [main]: service.AbstractService (AbstractService.java:start(104)) - Service:HiveServer2 is started.

</description>
			<version>1.0.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.service.cli.CLIService.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">9685</link>
			<link type="Reference" description="is related to">1808</link>
		</links>
	</bug>
	<bug id="10322" opendate="2015-04-13 19:07:10" fixdate="2015-04-13 20:34:56" resolution="Duplicate">
		<buginformation>
			<summary>TestJdbcWithMiniHS2.testNewConnectionConfiguration fails</summary>
			<description>Fix test org.apache.hive.jdbc.TestJdbcWithMiniHS2.testNewConnectionConfiguration failed with following error:


org.apache.hive.service.cli.HiveSQLException: Failed to open new session: org.apache.hive.service.cli.HiveSQLException: java.lang.IllegalArgumentException: hive configuration hive.server2.thrift.http.max.worker.threads does not exists.
	at org.apache.hive.jdbc.Utils.verifySuccess(Utils.java:243)
	at org.apache.hive.jdbc.Utils.verifySuccess(Utils.java:234)
	at org.apache.hive.jdbc.HiveConnection.openSession(HiveConnection.java:513)
	at org.apache.hive.jdbc.HiveConnection.&amp;lt;init&amp;gt;(HiveConnection.java:188)
	at org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:105)
	at java.sql.DriverManager.getConnection(DriverManager.java:571)
	at java.sql.DriverManager.getConnection(DriverManager.java:233)
	at org.apache.hive.jdbc.TestJdbcWithMiniHS2.testNewConnectionConfiguration(TestJdbcWithMiniHS2.java:275)
Caused by: org.apache.hive.service.cli.HiveSQLException: Failed to open new session: org.apache.hive.service.cli.HiveSQLException: java.lang.IllegalArgumentException: hive configuration hive.server2.thrift.http.max.worker.threads does not exists.


It seems related to HIVE-10271(remove hive.server2.thrift.http.min/max.worker.threads properties) </description>
			<version>1.2.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">10309</link>
		</links>
	</bug>
	<bug id="10309" opendate="2015-04-11 08:23:34" fixdate="2015-04-13 20:52:39" resolution="Fixed">
		<buginformation>
			<summary>TestJdbcWithMiniHS2.java broken because of the removal of hive.server2.thrift.http.max.worker.threads </summary>
			<description>HIVE-10271 removed hive.server2.thrift.http.min/max.worker.threads properties, however these properties are used in a few more places in hive code. For example, TestJdbcWithMiniHS2.java . We need to fix these as well.</description>
			<version>1.2.0</version>
			<fixedVersion>1.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">10322</link>
			<link type="Regression" description="is broken by">10271</link>
		</links>
	</bug>
	<bug id="10356" opendate="2015-04-16 01:35:30" fixdate="2015-04-16 18:36:07" resolution="Duplicate">
		<buginformation>
			<summary>LLAP: query80 fails with vectorization cast issue </summary>
			<description>Reducer 6 fails:

Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134524737E8
\N\N01.2847032699693155E96.41569738480791E7-5.956161019898126E8
\N\N04.682909323885761E82.288924051203157E7-5.995957665973593E7
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:332)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:180)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:172)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:172)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:168)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134524737E8
\N\N01.2847032699693155E96.41569738480791E7-5.956161019898126E8
\N\N04.682909323885761E82.288924051203157E7-5.995957665973593E7
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:267)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:254)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:148)
	... 14 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134524737E8
\N\N01.2847032699693155E96.41569738480791E7-5.956161019898126E8
\N\N04.682909323885761E82.288924051203157E7-5.995957665973593E7
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:394)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:252)
	... 16 more
Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.copyGroupKey(VectorGroupKeyHelper.java:94)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeGroupBatches.processBatch(VectorGroupByOperator.java:729)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.process(VectorGroupByOperator.java:878)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:378)
	... 17 more
]], Vertex failed as one or more tasks failed. failedTasks:1, Vertex vertex_1428572510173_0231_1_24 [Reducer 5] killed/failed due to:null]Vertex killed, vertexName=Reducer 6, vertexId=vertex_1428572510173_0231_1_25, diagnostics=[Vertex received Kill while in RUNNING state., Vertex killed as other vertex failed. failedTasks:0, Vertex vertex_1428572510173_0231_1_25 [Reducer 6] killed/failed due to:null]DAG failed due to vertex failure. failedVertices:1 killedVertices:1


How to repro: run query80 on scale factor 200. I might look tomorrow to see if this is specific to LLAP or not</description>
			<version>0.14.0</version>
			<fixedVersion></fixedVersion>
			<type>Sub-task</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">10244</link>
		</links>
	</bug>
	<bug id="10493" opendate="2015-04-27 06:16:41" fixdate="2015-04-27 23:23:01" resolution="Duplicate">
		<buginformation>
			<summary>Merge multiple joins when join keys are the same</summary>
			<description>CBO return path: auto_join3.q is joined on the same key from 3 sources. It is translated into 2 map joins. Need to merge them into a single one.</description>
			<version>1.2.0</version>
			<fixedVersion>1.2.0</fixedVersion>
			<type>Sub-task</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.HiveCalciteUtil.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveInsertExchange4JoinRule.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">10071</link>
		</links>
	</bug>
	<bug id="10523" opendate="2015-04-28 19:23:14" fixdate="2015-04-28 19:34:51" resolution="Duplicate">
		<buginformation>
			<summary>Hive HCatalog Core 1.2.0 can not be built with hadoop-1 profile</summary>
			<description>I tried to built Hive 1.2.0 with hadoop-1 profile and got the following error in HCatalog Core


$ git status
On branch branch-1.2
Your branch is up-to-date with &amp;amp;apos;origin/branch-1.2&amp;amp;apos;

$ mvn clean install -DskipTests -Phadoop-1
...
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /workhive/hive/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java:[515,19] cannot find symbol
  symbol:   method isFile()
  location: variable fileStatus of type org.apache.hadoop.fs.FileStatus
[ERROR] /workhive/hive/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java:[545,26] cannot find symbol
  symbol:   method isDirectory()
  location: variable fileStatus of type org.apache.hadoop.fs.FileStatus
[INFO] 2 errors 
[INFO] -------------------------------------------------------------
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive ............................................... SUCCESS [  3.181 s]
[INFO] Hive Shims Common .................................. SUCCESS [  4.292 s]
[INFO] Hive Shims 0.20S ................................... SUCCESS [  1.035 s]
[INFO] Hive Shims 0.23 .................................... SUCCESS [  5.692 s]
[INFO] Hive Shims Scheduler ............................... SUCCESS [  1.681 s]
[INFO] Hive Shims ......................................... SUCCESS [  1.302 s]
[INFO] Hive Common ........................................ SUCCESS [  4.787 s]
[INFO] Hive Serde ......................................... SUCCESS [  5.501 s]
[INFO] Hive Metastore ..................................... SUCCESS [ 15.634 s]
[INFO] Hive Ant Utilities ................................. SUCCESS [  0.695 s]
[INFO] Spark Remote Client ................................ SUCCESS [  9.376 s]
[INFO] Hive Query Language ................................ SUCCESS [01:19 min]
[INFO] Hive Service ....................................... SUCCESS [  5.310 s]
[INFO] Hive Accumulo Handler .............................. SUCCESS [  2.462 s]
[INFO] Hive JDBC .......................................... SUCCESS [  8.817 s]
[INFO] Hive Beeline ....................................... SUCCESS [  1.636 s]
[INFO] Hive CLI ........................................... SUCCESS [  4.843 s]
[INFO] Hive Contrib ....................................... SUCCESS [  1.501 s]
[INFO] Hive HBase Handler ................................. SUCCESS [ 11.925 s]
[INFO] Hive HCatalog ...................................... SUCCESS [  0.265 s]
[INFO] Hive HCatalog Core ................................. FAILURE [  1.003 s]
[INFO] Hive HCatalog Pig Adapter .......................... SKIPPED
[INFO] Hive HCatalog Server Extensions .................... SKIPPED
[INFO] Hive HCatalog Webhcat Java Client .................. SKIPPED
[INFO] Hive HCatalog Webhcat .............................. SKIPPED
[INFO] Hive HCatalog Streaming ............................ SKIPPED
[INFO] Hive HWI ........................................... SKIPPED
[INFO] Hive ODBC .......................................... SKIPPED
[INFO] Hive Shims Aggregator .............................. SKIPPED
[INFO] Hive TestUtils ..................................... SKIPPED
[INFO] Hive Packaging ..................................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 02:51 min
[INFO] Finished at: 2015-04-28T12:20:09-07:00
[INFO] Final Memory: 179M/649M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project hive-hcatalog-core: Compilation failure: Compilation failure:
[ERROR] /workhive/hive/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java:[515,19] cannot find symbol
[ERROR] symbol:   method isFile()
[ERROR] location: variable fileStatus of type org.apache.hadoop.fs.FileStatus
[ERROR] /workhive/hive/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java:[545,26] cannot find symbol
[ERROR] symbol:   method isDirectory()
[ERROR] location: variable fileStatus of type org.apache.hadoop.fs.FileStatus
[ERROR] -&amp;gt; [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn &amp;lt;goals&amp;gt; -rf :hive-hcatalog-core

</description>
			<version>1.2.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.LaunchMapper.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">10444</link>
			<link type="Reference" description="relates to">10370</link>
			<link type="Reference" description="relates to">10442</link>
		</links>
	</bug>
	<bug id="10483" opendate="2015-04-24 20:54:21" fixdate="2015-04-29 03:13:43" resolution="Fixed">
		<buginformation>
			<summary>insert overwrite partition deadlocks on itself with DbTxnManager</summary>
			<description>insert overwrite ta partition(part=xxxx) select xxx from tb join ta where part=xxxx
It seems like the Shared conflicts with the Exclusive lock for Insert Overwrite even though both are part of the same txn.
More precisely insert overwrite requires X lock on partition and the read side needs an S lock on the query.
A simpler case is
insert overwrite ta partition(part=xxxx) select * from ta</description>
			<version>1.0.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.DbLockManager.java</file>
			<file type="M">org.apache.hadoop.hive.common.JavaUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">7483</link>
			<link type="Reference" description="is related to">15077</link>
		</links>
	</bug>
	<bug id="10071" opendate="2015-03-24 18:01:47" fixdate="2015-04-30 06:25:41" resolution="Fixed">
		<buginformation>
			<summary>CBO (Calcite Return Path): Join to MultiJoin rule</summary>
			<description>CBO return path: auto_join3.q can be used to reproduce the problem.</description>
			<version>1.2.0</version>
			<fixedVersion>1.2.0</fixedVersion>
			<type>Sub-task</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.HiveCalciteUtil.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveInsertExchange4JoinRule.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">10493</link>
			<link type="Reference" description="is related to">10533</link>
		</links>
	</bug>
	<bug id="5672" opendate="2013-10-28 17:46:41" fixdate="2015-04-30 21:10:14" resolution="Fixed">
		<buginformation>
			<summary>Insert with custom separator not supported for non-local directory</summary>
			<description>https://issues.apache.org/jira/browse/HIVE-3682 is great but non local directory don&amp;amp;apos;t seem to be supported:


insert overwrite directory &amp;amp;apos;/tmp/test-02&amp;amp;apos;
row format delimited
FIELDS TERMINATED BY &amp;amp;apos;:&amp;amp;apos;
select description FROM sample_07




Error while compiling statement: FAILED: ParseException line 2:0 cannot recognize input near &amp;amp;apos;row&amp;amp;apos; &amp;amp;apos;format&amp;amp;apos; &amp;amp;apos;delimited&amp;amp;apos; in select clause


This works (with &amp;amp;apos;local&amp;amp;apos;):


insert overwrite local directory &amp;amp;apos;/tmp/test-02&amp;amp;apos;
row format delimited
FIELDS TERMINATED BY &amp;amp;apos;:&amp;amp;apos;
select code, description FROM sample_07

</description>
			<version>0.12.0</version>
			<fixedVersion>1.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.QB.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">6833</link>
			<link type="Duplicate" description="is duplicated by">6410</link>
			<link type="Reference" description="relates to">13064</link>
			<link type="Reference" description="relates to">3682</link>
			<link type="Reference" description="is related to">518</link>
		</links>
	</bug>
	<bug id="10444" opendate="2015-04-22 18:56:26" fixdate="2015-05-02 00:40:47" resolution="Fixed">
		<buginformation>
			<summary>HIVE-10223 breaks hadoop-1 build</summary>
			<description>FileStatus.isFile() and FileStatus.isDirectory() methods added in HIVE-10223 are not present in hadoop 1.</description>
			<version>1.2.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.LaunchMapper.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="is blocked by">10443</link>
			<link type="Duplicate" description="is duplicated by">10523</link>
			<link type="Reference" description="relates to">10370</link>
			<link type="Reference" description="is related to">10066</link>
			<link type="Reference" description="is related to">10151</link>
			<link type="Regression" description="is broken by">10223</link>
		</links>
	</bug>
	<bug id="10541" opendate="2015-04-29 21:20:43" fixdate="2015-05-04 00:13:52" resolution="Fixed">
		<buginformation>
			<summary>Beeline requires newline at the end of each query in a file</summary>
			<description>Beeline requires newline at the end of each query in a file.</description>
			<version>0.13.1</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.beeline.BeeLine.java</file>
			<file type="M">org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">7475</link>
		</links>
	</bug>
	<bug id="9486" opendate="2015-01-28 00:58:09" fixdate="2015-05-08 23:59:33" resolution="Fixed">
		<buginformation>
			<summary>Use session classloader instead of application loader</summary>
			<description>From http://www.mail-archive.com/dev@hive.apache.org/msg107615.html
Looks reasonable</description>
			<version>1.2.1</version>
			<fixedVersion>1.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.hcatalog.streaming.AbstractRecordWriter.java</file>
			<file type="M">org.apache.hadoop.hive.accumulo.Utils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java</file>
			<file type="M">org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsAggregator.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.HCatSplit.java</file>
			<file type="M">org.apache.hadoop.hive.accumulo.serde.AccumuloSerDeParameters.java</file>
			<file type="M">org.apache.hive.hcatalog.messaging.MessageFactory.java</file>
			<file type="M">org.apache.hadoop.hive.common.JavaUtils.java</file>
			<file type="M">org.apache.hadoop.hive.accumulo.predicate.PrimitiveComparisonFilter.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.PTFDeserializer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.AbstractSMBJoinProc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.JobState.java</file>
			<file type="M">org.apache.hadoop.hive.hbase.HBaseSerDeHelper.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsPublisher.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.HivePreWarmProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.hbase.HBaseSerDeParameters.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.WriterImpl.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFReflect.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.FosterStorageHandler.java</file>
			<file type="M">org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">11733</link>
			<link type="Reference" description="is related to">10066</link>
		</links>
	</bug>
	<bug id="3942" opendate="2013-01-25 06:54:14" fixdate="2015-05-18 20:45:39" resolution="Duplicate">
		<buginformation>
			<summary>Add UDF month_add and month_sub </summary>
			<description>hive (default)&amp;gt; desc function extended month_add;
month_add(start_date, num_months) - Returns the date that is num_months after start_date.
Synonyms: month_sub
start_date is a string in the format &amp;amp;apos;yyyy-MM-dd HH:mm:ss&amp;amp;apos; or &amp;amp;apos;yyyy-MM-dd&amp;amp;apos;. num_months is a number. The time part of start_date is ignored.
Example:
   SELECT month_add(&amp;amp;apos;2012-04-12&amp;amp;apos;, 1) FROM src LIMIT 1; --Return 2012-05-12
  SELECT month_add(&amp;amp;apos;2012-04-12 11:22:31&amp;amp;apos;, 1) FROM src LIMIT 1; --Return 2012-05-12
  SELECT month_add(cast(&amp;amp;apos;2012-04-12 11:22:31&amp;amp;apos; as timestamp), 1) FROM src LIMIT 1; --Return 2012-05-12
hive (default)&amp;gt; desc function extended month_sub;
month_sub(start_date, num_months) - Returns the date that is num_months after start_date.
Synonyms: month_add
start_date is a string in the format &amp;amp;apos;yyyy-MM-dd HH:mm:ss&amp;amp;apos; or &amp;amp;apos;yyyy-MM-dd&amp;amp;apos;. num_months is a number. The time part of start_date is ignored.
Example:
   SELECT month_sub(&amp;amp;apos;2012-04-12&amp;amp;apos;, 1) FROM src LIMIT 1; --Return 2012-05-12
  SELECT month_sub(&amp;amp;apos;2012-04-12 11:22:31&amp;amp;apos;, 1) FROM src LIMIT 1; --Return 2012-05-12
  SELECT month_sub(cast(&amp;amp;apos;2012-04-12 11:22:31&amp;amp;apos; as timestamp), 1) FROM src LIMIT 1; --Return 2012-05-12</description>
			<version>0.9.0</version>
			<fixedVersion></fixedVersion>
			<type>New Feature</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">9357</link>
		</links>
	</bug>
	<bug id="10709" opendate="2015-05-14 16:25:24" fixdate="2015-05-20 16:01:36" resolution="Fixed">
		<buginformation>
			<summary>Update Avro version to 1.7.7</summary>
			<description>We should update the avro version to 1.7.7 to consumer some of the nicer compatibility features.</description>
			<version>0.13.1</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">8690</link>
		</links>
	</bug>
	<bug id="10677" opendate="2015-05-12 00:13:04" fixdate="2015-05-23 19:36:37" resolution="Fixed">
		<buginformation>
			<summary>hive.exec.parallel=true has problem when it is used for analyze table column stats</summary>
			<description>To reproduce it, in q tests.


hive&amp;gt; set hive.exec.parallel;
hive.exec.parallel=true
hive&amp;gt; analyze table src compute statistics for columns;
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.ColumnStatsTask
java.lang.RuntimeException: Error caching map.xml: java.io.IOException: java.lang.InterruptedException
	at org.apache.hadoop.hive.ql.exec.Utilities.setBaseWork(Utilities.java:747)
	at org.apache.hadoop.hive.ql.exec.Utilities.setMapWork(Utilities.java:682)
	at org.apache.hadoop.hive.ql.exec.Utilities.setMapRedWork(Utilities.java:674)
	at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:375)
	at org.apache.hadoop.hive.ql.exec.mr.MapRedTask.execute(MapRedTask.java:137)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:88)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.run(TaskRunner.java:75)
Caused by: java.io.IOException: java.lang.InterruptedException
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:541)
	at org.apache.hadoop.util.Shell.run(Shell.java:455)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:702)
	at org.apache.hadoop.util.Shell.execCommand(Shell.java:791)
	at org.apache.hadoop.util.Shell.execCommand(Shell.java:774)
	at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:646)
	at org.apache.hadoop.fs.FilterFileSystem.setPermission(FilterFileSystem.java:472)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:460)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:426)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:784)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:773)
	at org.apache.hadoop.hive.ql.exec.Utilities.setBaseWork(Utilities.java:715)
	... 7 more
hive&amp;gt; Job Submission failed with exception &amp;amp;apos;java.lang.RuntimeException(Error caching map.xml: java.io.IOException: java.lang.InterruptedException)&amp;amp;apos;

</description>
			<version>0.12.0</version>
			<fixedVersion>1.2.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">11069</link>
			<link type="Reference" description="is related to">10404</link>
		</links>
	</bug>
	<bug id="8690" opendate="2014-11-01 00:39:05" fixdate="2015-05-26 22:24:34" resolution="Duplicate">
		<buginformation>
			<summary>Move Avro dependency to 1.7.7</summary>
			<description>Move Avro dependency from 1.7.5 to current release 1.7.7.</description>
			<version>0.13.1</version>
			<fixedVersion></fixedVersion>
			<type>New Feature</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">10709</link>
		</links>
	</bug>
	<bug id="10244" opendate="2015-04-07 21:25:05" fixdate="2015-05-28 23:19:50" resolution="Fixed">
		<buginformation>
			<summary>Vectorization : TPC-DS Q80 fails with java.lang.ClassCastException when hive.vectorized.execution.reduce.enabled is enabled</summary>
			<description>Query 


set hive.vectorized.execution.reduce.enabled=true;
with ssr as
 (select  s_store_id as store_id,
          sum(ss_ext_sales_price) as sales,
          sum(coalesce(sr_return_amt, 0)) as returns,
          sum(ss_net_profit - coalesce(sr_net_loss, 0)) as profit
  from store_sales left outer join store_returns on
         (ss_item_sk = sr_item_sk and ss_ticket_number = sr_ticket_number),
     date_dim,
     store,
     item,
     promotion
 where ss_sold_date_sk = d_date_sk
       and d_date between cast(&amp;amp;apos;1998-08-04&amp;amp;apos; as date) 
                  and (cast(&amp;amp;apos;1998-09-04&amp;amp;apos; as date))
       and ss_store_sk = s_store_sk
       and ss_item_sk = i_item_sk
       and i_current_price &amp;gt; 50
       and ss_promo_sk = p_promo_sk
       and p_channel_tv = &amp;amp;apos;N&amp;amp;apos;
 group by s_store_id)
 ,
 csr as
 (select  cp_catalog_page_id as catalog_page_id,
          sum(cs_ext_sales_price) as sales,
          sum(coalesce(cr_return_amount, 0)) as returns,
          sum(cs_net_profit - coalesce(cr_net_loss, 0)) as profit
  from catalog_sales left outer join catalog_returns on
         (cs_item_sk = cr_item_sk and cs_order_number = cr_order_number),
     date_dim,
     catalog_page,
     item,
     promotion
 where cs_sold_date_sk = d_date_sk
       and d_date between cast(&amp;amp;apos;1998-08-04&amp;amp;apos; as date)
                  and (cast(&amp;amp;apos;1998-09-04&amp;amp;apos; as date))
        and cs_catalog_page_sk = cp_catalog_page_sk
       and cs_item_sk = i_item_sk
       and i_current_price &amp;gt; 50
       and cs_promo_sk = p_promo_sk
       and p_channel_tv = &amp;amp;apos;N&amp;amp;apos;
group by cp_catalog_page_id)
 ,
 wsr as
 (select  web_site_id,
          sum(ws_ext_sales_price) as sales,
          sum(coalesce(wr_return_amt, 0)) as returns,
          sum(ws_net_profit - coalesce(wr_net_loss, 0)) as profit
  from web_sales left outer join web_returns on
         (ws_item_sk = wr_item_sk and ws_order_number = wr_order_number),
     date_dim,
     web_site,
     item,
     promotion
 where ws_sold_date_sk = d_date_sk
       and d_date between cast(&amp;amp;apos;1998-08-04&amp;amp;apos; as date)
                  and (cast(&amp;amp;apos;1998-09-04&amp;amp;apos; as date))
        and ws_web_site_sk = web_site_sk
       and ws_item_sk = i_item_sk
       and i_current_price &amp;gt; 50
       and ws_promo_sk = p_promo_sk
       and p_channel_tv = &amp;amp;apos;N&amp;amp;apos;
group by web_site_id)
  select  channel
        , id
        , sum(sales) as sales
        , sum(returns) as returns
        , sum(profit) as profit
 from 
 (select &amp;amp;apos;store channel&amp;amp;apos; as channel
        , concat(&amp;amp;apos;store&amp;amp;apos;, store_id) as id
        , sales
        , returns
        , profit
 from   ssr
 union all
 select &amp;amp;apos;catalog channel&amp;amp;apos; as channel
        , concat(&amp;amp;apos;catalog_page&amp;amp;apos;, catalog_page_id) as id
        , sales
        , returns
        , profit
 from  csr
 union all
 select &amp;amp;apos;web channel&amp;amp;apos; as channel
        , concat(&amp;amp;apos;web_site&amp;amp;apos;, web_site_id) as id
        , sales
        , returns
        , profit
 from   wsr
 ) x
 group by channel, id with rollup
 order by channel
         ,id
 limit 100


Exception 


Vertex failed, vertexName=Reducer 5, vertexId=vertex_1426707664723_1377_1_22, diagnostics=[Task failed, taskId=task_1426707664723_1377_1_22_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:330)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:267)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:248)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:148)
	... 14 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:394)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:252)
	... 16 more
Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.copyGroupKey(VectorGroupKeyHelper.java:94)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeGroupBatches.processBatch(VectorGroupByOperator.java:729)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.process(VectorGroupByOperator.java:878)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:378)
	... 17 more
], TaskAttempt 1 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:330)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:267)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:248)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:148)
	... 14 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:394)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:252)
	... 16 more
Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.copyGroupKey(VectorGroupKeyHelper.java:94)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeGroupBatches.processBatch(VectorGroupByOperator.java:729)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.process(VectorGroupByOperator.java:878)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:378)
	... 17 more
], TaskAttempt 2 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:330)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:267)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:248)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:148)
	... 14 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:394)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:252)
	... 16 more
Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.copyGroupKey(VectorGroupKeyHelper.java:94)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeGroupBatches.processBatch(VectorGroupByOperator.java:729)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.process(VectorGroupByOperator.java:878)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:378)
	... 17 more
], TaskAttempt 3 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:330)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:267)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:248)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:148)
	... 14 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:394)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:252)
	... 16 more
Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.copyGroupKey(VectorGroupKeyHelper.java:94)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeGroupBatches.processBatch(VectorGroupByOperator.java:729)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.process(VectorGroupByOperator.java:878)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:378)
	... 17 more
]], Vertex failed as one or more tasks failed. failedTasks:1, Vertex vertex_1426707664723_1377_1_22 [Reducer 5] killed/failed due to:null]
15/04/07 05:14:52 [main]: ERROR SessionState: Vertex failed, vertexName=Reducer 5, vertexId=vertex_1426707664723_1377_1_22, diagnostics=[Task failed, taskId=task_1426707664723_1377_1_22_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:330)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:267)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:248)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:148)
	... 14 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:394)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:252)
	... 16 more
Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.copyGroupKey(VectorGroupKeyHelper.java:94)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeGroupBatches.processBatch(VectorGroupByOperator.java:729)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.process(VectorGroupByOperator.java:878)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:378)
	... 17 more
], TaskAttempt 1 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:330)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:267)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:248)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:148)
	... 14 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:394)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:252)
	... 16 more
Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.copyGroupKey(VectorGroupKeyHelper.java:94)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeGroupBatches.processBatch(VectorGroupByOperator.java:729)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.process(VectorGroupByOperator.java:878)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:378)
	... 17 more
], TaskAttempt 2 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:330)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:267)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:248)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:148)
	... 14 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:394)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:252)
	... 16 more
Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.copyGroupKey(VectorGroupKeyHelper.java:94)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeGroupBatches.processBatch(VectorGroupByOperator.java:729)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.process(VectorGroupByOperator.java:878)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:378)
	... 17 more
], TaskAttempt 3 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:330)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:267)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:248)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:148)
	... 14 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:394)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:252)
	... 16 more
Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.copyGroupKey(VectorGroupKeyHelper.java:94)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeGroupBatches.processBatch(VectorGroupByOperator.java:729)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.process(VectorGroupByOperator.java:878)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:378)
	... 17 more
]], Vertex failed as one or more tasks failed. failedTasks:1, Vertex vertex_1426707664723_1377_1_22 [Reducer 5] killed/failed due to:null]
Vertex killed, vertexName=Reducer 6, vertexId=vertex_1426707664723_1377_1_23, diagnostics=[Vertex received Kill while in RUNNING state., Vertex killed as other vertex failed. failedTasks:0, Vertex vertex_1426707664723_1377_1_23 [Reducer 6] killed/failed due to:null]
15/04/07 05:14:52 [main]: ERROR SessionState: Vertex killed, vertexName=Reducer 6, vertexId=vertex_1426707664723_1377_1_23, diagnostics=[Vertex received Kill while in RUNNING state., Vertex killed as other vertex failed. failedTasks:0, Vertex vertex_1426707664723_1377_1_23 [Reducer 6] killed/failed due to:null]
DAG failed due to vertex failure. failedVertices:1 killedVertices:1
15/04/07 05:14:52 [main]: ERROR SessionState: DAG failed due to vertex failure. failedVertices:1 killedVertices:1
FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Reducer 5, vertexId=vertex_1426707664723_1377_1_22, diagnostics=[Task failed, taskId=task_1426707664723_1377_1_22_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:330)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:267)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:248)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:148)
	... 14 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:394)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:252)
	... 16 more
Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.copyGroupKey(VectorGroupKeyHelper.java:94)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeGroupBatches.processBatch(VectorGroupByOperator.java:729)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.process(VectorGroupByOperator.java:878)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:378)
	... 17 more
], TaskAttempt 1 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:330)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:267)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:248)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:148)
	... 14 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:394)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:252)
	... 16 more
Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.copyGroupKey(VectorGroupKeyHelper.java:94)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeGroupBatches.processBatch(VectorGroupByOperator.java:729)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.process(VectorGroupByOperator.java:878)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:378)
	... 17 more
], TaskAttempt 2 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:330)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:267)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:248)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:148)
	... 14 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:394)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:252)
	... 16 more
Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.copyGroupKey(VectorGroupKeyHelper.java:94)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeGroupBatches.processBatch(VectorGroupByOperator.java:729)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.process(VectorGroupByOperator.java:878)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:378)
	... 17 more
], TaskAttempt 3 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:330)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:267)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:248)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:148)
	... 14 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:394)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:252)
	... 16 more
Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.copyGroupKey(VectorGroupKeyHelper.java:94)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeGroupBatches.processBatch(VectorGroupByOperator.java:729)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.process(VectorGroupByOperator.java:878)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:378)
	... 17 more
]], Vertex failed as one or more tasks failed. failedTasks:1, Vertex vertex_1426707664723_1377_1_22 [Reducer 5] killed/failed due to:null]Vertex killed, vertexName=Reducer 6, vertexId=vertex_1426707664723_1377_1_23, diagnostics=[Vertex received Kill while in RUNNING state., Vertex killed as other vertex failed. failedTasks:0, Vertex vertex_1426707664723_1377_1_23 [Reducer 6] killed/failed due to:null]DAG failed due to vertex failure. failedVertices:1 killedVertices:1
15/04/07 05:14:52 [main]: ERROR ql.Driver: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Reducer 5, vertexId=vertex_1426707664723_1377_1_22, diagnostics=[Task failed, taskId=task_1426707664723_1377_1_22_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:330)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:267)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:248)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:148)
	... 14 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:394)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:252)
	... 16 more
Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.copyGroupKey(VectorGroupKeyHelper.java:94)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeGroupBatches.processBatch(VectorGroupByOperator.java:729)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.process(VectorGroupByOperator.java:878)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:378)
	... 17 more
], TaskAttempt 1 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:330)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:267)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:248)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:148)
	... 14 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:394)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:252)
	... 16 more
Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.copyGroupKey(VectorGroupKeyHelper.java:94)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeGroupBatches.processBatch(VectorGroupByOperator.java:729)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.process(VectorGroupByOperator.java:878)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:378)
	... 17 more
], TaskAttempt 2 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:330)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:267)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:248)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:148)
	... 14 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:394)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:252)
	... 16 more
Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.copyGroupKey(VectorGroupKeyHelper.java:94)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeGroupBatches.processBatch(VectorGroupByOperator.java:729)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.process(VectorGroupByOperator.java:878)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:378)
	... 17 more
], TaskAttempt 3 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:330)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:267)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:248)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:148)
	... 14 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8
\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7
\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:394)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:252)
	... 16 more
Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.copyGroupKey(VectorGroupKeyHelper.java:94)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeGroupBatches.processBatch(VectorGroupByOperator.java:729)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.process(VectorGroupByOperator.java:878)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:378)
	... 17 more
]], Vertex failed as one or more tasks failed. failedTasks:1, Vertex vertex_1426707664723_1377_1_22 [Reducer 5] killed/failed due to:null]Vertex killed, vertexName=Reducer 6, vertexId=vertex_1426707664723_1377_1_23, diagnostics=[Vertex received Kill while in RUNNING state., Vertex killed as other vertex failed. failedTasks:0, Vertex vertex_1426707664723_1377_1_23 [Reducer 6] killed/failed due to:null]DAG failed due to vertex failure. failedVertices:1 killedVertices:1


Plan 



STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
      Edges:
        Map 12 &amp;lt;- Map 14 (BROADCAST_EDGE), Map 15 (BROADCAST_EDGE), Map 16 (BROADCAST_EDGE), Map 17 (BROADCAST_EDGE), Map 18 (BROADCAST_EDGE)
        Map 19 &amp;lt;- Map 21 (BROADCAST_EDGE), Map 22 (BROADCAST_EDGE), Map 23 (BROADCAST_EDGE), Map 24 (BROADCAST_EDGE), Map 25 (BROADCAST_EDGE)
        Reducer 13 &amp;lt;- Map 12 (SIMPLE_EDGE), Union 4 (CONTAINS)
        Reducer 2 &amp;lt;- Map 1 (SIMPLE_EDGE), Map 10 (BROADCAST_EDGE), Map 11 (BROADCAST_EDGE), Map 7 (SIMPLE_EDGE), Map 8 (BROADCAST_EDGE), Map 9 (BROADCAST_EDGE)
        Reducer 20 &amp;lt;- Map 19 (SIMPLE_EDGE), Union 4 (CONTAINS)
        Reducer 3 &amp;lt;- Reducer 2 (SIMPLE_EDGE), Union 4 (CONTAINS)
        Reducer 5 &amp;lt;- Union 4 (SIMPLE_EDGE)
        Reducer 6 &amp;lt;- Reducer 5 (SIMPLE_EDGE)
      DagName: mmokhtar_20150407051226_eb6d232e-cb00-4174-8b2f-d70aa2b3fb15:1
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: store_sales
                  filterExpr: ((ss_item_sk is not null and ss_promo_sk is not null) and ss_store_sk is not null) (type: boolean)
                  Statistics: Num rows: 550076554 Data size: 47370018896 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: ((ss_item_sk is not null and ss_promo_sk is not null) and ss_store_sk is not null) (type: boolean)
                    Statistics: Num rows: 524469260 Data size: 14487496336 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: ss_item_sk (type: int), ss_store_sk (type: int), ss_promo_sk (type: int), ss_ticket_number (type: int), ss_ext_sales_price (type: float), ss_net_profit (type: float), ss_sold_date_sk (type: int)
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                      Statistics: Num rows: 524469260 Data size: 14487496336 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: int), _col3 (type: int)
                        sort order: ++
                        Map-reduce partition columns: _col0 (type: int), _col3 (type: int)
                        Statistics: Num rows: 524469260 Data size: 14487496336 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col1 (type: int), _col2 (type: int), _col4 (type: float), _col5 (type: float), _col6 (type: int)
            Execution mode: vectorized
        Map 10 
            Map Operator Tree:
                TableScan
                  alias: promotion
                  filterExpr: ((p_channel_tv = &amp;amp;apos;N&amp;amp;apos;) and p_promo_sk is not null) (type: boolean)
                  Statistics: Num rows: 450 Data size: 530848 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: ((p_channel_tv = &amp;amp;apos;N&amp;amp;apos;) and p_promo_sk is not null) (type: boolean)
                    Statistics: Num rows: 225 Data size: 20025 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: p_promo_sk (type: int)
                      outputColumnNames: _col0
                      Statistics: Num rows: 225 Data size: 900 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Statistics: Num rows: 225 Data size: 900 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Map 11 
            Map Operator Tree:
                TableScan
                  alias: store
                  filterExpr: s_store_sk is not null (type: boolean)
                  Statistics: Num rows: 212 Data size: 405680 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: s_store_sk is not null (type: boolean)
                    Statistics: Num rows: 212 Data size: 22048 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: s_store_sk (type: int), s_store_id (type: string)
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 212 Data size: 22048 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Statistics: Num rows: 212 Data size: 22048 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col1 (type: string)
            Execution mode: vectorized
        Map 12 
            Map Operator Tree:
                TableScan
                  alias: catalog_sales
                  filterExpr: ((cs_item_sk is not null and cs_promo_sk is not null) and cs_catalog_page_sk is not null) (type: boolean)
                  Statistics: Num rows: 286549727 Data size: 37743959324 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: ((cs_item_sk is not null and cs_promo_sk is not null) and cs_catalog_page_sk is not null) (type: boolean)
                    Statistics: Num rows: 285112475 Data size: 7974560516 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: cs_catalog_page_sk (type: int), cs_item_sk (type: int), cs_promo_sk (type: int), cs_order_number (type: int), cs_ext_sales_price (type: float), cs_net_profit (type: float), cs_sold_date_sk (type: int)
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                      Statistics: Num rows: 285112475 Data size: 7974560516 Basic stats: COMPLETE Column stats: COMPLETE
                      Map Join Operator
                        condition map:
                             Left Outer Join0 to 1
                        keys:
                          0 _col1 (type: int), _col3 (type: int)
                          1 _col0 (type: int), _col1 (type: int)
                        outputColumnNames: _col0, _col1, _col2, _col4, _col5, _col6, _col9, _col10
                        input vertices:
                          1 Map 14
                        Statistics: Num rows: 3412616 Data size: 109203712 Basic stats: COMPLETE Column stats: COMPLETE
                        Map Join Operator
                          condition map:
                               Inner Join 0 to 1
                          keys:
                            0 _col6 (type: int)
                            1 _col0 (type: int)
                          outputColumnNames: _col0, _col1, _col2, _col4, _col5, _col9, _col10
                          input vertices:
                            1 Map 15
                          Statistics: Num rows: 3815661 Data size: 106838508 Basic stats: COMPLETE Column stats: COMPLETE
                          Map Join Operator
                            condition map:
                                 Inner Join 0 to 1
                            keys:
                              0 _col1 (type: int)
                              1 _col0 (type: int)
                            outputColumnNames: _col0, _col2, _col4, _col5, _col9, _col10
                            input vertices:
                              1 Map 16
                            Statistics: Num rows: 1271887 Data size: 30525288 Basic stats: COMPLETE Column stats: COMPLETE
                            Map Join Operator
                              condition map:
                                   Inner Join 0 to 1
                              keys:
                                0 _col2 (type: int)
                                1 _col0 (type: int)
                              outputColumnNames: _col0, _col4, _col5, _col9, _col10
                              input vertices:
                                1 Map 17
                              Statistics: Num rows: 635944 Data size: 12718880 Basic stats: COMPLETE Column stats: COMPLETE
                              Map Join Operator
                                condition map:
                                     Inner Join 0 to 1
                                keys:
                                  0 _col0 (type: int)
                                  1 _col0 (type: int)
                                outputColumnNames: _col4, _col5, _col9, _col10, _col18
                                input vertices:
                                  1 Map 18
                                Statistics: Num rows: 635944 Data size: 73769504 Basic stats: COMPLETE Column stats: COMPLETE
                                Select Operator
                                  expressions: _col18 (type: string), _col4 (type: float), COALESCE(_col9,0) (type: float), (_col5 - COALESCE(_col10,0)) (type: float)
                                  outputColumnNames: _col0, _col1, _col2, _col3
                                  Statistics: Num rows: 635944 Data size: 73769504 Basic stats: COMPLETE Column stats: COMPLETE
                                  Group By Operator
                                    aggregations: sum(_col1), sum(_col2), sum(_col3)
                                    keys: _col0 (type: string)
                                    mode: hash
                                    outputColumnNames: _col0, _col1, _col2, _col3
                                    Statistics: Num rows: 10590 Data size: 1313160 Basic stats: COMPLETE Column stats: COMPLETE
                                    Reduce Output Operator
                                      key expressions: _col0 (type: string)
                                      sort order: +
                                      Map-reduce partition columns: _col0 (type: string)
                                      Statistics: Num rows: 10590 Data size: 1313160 Basic stats: COMPLETE Column stats: COMPLETE
                                      value expressions: _col1 (type: double), _col2 (type: double), _col3 (type: double)
            Execution mode: vectorized
        Map 14 
            Map Operator Tree:
                TableScan
                  alias: catalog_returns
                  filterExpr: cr_item_sk is not null (type: boolean)
                  Statistics: Num rows: 28798881 Data size: 2942039156 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: cr_item_sk is not null (type: boolean)
                    Statistics: Num rows: 28798881 Data size: 456171072 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: cr_item_sk (type: int), cr_order_number (type: int), cr_return_amount (type: float), cr_net_loss (type: float)
                      outputColumnNames: _col0, _col1, _col2, _col3
                      Statistics: Num rows: 28798881 Data size: 456171072 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: int), _col1 (type: int)
                        sort order: ++
                        Map-reduce partition columns: _col0 (type: int), _col1 (type: int)
                        Statistics: Num rows: 28798881 Data size: 456171072 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col2 (type: float), _col3 (type: float)
            Execution mode: vectorized
        Map 15 
            Map Operator Tree:
                TableScan
                  alias: date_dim
                  filterExpr: (d_date BETWEEN 1998-08-04 AND 1998-09-04 and d_date_sk is not null) (type: boolean)
                  Statistics: Num rows: 73049 Data size: 81741831 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: (d_date BETWEEN 1998-08-04 AND 1998-09-04 and d_date_sk is not null) (type: boolean)
                    Statistics: Num rows: 36524 Data size: 3579352 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: d_date_sk (type: int)
                      outputColumnNames: _col0
                      Statistics: Num rows: 36524 Data size: 146096 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Statistics: Num rows: 36524 Data size: 146096 Basic stats: COMPLETE Column stats: COMPLETE
                      Select Operator
                        expressions: _col0 (type: int)
                        outputColumnNames: _col0
                        Statistics: Num rows: 36524 Data size: 146096 Basic stats: COMPLETE Column stats: COMPLETE
                        Group By Operator
                          keys: _col0 (type: int)
                          mode: hash
                          outputColumnNames: _col0
                          Statistics: Num rows: 18262 Data size: 73048 Basic stats: COMPLETE Column stats: COMPLETE
                          Dynamic Partitioning Event Operator
                            Target Input: catalog_sales
                            Partition key expr: cs_sold_date_sk
                            Statistics: Num rows: 18262 Data size: 73048 Basic stats: COMPLETE Column stats: COMPLETE
                            Target column: cs_sold_date_sk
                            Target Vertex: Map 12
            Execution mode: vectorized
        Map 16 
            Map Operator Tree:
                TableScan
                  alias: item
                  filterExpr: ((i_current_price &amp;gt; 50.0) and i_item_sk is not null) (type: boolean)
                  Statistics: Num rows: 48000 Data size: 68732712 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: ((i_current_price &amp;gt; 50.0) and i_item_sk is not null) (type: boolean)
                    Statistics: Num rows: 16000 Data size: 127832 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: i_item_sk (type: int)
                      outputColumnNames: _col0
                      Statistics: Num rows: 16000 Data size: 64000 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Statistics: Num rows: 16000 Data size: 64000 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Map 17 
            Map Operator Tree:
                TableScan
                  alias: promotion
                  filterExpr: ((p_channel_tv = &amp;amp;apos;N&amp;amp;apos;) and p_promo_sk is not null) (type: boolean)
                  Statistics: Num rows: 450 Data size: 530848 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: ((p_channel_tv = &amp;amp;apos;N&amp;amp;apos;) and p_promo_sk is not null) (type: boolean)
                    Statistics: Num rows: 225 Data size: 20025 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: p_promo_sk (type: int)
                      outputColumnNames: _col0
                      Statistics: Num rows: 225 Data size: 900 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Statistics: Num rows: 225 Data size: 900 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Map 18 
            Map Operator Tree:
                TableScan
                  alias: catalog_page
                  filterExpr: cp_catalog_page_sk is not null (type: boolean)
                  Statistics: Num rows: 11718 Data size: 5400282 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: cp_catalog_page_sk is not null (type: boolean)
                    Statistics: Num rows: 11718 Data size: 1218672 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: cp_catalog_page_sk (type: int), cp_catalog_page_id (type: string)
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 11718 Data size: 1218672 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Statistics: Num rows: 11718 Data size: 1218672 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col1 (type: string)
            Execution mode: vectorized
        Map 19 
            Map Operator Tree:
                TableScan
                  alias: web_sales
                  filterExpr: ((ws_item_sk is not null and ws_promo_sk is not null) and ws_web_site_sk is not null) (type: boolean)
                  Statistics: Num rows: 143966864 Data size: 19001610332 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: ((ws_item_sk is not null and ws_promo_sk is not null) and ws_web_site_sk is not null) (type: boolean)
                    Statistics: Num rows: 143930635 Data size: 4029840544 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: ws_item_sk (type: int), ws_web_site_sk (type: int), ws_promo_sk (type: int), ws_order_number (type: int), ws_ext_sales_price (type: float), ws_net_profit (type: float), ws_sold_date_sk (type: int)
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                      Statistics: Num rows: 143930635 Data size: 4029840544 Basic stats: COMPLETE Column stats: COMPLETE
                      Map Join Operator
                        condition map:
                             Left Outer Join0 to 1
                        keys:
                          0 _col0 (type: int), _col3 (type: int)
                          1 _col0 (type: int), _col1 (type: int)
                        outputColumnNames: _col0, _col1, _col2, _col4, _col5, _col6, _col9, _col10
                        input vertices:
                          1 Map 21
                        Statistics: Num rows: 2406359 Data size: 77003488 Basic stats: COMPLETE Column stats: COMPLETE
                        Map Join Operator
                          condition map:
                               Inner Join 0 to 1
                          keys:
                            0 _col6 (type: int)
                            1 _col0 (type: int)
                          outputColumnNames: _col0, _col1, _col2, _col4, _col5, _col9, _col10
                          input vertices:
                            1 Map 22
                          Statistics: Num rows: 2690560 Data size: 75335680 Basic stats: COMPLETE Column stats: COMPLETE
                          Map Join Operator
                            condition map:
                                 Inner Join 0 to 1
                            keys:
                              0 _col0 (type: int)
                              1 _col0 (type: int)
                            outputColumnNames: _col1, _col2, _col4, _col5, _col9, _col10
                            input vertices:
                              1 Map 23
                            Statistics: Num rows: 896854 Data size: 21524496 Basic stats: COMPLETE Column stats: COMPLETE
                            Map Join Operator
                              condition map:
                                   Inner Join 0 to 1
                              keys:
                                0 _col2 (type: int)
                                1 _col0 (type: int)
                              outputColumnNames: _col1, _col4, _col5, _col9, _col10
                              input vertices:
                                1 Map 24
                              Statistics: Num rows: 448427 Data size: 8968540 Basic stats: COMPLETE Column stats: COMPLETE
                              Map Join Operator
                                condition map:
                                     Inner Join 0 to 1
                                keys:
                                  0 _col1 (type: int)
                                  1 _col0 (type: int)
                                outputColumnNames: _col4, _col5, _col9, _col10, _col18
                                input vertices:
                                  1 Map 25
                                Statistics: Num rows: 448427 Data size: 52017532 Basic stats: COMPLETE Column stats: COMPLETE
                                Select Operator
                                  expressions: _col18 (type: string), _col4 (type: float), COALESCE(_col9,0) (type: float), (_col5 - COALESCE(_col10,0)) (type: float)
                                  outputColumnNames: _col0, _col1, _col2, _col3
                                  Statistics: Num rows: 448427 Data size: 52017532 Basic stats: COMPLETE Column stats: COMPLETE
                                  Group By Operator
                                    aggregations: sum(_col1), sum(_col2), sum(_col3)
                                    keys: _col0 (type: string)
                                    mode: hash
                                    outputColumnNames: _col0, _col1, _col2, _col3
                                    Statistics: Num rows: 17 Data size: 2108 Basic stats: COMPLETE Column stats: COMPLETE
                                    Reduce Output Operator
                                      key expressions: _col0 (type: string)
                                      sort order: +
                                      Map-reduce partition columns: _col0 (type: string)
                                      Statistics: Num rows: 17 Data size: 2108 Basic stats: COMPLETE Column stats: COMPLETE
                                      value expressions: _col1 (type: double), _col2 (type: double), _col3 (type: double)
            Execution mode: vectorized
        Map 21 
            Map Operator Tree:
                TableScan
                  alias: web_returns
                  filterExpr: wr_item_sk is not null (type: boolean)
                  Statistics: Num rows: 13749816 Data size: 1237758344 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: wr_item_sk is not null (type: boolean)
                    Statistics: Num rows: 13749816 Data size: 217404672 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: wr_item_sk (type: int), wr_order_number (type: int), wr_return_amt (type: float), wr_net_loss (type: float)
                      outputColumnNames: _col0, _col1, _col2, _col3
                      Statistics: Num rows: 13749816 Data size: 217404672 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: int), _col1 (type: int)
                        sort order: ++
                        Map-reduce partition columns: _col0 (type: int), _col1 (type: int)
                        Statistics: Num rows: 13749816 Data size: 217404672 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col2 (type: float), _col3 (type: float)
            Execution mode: vectorized
        Map 22 
            Map Operator Tree:
                TableScan
                  alias: date_dim
                  filterExpr: (d_date BETWEEN 1998-08-04 AND 1998-09-04 and d_date_sk is not null) (type: boolean)
                  Statistics: Num rows: 73049 Data size: 81741831 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: (d_date BETWEEN 1998-08-04 AND 1998-09-04 and d_date_sk is not null) (type: boolean)
                    Statistics: Num rows: 36524 Data size: 3579352 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: d_date_sk (type: int)
                      outputColumnNames: _col0
                      Statistics: Num rows: 36524 Data size: 146096 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Statistics: Num rows: 36524 Data size: 146096 Basic stats: COMPLETE Column stats: COMPLETE
                      Select Operator
                        expressions: _col0 (type: int)
                        outputColumnNames: _col0
                        Statistics: Num rows: 36524 Data size: 146096 Basic stats: COMPLETE Column stats: COMPLETE
                        Group By Operator
                          keys: _col0 (type: int)
                          mode: hash
                          outputColumnNames: _col0
                          Statistics: Num rows: 18262 Data size: 73048 Basic stats: COMPLETE Column stats: COMPLETE
                          Dynamic Partitioning Event Operator
                            Target Input: web_sales
                            Partition key expr: ws_sold_date_sk
                            Statistics: Num rows: 18262 Data size: 73048 Basic stats: COMPLETE Column stats: COMPLETE
                            Target column: ws_sold_date_sk
                            Target Vertex: Map 19
            Execution mode: vectorized
        Map 23 
            Map Operator Tree:
                TableScan
                  alias: item
                  filterExpr: ((i_current_price &amp;gt; 50.0) and i_item_sk is not null) (type: boolean)
                  Statistics: Num rows: 48000 Data size: 68732712 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: ((i_current_price &amp;gt; 50.0) and i_item_sk is not null) (type: boolean)
                    Statistics: Num rows: 16000 Data size: 127832 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: i_item_sk (type: int)
                      outputColumnNames: _col0
                      Statistics: Num rows: 16000 Data size: 64000 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Statistics: Num rows: 16000 Data size: 64000 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Map 24 
            Map Operator Tree:
                TableScan
                  alias: promotion
                  filterExpr: ((p_channel_tv = &amp;amp;apos;N&amp;amp;apos;) and p_promo_sk is not null) (type: boolean)
                  Statistics: Num rows: 450 Data size: 530848 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: ((p_channel_tv = &amp;amp;apos;N&amp;amp;apos;) and p_promo_sk is not null) (type: boolean)
                    Statistics: Num rows: 225 Data size: 20025 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: p_promo_sk (type: int)
                      outputColumnNames: _col0
                      Statistics: Num rows: 225 Data size: 900 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Statistics: Num rows: 225 Data size: 900 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Map 25 
            Map Operator Tree:
                TableScan
                  alias: web_site
                  filterExpr: web_site_sk is not null (type: boolean)
                  Statistics: Num rows: 38 Data size: 70614 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: web_site_sk is not null (type: boolean)
                    Statistics: Num rows: 38 Data size: 3952 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: web_site_sk (type: int), web_site_id (type: string)
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 38 Data size: 3952 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Statistics: Num rows: 38 Data size: 3952 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col1 (type: string)
            Execution mode: vectorized
        Map 7 
            Map Operator Tree:
                TableScan
                  alias: store_returns
                  filterExpr: sr_item_sk is not null (type: boolean)
                  Statistics: Num rows: 55578005 Data size: 4155315616 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: sr_item_sk is not null (type: boolean)
                    Statistics: Num rows: 55578005 Data size: 881176504 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: sr_item_sk (type: int), sr_ticket_number (type: int), sr_return_amt (type: float), sr_net_loss (type: float)
                      outputColumnNames: _col0, _col1, _col2, _col3
                      Statistics: Num rows: 55578005 Data size: 881176504 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: int), _col1 (type: int)
                        sort order: ++
                        Map-reduce partition columns: _col0 (type: int), _col1 (type: int)
                        Statistics: Num rows: 55578005 Data size: 881176504 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col2 (type: float), _col3 (type: float)
            Execution mode: vectorized
        Map 8 
            Map Operator Tree:
                TableScan
                  alias: date_dim
                  filterExpr: (d_date BETWEEN 1998-08-04 AND 1998-09-04 and d_date_sk is not null) (type: boolean)
                  Statistics: Num rows: 73049 Data size: 81741831 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: (d_date BETWEEN 1998-08-04 AND 1998-09-04 and d_date_sk is not null) (type: boolean)
                    Statistics: Num rows: 36524 Data size: 3579352 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: d_date_sk (type: int)
                      outputColumnNames: _col0
                      Statistics: Num rows: 36524 Data size: 146096 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Statistics: Num rows: 36524 Data size: 146096 Basic stats: COMPLETE Column stats: COMPLETE
                      Select Operator
                        expressions: _col0 (type: int)
                        outputColumnNames: _col0
                        Statistics: Num rows: 36524 Data size: 146096 Basic stats: COMPLETE Column stats: COMPLETE
                        Group By Operator
                          keys: _col0 (type: int)
                          mode: hash
                          outputColumnNames: _col0
                          Statistics: Num rows: 18262 Data size: 73048 Basic stats: COMPLETE Column stats: COMPLETE
                          Dynamic Partitioning Event Operator
                            Target Input: store_sales
                            Partition key expr: ss_sold_date_sk
                            Statistics: Num rows: 18262 Data size: 73048 Basic stats: COMPLETE Column stats: COMPLETE
                            Target column: ss_sold_date_sk
                            Target Vertex: Map 1
            Execution mode: vectorized
        Map 9 
            Map Operator Tree:
                TableScan
                  alias: item
                  filterExpr: ((i_current_price &amp;gt; 50.0) and i_item_sk is not null) (type: boolean)
                  Statistics: Num rows: 48000 Data size: 68732712 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: ((i_current_price &amp;gt; 50.0) and i_item_sk is not null) (type: boolean)
                    Statistics: Num rows: 16000 Data size: 127832 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: i_item_sk (type: int)
                      outputColumnNames: _col0
                      Statistics: Num rows: 16000 Data size: 64000 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Statistics: Num rows: 16000 Data size: 64000 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Reducer 13 
            Reduce Operator Tree:
              Group By Operator
                aggregations: sum(VALUE._col0), sum(VALUE._col1), sum(VALUE._col2)
                keys: KEY._col0 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3
                Select Operator
                  expressions: &amp;amp;apos;catalog channel&amp;amp;apos; (type: string), concat(&amp;amp;apos;catalog_page&amp;amp;apos;, _col0) (type: string), _col1 (type: double), _col2 (type: double), _col3 (type: double)
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4
                  Group By Operator
                    aggregations: sum(_col2), sum(_col3), sum(_col4)
                    keys: _col0 (type: string), _col1 (type: string), &amp;amp;apos;0&amp;amp;apos; (type: string)
                    mode: hash
                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                    Reduce Output Operator
                      key expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string)
                      sort order: +++
                      Map-reduce partition columns: _col0 (type: string), _col1 (type: string), _col2 (type: string)
                      value expressions: _col3 (type: double), _col4 (type: double), _col5 (type: double)
        Reducer 2 
            Reduce Operator Tree:
              Merge Join Operator
                condition map:
                     Left Outer Join0 to 1
                keys:
                  0 _col0 (type: int), _col3 (type: int)
                  1 _col0 (type: int), _col1 (type: int)
                outputColumnNames: _col0, _col1, _col2, _col4, _col5, _col6, _col9, _col10
                Statistics: Num rows: 7811006 Data size: 249952192 Basic stats: COMPLETE Column stats: COMPLETE
                Map Join Operator
                  condition map:
                       Inner Join 0 to 1
                  keys:
                    0 _col6 (type: int)
                    1 _col0 (type: int)
                  outputColumnNames: _col0, _col1, _col2, _col4, _col5, _col9, _col10
                  input vertices:
                    1 Map 8
                  Statistics: Num rows: 8733520 Data size: 244538560 Basic stats: COMPLETE Column stats: COMPLETE
                  Map Join Operator
                    condition map:
                         Inner Join 0 to 1
                    keys:
                      0 _col0 (type: int)
                      1 _col0 (type: int)
                    outputColumnNames: _col1, _col2, _col4, _col5, _col9, _col10
                    input vertices:
                      1 Map 9
                    Statistics: Num rows: 2911174 Data size: 69868176 Basic stats: COMPLETE Column stats: COMPLETE
                    Map Join Operator
                      condition map:
                           Inner Join 0 to 1
                      keys:
                        0 _col2 (type: int)
                        1 _col0 (type: int)
                      outputColumnNames: _col1, _col4, _col5, _col9, _col10
                      input vertices:
                        1 Map 10
                      Statistics: Num rows: 1455587 Data size: 29111740 Basic stats: COMPLETE Column stats: COMPLETE
                      Map Join Operator
                        condition map:
                             Inner Join 0 to 1
                        keys:
                          0 _col1 (type: int)
                          1 _col0 (type: int)
                        outputColumnNames: _col4, _col5, _col9, _col10, _col18
                        input vertices:
                          1 Map 11
                        Statistics: Num rows: 1455587 Data size: 168848092 Basic stats: COMPLETE Column stats: COMPLETE
                        Select Operator
                          expressions: _col18 (type: string), _col4 (type: float), COALESCE(_col9,0) (type: float), (_col5 - COALESCE(_col10,0)) (type: float)
                          outputColumnNames: _col0, _col1, _col2, _col3
                          Statistics: Num rows: 1455587 Data size: 168848092 Basic stats: COMPLETE Column stats: COMPLETE
                          Group By Operator
                            aggregations: sum(_col1), sum(_col2), sum(_col3)
                            keys: _col0 (type: string)
                            mode: hash
                            outputColumnNames: _col0, _col1, _col2, _col3
                            Statistics: Num rows: 234 Data size: 29016 Basic stats: COMPLETE Column stats: COMPLETE
                            Reduce Output Operator
                              key expressions: _col0 (type: string)
                              sort order: +
                              Map-reduce partition columns: _col0 (type: string)
                              Statistics: Num rows: 234 Data size: 29016 Basic stats: COMPLETE Column stats: COMPLETE
                              value expressions: _col1 (type: double), _col2 (type: double), _col3 (type: double)
        Reducer 20 
            Reduce Operator Tree:
              Group By Operator
                aggregations: sum(VALUE._col0), sum(VALUE._col1), sum(VALUE._col2)
                keys: KEY._col0 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3
                Select Operator
                  expressions: &amp;amp;apos;web channel&amp;amp;apos; (type: string), concat(&amp;amp;apos;web_site&amp;amp;apos;, _col0) (type: string), _col1 (type: double), _col2 (type: double), _col3 (type: double)
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4
                  Group By Operator
                    aggregations: sum(_col2), sum(_col3), sum(_col4)
                    keys: _col0 (type: string), _col1 (type: string), &amp;amp;apos;0&amp;amp;apos; (type: string)
                    mode: hash
                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                    Reduce Output Operator
                      key expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string)
                      sort order: +++
                      Map-reduce partition columns: _col0 (type: string), _col1 (type: string), _col2 (type: string)
                      value expressions: _col3 (type: double), _col4 (type: double), _col5 (type: double)
        Reducer 3 
            Reduce Operator Tree:
              Group By Operator
                aggregations: sum(VALUE._col0), sum(VALUE._col1), sum(VALUE._col2)
                keys: KEY._col0 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3
                Select Operator
                  expressions: &amp;amp;apos;store channel&amp;amp;apos; (type: string), concat(&amp;amp;apos;store&amp;amp;apos;, _col0) (type: string), _col1 (type: double), _col2 (type: double), _col3 (type: double)
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4
                  Group By Operator
                    aggregations: sum(_col2), sum(_col3), sum(_col4)
                    keys: _col0 (type: string), _col1 (type: string), &amp;amp;apos;0&amp;amp;apos; (type: string)
                    mode: hash
                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                    Reduce Output Operator
                      key expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string)
                      sort order: +++
                      Map-reduce partition columns: _col0 (type: string), _col1 (type: string), _col2 (type: string)
                      value expressions: _col3 (type: double), _col4 (type: double), _col5 (type: double)
        Reducer 5 
            Reduce Operator Tree:
              Group By Operator
                aggregations: sum(VALUE._col0), sum(VALUE._col1), sum(VALUE._col2)
                keys: KEY._col0 (type: string), KEY._col1 (type: string), KEY._col2 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col3, _col4, _col5
                Statistics: Num rows: 1 Data size: 24 Basic stats: COMPLETE Column stats: COMPLETE
                pruneGroupingSetId: true
                Select Operator
                  expressions: _col0 (type: string), _col1 (type: string), _col3 (type: double), _col4 (type: double), _col5 (type: double)
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4
                  Statistics: Num rows: 1 Data size: 24 Basic stats: COMPLETE Column stats: COMPLETE
                  Reduce Output Operator
                    key expressions: _col0 (type: string), _col1 (type: string)
                    sort order: ++
                    Statistics: Num rows: 1 Data size: 24 Basic stats: COMPLETE Column stats: COMPLETE
                    TopN Hash Memory Usage: 0.04
                    value expressions: _col2 (type: double), _col3 (type: double), _col4 (type: double)
            Execution mode: vectorized
        Reducer 6 
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: string), KEY.reducesinkkey1 (type: string), VALUE._col0 (type: double), VALUE._col1 (type: double), VALUE._col2 (type: double)
                outputColumnNames: _col0, _col1, _col2, _col3, _col4
                Statistics: Num rows: 1 Data size: 24 Basic stats: COMPLETE Column stats: COMPLETE
                Limit
                  Number of rows: 100
                  Statistics: Num rows: 1 Data size: 24 Basic stats: COMPLETE Column stats: COMPLETE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 1 Data size: 24 Basic stats: COMPLETE Column stats: COMPLETE
                    table:
                        input format: org.apache.hadoop.mapred.TextInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized
        Union 4 
            Vertex: Union 4

  Stage: Stage-0
    Fetch Operator
      limit: 100
      Processor Tree:
        ListSink

</description>
			<version>0.14.0</version>
			<fixedVersion>1.2.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">10356</link>
		</links>
	</bug>
	<bug id="9718" opendate="2015-02-18 20:04:10" fixdate="2015-05-29 00:26:44" resolution="Duplicate">
		<buginformation>
			<summary>Insert into dynamic partitions with same column structure in the "distibute by" clause barfs</summary>
			<description>Sample reproducible query: 


SET hive.exec.dynamic.partition.mode=nonstrict;
SET hive.exec.dynamic.partition=true;

 insert overwrite table nation_new_p partition (some)
select n_name as name1, n_name as name2, n_name as name3 from nation distribute by name3;


Note: Make sure there is data in the source table to reproduce the issue. 
During the optimizations done for Jira: https://issues.apache.org/jira/browse/HIVE-4867, an optimization of deduplication of columns is done. But, when one of the columns is used as part of partitioned/distribute by, its not taken care of.  
The above query produces exception as follows:


Diagnostic Messages for this Task:
java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {"n_nationkey":0,"n_name":"ALGERIA","n_regionkey":0,"n_comment":" haggle. carefully final deposits detect slyly agai"}
	at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:185)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)
	at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runSubtask(LocalContainerLauncher.java:370)
	at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runTask(LocalContainerLauncher.java:295)
	at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.access$200(LocalContainerLauncher.java:181)
	at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler$1.run(LocalContainerLauncher.java:224)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {"n_nationkey":0,"n_name":"ALGERIA","n_regionkey":0,"n_comment":" haggle. carefully final deposits detect slyly agai"}
	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:503)
	at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:176)
	... 12 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: cannot find field _col2 from [0:_col0]
	at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.processOp(ReduceSinkOperator.java:397)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95)
	at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:157)
	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:493)
	... 13 more
Caused by: java.lang.RuntimeException: cannot find field _col2 from [0:_col0]
	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.getStandardStructFieldRef(ObjectInspectorUtils.java:410)
	at org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getStructFieldRef(StandardStructObjectInspector.java:147)
	at org.apache.hadoop.hive.ql.exec.ExprNodeColumnEvaluator.initialize(ExprNodeColumnEvaluator.java:55)
	at org.apache.hadoop.hive.ql.exec.Operator.initEvaluators(Operator.java:954)
	at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.processOp(ReduceSinkOperator.java:325)
	... 19 more


Tables used are: 


CREATE EXTERNAL TABLE `nation`(
  `n_nationkey` int,
  `n_name` string,
  `n_regionkey` int,
  `n_comment` string)
ROW FORMAT DELIMITED
  FIELDS TERMINATED BY &amp;amp;apos;|&amp;amp;apos;
STORED AS INPUTFORMAT
  &amp;amp;apos;org.apache.hadoop.mapred.TextInputFormat&amp;amp;apos;
OUTPUTFORMAT
  &amp;amp;apos;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&amp;amp;apos;;


and 


CREATE TABLE `nation_new_p`(
  `n_name1` string,
  `n_name2` string)
PARTITIONED BY (
  `some` string)
ROW FORMAT DELIMITED
  FIELDS TERMINATED BY &amp;amp;apos;|&amp;amp;apos;
STORED AS INPUTFORMAT
  &amp;amp;apos;org.apache.hadoop.mapred.TextInputFormat&amp;amp;apos;
OUTPUTFORMAT
  &amp;amp;apos;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&amp;amp;apos;


Sample data for the table is provided by the file attached with. </description>
			<version>0.14.0</version>
			<fixedVersion>1.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">9655</link>
		</links>
	</bug>
	<bug id="10879" opendate="2015-06-01 15:23:02" fixdate="2015-06-01 15:32:10" resolution="Duplicate">
		<buginformation>
			<summary>The bucket number is not respected in insert overwrite.</summary>
			<description>When hive.enforce.bucketing is true, the bucket number defined in the table is no longer respected in current master and 1.2. This is a regression.
Reproduce:

CREATE TABLE IF NOT EXISTS buckettestinput( 
data string 
) 
ROW FORMAT DELIMITED FIELDS TERMINATED BY &amp;amp;apos;,&amp;amp;apos;;
CREATE TABLE IF NOT EXISTS buckettestoutput1( 
data string 
)CLUSTERED BY(data) 
INTO 2 BUCKETS 
ROW FORMAT DELIMITED FIELDS TERMINATED BY &amp;amp;apos;,&amp;amp;apos;;
CREATE TABLE IF NOT EXISTS buckettestoutput2( 
data string 
)CLUSTERED BY(data) 
INTO 2 BUCKETS 
ROW FORMAT DELIMITED FIELDS TERMINATED BY &amp;amp;apos;,&amp;amp;apos;;
Then I inserted the following data into the "buckettestinput" table
firstinsert1 
firstinsert2 
firstinsert3 
firstinsert4 
firstinsert5 
firstinsert6 
firstinsert7 
firstinsert8 
secondinsert1 
secondinsert2 
secondinsert3 
secondinsert4 
secondinsert5 
secondinsert6 
secondinsert7 
secondinsert8
set hive.enforce.bucketing = true; 
set hive.enforce.sorting=true;
insert overwrite table buckettestoutput1 
select * from buckettestinput where data like &amp;amp;apos;first%&amp;amp;apos;;
set hive.auto.convert.sortmerge.join=true; 
set hive.optimize.bucketmapjoin = true; 
set hive.optimize.bucketmapjoin.sortedmerge = true; 
select * from buckettestoutput1 a join buckettestoutput2 b on (a.data=b.data);

Error: Error while compiling statement: FAILED: SemanticException [Error 10141]: Bucketed table metadata is not correct. Fix the metadata or don&amp;amp;apos;t use bucketed mapjoin, by setting hive.enforce.bucketmapjoin to false. The number of buckets for table buckettestoutput1 is 2, whereas the number of files is 1 (state=42000,code=10141)


The related debug information related to insert overwrite:

0: jdbc:hive2://localhost:10000&amp;gt; insert overwrite table buckettestoutput1 
select * from buckettestinput where data like &amp;amp;apos;first%&amp;amp;apos;insert overwrite table buckettestoutput1 
0: jdbc:hive2://localhost:10000&amp;gt; ;
select * from buckettestinput where data like &amp;amp;apos; 
first%&amp;amp;apos;;
INFO  : Number of reduce tasks determined at compile time: 2
INFO  : In order to change the average load for a reducer (in bytes):
INFO  :   set hive.exec.reducers.bytes.per.reducer=&amp;lt;number&amp;gt;
INFO  : In order to limit the maximum number of reducers:
INFO  :   set hive.exec.reducers.max=&amp;lt;number&amp;gt;
INFO  : In order to set a constant number of reducers:
INFO  :   set mapred.reduce.tasks=&amp;lt;number&amp;gt;
INFO  : Job running in-process (local Hadoop)
INFO  : 2015-06-01 11:09:29,650 Stage-1 map = 86%,  reduce = 100%
INFO  : Ended Job = job_local107155352_0001
INFO  : Loading data to table default.buckettestoutput1 from file:/user/hive/warehouse/buckettestoutput1/.hive-staging_hive_2015-06-01_11-09-28_166_3109203968904090801-1/-ext-10000
INFO  : Table default.buckettestoutput1 stats: [numFiles=1, numRows=4, totalSize=52, rawDataSize=48]
No rows affected (1.692 seconds)

</description>
			<version>1.2.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">10880</link>
		</links>
	</bug>
	<bug id="3217" opendate="2012-06-30 00:13:57" fixdate="2015-06-03 23:04:22" resolution="Duplicate">
		<buginformation>
			<summary>Implement HiveDatabaseMetaData.getFunctions() to retrieve registered UDFs. </summary>
			<description>Hive JDBC support currently throws UnsupportedException when getFunctions() is called. Hive CL provides a SHOW FUNCTIONS command to return the names of all registered UDFs. By getting a SQL Statement from the connection, getFunctions can execute( "SHOW FUNCTIONS") to retrieve all the registered functions (including those registered through create temporary function).</description>
			<version>0.9.0</version>
			<fixedVersion></fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.shims.HadoopShimsSecure.java</file>
			<file type="M">org.apache.hadoop.hive.ql.QTestUtil.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
			<file type="M">org.apache.hadoop.hive.ant.QTestGenTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.processors.SetProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.shims.HadoopShims.java</file>
			<file type="D">org.apache.hive.jdbc.beeline.OptionsProcessor.java</file>
			<file type="D">org.apache.hive.jdbc.beeline.HiveBeeline.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.CopyTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">11516</link>
			<link type="Duplicate" description="duplicates">2935</link>
			<link type="Reference" description="relates to">7676</link>
		</links>
	</bug>
	<bug id="10881" opendate="2015-06-01 15:27:11" fixdate="2015-06-06 14:25:12" resolution="Duplicate">
		<buginformation>
			<summary>The bucket number is not respected in insert overwrite.</summary>
			<description>When hive.enforce.bucketing is true, the bucket number defined in the table is no longer respected in current master and 1.2. This is a regression.
Reproduce:

CREATE TABLE IF NOT EXISTS buckettestinput( 
data string 
) 
ROW FORMAT DELIMITED FIELDS TERMINATED BY &amp;amp;apos;,&amp;amp;apos;;
CREATE TABLE IF NOT EXISTS buckettestoutput1( 
data string 
)CLUSTERED BY(data) 
INTO 2 BUCKETS 
ROW FORMAT DELIMITED FIELDS TERMINATED BY &amp;amp;apos;,&amp;amp;apos;;
CREATE TABLE IF NOT EXISTS buckettestoutput2( 
data string 
)CLUSTERED BY(data) 
INTO 2 BUCKETS 
ROW FORMAT DELIMITED FIELDS TERMINATED BY &amp;amp;apos;,&amp;amp;apos;;
Then I inserted the following data into the "buckettestinput" table
firstinsert1 
firstinsert2 
firstinsert3 
firstinsert4 
firstinsert5 
firstinsert6 
firstinsert7 
firstinsert8 
secondinsert1 
secondinsert2 
secondinsert3 
secondinsert4 
secondinsert5 
secondinsert6 
secondinsert7 
secondinsert8
set hive.enforce.bucketing = true; 
set hive.enforce.sorting=true;
insert overwrite table buckettestoutput1 
select * from buckettestinput where data like &amp;amp;apos;first%&amp;amp;apos;;
set hive.auto.convert.sortmerge.join=true; 
set hive.optimize.bucketmapjoin = true; 
set hive.optimize.bucketmapjoin.sortedmerge = true; 
select * from buckettestoutput1 a join buckettestoutput2 b on (a.data=b.data);

Error: Error while compiling statement: FAILED: SemanticException [Error 10141]: Bucketed table metadata is not correct. Fix the metadata or don&amp;amp;apos;t use bucketed mapjoin, by setting hive.enforce.bucketmapjoin to false. The number of buckets for table buckettestoutput1 is 2, whereas the number of files is 1 (state=42000,code=10141)


The related debug information related to insert overwrite:

0: jdbc:hive2://localhost:10000&amp;gt; insert overwrite table buckettestoutput1 
select * from buckettestinput where data like &amp;amp;apos;first%&amp;amp;apos;insert overwrite table buckettestoutput1 
0: jdbc:hive2://localhost:10000&amp;gt; ;
select * from buckettestinput where data like &amp;amp;apos; 
first%&amp;amp;apos;;
INFO  : Number of reduce tasks determined at compile time: 2
INFO  : In order to change the average load for a reducer (in bytes):
INFO  :   set hive.exec.reducers.bytes.per.reducer=&amp;lt;number&amp;gt;
INFO  : In order to limit the maximum number of reducers:
INFO  :   set hive.exec.reducers.max=&amp;lt;number&amp;gt;
INFO  : In order to set a constant number of reducers:
INFO  :   set mapred.reduce.tasks=&amp;lt;number&amp;gt;
INFO  : Job running in-process (local Hadoop)
INFO  : 2015-06-01 11:09:29,650 Stage-1 map = 86%,  reduce = 100%
INFO  : Ended Job = job_local107155352_0001
INFO  : Loading data to table default.buckettestoutput1 from file:/user/hive/warehouse/buckettestoutput1/.hive-staging_hive_2015-06-01_11-09-28_166_3109203968904090801-1/-ext-10000
INFO  : Table default.buckettestoutput1 stats: [numFiles=1, numRows=4, totalSize=52, rawDataSize=48]
No rows affected (1.692 seconds)

</description>
			<version>1.2.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">10866</link>
			<link type="Duplicate" description="duplicates">10880</link>
		</links>
	</bug>
	<bug id="10971" opendate="2015-06-09 08:52:01" fixdate="2015-06-10 07:09:42" resolution="Fixed">
		<buginformation>
			<summary>count(*) with count(distinct) gives wrong results when hive.groupby.skewindata=true</summary>
			<description>When hive.groupby.skewindata=true, the following query based on TPC-H gives wrong results:


set hive.groupby.skewindata=true;

select l_returnflag, count(*), count(distinct l_linestatus)
from lineitem
group by l_returnflag
limit 10;


The query plan shows that it generates only one MapReduce job instead of two theoretically, which is dictated by hive.groupby.skewindata=true.
The problem arises only when 

count(*)

 and 

count(distinct)

 exist together.</description>
			<version>1.2.0</version>
			<fixedVersion>1.2.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">7261</link>
		</links>
	</bug>
	<bug id="10965" opendate="2015-06-08 19:22:42" fixdate="2015-06-10 22:12:25" resolution="Fixed">
		<buginformation>
			<summary>direct SQL for stats fails in 0-column case</summary>
			<description></description>
			<version>1.0.0</version>
			<fixedVersion>1.2.1, 1.0.2</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
			<file type="M">org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">9748</link>
			<link type="Reference" description="is related to">12083</link>
		</links>
	</bug>
	<bug id="10841" opendate="2015-05-27 23:05:09" fixdate="2015-06-15 19:38:30" resolution="Fixed">
		<buginformation>
			<summary>[WHERE col is not null] does not work sometimes for queries with many JOIN statements</summary>
			<description>The result from the following SELECT query is 3 rows but it should be 1 row.
I checked it in MySQL - it returned 1 row.
To reproduce the issue in Hive
1. prepare tables


drop table if exists L;
drop table if exists LA;
drop table if exists FR;
drop table if exists A;
drop table if exists PI;
drop table if exists acct;

create table L as select 4436 id;
create table LA as select 4436 loan_id, 4748 aid, 4415 pi_id;
create table FR as select 4436 loan_id;
create table A as select 4748 id;
create table PI as select 4415 id;

create table acct as select 4748 aid, 10 acc_n, 122 brn;
insert into table acct values(4748, null, null);
insert into table acct values(4748, null, null);


2. run SELECT query


select
  acct.ACC_N,
  acct.brn
FROM L
JOIN LA ON L.id = LA.loan_id
JOIN FR ON L.id = FR.loan_id
JOIN A ON LA.aid = A.id
JOIN PI ON PI.id = LA.pi_id
JOIN acct ON A.id = acct.aid
WHERE
  L.id = 4436
  and acct.brn is not null;


the result is 3 rows


10	122
NULL	NULL
NULL	NULL


but it should be 1 row


10	122


2.1 "explain select ..." output for hive-1.3.0 MR


STAGE DEPENDENCIES:
  Stage-12 is a root stage
  Stage-9 depends on stages: Stage-12
  Stage-0 depends on stages: Stage-9

STAGE PLANS:
  Stage: Stage-12
    Map Reduce Local Work
      Alias -&amp;gt; Map Local Tables:
        a 
          Fetch Operator
            limit: -1
        acct 
          Fetch Operator
            limit: -1
        fr 
          Fetch Operator
            limit: -1
        l 
          Fetch Operator
            limit: -1
        pi 
          Fetch Operator
            limit: -1
      Alias -&amp;gt; Map Local Operator Tree:
        a 
          TableScan
            alias: a
            Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
            Filter Operator
              predicate: id is not null (type: boolean)
              Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
              HashTable Sink Operator
                keys:
                  0 _col5 (type: int)
                  1 id (type: int)
                  2 aid (type: int)
        acct 
          TableScan
            alias: acct
            Statistics: Num rows: 3 Data size: 31 Basic stats: COMPLETE Column stats: NONE
            Filter Operator
              predicate: aid is not null (type: boolean)
              Statistics: Num rows: 2 Data size: 20 Basic stats: COMPLETE Column stats: NONE
              HashTable Sink Operator
                keys:
                  0 _col5 (type: int)
                  1 id (type: int)
                  2 aid (type: int)
        fr 
          TableScan
            alias: fr
            Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
            Filter Operator
              predicate: (loan_id = 4436) (type: boolean)
              Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
              HashTable Sink Operator
                keys:
                  0 4436 (type: int)
                  1 4436 (type: int)
                  2 4436 (type: int)
        l 
          TableScan
            alias: l
            Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
            Filter Operator
              predicate: (id = 4436) (type: boolean)
              Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
              HashTable Sink Operator
                keys:
                  0 4436 (type: int)
                  1 4436 (type: int)
                  2 4436 (type: int)
        pi 
          TableScan
            alias: pi
            Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
            Filter Operator
              predicate: id is not null (type: boolean)
              Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
              HashTable Sink Operator
                keys:
                  0 _col6 (type: int)
                  1 id (type: int)

  Stage: Stage-9
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: la
            Statistics: Num rows: 1 Data size: 14 Basic stats: COMPLETE Column stats: NONE
            Filter Operator
              predicate: (((loan_id is not null and aid is not null) and pi_id is not null) and (loan_id = 4436)) (type: boolean)
              Statistics: Num rows: 1 Data size: 14 Basic stats: COMPLETE Column stats: NONE
              Map Join Operator
                condition map:
                     Inner Join 0 to 1
                     Inner Join 0 to 2
                keys:
                  0 4436 (type: int)
                  1 4436 (type: int)
                  2 4436 (type: int)
                outputColumnNames: _col5, _col6
                Statistics: Num rows: 2 Data size: 8 Basic stats: COMPLETE Column stats: NONE
                Map Join Operator
                  condition map:
                       Inner Join 0 to 1
                       Inner Join 1 to 2
                  keys:
                    0 _col5 (type: int)
                    1 id (type: int)
                    2 aid (type: int)
                  outputColumnNames: _col6, _col19, _col20
                  Statistics: Num rows: 4 Data size: 17 Basic stats: COMPLETE Column stats: NONE
                  Map Join Operator
                    condition map:
                         Inner Join 0 to 1
                    keys:
                      0 _col6 (type: int)
                      1 id (type: int)
                    outputColumnNames: _col19, _col20
                    Statistics: Num rows: 4 Data size: 18 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: _col19 (type: int), _col20 (type: int)
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 4 Data size: 18 Basic stats: COMPLETE Column stats: NONE
                      File Output Operator
                        compressed: false
                        Statistics: Num rows: 4 Data size: 18 Basic stats: COMPLETE Column stats: NONE
                        table:
                            input format: org.apache.hadoop.mapred.TextInputFormat
                            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
      Local Work:
        Map Reduce Local Work

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

Time taken: 0.57 seconds, Fetched: 142 row(s)


2.2. "explain select..." output for hive-0.13.1 Tez


STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-1
    Tez
      Edges:
        Reducer 2 &amp;lt;- Map 1 (SIMPLE_EDGE), Map 4 (SIMPLE_EDGE), Reducer 6 (SIMPLE_EDGE)
        Reducer 3 &amp;lt;- Reducer 2 (SIMPLE_EDGE), Map 9 (SIMPLE_EDGE)
        Reducer 6 &amp;lt;- Map 5 (SIMPLE_EDGE), Map 7 (SIMPLE_EDGE), Map 8 (SIMPLE_EDGE)
      DagName: lcapp_20150528111717_06c57a5b-8dc6-4ce9-bce7-b9e0a7818fe4:1
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: acct
                  Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: aid (type: int)
                    sort order: +
                    Map-reduce partition columns: aid (type: int)
                    Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
                    value expressions: acc_n (type: int), brn (type: int)
        Map 4 
            Map Operator Tree:
                TableScan
                  alias: a
                  Statistics: Num rows: 46 Data size: 187 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: id (type: int)
                    sort order: +
                    Map-reduce partition columns: id (type: int)
                    Statistics: Num rows: 46 Data size: 187 Basic stats: COMPLETE Column stats: NONE
        Map 5 
            Map Operator Tree:
                TableScan
                  alias: la
                  Statistics: Num rows: 28 Data size: 347 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: (loan_id = 4436) (type: boolean)
                    Statistics: Num rows: 14 Data size: 173 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: loan_id (type: int)
                      sort order: +
                      Map-reduce partition columns: loan_id (type: int)
                      Statistics: Num rows: 14 Data size: 173 Basic stats: COMPLETE Column stats: NONE
                      value expressions: aid (type: int), pi_id (type: int)
        Map 7 
            Map Operator Tree:
                TableScan
                  alias: fr
                  Statistics: Num rows: 46 Data size: 187 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: (loan_id = 4436) (type: boolean)
                    Statistics: Num rows: 23 Data size: 93 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: loan_id (type: int)
                      sort order: +
                      Map-reduce partition columns: loan_id (type: int)
                      Statistics: Num rows: 23 Data size: 93 Basic stats: COMPLETE Column stats: NONE
        Map 8 
            Map Operator Tree:
                TableScan
                  alias: l
                  Statistics: Num rows: 46 Data size: 187 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: (id = 4436) (type: boolean)
                    Statistics: Num rows: 23 Data size: 93 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: id (type: int)
                      sort order: +
                      Map-reduce partition columns: id (type: int)
                      Statistics: Num rows: 23 Data size: 93 Basic stats: COMPLETE Column stats: NONE
        Map 9 
            Map Operator Tree:
                TableScan
                  alias: pi
                  Statistics: Num rows: 46 Data size: 187 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: id (type: int)
                    sort order: +
                    Map-reduce partition columns: id (type: int)
                    Statistics: Num rows: 46 Data size: 187 Basic stats: COMPLETE Column stats: NONE
        Reducer 2 
            Reduce Operator Tree:
              Join Operator
                condition map:
                     Inner Join 0 to 1
                     Inner Join 1 to 2
                condition expressions:
                  0 {VALUE._col2}
                  1 
                  2 {VALUE._col1} {VALUE._col2}
                outputColumnNames: _col2, _col15, _col16
                Statistics: Num rows: 110 Data size: 448 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col2 (type: int)
                  sort order: +
                  Map-reduce partition columns: _col2 (type: int)
                  Statistics: Num rows: 110 Data size: 448 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col15 (type: int), _col16 (type: int)
        Reducer 3 
            Reduce Operator Tree:
              Join Operator
                condition map:
                     Inner Join 0 to 1
                condition expressions:
                  0 {VALUE._col1} {VALUE._col2}
                  1 
                outputColumnNames: _col1, _col2
                Statistics: Num rows: 121 Data size: 492 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: _col1 (type: int), _col2 (type: int)
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 121 Data size: 492 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 121 Data size: 492 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.TextInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Reducer 6 
            Reduce Operator Tree:
              Join Operator
                condition map:
                     Inner Join 0 to 1
                     Inner Join 0 to 2
                condition expressions:
                  0 
                  1 {VALUE._col1} {VALUE._col2}
                  2 
                outputColumnNames: _col4, _col5
                Statistics: Num rows: 50 Data size: 204 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col4 (type: int)
                  sort order: +
                  Map-reduce partition columns: _col4 (type: int)
                  Statistics: Num rows: 50 Data size: 204 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col5 (type: int)

  Stage: Stage-0
    Fetch Operator
      limit: -1

Time taken: 1.377 seconds, Fetched: 146 row(s)


3. The workaround is to put "acct.brn is not null" to join condition


select
  acct.ACC_N,
  acct.brn
FROM L
JOIN LA ON L.id = LA.loan_id
JOIN FR ON L.id = FR.loan_id
JOIN A ON LA.aid = A.id
JOIN PI ON PI.id = LA.pi_id
JOIN acct ON A.id = acct.aid and acct.brn is not null
WHERE
  L.id = 4436;

OK
10	122
Time taken: 23.479 seconds, Fetched: 1 row(s)


I tried it on hive-1.3.0 (MR) and hive-0.13.1 (MR and Tez) - all combinations have the issue</description>
			<version>0.13.0</version>
			<fixedVersion>1.3.0, 1.2.1, 2.0.0, 1.0.2</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">11034</link>
			<link type="Reference" description="relates to">4293</link>
			<link type="Reference" description="is related to">3847</link>
		</links>
	</bug>
	<bug id="11021" opendate="2015-06-16 14:20:35" fixdate="2015-06-16 14:32:55" resolution="Duplicate">
		<buginformation>
			<summary>ObjectStore should call closeAll() on JDO query object to release the resources</summary>
			<description>In ObjectStore class, in getMDatabase() and getMTable(), after retrieving the database and table info from the database, we should call closeAll() on JDO query to release the resource. It would cause the cursor leaking on the database otherwise.</description>
			<version>2.0.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.tools.HiveMetaTool.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">10895</link>
		</links>
	</bug>
	<bug id="7261" opendate="2014-06-20 03:22:11" fixdate="2015-06-17 02:24:16" resolution="Duplicate">
		<buginformation>
			<summary>Calculation works wrong when hive.groupby.skewindata  is true and count(*) count(distinct) group by work simultaneously </summary>
			<description>Phenomenon
The query results are not the same as when hive.groupby.skewindata was setted to true and false.
my question
I want to calculate the count and count(distinct) simultaneously ,otherwise it will cost 2 MR job to calculate. But when i set the hive.groupby.skewindata to be true, the count result shoud not be same as the count(distinct) , but the real result is same, so it&amp;amp;apos;s wrong. And I find the difference of its query plan which the "Reduce Operator Tree-&amp;gt;Group By Operator-&amp;gt;mode"  is mergepartial when skew is set to false and 
"Reduce Operator Tree-&amp;gt;Group By Operator-&amp;gt;mode"  is complete when skew is set to true. So i&amp;amp;apos;m confused the root cause of the error.
sql
select ds,appid,eventname,active,count(distinct(guid)), count from eventinfo_tmp where ds=&amp;amp;apos;20140612&amp;amp;apos; and length(eventname)&amp;lt;1000 and eventname like &amp;amp;apos;%alibaba%&amp;amp;apos; group by ds,appid,eventname,active;
the others hive configaration exclude hive.groupby.skewindata
hive.exec.compress.output=true
hive.exec.compress.intermediate=true
io.seqfile.compression.type=BLOCK
mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec
hive.map.aggr=true
hive.stats.autogather=false
hive.exec.scratchdir=/user/complat/tmp
mapred.job.queue.name=complat
hive.exec.mode.local.auto=false
hive.exec.mode.local.auto.inputbytes.max=500
hive.exec.mode.local.auto.tasks.max=10
hive.exec.mode.local.auto.input.files.max=1000
hive.exec.dynamic.partition=true
hive.exec.dynamic.partition.mode=nonstrict
hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat
mapred.max.split.size=100000000
mapred.min.split.size.per.node=100000000
mapred.min.split.size.per.rack=100000000
result
when hive.groupby.skewindata=true  the result is :
20140612	8	alibaba	1	87	147
when it=false the result is : 
20140612	8	alibaba	1	87	87
query plan
ABSTRACT SYNTAX TREE:
  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME eventinfo_tmp))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL ds)) (TOK_SELEXPR (TOK_TABLE_OR_COL appid)) (TOK_SELEXPR (TOK_TABLE_OR_COL eventname)) (TOK_SELEXPR (TOK_TABLE_OR_COL active)) (TOK_SELEXPR (TOK_FUNCTIONDI count (TOK_TABLE_OR_COL guid))) (TOK_SELEXPR (TOK_FUNCTIONSTAR count))) (TOK_WHERE (and (and (= (TOK_TABLE_OR_COL ds) &amp;amp;apos;20140612&amp;amp;apos;) (&amp;lt; (TOK_FUNCTION length (TOK_TABLE_OR_COL eventname)) 1000)) (like (TOK_TABLE_OR_COL eventname) &amp;amp;apos;%tvvideo_setting%&amp;amp;apos;))) (TOK_GROUPBY (TOK_TABLE_OR_COL ds) (TOK_TABLE_OR_COL appid) (TOK_TABLE_OR_COL eventname) (TOK_TABLE_OR_COL active))))
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 is a root stage
STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Alias -&amp;gt; Map Operator Tree:
        eventinfo_tmp 
          TableScan
            alias: eventinfo_tmp
            Filter Operator
              predicate:
                  expr: ((length(eventname) &amp;lt; 1000) and (eventname like &amp;amp;apos;%tvvideo_setting%&amp;amp;apos;))
                  type: boolean
              Select Operator
                expressions:
                      expr: ds
                      type: string
                      expr: appid
                      type: string
                      expr: eventname
                      type: string
                      expr: active
                      type: int
                      expr: guid
                      type: string
                outputColumnNames: ds, appid, eventname, active, guid
                Group By Operator
                  aggregations:
                        expr: count(DISTINCT guid)
                        expr: count()
                  bucketGroup: false
                  keys:
                        expr: ds
                        type: string
                        expr: appid
                        type: string
                        expr: eventname
                        type: string
                        expr: active
                        type: int
                        expr: guid
                        type: string
                  mode: hash
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                  Reduce Output Operator
                    key expressions:
                          expr: _col0
                          type: string
                          expr: _col1
                          type: string
                          expr: _col2
                          type: string
                          expr: _col3
                          type: int
                          expr: _col4
                          type: string
                    sort order: +++++
                    Map-reduce partition columns:
                          expr: _col0
                          type: string
                          expr: _col1
                          type: string
                          expr: _col2
                          type: string
                          expr: _col3
                          type: int
                    tag: -1
                    value expressions:
                          expr: _col5
                          type: bigint
                          expr: _col6
                          type: bigint
      Reduce Operator Tree:
        Group By Operator
          aggregations:
                expr: count(DISTINCT KEY._col4:0._col0)
                expr: count(VALUE._col1)
          bucketGroup: false
          keys:
                expr: KEY._col0
                type: string
                expr: KEY._col1
                type: string
                expr: KEY._col2
                type: string
                expr: KEY._col3
                type: int
          mode: mergepartial
          outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
          Select Operator
            expressions:
                  expr: _col0
                  type: string
                  expr: _col1
                  type: string
                  expr: _col2
                  type: string
                  expr: _col3
                  type: int
                  expr: _col4
                  type: bigint
                  expr: _col5
                  type: bigint
            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
            File Output Operator
              compressed: true
              GlobalTableId: 0
              table:
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
  Stage: Stage-0
    Fetch Operator
      limit: -1
ABSTRACT SYNTAX TREE:
  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME eventinfo_tmp))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL ds)) (TOK_SELEXPR (TOK_TABLE_OR_COL appid)) (TOK_SELEXPR (TOK_TABLE_OR_COL eventname)) (TOK_SELEXPR (TOK_TABLE_OR_COL active)) (TOK_SELEXPR (TOK_FUNCTIONDI count (TOK_TABLE_OR_COL guid))) (TOK_SELEXPR (TOK_FUNCTIONSTAR count))) (TOK_WHERE (and (and (= (TOK_TABLE_OR_COL ds) &amp;amp;apos;20140612&amp;amp;apos;) (&amp;lt; (TOK_FUNCTION length (TOK_TABLE_OR_COL eventname)) 1000)) (like (TOK_TABLE_OR_COL eventname) &amp;amp;apos;%tvvideo_setting%&amp;amp;apos;))) (TOK_GROUPBY (TOK_TABLE_OR_COL ds) (TOK_TABLE_OR_COL appid) (TOK_TABLE_OR_COL eventname) (TOK_TABLE_OR_COL active))))
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 is a root stage
STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Alias -&amp;gt; Map Operator Tree:
        eventinfo_tmp 
          TableScan
            alias: eventinfo_tmp
            Filter Operator
              predicate:
                  expr: ((length(eventname) &amp;lt; 1000) and (eventname like &amp;amp;apos;%tvvideo_setting%&amp;amp;apos;))
                  type: boolean
              Select Operator
                expressions:
                      expr: ds
                      type: string
                      expr: appid
                      type: string
                      expr: eventname
                      type: string
                      expr: active
                      type: int
                      expr: guid
                      type: string
                outputColumnNames: ds, appid, eventname, active, guid
                Group By Operator
                  aggregations:
                        expr: count(DISTINCT guid)
                        expr: count()
                  bucketGroup: false
                  keys:
                        expr: ds
                        type: string
                        expr: appid
                        type: string
                        expr: eventname
                        type: string
                        expr: active
                        type: int
                        expr: guid
                        type: string
                  mode: hash
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                  Reduce Output Operator
                    key expressions:
                          expr: _col0
                          type: string
                          expr: _col1
                          type: string
                          expr: _col2
                          type: string
                          expr: _col3
                          type: int
                          expr: _col4
                          type: string
                    sort order: +++++
                    Map-reduce partition columns:
                          expr: _col0
                          type: string
                          expr: _col1
                          type: string
                          expr: _col2
                          type: string
                          expr: _col3
                          type: int
                    tag: -1
                    value expressions:
                          expr: _col5
                          type: bigint
                          expr: _col6
                          type: bigint
      Reduce Operator Tree:
        Group By Operator
          aggregations:
                expr: count(DISTINCT KEY._col4:0._col0)
                expr: count(VALUE._col1)
          bucketGroup: false
          keys:
                expr: KEY._col0
                type: string
                expr: KEY._col1
                type: string
                expr: KEY._col2
                type: string
                expr: KEY._col3
                type: int
          mode: complete
          outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
          Select Operator
            expressions:
                  expr: _col0
                  type: string
                  expr: _col1
                  type: string
                  expr: _col2
                  type: string
                  expr: _col3
                  type: int
                  expr: _col4
                  type: bigint
                  expr: _col5
                  type: bigint
            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
            File Output Operator
              compressed: true
              GlobalTableId: 0
              table:
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
  Stage: Stage-0
    Fetch Operator
      limit: -1
</description>
			<version>0.12.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">10971</link>
		</links>
	</bug>
	<bug id="11000" opendate="2015-06-13 01:37:31" fixdate="2015-06-19 17:11:55" resolution="Duplicate">
		<buginformation>
			<summary>Hive not able to pass Hive&amp;apos;s Kerberos credential to spark-submit process [Spark Branch]</summary>
			<description>The end of the result is that manual kinit with Hive&amp;amp;apos;s keytab on the host where HS2 is running, or the following error may appear:


2015-04-29 15:49:34,614 INFO org.apache.hive.spark.client.SparkClientImpl: 15/04/29 15:49:34 WARN UserGroupInformation: PriviledgedActionException as:hive (auth:KERBEROS) cause:java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
2015-04-29 15:49:34,652 INFO org.apache.hive.spark.client.SparkClientImpl: Exception in thread "main" java.io.IOException: Failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]; Host Details : local host is: "secure-hos-1.ent.cloudera.com/10.20.77.79"; destination host is: "secure-hos-1.ent.cloudera.com":8032;
2015-04-29 15:49:34,653 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:772)
2015-04-29 15:49:34,653 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.hadoop.ipc.Client.call(Client.java:1472)
2015-04-29 15:49:34,654 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.hadoop.ipc.Client.call(Client.java:1399)
2015-04-29 15:49:34,654 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
2015-04-29 15:49:34,654 INFO org.apache.hive.spark.client.SparkClientImpl:      at com.sun.proxy.$Proxy11.getClusterMetrics(Unknown Source)
2015-04-29 15:49:34,655 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getClusterMetrics(ApplicationClientProtocolPBClientImpl.java:202)
2015-04-29 15:49:34,655 INFO org.apache.hive.spark.client.SparkClientImpl:      at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2015-04-29 15:49:34,655 INFO org.apache.hive.spark.client.SparkClientImpl:      at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
2015-04-29 15:49:34,656 INFO org.apache.hive.spark.client.SparkClientImpl:      at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2015-04-29 15:49:34,656 INFO org.apache.hive.spark.client.SparkClientImpl:      at java.lang.reflect.Method.invoke(Method.java:606)
2015-04-29 15:49:34,656 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
2015-04-29 15:49:34,657 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
2015-04-29 15:49:34,657 INFO org.apache.hive.spark.client.SparkClientImpl:      at com.sun.proxy.$Proxy12.getClusterMetrics(Unknown Source)
2015-04-29 15:49:34,657 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getYarnClusterMetrics(YarnClientImpl.java:461)
2015-04-29 15:49:34,657 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.spark.deploy.yarn.Client$$anonfun$submitApplication$1.apply(Client.scala:91)
2015-04-29 15:49:34,657 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.spark.deploy.yarn.Client$$anonfun$submitApplication$1.apply(Client.scala:91)
2015-04-29 15:49:34,657 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.spark.Logging$class.logInfo(Logging.scala:59)
2015-04-29 15:49:34,657 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.spark.deploy.yarn.Client.logInfo(Client.scala:49)
2015-04-29 15:49:34,657 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:90)
2015-04-29 15:49:34,658 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.spark.deploy.yarn.Client.run(Client.scala:619)
2015-04-29 15:49:34,658 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.spark.deploy.yarn.Client$.main(Client.scala:647)
2015-04-29 15:49:34,658 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.spark.deploy.yarn.Client.main(Client.scala)
2015-04-29 15:49:34,658 INFO org.apache.hive.spark.client.SparkClientImpl:      at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2015-04-29 15:49:34,658 INFO org.apache.hive.spark.client.SparkClientImpl:      at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
2015-04-29 15:49:34,658 INFO org.apache.hive.spark.client.SparkClientImpl:      at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2015-04-29 15:49:34,658 INFO org.apache.hive.spark.client.SparkClientImpl:      at java.lang.reflect.Method.invoke(Method.java:606)
2015-04-29 15:49:34,659 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:569)
2015-04-29 15:49:34,659 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:166)
2015-04-29 15:49:34,659 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:189)
2015-04-29 15:49:34,659 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:110)
2015-04-29 15:49:34,659 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
2015-04-29 15:49:34,660 INFO org.apache.hive.spark.client.SparkClientImpl: Caused by: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
2015-04-29 15:49:34,660 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:680)
2015-04-29 15:49:34,660 INFO org.apache.hive.spark.client.SparkClientImpl:      at java.security.AccessController.doPrivileged(Native Method)
2015-04-29 15:49:34,660 INFO org.apache.hive.spark.client.SparkClientImpl:      at javax.security.auth.Subject.doAs(Subject.java:415)
2015-04-29 15:49:34,660 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
2015-04-29 15:49:34,660 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:643)
2015-04-29 15:49:34,661 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:730)
2015-04-29 15:49:34,661 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
2015-04-29 15:49:34,661 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
2015-04-29 15:49:34,661 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.hadoop.ipc.Client.call(Client.java:1438)
2015-04-29 15:49:34,661 INFO org.apache.hive.spark.client.SparkClientImpl:      ... 29 more
2015-04-29 15:49:34,662 INFO org.apache.hive.spark.client.SparkClientImpl: Caused by: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
2015-04-29 15:49:34,662 INFO org.apache.hive.spark.client.SparkClientImpl:      at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:212)
2015-04-29 15:49:34,662 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:413)
2015-04-29 15:49:34,662 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:553)
2015-04-29 15:49:34,662 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.hadoop.ipc.Client$Connection.access$1800(Client.java:368)
2015-04-29 15:49:34,663 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:722)
2015-04-29 15:49:34,663 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:718)
2015-04-29 15:49:34,663 INFO org.apache.hive.spark.client.SparkClientImpl:      at java.security.AccessController.doPrivileged(Native Method)
2015-04-29 15:49:34,663 INFO org.apache.hive.spark.client.SparkClientImpl:      at javax.security.auth.Subject.doAs(Subject.java:415)
2015-04-29 15:49:34,663 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
2015-04-29 15:49:34,664 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:717)
2015-04-29 15:49:34,664 INFO org.apache.hive.spark.client.SparkClientImpl:      ... 32 more
2015-04-29 15:49:34,664 INFO org.apache.hive.spark.client.SparkClientImpl: Caused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)
2015-04-29 15:49:34,664 INFO org.apache.hive.spark.client.SparkClientImpl:      at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:147)
2015-04-29 15:49:34,664 INFO org.apache.hive.spark.client.SparkClientImpl:      at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:121)
2015-04-29 15:49:34,665 INFO org.apache.hive.spark.client.SparkClientImpl:      at sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:187)
2015-04-29 15:49:34,665 INFO org.apache.hive.spark.client.SparkClientImpl:      at sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:223)
2015-04-29 15:49:34,665 INFO org.apache.hive.spark.client.SparkClientImpl:      at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:212)
2015-04-29 15:49:34,665 INFO org.apache.hive.spark.client.SparkClientImpl:      at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179)
2015-04-29 15:49:34,665 INFO org.apache.hive.spark.client.SparkClientImpl:      at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:193)
2015-04-29 15:49:34,665 INFO org.apache.hive.spark.client.SparkClientImpl:      ... 41 more...


We need to find way pass the HS2&amp;amp;apos;s ticket to spark-submit.sh when it&amp;amp;apos;s launched.</description>
			<version>1.1.0</version>
			<fixedVersion></fixedVersion>
			<type>Sub-task</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.spark.client.SparkClientImpl.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">10594</link>
		</links>
	</bug>
	<bug id="11034" opendate="2015-06-17 02:42:20" fixdate="2015-06-19 18:34:50" resolution="Duplicate">
		<buginformation>
			<summary>Joining multiple tables producing different results with different order of join</summary>
			<description>
Join between tables with different join columns from main table yielding wrong results in hive. 
Changing the order of the joins between main table and other tables is producing different results.

Please see below for the steps to reproduce the issue:
1. Create tables as follows:
    create table p(ck string, email string);
    create table a1(ck string, flag string);
    create table a2(email string, flag string);
    create table a3(ck string, flag string);
2. Load data into the tables as follows:
    P


ck
email


10
e10


20
e20


30
e30


40
e40


    A1


ck
flag


10
N


20
Y


30
Y


40
Y


    A2


email
flag


e10
Y


e20
N


e30
Y


e40
Y


    A3


ck
flag


10
Y


20
Y


30
N


40
Y


 3. Good query:

select p.ck 
from p 
left outer join a1 on p.ck = a1.ck 
left outer join a3 on p.ck = a3.ck 
left outer join a2 on p.email = a2.email 
where a1.flag = &amp;amp;apos;Y&amp;amp;apos;
  and a3.flag = &amp;amp;apos;Y&amp;amp;apos;
  and a2.flag = &amp;amp;apos;Y&amp;amp;apos;
;

and results are
  40
4. Bad query

select p.ck 
from p 
left outer join a1 on p.ck = a1.ck 
left outer join a2 on p.email = a2.email 
left outer join a3 on p.ck = a3.ck 
where a1.flag = &amp;amp;apos;Y&amp;amp;apos;
  and a2.flag = &amp;amp;apos;Y&amp;amp;apos;
  and a3.flag = &amp;amp;apos;Y&amp;amp;apos;
;

 Producing results as:
 30
 40
</description>
			<version>0.13.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">10841</link>
		</links>
	</bug>
	<bug id="11069" opendate="2015-06-22 07:31:54" fixdate="2015-06-22 07:39:32" resolution="Duplicate">
		<buginformation>
			<summary>ColumnStatsTask doesn&amp;apos;t work with hive.exec.parallel</summary>
			<description>Try a simple query:


hive&amp;gt; set hive.exec.parallel=true;
hive&amp;gt; analyze table src compute statistics for columns;


It fails with errors similar to:


FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.ColumnStatsTask
hive&amp;gt; java.lang.RuntimeException: Error caching map.xml: java.io.IOException: java.lang.InterruptedException
	at org.apache.hadoop.hive.ql.exec.Utilities.setBaseWork(Utilities.java:747)
	at org.apache.hadoop.hive.ql.exec.Utilities.setMapWork(Utilities.java:682)
	at org.apache.hadoop.hive.ql.exec.Utilities.setMapRedWork(Utilities.java:674)
	at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:375)
	at org.apache.hadoop.hive.ql.exec.mr.MapRedTask.execute(MapRedTask.java:137)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:88)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.run(TaskRunner.java:75)
Caused by: java.io.IOException: java.lang.InterruptedException
	at org.apache.hadoop.ipc.Client.call(Client.java:1450)
	at org.apache.hadoop.ipc.Client.call(Client.java:1402)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy14.mkdirs(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:539)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy15.mkdirs(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2758)
	at org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2729)
	at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:866)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:866)
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:859)
	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1954)
	at org.apache.hadoop.hive.ql.exec.Utilities.setPlanPath(Utilities.java:765)
	at org.apache.hadoop.hive.ql.exec.Utilities.setBaseWork(Utilities.java:691)
	... 7 more
Caused by: java.lang.InterruptedException
	at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:400)
	at java.util.concurrent.FutureTask.get(FutureTask.java:187)
	at org.apache.hadoop.ipc.Client$Connection.sendRpcRequest(Client.java:1049)
	at org.apache.hadoop.ipc.Client.call(Client.java:1444)
	... 28 more
Job Submission failed with exception &amp;amp;apos;java.lang.RuntimeException(Error caching map.xml: java.io.IOException: java.lang.InterruptedException)&amp;amp;apos;


The problem is the Column Stats Task doesn&amp;amp;apos;t depend on the root task which causes errors. Here&amp;amp;apos;s the explain output:


hive&amp;gt; explain analyze table src compute statistics for columns;
OK
STAGE DEPENDENCIES:
  Stage-0 is a root stage
  Stage-1 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: src
            Select Operator
              expressions: key (type: string), value (type: string)
              outputColumnNames: key, value
              Group By Operator
                aggregations: compute_stats(key, 16), compute_stats(value, 16)
                mode: hash
                outputColumnNames: _col0, _col1
                Reduce Output Operator
                  sort order:
                  value expressions: _col0 (type: struct&amp;lt;columntype:string,maxlength:bigint,sumlength:bigint,count:bigint,countnulls:bigint,bitvector:string,numbitvectors:int&amp;gt;), _col1 (type: struct&amp;lt;columntype:string,maxlength:bigint,sumlength:bigint,count:bigint,countnulls:bigint,bitvector:string,numbitvectors:int&amp;gt;)
      Reduce Operator Tree:
        Group By Operator
          aggregations: compute_stats(VALUE._col0), compute_stats(VALUE._col1)
          mode: mergepartial
          outputColumnNames: _col0, _col1
          File Output Operator
            compressed: false
            table:
                input format: org.apache.hadoop.mapred.TextInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-1
    Column Stats Work
      Column Stats Desc:
          Columns: key, value
          Column Types: string, string
          Table: default.src

Time taken: 0.761 seconds, Fetched: 39 row(s)


For reference, here&amp;amp;apos;s the corresponding output in Hive 0.13:


hive&amp;gt; explain analyze table orders compute statistics for columns;
OK
STAGE DEPENDENCIES:
  Stage-0 is a root stage
  Stage-1 depends on stages: Stage-0

STAGE PLANS:
  Stage: Stage-0
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: orders
            Statistics: Num rows: 0 Data size: 13310103552 Basic stats: PARTIAL Column stats: COMPLETE

  Stage: Stage-1
    Stats-Aggr Operator

Time taken: 2.142 seconds, Fetched: 15 row(s)

</description>
			<version>1.2.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">10677</link>
		</links>
	</bug>
	<bug id="10594" opendate="2015-05-04 16:48:01" fixdate="2015-06-22 20:10:26" resolution="Fixed">
		<buginformation>
			<summary>Remote Spark client doesn&amp;apos;t use Kerberos keytab to authenticate [Spark Branch]</summary>
			<description>Reporting problem found by one of the HoS users:
Currently, if user is running Beeline on a different host than HS2, and he/she didn&amp;amp;apos;t do kinit on the HS2 host, then he/she may get the following error:


2015-04-29 15:49:34,614 INFO org.apache.hive.spark.client.SparkClientImpl: 15/04/29 15:49:34 WARN UserGroupInformation: PriviledgedActionException as:hive (auth:KERBEROS) cause:java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
2015-04-29 15:49:34,652 INFO org.apache.hive.spark.client.SparkClientImpl: Exception in thread "main" java.io.IOException: Failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]; Host Details : local host is: "secure-hos-1.ent.cloudera.com/10.20.77.79"; destination host is: "secure-hos-1.ent.cloudera.com":8032;
2015-04-29 15:49:34,653 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:772)
2015-04-29 15:49:34,653 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.hadoop.ipc.Client.call(Client.java:1472)
2015-04-29 15:49:34,654 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.hadoop.ipc.Client.call(Client.java:1399)
2015-04-29 15:49:34,654 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
2015-04-29 15:49:34,654 INFO org.apache.hive.spark.client.SparkClientImpl:      at com.sun.proxy.$Proxy11.getClusterMetrics(Unknown Source)
2015-04-29 15:49:34,655 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getClusterMetrics(ApplicationClientProtocolPBClientImpl.java:202)
2015-04-29 15:49:34,655 INFO org.apache.hive.spark.client.SparkClientImpl:      at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2015-04-29 15:49:34,655 INFO org.apache.hive.spark.client.SparkClientImpl:      at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
2015-04-29 15:49:34,656 INFO org.apache.hive.spark.client.SparkClientImpl:      at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2015-04-29 15:49:34,656 INFO org.apache.hive.spark.client.SparkClientImpl:      at java.lang.reflect.Method.invoke(Method.java:606)
2015-04-29 15:49:34,656 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
2015-04-29 15:49:34,657 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
2015-04-29 15:49:34,657 INFO org.apache.hive.spark.client.SparkClientImpl:      at com.sun.proxy.$Proxy12.getClusterMetrics(Unknown Source)
2015-04-29 15:49:34,657 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getYarnClusterMetrics(YarnClientImpl.java:461)
2015-04-29 15:49:34,657 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.spark.deploy.yarn.Client$$anonfun$submitApplication$1.apply(Client.scala:91)
2015-04-29 15:49:34,657 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.spark.deploy.yarn.Client$$anonfun$submitApplication$1.apply(Client.scala:91)
2015-04-29 15:49:34,657 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.spark.Logging$class.logInfo(Logging.scala:59)
2015-04-29 15:49:34,657 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.spark.deploy.yarn.Client.logInfo(Client.scala:49)
2015-04-29 15:49:34,657 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:90)
2015-04-29 15:49:34,658 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.spark.deploy.yarn.Client.run(Client.scala:619)
2015-04-29 15:49:34,658 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.spark.deploy.yarn.Client$.main(Client.scala:647)
2015-04-29 15:49:34,658 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.spark.deploy.yarn.Client.main(Client.scala)
2015-04-29 15:49:34,658 INFO org.apache.hive.spark.client.SparkClientImpl:      at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2015-04-29 15:49:34,658 INFO org.apache.hive.spark.client.SparkClientImpl:      at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
2015-04-29 15:49:34,658 INFO org.apache.hive.spark.client.SparkClientImpl:      at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2015-04-29 15:49:34,658 INFO org.apache.hive.spark.client.SparkClientImpl:      at java.lang.reflect.Method.invoke(Method.java:606)
2015-04-29 15:49:34,659 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:569)
2015-04-29 15:49:34,659 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:166)
2015-04-29 15:49:34,659 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:189)
2015-04-29 15:49:34,659 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:110)
2015-04-29 15:49:34,659 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
2015-04-29 15:49:34,660 INFO org.apache.hive.spark.client.SparkClientImpl: Caused by: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
2015-04-29 15:49:34,660 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:680)
2015-04-29 15:49:34,660 INFO org.apache.hive.spark.client.SparkClientImpl:      at java.security.AccessController.doPrivileged(Native Method)
2015-04-29 15:49:34,660 INFO org.apache.hive.spark.client.SparkClientImpl:      at javax.security.auth.Subject.doAs(Subject.java:415)
2015-04-29 15:49:34,660 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
2015-04-29 15:49:34,660 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:643)
2015-04-29 15:49:34,661 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:730)
2015-04-29 15:49:34,661 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
2015-04-29 15:49:34,661 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
2015-04-29 15:49:34,661 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.hadoop.ipc.Client.call(Client.java:1438)
2015-04-29 15:49:34,661 INFO org.apache.hive.spark.client.SparkClientImpl:      ... 29 more
2015-04-29 15:49:34,662 INFO org.apache.hive.spark.client.SparkClientImpl: Caused by: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
2015-04-29 15:49:34,662 INFO org.apache.hive.spark.client.SparkClientImpl:      at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:212)
2015-04-29 15:49:34,662 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:413)
2015-04-29 15:49:34,662 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:553)
2015-04-29 15:49:34,662 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.hadoop.ipc.Client$Connection.access$1800(Client.java:368)
2015-04-29 15:49:34,663 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:722)
2015-04-29 15:49:34,663 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:718)
2015-04-29 15:49:34,663 INFO org.apache.hive.spark.client.SparkClientImpl:      at java.security.AccessController.doPrivileged(Native Method)
2015-04-29 15:49:34,663 INFO org.apache.hive.spark.client.SparkClientImpl:      at javax.security.auth.Subject.doAs(Subject.java:415)
2015-04-29 15:49:34,663 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
2015-04-29 15:49:34,664 INFO org.apache.hive.spark.client.SparkClientImpl:      at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:717)
2015-04-29 15:49:34,664 INFO org.apache.hive.spark.client.SparkClientImpl:      ... 32 more
2015-04-29 15:49:34,664 INFO org.apache.hive.spark.client.SparkClientImpl: Caused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)
2015-04-29 15:49:34,664 INFO org.apache.hive.spark.client.SparkClientImpl:      at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:147)
2015-04-29 15:49:34,664 INFO org.apache.hive.spark.client.SparkClientImpl:      at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:121)
2015-04-29 15:49:34,665 INFO org.apache.hive.spark.client.SparkClientImpl:      at sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:187)
2015-04-29 15:49:34,665 INFO org.apache.hive.spark.client.SparkClientImpl:      at sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:223)
2015-04-29 15:49:34,665 INFO org.apache.hive.spark.client.SparkClientImpl:      at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:212)
2015-04-29 15:49:34,665 INFO org.apache.hive.spark.client.SparkClientImpl:      at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179)
2015-04-29 15:49:34,665 INFO org.apache.hive.spark.client.SparkClientImpl:      at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:193)
2015-04-29 15:49:34,665 INFO org.apache.hive.spark.client.SparkClientImpl:      ... 41 more...


For MR, it works fine. I reproduced the issue on a newly provisioned CDH-5.4.1 cluster, by explicitly kdestroy on the HS2 host first, and later submitting HoS query through beeline on a different host.
According to the user&amp;amp;apos;s investigation, it might be an issue with spark-submit.sh.
Here&amp;amp;apos;s the quote:

I think I found it .. HS2 calls out via Bash to set up the spark container for the HS2 session  I am not seeing any keytab access for this new session  so having a krb5cc_xx available solves the hive need for an active TGT.
Basically bash is laundering the access .. the TGT that HS2 is carrying isnt passed on through to bash  because bash doesnt have a KRB5CCNAME to go looking for so it cannot pass the TGT on even if one existed.</description>
			<version>1.1.0</version>
			<fixedVersion>spark-branch, 1.3.0, 2.0.0</fixedVersion>
			<type>Sub-task</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.spark.client.SparkClientImpl.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">11000</link>
			<link type="Reference" description="relates to">14383</link>
		</links>
	</bug>
	<bug id="10996" opendate="2015-06-13 00:46:24" fixdate="2015-06-27 22:22:21" resolution="Fixed">
		<buginformation>
			<summary>Aggregation / Projection over Multi-Join Inner Query producing incorrect results</summary>
			<description>We see the following problem on 1.1.0 and 1.2.0 but not 0.13 which seems like a regression.
The following query (Q1) produces no results:


select s
from (
  select last.*, action.st2, action.n
  from (
    select purchase.s, purchase.timestamp, max (mevt.timestamp) as last_stage_timestamp
    from (select * from purchase_history) purchase
    join (select * from cart_history) mevt
    on purchase.s = mevt.s
    where purchase.timestamp &amp;gt; mevt.timestamp
    group by purchase.s, purchase.timestamp
  ) last
  join (select * from events) action
  on last.s = action.s and last.last_stage_timestamp = action.timestamp
) list;



While this one (Q2) does produce results :


select *
from (
  select last.*, action.st2, action.n
  from (
    select purchase.s, purchase.timestamp, max (mevt.timestamp) as last_stage_timestamp
    from (select * from purchase_history) purchase
    join (select * from cart_history) mevt
    on purchase.s = mevt.s
    where purchase.timestamp &amp;gt; mevt.timestamp
    group by purchase.s, purchase.timestamp
  ) last
  join (select * from events) action
  on last.s = action.s and last.last_stage_timestamp = action.timestamp
) list;
1	21	20	Bob	1234
1	31	30	Bob	1234
3	51	50	Jeff	1234



The setup to test this is:


create table purchase_history (s string, product string, price double, timestamp int);
insert into purchase_history values (&amp;amp;apos;1&amp;amp;apos;, &amp;amp;apos;Belt&amp;amp;apos;, 20.00, 21);
insert into purchase_history values (&amp;amp;apos;1&amp;amp;apos;, &amp;amp;apos;Socks&amp;amp;apos;, 3.50, 31);
insert into purchase_history values (&amp;amp;apos;3&amp;amp;apos;, &amp;amp;apos;Belt&amp;amp;apos;, 20.00, 51);
insert into purchase_history values (&amp;amp;apos;4&amp;amp;apos;, &amp;amp;apos;Shirt&amp;amp;apos;, 15.50, 59);

create table cart_history (s string, cart_id int, timestamp int);
insert into cart_history values (&amp;amp;apos;1&amp;amp;apos;, 1, 10);
insert into cart_history values (&amp;amp;apos;1&amp;amp;apos;, 2, 20);
insert into cart_history values (&amp;amp;apos;1&amp;amp;apos;, 3, 30);
insert into cart_history values (&amp;amp;apos;1&amp;amp;apos;, 4, 40);
insert into cart_history values (&amp;amp;apos;3&amp;amp;apos;, 5, 50);
insert into cart_history values (&amp;amp;apos;4&amp;amp;apos;, 6, 60);

create table events (s string, st2 string, n int, timestamp int);
insert into events values (&amp;amp;apos;1&amp;amp;apos;, &amp;amp;apos;Bob&amp;amp;apos;, 1234, 20);
insert into events values (&amp;amp;apos;1&amp;amp;apos;, &amp;amp;apos;Bob&amp;amp;apos;, 1234, 30);
insert into events values (&amp;amp;apos;1&amp;amp;apos;, &amp;amp;apos;Bob&amp;amp;apos;, 1234, 25);
insert into events values (&amp;amp;apos;2&amp;amp;apos;, &amp;amp;apos;Sam&amp;amp;apos;, 1234, 30);
insert into events values (&amp;amp;apos;3&amp;amp;apos;, &amp;amp;apos;Jeff&amp;amp;apos;, 1234, 50);
insert into events values (&amp;amp;apos;4&amp;amp;apos;, &amp;amp;apos;Ted&amp;amp;apos;, 1234, 60);



I realize select * and select s are not all that interesting in this context but what lead us to this issue was select count(distinct s) was not returning results. The above queries are the simplified queries that produce the issue. 
I will note that if I convert the inner join to a table and select from that the issue does not appear.
Update: Found that turning off  hive.optimize.remove.identity.project fixes this issue. This optimization was introduced in https://issues.apache.org/jira/browse/HIVE-8435</description>
			<version>1.1.0</version>
			<fixedVersion>1.1.1, 2.0.0, 1.2.2</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcCtx.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ColumnInfo.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">11931</link>
			<link type="Reference" description="relates to">11088</link>
			<link type="Reference" description="relates to">11056</link>
		</links>
	</bug>
	<bug id="11112" opendate="2015-06-25 17:18:34" fixdate="2015-06-29 14:23:36" resolution="Fixed">
		<buginformation>
			<summary>ISO-8859-1 text output has fragments of previous longer rows appended</summary>
			<description>If a LazySimpleSerDe table is created using ISO 8859-1 encoding, query results for a string column are incorrect for any row that was preceded by a row containing a longer string.
Example steps to reproduce:
1. Create a table using ISO 8859-1 encoding:


CREATE TABLE person_lat1 (name STRING)
ROW FORMAT SERDE &amp;amp;apos;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe&amp;amp;apos; WITH SERDEPROPERTIES (&amp;amp;apos;serialization.encoding&amp;amp;apos;=&amp;amp;apos;ISO8859_1&amp;amp;apos;);


2. Copy an ISO-8859-1 encoded text file into the appropriate warehouse folder in HDFS. I&amp;amp;apos;ll attach an example file containing the following text: 

Mller,Thomas
Jrgensen,Jrgen
Pea,Andrs
Nm,Fk


3. Execute SELECT * FROM person_lat1
Result - The following output appears:

+-------------------+--+
| person_lat1.name |
+-------------------+--+
| Mller,Thomas |
| Jrgensen,Jrgen |
| Pea,Andrsrgen |
| Nm,Fkdrsrgen |
+-------------------+--+

</description>
			<version>1.2.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.serde2.SerDeUtils.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">10983</link>
			<link type="Incorporates" description="is part of">10983</link>
			<link type="Reference" description="relates to">11095</link>
		</links>
	</bug>
	<bug id="11095" opendate="2015-06-24 13:30:34" fixdate="2015-06-30 12:24:52" resolution="Fixed">
		<buginformation>
			<summary>SerDeUtils  another bug ,when Text is reused</summary>
			<description>
The method transformTextFromUTF8 have a  error bug, It invoke a bad method of Text,getBytes()!
The method getBytes of Text returns the raw bytes; however, only data up to Text.length is valid.A better way is  use copyBytes()  if you need the returned array to be precisely the length of the data.
But the copyBytes is added behind hadoop1. 


How I found this bug
When i query data from a lzo table  I found in results  the length of the current row is always largr than the previous row and sometimesthe current row contains the contents of the previous row For example i execute a sql ,


select * from web_searchhub where logdate=2015061003


the result of sql see blow.Notice that ,the second row content contains the first row content.

INFO [03:00:05.589] HttpFrontServer::FrontSH msgRecv:Remote=/10.13.193.68:42098,session=3151,thread=254 2015061003
INFO [03:00:05.594] &amp;lt;18941e66-9962-44ad-81bc-3519f47ba274&amp;gt; session=901,thread=223ession=3151,thread=254 2015061003


The content of origin lzo file content see below ,just 2 rows.

INFO [03:00:05.635] &amp;lt;b88e0473-7530-494c-82d8-e2d2ebd2666c_forweb&amp;gt; session=3148,thread=285
INFO [03:00:05.635] HttpFrontServer::FrontSH msgRecv:Remote=/10.13.193.68:42095,session=3148,thread=285


I think this error is caused by the Text reuse,and I found the solutions .
Addicational, table create sql is : 


CREATE EXTERNAL TABLE `web_searchhub`(
`line` string)
PARTITIONED BY (
`logdate` string)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY &amp;amp;apos;
U0000&amp;amp;apos;
WITH SERDEPROPERTIES (
&amp;amp;apos;serialization.encoding&amp;amp;apos;=&amp;amp;apos;GBK&amp;amp;apos;)
STORED AS INPUTFORMAT "com.hadoop.mapred.DeprecatedLzoTextInputFormat"
OUTPUTFORMAT "org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat";
LOCATION
&amp;amp;apos;viewfs://nsX/user/hive/warehouse/raw.db/web/web_searchhub&amp;amp;apos; 

</description>
			<version>0.14.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.serde2.SerDeUtils.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">10983</link>
			<link type="Reference" description="relates to">10983</link>
			<link type="Reference" description="is related to">11112</link>
		</links>
	</bug>
	<bug id="10983" opendate="2015-06-11 09:07:49" fixdate="2015-06-30 17:51:14" resolution="Duplicate">
		<buginformation>
			<summary>SerDeUtils bug  ,when Text is reused </summary>
			<description>
The mothod transformTextToUTF8 and transformTextFromUTF8  have a error bug,It invoke a bad method of Text,getBytes()!
The method getBytes of Text returns the raw bytes; however, only data up to Text.length is valid.A better way is  use copyBytes()  if you need the returned array to be precisely the length of the data.
But the copyBytes is added behind hadoop1. 


When i query data from a lzo table  I found  in results  the length of the current row is always largr  than the previous row and sometimesthe current  row contains the contents of the previous row For example i execute a sql ,


select *   from web_searchhub where logdate=2015061003


the result of sql see blow.Notice that ,the second row content contains the first row content.

INFO [03:00:05.589] HttpFrontServer::FrontSH msgRecv:Remote=/10.13.193.68:42098,session=3151,thread=254 2015061003
INFO [03:00:05.594] &amp;lt;18941e66-9962-44ad-81bc-3519f47ba274&amp;gt; session=901,thread=223ession=3151,thread=254 2015061003


The content  of origin lzo file content see below ,just 2 rows.

INFO [03:00:05.635] &amp;lt;b88e0473-7530-494c-82d8-e2d2ebd2666c_forweb&amp;gt; session=3148,thread=285
INFO [03:00:05.635] HttpFrontServer::FrontSH msgRecv:Remote=/10.13.193.68:42095,session=3148,thread=285


I think this error is caused by the Text reuse,and I found the solutions .
Addicational, table create sql is : 


CREATE EXTERNAL TABLE `web_searchhub`(
  `line` string)
PARTITIONED BY (
  `logdate` string)
ROW FORMAT DELIMITED
  FIELDS TERMINATED BY &amp;amp;apos;\\U0000&amp;amp;apos;
WITH SERDEPROPERTIES (
  &amp;amp;apos;serialization.encoding&amp;amp;apos;=&amp;amp;apos;GBK&amp;amp;apos;)
STORED AS INPUTFORMAT  "com.hadoop.mapred.DeprecatedLzoTextInputFormat"
          OUTPUTFORMAT "org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat";

LOCATION
  &amp;amp;apos;viewfs://nsX/user/hive/warehouse/raw.db/web/web_searchhub&amp;amp;apos; 

</description>
			<version>0.14.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.serde2.SerDeUtils.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">11095</link>
			<link type="Duplicate" description="duplicates">11112</link>
			<link type="Incorporates" description="incorporates">11112</link>
			<link type="Reference" description="is related to">11095</link>
		</links>
	</bug>
	<bug id="10895" opendate="2015-06-02 19:54:46" fixdate="2015-07-09 17:26:04" resolution="Fixed">
		<buginformation>
			<summary>ObjectStore does not close Query objects in some calls, causing a potential leak in some metastore db resources</summary>
			<description>During testing, we&amp;amp;apos;ve noticed Oracle db running out of cursors. Might be related to this.</description>
			<version>0.13</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.tools.HiveMetaTool.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">11021</link>
		</links>
	</bug>
	<bug id="4239" opendate="2013-03-27 23:54:48" fixdate="2015-07-09 18:33:34" resolution="Fixed">
		<buginformation>
			<summary>Remove lock on compilation stage</summary>
			<description></description>
			<version>0.12.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.GenTezProcContext.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hive.service.cli.CLIServiceTest.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.RemoveDynamicPruningBySize.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.GenTezWork.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.ql.session.SessionState.java</file>
			<file type="M">org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
			<file type="M">org.apache.hive.service.cli.session.HiveSessionImplwithUGI.java</file>
		</fixedFiles>
		<links>
			<link type="Blocked" description="Blocked">13193</link>
			<link type="Duplicate" description="is duplicated by">10827</link>
			<link type="Duplicate" description="is duplicated by">8440</link>
			<link type="Duplicate" description="is duplicated by">11677</link>
			<link type="Duplicate" description="is duplicated by">5244</link>
			<link type="Reference" description="relates to">8134</link>
			<link type="Reference" description="relates to">10876</link>
			<link type="Reference" description="is related to">8440</link>
			<link type="Reference" description="is related to">11402</link>
			<link type="Reference" description="is related to">13882</link>
			<link type="Reference" description="is related to">2935</link>
			<link type="dependent" description="is depended upon by">11165</link>
		</links>
	</bug>
	<bug id="11237" opendate="2015-07-13 16:30:52" fixdate="2015-07-13 17:10:16" resolution="Duplicate">
		<buginformation>
			<summary>Unable to drop a default partition with "int" type</summary>
			<description>
CREATE TABLE test (col1 string) PARTITIONED BY (p1 int) ROW FORMAT DELIMITED FIELDS TERMINATED BY &amp;amp;apos;\001&amp;amp;apos; STORED AS TEXTFILE;
INSERT OVERWRITE TABLE test PARTITION (p1) SELECT code, IF(salary &amp;gt; 60000, 100, null) as p1 FROM default.sample_07;
hive&amp;gt; SHOW PARTITIONS test;
OK
p1=100
p1=__HIVE_DEFAULT_PARTITION__
Time taken: 0.124 seconds, Fetched: 2 row(s)

hive&amp;gt; ALTER TABLE test DROP partition (p1 = &amp;amp;apos;__HIVE_DEFAULT_PARTITION__&amp;amp;apos;);
FAILED: SemanticException Unexpected unknown partitions for (p1 = null)


The default partition name &amp;amp;apos;_HIVE_DEFAULT_PARTITION_&amp;amp;apos; cannot be deleted.</description>
			<version>2.0.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ppr.PartExprEvalUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqual.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNotEqual.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">11208</link>
		</links>
	</bug>
	<bug id="7566" opendate="2014-07-31 02:16:10" fixdate="2015-08-06 16:44:18" resolution="Duplicate">
		<buginformation>
			<summary>HIVE can&amp;apos;t count hbase NULL column value properly</summary>
			<description>HBase table structure is like this:
table name : &amp;amp;apos;testtable&amp;amp;apos;
column family : &amp;amp;apos;data&amp;amp;apos;
column 1 : &amp;amp;apos;name&amp;amp;apos;
column 2 : &amp;amp;apos;color&amp;amp;apos;
HIVE mapping table is structure is like this:
table name : &amp;amp;apos;hb_testtable&amp;amp;apos;
column 1 : &amp;amp;apos;name&amp;amp;apos;
column 2 : &amp;amp;apos;color&amp;amp;apos;
in hbase, put two rows
James, blue
May
then do select in hive
select * from hb_testtable where color is null
the result is 
May, NULL
then try count 
select count from hb_testtable where color is null
the result is 0, which should be 1</description>
			<version>0.13.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.hbase.HiveHBaseInputFormatUtil.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">5277</link>
		</links>
	</bug>
	<bug id="10880" opendate="2015-06-01 15:24:59" fixdate="2015-08-07 13:20:36" resolution="Fixed">
		<buginformation>
			<summary>The bucket number is not respected in insert overwrite.</summary>
			<description>When hive.enforce.bucketing is true, the bucket number defined in the table is no longer respected in current master and 1.2. 
Reproduce:


CREATE TABLE IF NOT EXISTS buckettestinput( 
data string 
) 
ROW FORMAT DELIMITED FIELDS TERMINATED BY &amp;amp;apos;,&amp;amp;apos;;

CREATE TABLE IF NOT EXISTS buckettestoutput1( 
data string 
)CLUSTERED BY(data) 
INTO 2 BUCKETS 
ROW FORMAT DELIMITED FIELDS TERMINATED BY &amp;amp;apos;,&amp;amp;apos;;

CREATE TABLE IF NOT EXISTS buckettestoutput2( 
data string 
)CLUSTERED BY(data) 
INTO 2 BUCKETS 
ROW FORMAT DELIMITED FIELDS TERMINATED BY &amp;amp;apos;,&amp;amp;apos;;


Then I inserted the following data into the "buckettestinput" table:

firstinsert1 
firstinsert2 
firstinsert3 
firstinsert4 
firstinsert5 
firstinsert6 
firstinsert7 
firstinsert8 
secondinsert1 
secondinsert2 
secondinsert3 
secondinsert4 
secondinsert5 
secondinsert6 
secondinsert7 
secondinsert8




set hive.enforce.bucketing = true; 
set hive.enforce.sorting=true;
insert overwrite table buckettestoutput1 
select * from buckettestinput where data like &amp;amp;apos;first%&amp;amp;apos;;
set hive.auto.convert.sortmerge.join=true; 
set hive.optimize.bucketmapjoin = true; 
set hive.optimize.bucketmapjoin.sortedmerge = true; 
select * from buckettestoutput1 a join buckettestoutput2 b on (a.data=b.data);



Error: Error while compiling statement: FAILED: SemanticException [Error 10141]: Bucketed table metadata is not correct. Fix the metadata or don&amp;amp;apos;t use bucketed mapjoin, by setting hive.enforce.bucketmapjoin to false. The number of buckets for table buckettestoutput1 is 2, whereas the number of files is 1 (state=42000,code=10141)


The related debug information related to insert overwrite:

0: jdbc:hive2://localhost:10000&amp;gt; insert overwrite table buckettestoutput1 
select * from buckettestinput where data like &amp;amp;apos;first%&amp;amp;apos;insert overwrite table buckettestoutput1 
0: jdbc:hive2://localhost:10000&amp;gt; ;
select * from buckettestinput where data like &amp;amp;apos; 
first%&amp;amp;apos;;
INFO  : Number of reduce tasks determined at compile time: 2
INFO  : In order to change the average load for a reducer (in bytes):
INFO  :   set hive.exec.reducers.bytes.per.reducer=&amp;lt;number&amp;gt;
INFO  : In order to limit the maximum number of reducers:
INFO  :   set hive.exec.reducers.max=&amp;lt;number&amp;gt;
INFO  : In order to set a constant number of reducers:
INFO  :   set mapred.reduce.tasks=&amp;lt;number&amp;gt;
INFO  : Job running in-process (local Hadoop)
INFO  : 2015-06-01 11:09:29,650 Stage-1 map = 86%,  reduce = 100%
INFO  : Ended Job = job_local107155352_0001
INFO  : Loading data to table default.buckettestoutput1 from file:/user/hive/warehouse/buckettestoutput1/.hive-staging_hive_2015-06-01_11-09-28_166_3109203968904090801-1/-ext-10000
INFO  : Table default.buckettestoutput1 stats: [numFiles=1, numRows=4, totalSize=52, rawDataSize=48]
No rows affected (1.692 seconds)


Insert use dynamic partition does not have the issue. </description>
			<version>1.2.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">10879</link>
			<link type="Duplicate" description="is duplicated by">10881</link>
			<link type="Reference" description="relates to">11360</link>
		</links>
	</bug>
	<bug id="10625" opendate="2015-05-06 09:38:33" fixdate="2015-08-11 01:52:34" resolution="Duplicate">
		<buginformation>
			<summary>Handle Authorization for  &amp;apos;select &lt;expr&gt;&amp;apos; hive queries in  SQL Standard Authorization</summary>
			<description>Hive internally rewrites this &amp;amp;apos;select &amp;lt;expression&amp;gt;&amp;amp;apos; query into &amp;amp;apos;select &amp;lt;expression&amp;gt; from _dummy_database._dummy_table&amp;amp;apos;, where these dummy db and table are temp entities for the current query.
The SQL Standard Authorization  need to handle these special objects.
Typing "select reverse("123");" in beeline,will get this error :


Error: Error while compiling statement: FAILED: HiveAuthzPluginException Error getting object from metastore for Object [type=TABLE_OR_VIEW, name=_dummy_database._dummy_table] (state=42000,code=40000)

</description>
			<version>1.1.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">11498</link>
		</links>
	</bug>
	<bug id="11498" opendate="2015-08-07 05:03:41" fixdate="2015-08-11 03:31:28" resolution="Fixed">
		<buginformation>
			<summary>HIVE Authorization v2 should not check permission for dummy entity</summary>
			<description>The queries like SELECT 1+1;, The target table and database will set to _dummy_database _dummy_table, authorization should skip these kinds of databases or tables.
For authz v1. it has skip them.
eg1. Source code at github

for (WriteEntity write : outputs) {
        if (write.isDummy() || write.isPathType()) {
          continue;
        }


eg2. Source code at github

for (ReadEntity read : inputs) {
        if (read.isDummy() || read.isPathType()) {
          continue;
        }
       ...
        }


...
This patch will fix authz v2.</description>
			<version>1.2.0</version>
			<fixedVersion>1.3.0, 1.2.1, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">10625</link>
		</links>
	</bug>
	<bug id="11502" opendate="2015-08-08 21:34:46" fixdate="2015-08-20 01:23:26" resolution="Fixed">
		<buginformation>
			<summary>Map side aggregation is extremely slow</summary>
			<description>For the query as following:

create table tbl2 as 
select col1, max(col2) as col2 
from tbl1 group by col1;


If the column for group by has many different values (for example 400000) and it is in type double, the map side aggregation is very slow. I ran the query which took more than 3 hours , after 3 hours, I have to kill the query.
The same query can finish in 7 seconds, if I turn off map side aggregation by:

set hive.map.aggr = false;

</description>
			<version>1.2.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.KeyWrapperFactory.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">9495</link>
			<link type="Duplicate" description="is duplicated by">12093</link>
			<link type="Reference" description="relates to">11761</link>
			<link type="Reference" description="is related to">12217</link>
		</links>
	</bug>
	<bug id="11607" opendate="2015-08-19 23:18:47" fixdate="2015-08-21 17:12:59" resolution="Fixed">
		<buginformation>
			<summary>Export tables broken for data &gt; 32 MB</summary>
			<description>Broken for both hadoop-1 as well as hadoop-2 line</description>
			<version>1.0.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
			<file type="M">org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">12087</link>
			<link type="Reference" description="is related to">11820</link>
			<link type="Regression" description="breaks">13704</link>
			<link type="Regression" description="is broken by">9264</link>
		</links>
	</bug>
	<bug id="5277" opendate="2013-09-12 02:15:12" fixdate="2015-08-21 18:22:13" resolution="Fixed">
		<buginformation>
			<summary>HBase handler skips rows with null valued first cells when only row key is selected</summary>
			<description>HBaseStorageHandler skips rows with null valued first cells when only row key is selected.

SELECT key, col1, col2 FROM hbase_table;
key1	cell1	cell2 
key2	NULL	cell3

SELECT COUNT(key) FROM hbase_table;
1


HiveHBaseTableInputFormat.getRecordReader makes first cell selected to avoid skipping rows. But when the first cell is null, HBase skips that row.
http://hbase.apache.org/book/perf.reading.html 12.9.6. Optimal Loading of Row Keys describes how to deal with this problem.
I tried to find an existing issue, but I couldn&amp;amp;apos;t. If you find a same issue, please make this issue duplicated.</description>
			<version>0.11.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.hbase.HiveHBaseInputFormatUtil.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">7566</link>
			<link type="Reference" description="relates to">11849</link>
		</links>
	</bug>
	<bug id="11123" opendate="2015-06-26 04:14:34" fixdate="2015-08-27 18:16:26" resolution="Fixed">
		<buginformation>
			<summary>Fix how to confirm the RDBMS product name at Metastore.</summary>
			<description>I use PostgreSQL to Hive Metastore. And I saw the following message at PostgreSQL log.


&amp;lt; 2015-06-26 10:58:15.488 JST &amp;gt;ERROR:  syntax error at or near "@@" at character 5
&amp;lt; 2015-06-26 10:58:15.488 JST &amp;gt;STATEMENT:  SET @@session.sql_mode=ANSI_QUOTES
&amp;lt; 2015-06-26 10:58:15.489 JST &amp;gt;ERROR:  relation "v$instance" does not exist at character 21
&amp;lt; 2015-06-26 10:58:15.489 JST &amp;gt;STATEMENT:  SELECT version FROM v$instance
&amp;lt; 2015-06-26 10:58:15.490 JST &amp;gt;ERROR:  column "version" does not exist at character 10
&amp;lt; 2015-06-26 10:58:15.490 JST &amp;gt;STATEMENT:  SELECT @@version


When Hive CLI and Beeline embedded mode are carried out, this message is output to PostgreSQL log.
These queries are called from MetaStoreDirectSql#determineDbType. And if we use MetaStoreDirectSql#getProductName, we need not to call these queries.</description>
			<version>1.2.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">11859</link>
		</links>
	</bug>
	<bug id="11504" opendate="2015-08-10 03:29:01" fixdate="2015-09-01 01:13:41" resolution="Fixed">
		<buginformation>
			<summary>Predicate pushing down doesn&amp;apos;t work for float type for Parquet</summary>
			<description>Predicate builder should use PrimitiveTypeName type in parquet side to construct predicate leaf instead of the type provided by PredicateLeaf.</description>
			<version>1.2.1</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Sub-task</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.parquet.read.TestParquetFilterPredicate.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.parquet.LeafFilterFactory.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">15004</link>
			<link type="Duplicate" description="duplicates">13114</link>
		</links>
	</bug>
	<bug id="11566" opendate="2015-08-14 20:28:02" fixdate="2015-09-01 23:09:09" resolution="Duplicate">
		<buginformation>
			<summary>Hybrid grace hash join should only allocate write buffer for a hash partition when first write happens</summary>
			<description>Currently it&amp;amp;apos;s allocating one write buffer for a number of hash partitions up front, which can cause GC pause.
It&amp;amp;apos;s better to do the write buffer allocation on demand.</description>
			<version>1.2.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.WriteBuffers.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.HashTableLoader.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">11587</link>
		</links>
	</bug>
	<bug id="11657" opendate="2015-08-26 21:58:38" fixdate="2015-09-04 02:58:13" resolution="Fixed">
		<buginformation>
			<summary>HIVE-2573 introduces some issues during metastore init (and CLI init)</summary>
			<description>HIVE-2573 introduced static reload functions call.
It has a few problems:
1) When metastore client is initialized using an externally supplied config (i.e. Hive.get(HiveConf)), it still gets called during static init using the main service config. In my case, even though I have uris in the supplied config to connect to remote MS (which eventually happens), the static call creates objectstore, which is undesirable.
2) It breaks compat - old metastores do not support this call so new clients will fail, and there&amp;amp;apos;s no workaround like not using a new feature because the static call is always made</description>
			<version>0.14.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FunctionTask.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">11801</link>
			<link type="Reference" description="is related to">2573</link>
		</links>
	</bug>
	<bug id="11587" opendate="2015-08-17 21:09:30" fixdate="2015-09-10 19:02:28" resolution="Fixed">
		<buginformation>
			<summary>Fix memory estimates for mapjoin hashtable</summary>
			<description>Due to the legacy in in-memory mapjoin and conservative planning, the memory estimation code for mapjoin hashtable is currently not very good. It allocates the probe erring on the side of more memory, not taking data into account because unlike the probe, it&amp;amp;apos;s free to resize, so it&amp;amp;apos;s better for perf to allocate big probe and hope for the best with regard to future data size. It is not true for hybrid case.
There&amp;amp;apos;s code to cap the initial allocation based on memory available (memUsage argument), but due to some code rot, the memory estimates from planning are not even passed to hashtable anymore (there used to be two config settings, hashjoin size fraction by itself, or hashjoin size fraction for group by case), so it never caps the memory anymore below 1 Gb. 
Initial capacity is estimated from input key count, and in hybrid join cache can exceed Java memory due to number of segments.
There needs to be a review and fix of all this code.
Suggested improvements:
1) Make sure "initialCapacity" argument from Hybrid case is correct given the number of segments. See how it&amp;amp;apos;s calculated from keys for regular case; it needs to be adjusted accordingly for hybrid case if not done already.
1.5) Note that, knowing the number of rows, the maximum capacity one will ever need for probe size (in longs) is row count (assuming key per row, i.e. maximum possible number of keys) divided by load factor, plus some very small number to round up. That is for flat case. For hybrid case it may be more complex due to skew, but that is still a good upper bound for the total probe capacity of all segments.
2) Rename memUsage to maxProbeSize, or something, make sure it&amp;amp;apos;s passed correctly based on estimates that take into account both probe and data size, esp. in hybrid case.
3) Make sure that memory estimation for hybrid case also doesn&amp;amp;apos;t come up with numbers that are too small, like 1-byte hashtable. I am not very familiar with that code but it has happened in the past.
Other issues we have seen:
4) Cap single write buffer size to 8-16Mb. The whole point of WBs is that you should not allocate large array in advance. Even if some estimate passes 500Mb or 40Mb or whatever, it doesn&amp;amp;apos;t make sense to allocate that.
5) For hybrid, don&amp;amp;apos;t pre-allocate WBs - only allocate on write.
6) Change everywhere rounding up to power of two is used to rounding down, at least for hybrid case 
I wanted to put all of these items in single JIRA so we could keep track of fixing all of them.
I think there are JIRAs for some of these already, feel free to link them to this one.</description>
			<version>1.2.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.WriteBuffers.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.HashTableLoader.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">11566</link>
		</links>
	</bug>
	<bug id="11801" opendate="2015-09-11 18:21:16" fixdate="2015-09-11 19:21:11" resolution="Duplicate">
		<buginformation>
			<summary>In HMS HA env, "show databases" fails when"current" HMS is stopped.</summary>
			<description>Reproduce steps:

Enable HMS HA on a cluster
Use beeline to connect to HS2 and execute command show databases. Don&amp;amp;apos;t quit beeline after command has finished
Stop the first HMS in configuration hive.metastore.uri
Execute show databases in beeline again. Will get below error:

MetaException(message:Got exception: org.apache.thrift.transport.TTransportException java.net.SocketException: Broken pipe)



The error message in HS2 is as below:

2015-09-08 12:06:53,236 ERROR hive.log: Got exception: org.apache.thrift.transport.TTransportException java.net.SocketException: Broken pipe
org.apache.thrift.transport.TTransportException: java.net.SocketException: Broken pipe
	at org.apache.thrift.transport.TIOStreamTransport.flush(TIOStreamTransport.java:161)
	at org.apache.thrift.transport.TSaslTransport.flush(TSaslTransport.java:501)
	at org.apache.thrift.transport.TSaslClientTransport.flush(TSaslClientTransport.java:37)
	at org.apache.hadoop.hive.thrift.TFilterTransport.flush(TFilterTransport.java:77)
	at org.apache.thrift.TServiceClient.sendBase(TServiceClient.java:65)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.send_get_databases(ThriftHiveMetastore.java:692)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_databases(ThriftHiveMetastore.java:684)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDatabases(HiveMetaStoreClient.java:964)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:91)
	at com.sun.proxy.$Proxy6.getDatabases(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:1909)
	at com.sun.proxy.$Proxy6.getDatabases(Unknown Source)
	at org.apache.hive.service.cli.operation.GetSchemasOperation.runInternal(GetSchemasOperation.java:59)
	at org.apache.hive.service.cli.operation.Operation.run(Operation.java:257)
	at org.apache.hive.service.cli.session.HiveSessionImpl.getSchemas(HiveSessionImpl.java:462)
	at org.apache.hive.service.cli.CLIService.getSchemas(CLIService.java:296)
	at org.apache.hive.service.cli.thrift.ThriftCLIService.GetSchemas(ThriftCLIService.java:534)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$GetSchemas.getResult(TCLIService.java:1373)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$GetSchemas.getResult(TCLIService.java:1358)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:692)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Broken pipe
	at java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109)
	at java.net.SocketOutputStream.write(SocketOutputStream.java:153)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
	at org.apache.thrift.transport.TIOStreamTransport.flush(TIOStreamTransport.java:159)
	... 31 more
2015-09-08 12:06:53,238 ERROR hive.log: Converting exception to MetaException
2015-09-08 12:06:53,238 WARN org.apache.hive.service.cli.thrift.ThriftCLIService: Error getting schemas:
org.apache.hive.service.cli.HiveSQLException: MetaException(message:Got exception: org.apache.thrift.transport.TTransportException java.net.SocketException: Broken pipe)
	at org.apache.hive.service.cli.operation.GetSchemasOperation.runInternal(GetSchemasOperation.java:65)
	at org.apache.hive.service.cli.operation.Operation.run(Operation.java:257)
	at org.apache.hive.service.cli.session.HiveSessionImpl.getSchemas(HiveSessionImpl.java:462)
	at org.apache.hive.service.cli.CLIService.getSchemas(CLIService.java:296)
	at org.apache.hive.service.cli.thrift.ThriftCLIService.GetSchemas(ThriftCLIService.java:534)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$GetSchemas.getResult(TCLIService.java:1373)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$GetSchemas.getResult(TCLIService.java:1358)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:692)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: MetaException(message:Got exception: org.apache.thrift.transport.TTransportException java.net.SocketException: Broken pipe)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.logAndThrowMetaException(MetaStoreUtils.java:1178)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDatabases(HiveMetaStoreClient.java:966)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:91)
	at com.sun.proxy.$Proxy6.getDatabases(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:1909)
	at com.sun.proxy.$Proxy6.getDatabases(Unknown Source)
	at org.apache.hive.service.cli.operation.GetSchemasOperation.runInternal(GetSchemasOperation.java:59)
	... 13 more

</description>
			<version>0.14.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FunctionTask.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">11657</link>
		</links>
	</bug>
	<bug id="11859" opendate="2015-09-17 03:42:01" fixdate="2015-09-17 18:42:14" resolution="Duplicate">
		<buginformation>
			<summary>Bunch of SQL error reported for Hive with SQL Server as Metastore</summary>
			<description>We are getting a lot of SQL errors reported at SQL Server end. Our set up is a Two Node Hive cluster using SQL Server as MetaStore.
Here is a snippet of SQL Server errors
Invalid object name &amp;amp;apos;v$instance&amp;amp;apos;. Caused by: SELECT version FROM v$instance
Must declare the scalar variable "@@session".Caused by: SET @@session.sql_mode=ANSI_QUOTES
This is filling our logs at a very rapid rate, which we are forced to purge. 
Please note these queries are for MySQL / Oracle.</description>
			<version>1.2.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">11123</link>
		</links>
	</bug>
	<bug id="11850" opendate="2015-09-16 20:04:56" fixdate="2015-09-22 20:50:13" resolution="Duplicate">
		<buginformation>
			<summary>On Windows, creating udf function using wasb fail throwing java.lang.RuntimeException: invalid url: wasb:///...  expecting ( file | hdfs | ivy)  as url scheme.</summary>
			<description>
hive&amp;gt; drop function if exists gencounter;
OK
Time taken: 2.614 seconds
On Humboldt, creating UDF function fail as follows:
hive&amp;gt; create function gencounter as &amp;amp;apos;org.apache.hive.udf.generic.GenericUDFGenCounter&amp;amp;apos; using jar &amp;amp;apos;wasb:///tmp/hive-udfs-0.1.jar&amp;amp;apos;;
invalid url: wasb:///tmp/hive-udfs-0.1.jar, expecting ( file | hdfs | ivy)  as url scheme.
Failed to register default.gencounter using class org.apache.hive.udf.generic.GenericUDFGenCounter
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.FunctionTask


The jar exists in wasb dir:

hrt_qa@headnode0:~$ hadoop fs -ls wasb:///tmp/
Found 2 items
-rw-r--r--   1 hrt_qa supergroup       4472 2015-09-16 11:50 wasb:///tmp/hive-udfs-0.1.jar
drwxrwxrwx   - hdfs   supergroup          0 2015-09-16 12:00 wasb:///tmp/aa

</description>
			<version>1.2.1</version>
			<fixedVersion>1.2.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.session.SessionState.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">11920</link>
		</links>
	</bug>
	<bug id="9748" opendate="2015-02-21 08:33:18" fixdate="2015-09-24 22:56:00" resolution="Duplicate">
		<buginformation>
			<summary>MetastoreDirectSql fails for zero item queries</summary>
			<description>Metastore Direct SQL throws a SQL Exception 


2015-02-21 00:29:00,238 WARN  [pool-3-thread-10]: metastore.ObjectStore (ObjectStore.java:handleDirectSqlError(2400)) - Direct SQL failed
MetaException(message:See previous errors; Error executing SQL query "select count("COLUMN_NAME") from "PART_COL_STATS" where "DB_NAME" = ? and "TABLE_NAME" = ?  and "COLUMN_NAME" in () and "PARTITION_NAME" in () group by "PARTITION_NAME"
".)
        at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.executeWithArray(MetaStoreDirectSql.java:1448)
        at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.partsFoundForPartitions(MetaStoreDirectSql.java:1098)
        at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.aggrColStatsForPartitions(MetaStoreDirectSql.java:1081)
        at org.apache.hadoop.hive.metastore.ObjectStore$8.getSqlResult(ObjectStore.java:6100)
        at org.apache.hadoop.hive.metastore.ObjectStore$8.getSqlResult(ObjectStore.java:6096)
        at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2365)
        at org.apache.hadoop.hive.metastore.ObjectStore.get_aggr_stats_for(ObjectStore.java:6115)


The query to trigger the issue an EXPLAIN query with column + partition stats on.


 explain select count(1) from store_sales where &amp;amp;apos;2014-10-01&amp;amp;apos; =ss_sold_date ;

</description>
			<version>1.0.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
			<file type="M">org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">10965</link>
		</links>
	</bug>
	<bug id="10048" opendate="2015-03-21 02:29:10" fixdate="2015-09-28 03:12:47" resolution="Fixed">
		<buginformation>
			<summary>JDBC - Support SSL encryption regardless of Authentication mechanism</summary>
			<description>JDBC driver currently only supports SSL Transport if the Authentication mechanism is SASL Plain with username and password. SSL transport  should be decoupled from Authentication mechanism. If the customer chooses to do Kerberos Authentication and SSL encryption over the wire it should be supported. The Server side already supports this but the driver does not.</description>
			<version>1.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.jdbc.HiveConnection.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">14019</link>
		</links>
	</bug>
	<bug id="11920" opendate="2015-09-22 20:39:28" fixdate="2015-09-30 18:45:29" resolution="Fixed">
		<buginformation>
			<summary>ADD JAR failing with URL schemes other than file/ivy/hdfs</summary>
			<description>Example stack trace below. It looks like this was introduced by HIVE-9664.

015-09-16 19:53:16,502 ERROR [main]: SessionState (SessionState.java:printError(960)) - invalid url: wasb:///tmp/hive-udfs-0.1.jar, expecting ( file | hdfs | ivy)  as url scheme.
java.lang.RuntimeException: invalid url: wasb:///tmp/hive-udfs-0.1.jar, expecting ( file | hdfs | ivy)  as url scheme.
        at org.apache.hadoop.hive.ql.session.SessionState.getURLType(SessionState.java:1230)
        at org.apache.hadoop.hive.ql.session.SessionState.resolveAndDownload(SessionState.java:1237)
        at org.apache.hadoop.hive.ql.session.SessionState.add_resources(SessionState.java:1163)
        at org.apache.hadoop.hive.ql.session.SessionState.add_resources(SessionState.java:1149)
        at org.apache.hadoop.hive.ql.exec.FunctionTask.addFunctionResources(FunctionTask.java:301)
        at org.apache.hadoop.hive.ql.exec.Registry.registerToSessionRegistry(Registry.java:453)
        at org.apache.hadoop.hive.ql.exec.Registry.registerPermanentFunction(Registry.java:200)
        at org.apache.hadoop.hive.ql.exec.FunctionRegistry.registerPermanentFunction(FunctionRegistry.java:1495)
        at org.apache.hadoop.hive.ql.exec.FunctionTask.createPermanentFunction(FunctionTask.java:136)
        at org.apache.hadoop.hive.ql.exec.FunctionTask.execute(FunctionTask.java:75)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89)
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1655)
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1414)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1195)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:736)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)

</description>
			<version>1.2.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.session.SessionState.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">11850</link>
			<link type="Reference" description="is related to">9664</link>
		</links>
	</bug>
	<bug id="12006" opendate="2015-10-01 17:28:02" fixdate="2015-10-01 17:36:56" resolution="Duplicate">
		<buginformation>
			<summary>Enable Columnar Pushdown for RC/ORC File for HCatLoader</summary>
			<description>This initially enabled by HIVE-5193. However, HIVE-10752 reverted it since there is issue in original implementation.
We shall fix the issue an reenable it.</description>
			<version>1.2.1</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.hcatalog.pig.HCatLoader.java</file>
			<file type="M">org.apache.hive.hcatalog.pig.TestHCatLoader.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.ColumnProjectionUtils.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">10755</link>
			<link type="Supercedes" description="supercedes">5193</link>
		</links>
	</bug>
	<bug id="12040" opendate="2015-10-06 00:02:13" fixdate="2015-10-06 00:06:00" resolution="Duplicate">
		<buginformation>
			<summary>CBO: Use CBO, even for the 1 JOIN + GROUP BY case</summary>
			<description>Hive CBO kicks in for a query only if a query has &amp;gt;1 joins, which is an archaic result of only re-ordering joins in the original impl.
As more group-by &amp;amp; filter optimizers have been added, modify the CBO impl to kick in even if the query has a single JOIN.</description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveAggregateProjectMergeRule.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelOptUtil.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.translator.SqlFunctionConverter.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">12017</link>
		</links>
	</bug>
	<bug id="7316" opendate="2014-06-30 20:30:46" fixdate="2015-10-06 18:33:07" resolution="Duplicate">
		<buginformation>
			<summary>Hive fails on zero length files</summary>
			<description>Flume will, at times, generate zero length files. This causes queries to fail on Avro data and likely sequence file as well.</description>
			<version>0.14.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.avro.AvroGenericRecordReader.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">11977</link>
			<link type="Reference" description="relates to">12607</link>
			<link type="Reference" description="relates to">1530</link>
		</links>
	</bug>
	<bug id="11977" opendate="2015-09-28 18:25:52" fixdate="2015-10-07 16:26:00" resolution="Fixed">
		<buginformation>
			<summary>Hive should handle an external avro table with zero length files present</summary>
			<description>If a zero length file is in the top level directory housing an external avro table,  all hive queries on the table fail.
This issue is that org.apache.hadoop.hive.ql.io.avro.AvroGenericRecordReader creates a new org.apache.avro.file.DataFileReader and DataFileReader throws an exception when trying to read an empty file (because the empty file lacks the magic number marking it as avro).  
AvroGenericRecordReader should detect an empty file and then behave reasonably.
Caused by: java.io.IOException: Not a data file.
at org.apache.avro.file.DataFileStream.initialize(DataFileStream.java:102)
at org.apache.avro.file.DataFileReader.&amp;lt;init&amp;gt;(DataFileReader.java:97)
at org.apache.hadoop.hive.ql.io.avro.AvroGenericRecordReader.&amp;lt;init&amp;gt;(AvroGenericRecordReader.java:81)
at org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.getRecordReader(AvroContainerInputFormat.java:51)
at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:246)
... 25 more</description>
			<version>0.14.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.avro.AvroGenericRecordReader.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">7316</link>
			<link type="Reference" description="relates to">12607</link>
			<link type="Reference" description="relates to">1530</link>
		</links>
	</bug>
	<bug id="11798" opendate="2015-09-11 06:21:53" fixdate="2015-10-09 18:46:07" resolution="Duplicate">
		<buginformation>
			<summary>The Beeline report should not display the header when --showHeader is set to false.</summary>
			<description>In Beeline tool User sets the --showheader option as false.
In command line interface user inputs the command bin/beeline -u jdbc:hive2://10.19.92.183:10000  --showHeader=false
Actual Result : The Beeline report displays the column name.
Expected Result : The Beeline report should not display the header when --showHeader is set to false.</description>
			<version>0.13.0</version>
			<fixedVersion>0.14.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.beeline.TableOutputFormat.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">7200</link>
		</links>
	</bug>
	<bug id="11931" opendate="2015-09-23 06:22:22" fixdate="2015-10-10 01:16:53" resolution="Duplicate">
		<buginformation>
			<summary>Join sql cannot get result</summary>
			<description>I found a join issue in hive-1.2.1 and hive-1.1.1.
The create table sql is as below.


CREATE TABLE IF NOT EXISTS join_case(
    orderid  bigint,
    tradeitemid bigint,
    id bigint
) ROW FORMAT DELIMITED
FIELDS TERMINATED BY &amp;amp;apos;,&amp;amp;apos; 
LINES TERMINATED BY &amp;amp;apos;\n&amp;amp;apos;
STORED AS TEXTFILE;


Please put attached sample data file 000000_0 in /tmp/join_case folder.
Then load data.


LOAD DATA LOCAL INPATH &amp;amp;apos;/tmp/join_case/000000_0&amp;amp;apos; OVERWRITE INTO TABLE join_case;


Run the following sql, but cannot get searching result.


select a.id from 
(
select orderid as orderid, max(id) as id from join_case group by orderid
) a 
join 
(
select id as id , orderid as orderid from join_case
) b
on a.id = b.id limit 10;


This issue also occurs in hive-1.1.0-cdh5.4.5.
But in apache hive-1.0.1 the above sql can return 10 rows.
After exchanging the sequence of "orderid as orderid" and "max(id) as id", the following sql can get result in hive-1.2.1 and hive-1.1.1.


select a.id from 
(
select max(id) as id, orderid as orderid from join_case group by orderid
) a 
join 
(
select id as id , orderid as orderid from join_case
) b
on a.id = b.id limit 10;


Also, the following sql can get results in hive-1.2.1 and hive-1.1.1.


select a.id from 
(
select orderid as orderid, id as id from join_case group by orderid, id
) a 
join 
(
select id as id , orderid as orderid from join_case
) b
on a.id = b.id limit 10; 


Anyone can take a look at this issue? 
Thanks.</description>
			<version>1.1.1</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcCtx.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ColumnInfo.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">10996</link>
		</links>
	</bug>
	<bug id="12087" opendate="2015-10-11 07:18:07" fixdate="2015-10-11 15:22:25" resolution="Duplicate">
		<buginformation>
			<summary>IMPORT TABLE fails</summary>
			<description>IMPORT TABLE fails for larger tables with:


0: jdbc:hive2://hdpprdhiv01.prd.xxx:10001/&amp;gt; import from &amp;amp;apos;/tmp/export/repository/res_sales_navigator&amp;amp;apos;;
INFO  : Copying data from hdfs://hdpprdmas01.prd.xxx:8020/tmp/export/repository/res_sales_navigator/valid_from=201508250000 to hdfs://hdpprdmas01.prd.xxx:8020/tmp/export/repository/res_sales_navigator/.hive-staging_hive_2015-10-07_20-55-37_456_5706704167497413401-2/-ext-10000
INFO  : Copying file: hdfs://hdpprdmas01.prd.xxx:8020/tmp/export/repository/res_sales_navigator/valid_from=201508250000/part-r-00000
ERROR : Failed with exception Cannot get DistCp constructor: org.apache.hadoop.tools.DistCp.&amp;lt;init&amp;gt;()
java.io.IOException: Cannot get DistCp constructor: org.apache.hadoop.tools.DistCp.&amp;lt;init&amp;gt;()
	at org.apache.hadoop.hive.shims.Hadoop23Shims.runDistCp(Hadoop23Shims.java:1160)
	at org.apache.hadoop.hive.common.FileUtils.copy(FileUtils.java:553)
	at org.apache.hadoop.hive.ql.exec.CopyTask.execute(CopyTask.java:82)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:88)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1653)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1412)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1195)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1054)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:154)
	at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:71)
	at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:206)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:218)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Error: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.CopyTask (state=08S01,code=1)

</description>
			<version>1.2.1</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
			<file type="M">org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">11607</link>
		</links>
	</bug>
	<bug id="10755" opendate="2015-05-19 18:32:32" fixdate="2015-10-14 20:50:52" resolution="Fixed">
		<buginformation>
			<summary>Rework on HIVE-5193 to enhance the column oriented table access</summary>
			<description>Add the support of column pruning for column oriented table access which was done in HIVE-5193 but was reverted due to the join issue in HIVE-10720.
In 1.3.0, the patch posted by Viray didn&amp;amp;apos;t work, probably due to some jar reference. That seems to get fixed and that patch works in 2.0.0 now.</description>
			<version>1.2.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Sub-task</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.hcatalog.pig.HCatLoader.java</file>
			<file type="M">org.apache.hive.hcatalog.pig.TestHCatLoader.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.ColumnProjectionUtils.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">12006</link>
		</links>
	</bug>
	<bug id="12214" opendate="2015-10-19 16:03:45" fixdate="2015-10-20 15:06:24" resolution="Duplicate">
		<buginformation>
			<summary>Investigate the test failure TestSSL.testSSLVersion</summary>
			<description></description>
			<version>1.3.0</version>
			<fixedVersion></fixedVersion>
			<type>Test</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.jdbc.TestSSL.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">12039</link>
		</links>
	</bug>
	<bug id="12218" opendate="2015-10-20 22:15:26" fixdate="2015-10-21 21:44:14" resolution="Fixed">
		<buginformation>
			<summary>Unable to create a like table for an hbase backed table</summary>
			<description>For an HBase backed table:


CREATE TABLE hbasetbl (key string, state string, country string, country_id int)
STORED BY &amp;amp;apos;org.apache.hadoop.hive.hbase.HBaseStorageHandler&amp;amp;apos;
WITH SERDEPROPERTIES (
"hbase.columns.mapping" = "info:state,info:country,info:country_id"
);


Create its like table using query such as 
create table hbasetbl_like like hbasetbl;
It fails with error:
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. org.apache.hadoop.hive.ql.metadata.HiveException: must specify an InputFormat class
</description>
			<version>1.2.1</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
		</fixedFiles>
	</bug>
	<bug id="12204" opendate="2015-10-16 21:09:47" fixdate="2015-10-22 23:02:11" resolution="Fixed">
		<buginformation>
			<summary>Tez queries stopped running with ApplicationNotRunningException</summary>
			<description>In some error cases, if hive can no longer submit DAGs to tez, there is no use retrying to submit. We need to exit by throwing exception in this case.</description>
			<version>1.0.1</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TestTezSessionPool.java</file>
		</fixedFiles>
	</bug>
	<bug id="11733" opendate="2015-09-04 07:07:36" fixdate="2015-10-25 01:03:34" resolution="Duplicate">
		<buginformation>
			<summary>UDF GenericUDFReflect cannot find classes added by "ADD JAR"</summary>
			<description>When run below command:

hive -e "add jar /root/hive/TestReflect.jar; \
select reflect(&amp;amp;apos;com.yshi.hive.TestReflect&amp;amp;apos;, &amp;amp;apos;testReflect&amp;amp;apos;, code) from sample_07 limit 3"
Get below error:

Failed with exception java.io.IOException:org.apache.hadoop.hive.ql.metadata.HiveException: UDFReflect evaluate


The full stack trace is:

15/09/04 07:00:37 [main]: INFO compress.CodecPool: Got brand-new decompressor [.bz2]
Failed with exception java.io.IOException:org.apache.hadoop.hive.ql.metadata.HiveException: UDFReflect evaluate
15/09/04 07:00:37 [main]: ERROR CliDriver: Failed with exception java.io.IOException:org.apache.hadoop.hive.ql.metadata.HiveException: UDFReflect evaluate
java.io.IOException: org.apache.hadoop.hive.ql.metadata.HiveException: UDFReflect evaluate
	at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:152)
	at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1657)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:227)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:159)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:370)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:756)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:615)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: UDFReflect evaluate
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDFReflect.evaluate(GenericUDFReflect.java:107)
	at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:185)
	at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:77)
	at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:65)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:77)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95)
	at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:424)
	at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:416)
	at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:138)
	... 13 more
Caused by: java.lang.ClassNotFoundException: com.yshi.hive.TestReflect
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:190)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDFReflect.evaluate(GenericUDFReflect.java:105)
	... 22 more

</description>
			<version>1.2.1</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.hcatalog.streaming.AbstractRecordWriter.java</file>
			<file type="M">org.apache.hadoop.hive.accumulo.Utils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java</file>
			<file type="M">org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsAggregator.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.HCatSplit.java</file>
			<file type="M">org.apache.hadoop.hive.accumulo.serde.AccumuloSerDeParameters.java</file>
			<file type="M">org.apache.hive.hcatalog.messaging.MessageFactory.java</file>
			<file type="M">org.apache.hadoop.hive.common.JavaUtils.java</file>
			<file type="M">org.apache.hadoop.hive.accumulo.predicate.PrimitiveComparisonFilter.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.PTFDeserializer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.AbstractSMBJoinProc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.tool.JobState.java</file>
			<file type="M">org.apache.hadoop.hive.hbase.HBaseSerDeHelper.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsPublisher.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.HivePreWarmProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.hbase.HBaseSerDeParameters.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.WriterImpl.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFReflect.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.FosterStorageHandler.java</file>
			<file type="M">org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">9486</link>
		</links>
	</bug>
	<bug id="12234" opendate="2015-10-22 21:09:55" fixdate="2015-10-26 05:17:21" resolution="Fixed">
		<buginformation>
			<summary>Beeline quit tries to connect again if no existing connections</summary>
			<description>Beeline !quit calls close(), which then does the following check:

beeLine.getDatabaseConnection().getConnection() != null


This inadvertently tries to connect again.</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.beeline.DatabaseConnection.java</file>
			<file type="M">org.apache.hive.beeline.Commands.java</file>
		</fixedFiles>
	</bug>
	<bug id="12261" opendate="2015-10-25 05:48:48" fixdate="2015-10-26 06:51:51" resolution="Fixed">
		<buginformation>
			<summary>schematool version info exit status should depend on compatibility, not equality</summary>
			<description>Newer versions of metastore schema are compatible with older versions of hive, as only new tables or columns are added with additional information.
HIVE-11613 added a check in hive schematool -info command to see if schema version is equal. 
However, the state where db schema version is ahead of hive software version is often seen when a &amp;amp;apos;rolling upgrade&amp;amp;apos; or &amp;amp;apos;rolling downgrade&amp;amp;apos; is happening. This is a state where hive is functional and returning non zero status for it is misleading.</description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreSchemaInfo.java</file>
			<file type="M">org.apache.hive.beeline.HiveSchemaTool.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">14080</link>
			<link type="Reference" description="is related to">11613</link>
		</links>
	</bug>
	<bug id="12250" opendate="2015-10-23 21:31:00" fixdate="2015-10-27 18:10:43" resolution="Fixed">
		<buginformation>
			<summary>Zookeeper connection leaks in Hive&amp;apos;s HBaseHandler.</summary>
			<description>HiveServer2 performance regresses severely due to what appears to be a leak in the ZooKeeper connections. lsof output on the HS2 process shows about 8000 TCP connections to the ZK ensemble nodes.
grep TCP lsof-hive-node11 | grep node11 | grep -E "node03|node04|node05" | wc -l
    7866 
grep TCP lsof-hive-node11 | grep node11 | grep -E "node03" | wc -l
    2615
grep TCP lsof-hive-node11 | grep node11 | grep -E "node04" | wc -l
    2622
grep TCP lsof-hive-node11 | grep node11 | grep -E "node05" | wc -l
    2629
node11 - HMS node
node03, node04 and node05 are the hosts for zookeeper ensemble.</description>
			<version>1.1.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.hbase.HiveHBaseTableOutputFormat.java</file>
		</fixedFiles>
		<links>
			<link type="dependent" description="is depended upon by">12418</link>
		</links>
	</bug>
	<bug id="12276" opendate="2015-10-27 17:52:40" fixdate="2015-10-28 23:02:17" resolution="Fixed">
		<buginformation>
			<summary>Fix messages in InvalidTable</summary>
			<description>follow up to HIVE-12003 to make exception message meaningful</description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.hcatalog.streaming.InvalidTable.java</file>
		</fixedFiles>
	</bug>
	<bug id="12275" opendate="2015-10-27 17:52:20" fixdate="2015-10-29 17:24:24" resolution="Duplicate">
		<buginformation>
			<summary>Query results on macro_duplicate.q are in different order on some environments</summary>
			<description>On my Linux VM, the order is different from the golden file. Seems to work on Mac as well as on the Pre-Commit tests. We can add an order-by to make the results deterministic across environments.

55d54
&amp;lt; 16    25      24      120     8       10      6
56a56
&amp;gt; 16    25      24      120     8       10      6
Exception: Client Execution results failed with error code = 1

</description>
			<version>1.2.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">12277</link>
		</links>
	</bug>
	<bug id="9495" opendate="2015-01-28 08:53:16" fixdate="2015-10-30 08:25:47" resolution="Duplicate">
		<buginformation>
			<summary>Map Side aggregation affecting map performance</summary>
			<description>When trying to run a simple aggregation query with hive.map.aggr=true, map tasks take a lot of time in Hive 0.14 as against  with hive.map.aggr=false.
e.g.
Consider the query:


INSERT OVERWRITE TABLE lineitem_tgt_agg
select alias.a0 as a0,
 alias.a2 as a1,
 alias.a1 as a2,
 alias.a3 as a3,
 alias.a4 as a4
from (
 select alias.a0 as a0,
  SUM(alias.a1) as a1,
  SUM(alias.a2) as a2,
  SUM(alias.a3) as a3,
  SUM(alias.a4) as a4
 from (
  select lineitem_sf500.l_orderkey as a0,
   CAST(lineitem_sf500.l_quantity * lineitem_sf500.l_extendedprice * (1 - lineitem_sf500.l_discount) * (1 + lineitem_sf500.l_tax) as double) as a1,
   lineitem_sf500.l_quantity as a2,
   CAST(lineitem_sf500.l_quantity * lineitem_sf500.l_extendedprice * lineitem_sf500.l_discount as double) as a3,
   CAST(lineitem_sf500.l_quantity * lineitem_sf500.l_extendedprice * lineitem_sf500.l_tax as double) as a4
  from lineitem_sf500
  ) alias
 group by alias.a0
 ) alias;


The above query was run with ~376GB of data / ~3billion records in the source.
It takes ~10 minutes with hive.map.aggr=false.
With map side aggregation set to true, the map tasks don&amp;amp;apos;t complete even after an hour.
</description>
			<version>0.14.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.KeyWrapperFactory.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">11502</link>
		</links>
	</bug>
	<bug id="12277" opendate="2015-10-27 18:39:00" fixdate="2015-10-30 17:18:19" resolution="Fixed">
		<buginformation>
			<summary>Hive macro results on macro_duplicate.q different after adding ORDER BY</summary>
			<description>Added an order-by to the query in macro_duplicate.q:

-select math_square(a), math_square(b),factorial(a), factorial(b), math_add(a), math_add(b),int(c) from macro_testing;
\ No newline at end of file
+select math_square(a), math_square(b),factorial(a), factorial(b), math_add(a), math_add(b),int(c) from macro_testing order by int(c);


And the results from math_add() changed unexpectedly:

-1      4       1       2       2       4       3
-16     25      24      120     8       10      6
+1      4       1       2       1       4       3
+16     25      24      120     16      25      6

</description>
			<version>1.2.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">12275</link>
			<link type="Reference" description="relates to">2655</link>
		</links>
	</bug>
	<bug id="12280" opendate="2015-10-28 00:47:50" fixdate="2015-10-30 17:47:00" resolution="Fixed">
		<buginformation>
			<summary>HiveConnection does not try other HS2 after failure for service discovery</summary>
			<description>Found this while mocking some bad connection data in znode.. will try to add a test for this.</description>
			<version>1.2.1</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.jdbc.ZooKeeperHiveClientHelper.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">11581</link>
		</links>
	</bug>
	<bug id="12248" opendate="2015-10-23 19:50:20" fixdate="2015-10-30 18:45:22" resolution="Duplicate">
		<buginformation>
			<summary>The rawStore used in DBTokenStore should be thread-safe</summary>
			<description>A non-thread-safe implementation of RawStore, particularly ObjectStore, set in DBTokenStore is being shared by multi-threads, which causes the race condition in DataNuclues to access the backend DB. 
The DN PersistenceManager(PM) in ObjectStore is not thread safe, so DBTokenStore should use a ThreadLocal ObjectStore.
Following errors might be root caused by the race condition in DN PM.


Object of type "org.apache.hadoop.hive.metastore.model.MDelegationToken" is detached. Detached objects cannot be used with this operation.
org.datanucleus.exceptions.ObjectDetachedException: Object of type "org.apache.hadoop.hive.metastore.model.MDelegationToken" is detached. Detached objects cannot be used with this operation.
at org.datanucleus.ExecutionContextImpl.assertNotDetached(ExecutionContextImpl.java:5728)
at org.datanucleus.ExecutionContextImpl.retrieveObject(ExecutionContextImpl.java:1859)
at org.datanucleus.ExecutionContextThreadedImpl.retrieveObject(ExecutionContextThreadedImpl.java:203)
at org.datanucleus.api.jdo.JDOPersistenceManager.jdoRetrieve(JDOPersistenceManager.java:605)
at org.datanucleus.api.jdo.JDOPersistenceManager.retrieveAll(JDOPersistenceManager.java:693)
at org.datanucleus.api.jdo.JDOPersistenceManager.retrieveAll(JDOPersistenceManager.java:713)
at org.apache.hadoop.hive.metastore.ObjectStore.getAllTokenIdentifiers(ObjectStore.java:6517) 

</description>
			<version>1.2.1</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.java</file>
			<file type="M">org.apache.hive.service.auth.HiveAuthFactory.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.DBTokenStore.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.TestDBTokenStore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">11616</link>
		</links>
	</bug>
	<bug id="11616" opendate="2015-08-21 09:00:07" fixdate="2015-10-30 21:53:30" resolution="Fixed">
		<buginformation>
			<summary>DelegationTokenSecretManager reuses the same objectstore, which has concurrency issues</summary>
			<description>sometime in metastore log, will get below exception,  after analysis, we found that :
when hivemetastore start, the DelegationTokenSecretManager will maintain the same objectstore, see here


saslServer.startDelegationTokenSecretManager(conf, *baseHandler.getMS()*, ServerMode.METASTORE);


this lead to the cocurrent issue.


2015-08-18 20:59:10,520 | ERROR | pool-6-thread-200 | Error occurred during processing of message. | org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:296)
org.apache.hadoop.hive.thrift.DelegationTokenStore$TokenStoreException: org.datanucleus.transaction.NucleusTransactionException: Invalid state. Transaction has already started
	at org.apache.hadoop.hive.thrift.DBTokenStore.invokeOnRawStore(DBTokenStore.java:154)
	at org.apache.hadoop.hive.thrift.DBTokenStore.getToken(DBTokenStore.java:88)
	at org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.retrievePassword(TokenStoreDelegationTokenSecretManager.java:112)
	at org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.retrievePassword(TokenStoreDelegationTokenSecretManager.java:56)
	at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$SaslDigestCallbackHandler.getPassword(HadoopThriftAuthBridge.java:565)
	at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$SaslDigestCallbackHandler.handle(HadoopThriftAuthBridge.java:596)
	at com.sun.security.sasl.digest.DigestMD5Server.validateClientResponse(DigestMD5Server.java:589)
	at com.sun.security.sasl.digest.DigestMD5Server.evaluateResponse(DigestMD5Server.java:244)
	at org.apache.thrift.transport.TSaslTransport$SaslParticipant.evaluateChallengeOrResponse(TSaslTransport.java:539)
	at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:283)
	at org.apache.thrift.transport.HiveTSaslServerTransport.open(HiveTSaslServerTransport.java:133)
	at org.apache.thrift.transport.HiveTSaslServerTransport$Factory.getTransport(HiveTSaslServerTransport.java:261)
	at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingTransportFactory$1.run(HadoopThriftAuthBridge.java:739)
	at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingTransportFactory$1.run(HadoopThriftAuthBridge.java:736)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:360)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1652)
	at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingTransportFactory.getTransport(HadoopThriftAuthBridge.java:736)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:268)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.datanucleus.transaction.NucleusTransactionException: Invalid state. Transaction has already started
	at org.datanucleus.transaction.TransactionManager.begin(TransactionManager.java:47)
	at org.datanucleus.TransactionImpl.begin(TransactionImpl.java:131)
	at org.datanucleus.api.jdo.JDOTransaction.internalBegin(JDOTransaction.java:88)
	at org.datanucleus.api.jdo.JDOTransaction.begin(JDOTransaction.java:80)
	at org.apache.hadoop.hive.metastore.ObjectStore.openTransaction(ObjectStore.java:420)
	at org.apache.hadoop.hive.metastore.ObjectStore.getToken(ObjectStore.java:6455)
	at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98)
	at com.sun.proxy.$Proxy4.getToken(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.hive.thrift.DBTokenStore.invokeOnRawStore(DBTokenStore.java:146)
	... 21 more

</description>
			<version>1.2.1</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.java</file>
			<file type="M">org.apache.hive.service.auth.HiveAuthFactory.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.DBTokenStore.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.TestDBTokenStore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">12248</link>
			<link type="Reference" description="relates to">12270</link>
		</links>
	</bug>
	<bug id="12249" opendate="2015-10-23 20:56:25" fixdate="2015-10-30 22:05:35" resolution="Fixed">
		<buginformation>
			<summary>Improve logging with tez</summary>
			<description>We need to improve logging across the board. TEZ-2851 added a caller context so that one can correlate logs with the application. This jira adds a new configuration for users that can be used to correlate the logs.</description>
			<version>1.2.1</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.cli.CliDriver.java</file>
			<file type="M">org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
			<file type="M">org.apache.hadoop.hive.ql.hooks.ATSHook.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="is blocked by">9184</link>
			<link type="Reference" description="relates to">12419</link>
		</links>
	</bug>
	<bug id="12232" opendate="2015-10-22 12:04:16" fixdate="2015-10-30 22:32:46" resolution="Duplicate">
		<buginformation>
			<summary>Create external table failed when enabled StorageBasedAuthorization</summary>
			<description>Please look at the stacktrace, when enabled StorageBasedAuthorization, creating external table will failed with write permission about the default warehouse path "/user/hive/warehouse": 
&amp;gt; CREATE EXTERNAL TABLE test(id int) LOCATION &amp;amp;apos;/tmp/wangmeng/test&amp;amp;apos;  ;
Error: Error while compiling statement: FAILED: HiveException java.security.AccessControlException: Permission denied: user=wangmeng, access=WRITE, inode="/user/hive/warehouse":hive:hive:drwxr-x--t.</description>
			<version>1.2.1</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.BucketingSortingReduceSinkOptimizer.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">12231</link>
		</links>
	</bug>
	<bug id="12215" opendate="2015-10-19 19:24:35" fixdate="2015-11-02 17:22:56" resolution="Fixed">
		<buginformation>
			<summary>Exchange partition does not show outputs field for post/pre execute hooks</summary>
			<description>The pre/post execute hook interface has fields that indicate which Hive objects were read / written to as a result of running the query. For the exchange partition operation, these fields (ReadEntity and WriteEntity) are empty. 
This is an important issue as the hook interface may be configured to perform critical warehouse operations.
See

ql/src/test/results/clientpositive/exchange_partition3.q.out



PREHOOK: query: -- This will exchange both partitions hr=1 and hr=2
ALTER TABLE exchange_part_test1 EXCHANGE PARTITION (ds=&amp;amp;apos;2013-04-05&amp;amp;apos;) WITH TABLE exchange_part_test2
PREHOOK: type: ALTERTABLE_EXCHANGEPARTITION
POSTHOOK: query: -- This will exchange both partitions hr=1 and hr=2
ALTER TABLE exchange_part_test1 EXCHANGE PARTITION (ds=&amp;amp;apos;2013-04-05&amp;amp;apos;) WITH TABLE exchange_part_test2
POSTHOOK: type: ALTERTABLE_EXCHANGEPARTITION


Seems it should also print output fields.</description>
			<version>2.0.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">11554</link>
		</links>
	</bug>
	<bug id="11293" opendate="2015-07-17 18:45:46" fixdate="2015-11-02 23:54:51" resolution="Fixed">
		<buginformation>
			<summary>HiveConnection.setAutoCommit(true) throws exception</summary>
			<description>Effectively autoCommit is always true for HiveConnection, however setAutoCommit(true) throws exception, causing problems in existing JDBC code.
Should be 


  @Override
  public void setAutoCommit(boolean autoCommit) throws SQLException {
    if (!autoCommit) {
      throw new SQLException("disabling autocommit is not supported");
    }
  }

</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.jdbc.HiveConnection.java</file>
			<file type="M">org.apache.hive.jdbc.TestJdbcDriver2.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">6712</link>
		</links>
	</bug>
	<bug id="12281" opendate="2015-10-28 00:48:23" fixdate="2015-11-03 03:51:20" resolution="Fixed">
		<buginformation>
			<summary>Vectorized MapJoin - use Operator::isLogDebugEnabled</summary>
			<description>
</description>
			<version>1.3.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerLongOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinCommonOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinOuterMultiKeyOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerMultiKeyOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiLongOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyStringOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiStringOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastLongHashTable.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashTable.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyMultiKeyOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinOuterStringOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyLongOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerStringOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiMultiKeyOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinOuterLongOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinOuterGenerateResultOperator.java</file>
		</fixedFiles>
	</bug>
	<bug id="7428" opendate="2014-07-16 17:44:21" fixdate="2015-11-03 04:10:57" resolution="Duplicate">
		<buginformation>
			<summary>OrcSplit fails to account for columnar projections in its size estimates</summary>
			<description>Currently, ORC generates splits based on stripe offset + stripe length.
This means that the splits for all columnar projections are exactly the same size, despite reading the footer which gives the estimated sizes for each column.
This is a hold-out from FileSplit which uses getLen() as the I/O cost of reading a file in a map-task.
RCFile didn&amp;amp;apos;t have a footer with column statistics information, but for ORC this would be extremely useful to reduce task overheads when processing extremely wide tables with highly selective column projections.</description>
			<version>1.2.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.SplitGrouper.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcSplit.java</file>
			<file type="D">org.apache.hadoop.hive.ql.io.ColumnarSplit.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="is blocked by">10497</link>
			<link type="Duplicate" description="is duplicated by">10397</link>
			<link type="Reference" description="relates to">11546</link>
			<link type="Reference" description="relates to">1993</link>
			<link type="Supercedes" description="supercedes">10397</link>
		</links>
	</bug>
	<bug id="12238" opendate="2015-10-23 01:10:57" fixdate="2015-11-03 04:11:43" resolution="Fixed">
		<buginformation>
			<summary>Vectorization: Thread-safety errors in VectorUDFDate</summary>
			<description>

Caused by: java.lang.NumberFormatException: For input string: ""
        at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
        at java.lang.Long.parseLong(Long.java:601)
        at java.lang.Long.parseLong(Long.java:631)
        at java.text.DigitList.getLong(DigitList.java:195)
        at java.text.DecimalFormat.parse(DecimalFormat.java:2051)
        at java.text.SimpleDateFormat.subParse(SimpleDateFormat.java:1869)
        at java.text.SimpleDateFormat.parse(SimpleDateFormat.java:1514)        at java.text.DateFormat.parse(DateFormat.java:364)
        at org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateString$1.evaluate(VectorUDFDateString.java:48)
        at org.apache.hadoop.hive.ql.exec.vector.expressions.StringUnaryUDF.evaluate(StringUnaryUDF.java:90)        at org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression.evaluateChildren(VectorExpression.java:121)
        at org.apache.hadoop.hive.ql.exec.vector.expressions.StringGroupColConcatStringScalar.evaluate(StringGroupColConcatStringScalar.java:50)
        at org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression.evaluateChildren(VectorExpression.java:121)
        at org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFAdaptor.evaluate(VectorUDFAdaptor.java:112)
        at org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression.evaluateChildren(VectorExpression.java:121)
        at org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldLong.evaluate(VectorUDFTimestampFieldLong.java:93)
        at org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.process(VectorSelectOperator.java:123)
        ... 22 more

</description>
			<version>1.2.1</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateString.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateExpressions.java</file>
		</fixedFiles>
	</bug>
	<bug id="12093" opendate="2015-10-12 09:45:32" fixdate="2015-11-03 09:11:59" resolution="Duplicate">
		<buginformation>
			<summary> launch local task to process map join cost long time </summary>
			<description> launch local task to process map join cost long time   
2015-10-08 19:34:35 INFO 2015-10-08 19:34:35  Starting to launch local task to process map join;  maximum memory = 1908932608
2015-10-08 20:07:43 INFO 2015-10-08 20:07:43  Dump the side-table for tag: 1 with group count: 148024 into file: file:/tmp/test/6b99a4b8-0db3-4c62-a0f3-20547504b2b4/hive_2015-10-08_19-30-11_948_5184081524408167915-1/-local-10015/HashTable-Stage-33/MapJoin-mapfile71--.hashtable
2015-10-08 20:07:43 INFO 2015-10-08 20:07:43  Uploaded 1 File to: file:/tmp/test/6b99a4b8-0db3-4c62-a0f3-20547504b2b4/hive_2015-10-08_19-30-11_948_5184081524408167915-1/-local-10015/HashTable-Stage-33/MapJoin-mapfile71--.hashtable (8922201 bytes)
2015-10-08 20:07:43 INFO 2015-10-08 20:07:43  End of local task; Time Taken: 1987.642 sec.</description>
			<version>1.2.1</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.KeyWrapperFactory.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">11502</link>
		</links>
	</bug>
	<bug id="12202" opendate="2015-10-16 10:58:11" fixdate="2015-11-03 18:59:06" resolution="Fixed">
		<buginformation>
			<summary>NPE thrown when reading legacy ACID delta files</summary>
			<description>When reading legacy ACID deltas of the form delta_$startTxnId_$endTxnId a NullPointerException is thrown on:
org.apache.hadoop.hive.ql.io.AcidUtils.deserializeDeltas#371

if(dmd.getStmtIds().isEmpty()) {


The older ACID data format (pre-Hive 1.3.0) which does not include the statement ID, and code written for that format should still be supported. Therefore the above condition should also include a null check or alternatively AcidInputFormat.DeltaMetaData should never return null, and return an empty list in this specific scenario.</description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.AcidInputFormat.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">12030</link>
			<link type="Regression" description="is broken by">11030</link>
		</links>
	</bug>
	<bug id="12266" opendate="2015-10-26 19:50:30" fixdate="2015-11-03 19:48:46" resolution="Fixed">
		<buginformation>
			<summary>When client exists abnormally, it doesn&amp;apos;t release ACID locks</summary>
			<description>if you start Hive CLI (locking enabled) and run some command that acquires locks and ^C the shell before command completes the locks for the command remain until they timeout.
I believe Beeline has the same issue.
Need to add proper hooks to release locks when command dies. (As much as possible)</description>
			<version>1.0.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">12583</link>
			<link type="Reference" description="is related to">12453</link>
		</links>
	</bug>
	<bug id="12223" opendate="2015-10-21 17:54:30" fixdate="2015-11-05 08:40:13" resolution="Fixed">
		<buginformation>
			<summary>Filter on Grouping__ID does not work properly</summary>
			<description>Consider the following query:

SELECT key, value, GROUPING__ID, count(*)
FROM T1
GROUP BY key, value
GROUPING SETS ((), (key))
HAVING GROUPING__ID = 1


This query will not return results.
The reason is that a "constant" placeholder is introduced by SemanticAnalyzer for the GROUPING__ID column. At execution time, this placeholder is replaced by the actual value of the GROUPING__ID. As the column is a constant, the Hive optimizer (combination of PPD and constant folding) will evaluate statically whether the condition is met or not, leading to incorrect results.
We should be able to recognize the placeholder and avoid PPD pushing the filter predicate to the GroupBy operator.</description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.ppd.ExprWalkerProcFactory.java</file>
		</fixedFiles>
	</bug>
	<bug id="12304" opendate="2015-10-30 17:17:28" fixdate="2015-11-05 16:53:26" resolution="Fixed">
		<buginformation>
			<summary>"drop database cascade" needs to unregister functions</summary>
			<description>Currently "drop database cascade" command doesn&amp;amp;apos;t unregister the functions under the database. If the functions are not unregistered, in some cases like "describe db1.func1" will still show the info for the function; or if the same database is recreated, "drop if exists db1.func1" will throw an exception since the function is considered existing from the registry while it doesn&amp;amp;apos;t exist in metastore.</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Registry.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">14631</link>
		</links>
	</bug>
	<bug id="12252" opendate="2015-10-23 23:03:38" fixdate="2015-11-05 18:18:06" resolution="Fixed">
		<buginformation>
			<summary>Streaming API HiveEndPoint can be created w/o partitionVals for partitioned table</summary>
			<description>When this happens, the write from Streaming API to this end point will succeed but it will place the data in the table directory which is not correct
Need to make the API throw in this case.
</description>
			<version>0.14.0</version>
			<fixedVersion>1.3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.hcatalog.streaming.HiveEndPoint.java</file>
			<file type="M">org.apache.hive.hcatalog.streaming.TestStreaming.java</file>
			<file type="M">org.apache.hive.hcatalog.streaming.ConnectionError.java</file>
		</fixedFiles>
	</bug>
	<bug id="12207" opendate="2015-10-18 06:34:52" fixdate="2015-11-05 22:17:13" resolution="Fixed">
		<buginformation>
			<summary>Query fails when non-ascii characters are used in string literals</summary>
			<description>While debugging HIVE-11721 I found that using non-ascii characters in string literals causes calcite planner to throw the following exception:


2015-10-17T23:07:20,586 ERROR [main]: parse.CalcitePlanner (CalcitePlanner.java:genOPTree(292)) - CBO failed, skipping CBO.
org.apache.calcite.runtime.CalciteException: Failed to encode &amp;amp;apos;&amp;amp;apos; in character set &amp;amp;apos;ISO-8859-1&amp;amp;apos;


The query is:


select concat("", "") from src limit 1;


Other queries with non-ascii literals fail as well.</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.translator.RexNodeConverter.java</file>
		</fixedFiles>
	</bug>
	<bug id="12315" opendate="2015-11-02 18:05:25" fixdate="2015-11-06 01:18:15" resolution="Fixed">
		<buginformation>
			<summary>vectorization_short_regress.q has a wrong result issue for a double calculation</summary>
			<description>I suspect it is related to the fancy optimizations in vectorized double divide that try to quickly process the batch without checking each row for null.


 public static void setNullAndDivBy0DataEntriesDouble(
      DoubleColumnVector v, boolean selectedInUse, int[] sel, int n, DoubleColumnVector denoms) {
    assert v.isRepeating || !denoms.isRepeating;
    v.noNulls = false;
    double[] vector = denoms.vector;
    if (v.isRepeating &amp;amp;&amp;amp; (v.isNull[0] = (v.isNull[0] || vector[0] == 0))) {
      v.vector[0] = DoubleColumnVector.NULL_VALUE;

</description>
			<version>0.14.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil.java</file>
		</fixedFiles>
	</bug>
	<bug id="12346" opendate="2015-11-05 13:49:20" fixdate="2015-11-06 13:59:39" resolution="Fixed">
		<buginformation>
			<summary>Internally used variables in HiveConf should not be settable via command</summary>
			<description>Some HiveConf variables such as hive.added.jars.path are only for internal use and should not be settable via set command. 
We saw a lot of cases that users mistakenly set these variables using set command despite some of them have been documented as "internal parameter" in Hive. The command usually succeeds but it sometimes does not effect, which causes some confusions. For example, the hive.added.jars.path can be set via set command but it is sometimes overridden by session resource jars during runtime.</description>
			<version>1.2.1</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
	</bug>
	<bug id="12344" opendate="2015-11-05 08:14:51" fixdate="2015-11-06 16:46:58" resolution="Fixed">
		<buginformation>
			<summary>Wrong types inferred for SemiJoin generation in CBO</summary>
			<description>The method projectNonColumnEquiConditions in HiveCalciteUtil will assign the type wrongly for newly created conditions. The problem is in this block:

      RexNode cond = rexBuilder.makeCall(SqlStdOperatorTable.EQUALS,
          rexBuilder.makeInputRef(newLeftFields.get(i).getType(), newLeftOffset + i),
          rexBuilder.makeInputRef(newLeftFields.get(i).getType(), newRightOffset + i));


It looks like a code copy-paste mistake. In addition, index i is incorrect, as newLeftFields contains all the fields, not only the ones of the new condition.</description>
			<version>0.14.0</version>
			<fixedVersion>0.14.1, 1.3.0, 2.0.0, 1.0.2, 1.2.2, 1.1.2</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.HiveCalciteUtil.java</file>
		</fixedFiles>
	</bug>
	<bug id="12263" opendate="2015-10-26 03:16:41" fixdate="2015-11-07 19:17:57" resolution="Fixed">
		<buginformation>
			<summary>Hive SchemaTool does not tolerate leading spaces in JDBC url</summary>
			<description>With configuration as below:
&amp;lt;property&amp;gt;
&amp;lt;name&amp;gt;
javax.jdo.option.ConnectionURL
&amp;lt;/name&amp;gt;
&amp;lt;value&amp;gt;
jdbc:mysql://host/hive?createDatabaseIfNotExist=true
&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
SchemaTool will failed:
export HIVE_CONF_DIR=$HIVE_CONF_DIR; schematool -dbType mysql -userName hive -passWord xxx -initSchema -verbose
exception as below:
Metastore connection URL:
jdbc:mysql://host/hive?createDatabaseIfNotExist=true
Metastore Connection Driver : com.mysql.jdbc.Driver
Metastore connection User: hive
org.apache.hadoop.hive.metastore.HiveMetaException: Failed to get schema version.
org.apache.hadoop.hive.metastore.HiveMetaException: Failed to get schema version.
at org.apache.hive.beeline.HiveSchemaHelper.getConnectionToMetastore(HiveSchemaHelper.java:77)
at org.apache.hive.beeline.HiveSchemaTool.getConnectionToMetastore(HiveSchemaTool.java:113)
at org.apache.hive.beeline.HiveSchemaTool.testConnectionToMetastore(HiveSchemaTool.java:159)
at org.apache.hive.beeline.HiveSchemaTool.doInit(HiveSchemaTool.java:257)
at org.apache.hive.beeline.HiveSchemaTool.doInit(HiveSchemaTool.java:243)
at org.apache.hive.beeline.HiveSchemaTool.main(HiveSchemaTool.java:473)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:497)
at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.sql.SQLException: No suitable driver found for
jdbc:mysql://host/hive?createDatabaseIfNotExist=true
at java.sql.DriverManager.getConnection(DriverManager.java:689)
at java.sql.DriverManager.getConnection(DriverManager.java:247)
at org.apache.hive.beeline.HiveSchemaHelper.getConnectionToMetastore(HiveSchemaHelper.java:73)
... 11 more</description>
			<version>1.2.1</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.beeline.HiveSchemaHelper.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">11287</link>
		</links>
	</bug>
	<bug id="12311" opendate="2015-10-31 15:18:34" fixdate="2015-11-09 01:21:48" resolution="Fixed">
		<buginformation>
			<summary>explain CTAS fails if the table already exists</summary>
			<description>Explain of a CTAS will fail if the table already exists.
This is an annoyance when you&amp;amp;apos;re seeing if a large body of SQL queries will function by putting explain in front of every query. 


hive&amp;gt; create table temp (x int);
OK
Time taken: 0.252 seconds
hive&amp;gt; create table temp2 (x int);
OK
Time taken: 0.407 seconds
hive&amp;gt; explain create table temp as select * from temp2;
FAILED: SemanticException org.apache.hadoop.hive.ql.parse.SemanticException: Table already exists: mydb.temp


If we compare to Postgres "The Zinc Standard of SQL Compliance":


carter=# create table temp (x int);
CREATE TABLE
carter=# create table temp2 (x int);
CREATE TABLE
carter=# explain create table temp as select * from temp2;
                       QUERY PLAN
---------------------------------------------------------
 Seq Scan on temp2  (cost=0.00..34.00 rows=2400 width=4)
(1 row)


If the CTAS is something complex it would be nice to see the query plan in advance.</description>
			<version>1.2.1</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
	</bug>
	<bug id="12312" opendate="2015-11-01 03:48:37" fixdate="2015-11-09 18:27:46" resolution="Fixed">
		<buginformation>
			<summary>Excessive logging in PPD code</summary>
			<description>One of my very complex queries takes about 14 minutes to compile with PPD on. Profiling it I saw a lot of time spent in this stack which is called many many thousands of times.


java.lang.Throwable.getStackTraceElement(-2)
java.lang.Throwable.getOurStackTrace(827)
java.lang.Throwable.getStackTrace(816)
sun.reflect.GeneratedMethodAccessor5.invoke(-1)
sun.reflect.DelegatingMethodAccessorImpl.invoke(43)
java.lang.reflect.Method.invoke(497)
org.apache.log4j.spi.LocationInfo.&amp;lt;init&amp;gt;(139)
org.apache.log4j.spi.LoggingEvent.getLocationInformation(253)
org.apache.log4j.helpers.PatternParser$LocationPatternConverter.convert(500)
org.apache.log4j.helpers.PatternConverter.format(65)
org.apache.log4j.PatternLayout.format(506)
org.apache.log4j.WriterAppender.subAppend(310)
org.apache.log4j.DailyRollingFileAppender.subAppend(369)
org.apache.log4j.WriterAppender.append(162)
org.apache.log4j.AppenderSkeleton.doAppend(251)
org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(66)
org.apache.log4j.Category.callAppenders(206)
org.apache.log4j.Category.forcedLog(391)
org.apache.log4j.Category.log(856)
org.apache.commons.logging.impl.Log4JLogger.info(176)
org.apache.hadoop.hive.ql.ppd.OpProcFactory$DefaultPPD.logExpr(707)
org.apache.hadoop.hive.ql.ppd.OpProcFactory$DefaultPPD.mergeWithChildrenPred(752)
org.apache.hadoop.hive.ql.ppd.OpProcFactory$FilterPPD.process(437)


logExpr is set to log at INFO level, but I think DEBUG is more appropriate. When I set log level to debug I see &amp;gt; 20% speedup in compile time:
Before:


real    14m47.972s
user    15m25.609s
sys    0m20.282s


After:


real    11m30.946s
user    12m10.870s
sys    0m7.320s


It looks like there&amp;amp;apos;s a lot of stuff in the PPD code that could be optimized, when I turn PPD off the query compiles in 2m 30s. But this seems like an easy and low risk win.</description>
			<version>1.2.1</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
		</fixedFiles>
	</bug>
	<bug id="12364" opendate="2015-11-07 02:04:25" fixdate="2015-11-10 02:00:24" resolution="Fixed">
		<buginformation>
			<summary>Distcp job fails when run under Tez</summary>
			<description>PROBLEM:
insert into/overwrite directory &amp;amp;apos;/path&amp;amp;apos; invokes distcp for moveTask and fails
query when execution engine is Tez 
set hive.exec.copyfile.maxsize=40000;
insert overwrite into &amp;amp;apos;/tmp/testinser&amp;amp;apos; select * from customer;
failed at moveTask
hive client log:


2015-11-05 16:02:53,254 INFO  [main]: exec.FileSinkOperator (Utilities.java:mvFileToFinalPath(1882)) - Moving tmp dir: hdfs://hdpsecehdfs/tmp/testindir/.hive-staging_hive_2015-11-05_15-59-44_557_1429894387987411483-1/_tmp.-ext-10000 to: hdfs://hdpsecehdfs/tmp/testindir/.hive-staging_hive_2015-11-05_15-59-44_557_1429894387987411483-1/-ext-10000
2015-11-05 16:02:53,611 INFO  [main]: log.PerfLogger (PerfLogger.java:PerfLogBegin(121)) - &amp;lt;PERFLOG method=task.DEPENDENCY_COLLECTION.Stage-2 from=org.apache.hadoop.hive.ql.Driver&amp;gt;
2015-11-05 16:02:53,612 INFO  [main]: ql.Driver (Driver.java:launchTask(1653)) - Starting task [Stage-2:DEPENDENCY_COLLECTION] in serial mode
2015-11-05 16:02:53,612 INFO  [main]: log.PerfLogger (PerfLogger.java:PerfLogBegin(121)) - &amp;lt;PERFLOG method=task.MOVE.Stage-0 from=org.apache.hadoop.hive.ql.Driver&amp;gt;
2015-11-05 16:02:53,612 INFO  [main]: ql.Driver (Driver.java:launchTask(1653)) - Starting task [Stage-0:MOVE] in serial mode
2015-11-05 16:02:53,612 INFO  [main]: exec.Task (SessionState.java:printInfo(951)) - Moving data to: /tmp/testindir from hdfs://hdpsecehdfs/tmp/testindir/.hive-staging_hive_2015-11-05_15-59-44_557_1429894387987411483-1/-ext-10000
2015-11-05 16:02:53,637 INFO  [main]: common.FileUtils (FileUtils.java:copy(551)) - Source is 491763261 bytes. (MAX: 40000)
2015-11-05 16:02:53,638 INFO  [main]: common.FileUtils (FileUtils.java:copy(552)) - Launch distributed copy (distcp) job.
2015-11-05 16:03:03,924 INFO  [main]: impl.TimelineClientImpl (TimelineClientImpl.java:serviceInit(296)) - Timeline service address: http://hdpsece02.sece.hwxsup.com:8188/ws/v1/timeline/
2015-11-05 16:03:04,081 INFO  [main]: impl.TimelineClientImpl (TimelineClientImpl.java:serviceInit(296)) - Timeline service address: http://hdpsece02.sece.hwxsup.com:8188/ws/v1/timeline/
2015-11-05 16:03:20,210 INFO  [main]: hdfs.DFSClient (DFSClient.java:getDelegationToken(1047)) - Created HDFS_DELEGATION_TOKEN token 1069 for haha on ha-hdfs:hdpsecehdfs
2015-11-05 16:03:20,249 INFO  [main]: security.TokenCache (TokenCache.java:obtainTokensForNamenodesInternal(125)) - Got dt for hdfs://hdpsecehdfs; Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:hdpsecehdfs, Ident: (HDFS_DELEGATION_TOKEN token 1069 for haha)
2015-11-05 16:03:20,250 WARN  [main]: token.Token (Token.java:getClassForIdentifier(121)) - Cannot find class for token kind kms-dt
2015-11-05 16:03:20,250 INFO  [main]: security.TokenCache (TokenCache.java:obtainTokensForNamenodesInternal(125)) - Got dt for hdfs://hdpsecehdfs; Kind: kms-dt, Service: 172.25.17.102:9292, Ident: 00 04 68 61 68 61 02 72 6d 00 8a 01 50 da 1a ca 29 8a 01 50 fe 27 4e 29 03 02
2015-11-05 16:03:22,561 INFO  [main]: Configuration.deprecation (Configuration.java:warnOnceIfDeprecated(1173)) - io.sort.mb is deprecated. Instead, use mapreduce.task.io.sort.mb
2015-11-05 16:03:22,562 INFO  [main]: Configuration.deprecation (Configuration.java:warnOnceIfDeprecated(1173)) - io.sort.factor is deprecated. Instead, use mapreduce.task.io.sort.factor
2015-11-05 16:03:33,733 ERROR [main]: exec.Task (SessionState.java:printError(960)) - Failed with exception Unable to move source hdfs://hdpsecehdfs/tmp/testindir/.hive-staging_hive_2015-11-05_15-59-44_557_1429894387987411483-1/-ext-10000 to destination /tmp/testindir
org.apache.hadoop.hive.ql.metadata.HiveException: Unable to move source hdfs://hdpsecehdfs/tmp/testindir/.hive-staging_hive_2015-11-05_15-59-44_557_1429894387987411483-1/-ext-10000 to destination /tmp/testindir
        at org.apache.hadoop.hive.ql.metadata.Hive.moveFile(Hive.java:2665)
        at org.apache.hadoop.hive.ql.exec.MoveTask.moveFile(MoveTask.java:105)
        at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:222)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89)
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1655)
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1414)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1195)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:736)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.io.IOException: Cannot execute DistCp process: java.io.IOException: mapreduce.job.inputformat.class is incompatible with map compatability mode.
        at org.apache.hadoop.hive.shims.Hadoop23Shims.runDistCp(Hadoop23Shims.java:1156)
        at org.apache.hadoop.hive.common.FileUtils.copy(FileUtils.java:553)
        at org.apache.hadoop.hive.ql.metadata.Hive.moveFile(Hive.java:2647)
        ... 21 more
Caused by: java.io.IOException: mapreduce.job.inputformat.class is incompatible with map compatability mode.
        at org.apache.hadoop.mapreduce.Job.ensureNotSet(Job.java:1194)
        at org.apache.hadoop.mapreduce.Job.setUseNewAPI(Job.java:1229)
        at org.apache.hadoop.mapreduce.Job.submit(Job.java:1283)
        at org.apache.hadoop.tools.DistCp.createAndSubmitJob(DistCp.java:183)
        at org.apache.hadoop.tools.DistCp.execute(DistCp.java:153)
        at org.apache.hadoop.hive.shims.Hadoop23Shims.runDistCp(Hadoop23Shims.java:1153)
        ... 23 more

2015-11-05 16:03:33,734 INFO  [main]: hooks.ATSHook (ATSHook.java:&amp;lt;init&amp;gt;(84)) - Created ATS Hook


</description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
		</fixedFiles>
		<links>
			<link type="Regression" description="is broken by">2931</link>
		</links>
	</bug>
	<bug id="12363" opendate="2015-11-07 01:29:03" fixdate="2015-11-11 14:47:17" resolution="Fixed">
		<buginformation>
			<summary>Incorrect results with orc ppd across ORC versions</summary>
			<description>Run vector_decimal_cast.q on tez cli driver.
The issue is related to the ORC Timestamp column stats, which does not exist in all ORC files. 
When the timestamp column is missing stats, default to YES_NO_NULL instead of assuming the column is all nulls.</description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
		</fixedFiles>
		<links>
			<link type="Required" description="is required by">12342</link>
		</links>
	</bug>
	<bug id="12208" opendate="2015-10-18 07:10:26" fixdate="2015-11-11 21:48:53" resolution="Fixed">
		<buginformation>
			<summary>Vectorized JOIN NPE on dynamically partitioned hash-join + map-join</summary>
			<description>TPC-DS Q82 with reducer vectorized join optimizations


  Reducer 5 &amp;lt;- Map 1 (CUSTOM_SIMPLE_EDGE), Map 2 (CUSTOM_SIMPLE_EDGE), Map 3 (BROADCAST_EDGE), Map 4 (CUSTOM_SIMPLE_EDGE)




set hive.optimize.dynamic.partition.hashjoin=true;
set hive.vectorized.execution.reduce.enabled=true;
set hive.mapjoin.hybridgrace.hashtable=false;

select  i_item_id
       ,i_item_desc
       ,i_current_price
 from item, inventory, date_dim, store_sales
 where i_current_price between 30 and 30+30
 and inv_item_sk = i_item_sk
 and d_date_sk=inv_date_sk
 and d_date between &amp;amp;apos;2002-05-30&amp;amp;apos; and &amp;amp;apos;2002-07-30&amp;amp;apos;
 and i_manufact_id in (437,129,727,663)
 and inv_quantity_on_hand between 100 and 500
 and ss_item_sk = i_item_sk
 group by i_item_id,i_item_desc,i_current_price
 order by i_item_id
 limit 100


possibly a trivial plan setup issue, since the NPE is pretty much immediate.


Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerLongOperator.process(VectorMapJoinInnerLongOperator.java:368)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:852)
	at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator.forwardBigTableBatch(VectorMapJoinGenerateResultOperator.java:603)
	at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerLongOperator.process(VectorMapJoinInnerLongOperator.java:362)
	... 19 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerGenerateResultOperator.commonSetup(VectorMapJoinInnerGenerateResultOperator.java:112)
	at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerLongOperator.process(VectorMapJoinInnerLongOperator.java:96)
	... 22 more

</description>
			<version>2.0.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinCommonOperator.java</file>
		</fixedFiles>
	</bug>
	<bug id="12365" opendate="2015-11-07 15:30:42" fixdate="2015-11-11 23:54:51" resolution="Fixed">
		<buginformation>
			<summary>Added resource path is sent to cluster as an empty string when externally removed</summary>
			<description>Sometimes the resources (e.g. jar) added via command like "add jars &amp;lt;filepath&amp;gt;" are removed externally from their filepath for some reasons. Their paths are sent to cluster as empty strings which causes the failures to the query that even do not need these jars in execution. The error look like as following:


15/11/06 21:56:44 INFO mapreduce.JobSubmitter: Cleaning up the staging area file:/tmp/hadoop-ctang/mapred/staging/ctang734817191/.staging/job_local734817191_0003
java.lang.IllegalArgumentException: Can not create a Path from an empty string
	at org.apache.hadoop.fs.Path.checkPathArg(Path.java:127)
	at org.apache.hadoop.fs.Path.&amp;lt;init&amp;gt;(Path.java:135)
	at org.apache.hadoop.mapreduce.JobSubmitter.copyAndConfigureFiles(JobSubmitter.java:215)
	at org.apache.hadoop.mapreduce.JobSubmitter.copyAndConfigureFiles(JobSubmitter.java:390)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:483)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1296)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1293)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1293)

</description>
			<version>1.2.1</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
		</fixedFiles>
	</bug>
	<bug id="7129" opendate="2014-05-27 22:49:21" fixdate="2015-11-12 21:33:26" resolution="Duplicate">
		<buginformation>
			<summary>Change datanucleus.fixedDatastore config to true</summary>
			<description>Much safer in production environment to have this as true.</description>
			<version>0.6.0</version>
			<fixedVersion></fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hive.beeline.cli.TestHiveCli.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">1841</link>
		</links>
	</bug>
	<bug id="12391" opendate="2015-11-12 14:11:16" fixdate="2015-11-13 09:23:01" resolution="Fixed">
		<buginformation>
			<summary>SkewJoinOptimizer might not kick in if columns are renamed after TableScanOperator</summary>
			<description>SkewJoinOptimizer will not kick in if the columns are just renamed after the TS e.g. by the creation of a derived table.
To reproduce, consider the following example:


set hive.optimize.skewjoin.compiletime = true;

CREATE TABLE T1(key STRING, val STRING)
SKEWED BY (key) ON ((2)) STORED AS TEXTFILE;

CREATE TABLE T2(key STRING, val STRING)
SKEWED BY (key) ON ((3)) STORED AS TEXTFILE;


For this query, SkewJoinOptimizer kicks in:


SELECT a.*, b.*
FROM T1 a JOIN T2 b
ON a.key = b.key


For this one, it does not:


SELECT a.*, b.*
FROM 
  (SELECT key as k, val as v FROM T1) a
  JOIN
  (SELECT key as k, val as v FROM T2) b
ON a.k = b.k;


The reason is that SkewJoinOptimizer does not backtrack the origin of the column. Instead it just uses its name to know if it is produced by a certain TS.</description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.SkewJoinOptimizer.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">12017</link>
		</links>
	</bug>
	<bug id="12396" opendate="2015-11-12 19:28:14" fixdate="2015-11-13 18:15:02" resolution="Fixed">
		<buginformation>
			<summary>BucketingSortingReduceSinkOptimizer may still throw IOB exception for duplicate columns</summary>
			<description>HIVE-12332 didn&amp;amp;apos;t fix the issue completely.</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.BucketingSortingReduceSinkOptimizer.java</file>
		</fixedFiles>
	</bug>
	<bug id="12407" opendate="2015-11-13 15:10:16" fixdate="2015-11-14 21:13:58" resolution="Fixed">
		<buginformation>
			<summary>Check fetch property to determine if a SortLimit contains a limit operation</summary>
			<description>Now that Calcite 1.5 went in, sometimes we end up with Sort and Limit operations in the same operator. limitRelNode in HiveCalciteUtil should check the fetch property of the SortLimit operator to determine if an operator is a Limit.</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.HiveCalciteUtil.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.translator.PlanModifierForASTConv.java</file>
		</fixedFiles>
	</bug>
	<bug id="1841" opendate="2010-12-08 17:26:11" fixdate="2015-11-15 04:45:40" resolution="Fixed">
		<buginformation>
			<summary> datanucleus.fixedDatastore should be true in hive-default.xml</summary>
			<description>Two datanucleus variables:

&amp;lt;property&amp;gt;
 &amp;lt;name&amp;gt;datanucleus.autoCreateSchema&amp;lt;/name&amp;gt;
 &amp;lt;value&amp;gt;false&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;

&amp;lt;property&amp;gt;
 &amp;lt;name&amp;gt;datanucleus.fixedDatastore&amp;lt;/name&amp;gt;
 &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;


are dangerous.  We do want the schema to auto-create itself, but we do not want the schema to auto update itself. 
Someone might accidentally point a trunk at the wrong meta-store and unknowingly update. I believe we should set this to false and possibly trap exceptions stemming from hive wanting to do any update. This way someone has to actively acknowledge the update, by setting this to true and then starting up hive, or leaving it false, removing schema modifies for the user that hive usages, and doing all the time and doing the updates by hand. </description>
			<version>0.6.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hive.beeline.cli.TestHiveCli.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">7129</link>
			<link type="Reference" description="relates to">14152</link>
			<link type="Reference" description="relates to">14322</link>
			<link type="Reference" description="relates to">3764</link>
			<link type="Reference" description="is related to">6113</link>
			<link type="dependent" description="depends upon">1530</link>
		</links>
	</bug>
	<bug id="12378" opendate="2015-11-10 20:15:34" fixdate="2015-11-16 17:03:23" resolution="Fixed">
		<buginformation>
			<summary>Exception on HBaseSerDe.serialize binary field</summary>
			<description>An issue was reproduced with the binary typed HBase columns in Hive:
It works fine as below:
CREATE TABLE test9 (key int, val string)
STORED BY &amp;amp;apos;org.apache.hadoop.hive.hbase.HBaseStorageHandler&amp;amp;apos;
WITH SERDEPROPERTIES (
"hbase.columns.mapping" = ":key,cf:val#b"
);
insert into test9 values(1,"hello");
But when string type is changed to binary as:
CREATE TABLE test2 (key int, val binary)
STORED BY &amp;amp;apos;org.apache.hadoop.hive.hbase.HBaseStorageHandler&amp;amp;apos;
WITH SERDEPROPERTIES (
"hbase.columns.mapping" = ":key,cf:val#b"
);
insert into table test2 values(1, &amp;amp;apos;hello&amp;amp;apos;);
The following exception is thrown:
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
{"tmp_values_col1":"1","tmp_values_col2":"hello"}
...
Caused by: java.lang.RuntimeException: Hive internal error.
at org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitive(LazyUtils.java:322)
at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:220)
at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serializeField(HBaseRowSerializer.java:194)
at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:118)
at org.apache.hadoop.hive.hbase.HBaseSerDe.serialize(HBaseSerDe.java:282)
... 16 more
We should support hive binary type column for hbase.</description>
			<version>1.0.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyFactory.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyUtils.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyBinary.java</file>
		</fixedFiles>
	</bug>
	<bug id="11036" opendate="2015-06-17 20:34:47" fixdate="2015-11-16 19:56:02" resolution="Duplicate">
		<buginformation>
			<summary>Race condition in DataNucleus makes Metastore to hang</summary>
			<description>Under moderate to high concurrent query workload Metastore gets deadlocked in DataNucleus</description>
			<version>0.14.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.TestMetastoreVersion.java</file>
			<file type="M">org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			<file type="M">org.apache.hive.beeline.cli.TestHiveCli.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">6113</link>
		</links>
	</bug>
	<bug id="12384" opendate="2015-11-11 21:12:51" fixdate="2015-11-17 23:08:53" resolution="Fixed">
		<buginformation>
			<summary>Union Operator may produce incorrect result on TEZ</summary>
			<description>Union queries may produce incorrect result on TEZ.
TEZ removes union op, thus might loose the implicit cast in union op.
Reproduction test case:
set hive.cbo.enable=false;
set hive.execution.engine=tez;
select (x/sum over())  as y from(select cast(1 as decimal(10,0))  as x from (select * from src limit 2)s1 union all select cast(1 as decimal(10,0)) x from (select * from src limit 2) s2 union all select &amp;amp;apos;100000000&amp;amp;apos; x from (select * from src limit 2) s3)u order by y;
select (x/sum over()) as y from(select cast(1 as decimal(10,0))  as x from (select * from src limit 2)s1 union all select cast(1 as decimal(10,0)) x from (select * from src limit 2) s2 union all select cast (null as string) x from (select * from src limit 2) s3)u order by y;</description>
			<version>0.14.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">12423</link>
		</links>
	</bug>
	<bug id="12418" opendate="2015-11-16 19:49:35" fixdate="2015-11-19 14:11:11" resolution="Fixed">
		<buginformation>
			<summary>HiveHBaseTableInputFormat.getRecordReader() causes Zookeeper connection leak.</summary>
			<description>  @Override
  public RecordReader&amp;lt;ImmutableBytesWritable, ResultWritable&amp;gt; getRecordReader(
...
...
 setHTable(HiveHBaseInputFormatUtil.getTable(jobConf));
...
The HiveHBaseInputFormatUtil.getTable() creates new ZooKeeper connections(when HTable instance is created) which are never closed.</description>
			<version>1.1.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.java</file>
		</fixedFiles>
		<links>
			<link type="dependent" description="depends upon">12250</link>
		</links>
	</bug>
	<bug id="12405" opendate="2015-11-13 07:11:46" fixdate="2015-11-19 22:16:22" resolution="Fixed">
		<buginformation>
			<summary>Comparison bug in HiveSplitGenerator.InputSplitComparator#compare()</summary>
			<description>"compare()" method in HiveSplitGenerator.InputSplitComparator has the following condition on line 281 which is always false and is most likely a typo:


if (startPos1 &amp;gt; startPos1) {


As a result, in certain conditions splits might be sorted in incorrect order.</description>
			<version>2.0.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.java</file>
		</fixedFiles>
	</bug>
	<bug id="12450" opendate="2015-11-18 01:08:12" fixdate="2015-11-20 07:25:49" resolution="Fixed">
		<buginformation>
			<summary>OrcFileMergeOperator does not use correct compression buffer size</summary>
			<description>OrcFileMergeOperator checks for compatibility before merging orc files. This compatibility check include checking compression buffer size. But the output file that is created does not honor the compression buffer size and always defaults to 256KB. This will not be a problem when reading the orc file but can create unwanted memory pressure because of wasted space within compression buffer.
This issue also can make the merged file unreadable under certain cases. For example, if the original compression buffer size is 8KB and if  hive.exec.orc.default.buffer.size is set to 4KB. The merge file operator will use 4KB instead of actual 8KB which can result in hanging of ORC reader (more specifically ZlibCodec will wait for more compression buffers). 
jstack output for hanging issue

"main" prio=5 tid=0x00007fc073000000 nid=0x1703 runnable [0x0000700000218000]
   java.lang.Thread.State: RUNNABLE
	at java.util.zip.Inflater.inflateBytes(Native Method)
	at java.util.zip.Inflater.inflate(Inflater.java:259)
	- locked &amp;lt;0x00000007f5d5fdc8&amp;gt; (a java.util.zip.ZStreamRef)
	at org.apache.hadoop.hive.ql.io.orc.ZlibCodec.decompress(ZlibCodec.java:94)
	at org.apache.hadoop.hive.ql.io.orc.InStream$CompressedStream.readHeader(InStream.java:238)
	at org.apache.hadoop.hive.ql.io.orc.InStream$CompressedStream.read(InStream.java:262)
	at java.io.InputStream.read(InputStream.java:101)
	at com.google.protobuf.CodedInputStream.refillBuffer(CodedInputStream.java:737)
	at com.google.protobuf.CodedInputStream.isAtEnd(CodedInputStream.java:701)
	at com.google.protobuf.CodedInputStream.readTag(CodedInputStream.java:99)
	at org.apache.hadoop.hive.ql.io.orc.OrcProto$StripeFooter.&amp;lt;init&amp;gt;(OrcProto.java:10661)
	at org.apache.hadoop.hive.ql.io.orc.OrcProto$StripeFooter.&amp;lt;init&amp;gt;(OrcProto.java:10625)
	at org.apache.hadoop.hive.ql.io.orc.OrcProto$StripeFooter$1.parsePartialFrom(OrcProto.java:10730)
	at org.apache.hadoop.hive.ql.io.orc.OrcProto$StripeFooter$1.parsePartialFrom(OrcProto.java:10725)
	at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:89)
	at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:95)
	at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:49)
	at org.apache.hadoop.hive.ql.io.orc.OrcProto$StripeFooter.parseFrom(OrcProto.java:10958)
	at org.apache.hadoop.hive.ql.io.orc.MetadataReaderImpl.readStripeFooter(MetadataReaderImpl.java:114)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.readStripeFooter(RecordReaderImpl.java:240)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.beginReadStripe(RecordReaderImpl.java:847)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.readStripe(RecordReaderImpl.java:818)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.advanceStripe(RecordReaderImpl.java:1033)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.advanceToNextRow(RecordReaderImpl.java:1068)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.&amp;lt;init&amp;gt;(RecordReaderImpl.java:217)
	at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.rowsOptions(ReaderImpl.java:638)
	at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.rows(ReaderImpl.java:625)
	at org.apache.hadoop.hive.ql.io.orc.FileDump.printMetaData(FileDump.java:162)
	at org.apache.hadoop.hive.ql.io.orc.FileDump.main(FileDump.java:110)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)

</description>
			<version>1.2.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcFileKeyWrapper.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.OrcFileMergeOperator.java</file>
		</fixedFiles>
	</bug>
	<bug id="12017" opendate="2015-10-02 15:41:01" fixdate="2015-11-20 22:34:08" resolution="Fixed">
		<buginformation>
			<summary>Do not disable CBO by default when number of joins in a query is equal or less than 1</summary>
			<description>Instead, we could disable some parts of CBO that are not relevant if the query contains 1 or 0 joins. Implementation should be able to define easily other query patterns for which we might disable some parts of CBO (in case we want to do it in the future).</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveAggregateProjectMergeRule.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelOptUtil.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.translator.SqlFunctionConverter.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="is blocked by">12391</link>
			<link type="Duplicate" description="duplicates">12040</link>
			<link type="Incorporates" description="incorporates">12509</link>
			<link type="Reference" description="relates to">12465</link>
			<link type="Reference" description="relates to">12477</link>
		</links>
	</bug>
	<bug id="12437" opendate="2015-11-17 19:26:54" fixdate="2015-11-20 22:56:19" resolution="Fixed">
		<buginformation>
			<summary>SMB join in tez fails when one of the tables is empty</summary>
			<description>It looks like a better check for empty tables is to depend on the existence of the record reader for the input from tez. </description>
			<version>1.0.1</version>
			<fixedVersion>1.3.0, 2.0.0, 1.2.2</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex.java</file>
		</fixedFiles>
	</bug>
	<bug id="12409" opendate="2015-11-13 19:33:56" fixdate="2015-11-23 17:31:18" resolution="Fixed">
		<buginformation>
			<summary>make sure SessionState.initTxnMgr() is thread safe</summary>
			<description>make this method synchronized since HS2 may run multiple threads in a Session.  see HIVE-11402</description>
			<version>1.0.0</version>
			<fixedVersion>1.3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.session.SessionState.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">11402</link>
		</links>
	</bug>
	<bug id="12389" opendate="2015-11-12 02:45:50" fixdate="2015-11-23 17:31:50" resolution="Fixed">
		<buginformation>
			<summary>CompactionTxnHandler.cleanEmptyAbortedTxns() should safeguard against huge IN clauses</summary>
			<description>in extreme situations, due to misconfigurations, it may be possible to have 100Ks or even 1Ms of aborted txns.
This causes delete from TXNS where txn_id in (...) to have a huge IN clause and DB chokes.  
Should use something like TxnHandler.TIMED_OUT_TXN_ABORT_BATCH_SIZE to break up delete into multiple queries.  (Incidentally the batch size should likely be 1000, not 100, maybe even configurable).
On MySQL for example, it can cause query to fail with
Packet for query is too large (9288598 &amp;gt; 1048576). You can change this value on the server by setting the max_allowed_packet&amp;amp;apos; variable.</description>
			<version>1.0.0</version>
			<fixedVersion>1.3.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.txn.compactor.TestInitiator.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="is blocked by">11948</link>
		</links>
	</bug>
	<bug id="12456" opendate="2015-11-18 20:18:16" fixdate="2015-11-23 18:05:12" resolution="Fixed">
		<buginformation>
			<summary>QueryId can&amp;apos;t be stored in the configuration of the SessionState since multiple queries can run in a single session</summary>
			<description>Follow up on HIVE-11488 which stores the queryId in the sessionState conf. If multiple queries run at  the same time, then the logging will get wrong queryId from the sessionState.</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.service.cli.operation.ExecuteStatementOperation.java</file>
			<file type="M">org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
			<file type="M">org.apache.hive.service.cli.operation.Operation.java</file>
			<file type="M">org.apache.hive.service.cli.operation.SQLOperation.java</file>
		</fixedFiles>
		<links>
			<link type="Required" description="is required by">11488</link>
		</links>
	</bug>
	<bug id="12489" opendate="2015-11-20 21:52:52" fixdate="2015-11-23 19:47:40" resolution="Fixed">
		<buginformation>
			<summary>Analyze for partition fails if partition value has special characters</summary>
			<description>When analyzing a partition that has a special characters in the value, the analyze command fails with an exception. 
Example:
hive&amp;gt; create table testtable (a int) partitioned by (b string);
hive&amp;gt; insert into table testtable  partition (b="p\"1") values (1);
hive&amp;gt; ANALYZE TABLE testtable  PARTITION(b="p\"1") COMPUTE STATISTICS for columns a;</description>
			<version>0.12.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.java</file>
		</fixedFiles>
	</bug>
	<bug id="12406" opendate="2015-11-13 07:39:39" fixdate="2015-11-23 20:47:35" resolution="Fixed">
		<buginformation>
			<summary>HIVE-9500 introduced incompatible change to LazySimpleSerDe public interface</summary>
			<description>In the process of fixing HIVE-9500, an incompatibility was introduced that will break 3rd party code that relies on LazySimpleSerde. In HIVE-9500, the nested class SerDeParamaters was removed and the method LazySimpleSerDe.initSerdeParms was also removed. They were replaced by a standalone class LazySerDeParameters.
Since this has already been released, I don&amp;amp;apos;t think we should revert the change since that would mean breaking compatibility again. Instead, the best approach would be to support both interfaces, if possible. </description>
			<version>1.2.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.TestLazySimpleSerDe.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazySerDeParameters.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">9500</link>
		</links>
	</bug>
	<bug id="12432" opendate="2015-11-17 09:28:18" fixdate="2015-11-24 01:04:15" resolution="Duplicate">
		<buginformation>
			<summary>Hive on Spark Counter "RECORDS_OUT" always  be zero</summary>
			<description>A simple way to reproduce :
set hive.execution.engine=spark;
CREATE TABLE  test(id INT);
insert into test values (1) ,(2);</description>
			<version>1.2.1</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.SparkTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">12466</link>
			<link type="Reference" description="relates to">12382</link>
		</links>
	</bug>
	<bug id="12466" opendate="2015-11-19 12:29:55" fixdate="2015-11-25 03:04:46" resolution="Fixed">
		<buginformation>
			<summary>SparkCounter not initialized error</summary>
			<description>During a query, lots of the following error found in executor&amp;amp;apos;s log:

03:47:28.759 [Executor task launch worker-0] ERROR org.apache.hive.spark.counter.SparkCounters - counter[HIVE, RECORDS_OUT_0] has not initialized before.
03:47:28.762 [Executor task launch worker-1] ERROR org.apache.hive.spark.counter.SparkCounters - counter[HIVE, RECORDS_OUT_0] has not initialized before.
03:47:30.707 [Executor task launch worker-1] ERROR org.apache.hive.spark.counter.SparkCounters - counter[HIVE, RECORDS_OUT_1_default.tmp_tmp] has not initialized before.
03:47:33.385 [Executor task launch worker-1] ERROR org.apache.hive.spark.counter.SparkCounters - counter[HIVE, RECORDS_OUT_1_default.test_table] has not initialized before.
03:47:33.388 [Executor task launch worker-0] ERROR org.apache.hive.spark.counter.SparkCounters - counter[HIVE, RECORDS_OUT_1_default.test_table] has not initialized before.
03:47:33.495 [Executor task launch worker-0] ERROR org.apache.hive.spark.counter.SparkCounters - counter[HIVE, RECORDS_OUT_1_default.test_table] has not initialized before.
03:47:35.141 [Executor task launch worker-1] ERROR org.apache.hive.spark.counter.SparkCounters - counter[HIVE, RECORDS_OUT_1_default.test_table] has not initialized before.

...........

</description>
			<version>1.2.1</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.SparkTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">12432</link>
		</links>
	</bug>
	<bug id="12399" opendate="2015-11-12 22:08:00" fixdate="2015-11-25 08:53:54" resolution="Fixed">
		<buginformation>
			<summary>Native Vector MapJoin can encounter  "Null key not expected in MapJoin" and "Unexpected NULL in map join small table" exceptions</summary>
			<description>Instead of throw exception, just filter out NULLs in the Native Vector MapJoin operators.</description>
			<version>2.0.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerLongOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastStringCommon.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyStringOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiLongOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerMultiKeyOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiStringOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastLongHashTable.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyMultiKeyOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyLongOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerStringOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiMultiKeyOperator.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">12533</link>
		</links>
	</bug>
	<bug id="12463" opendate="2015-11-19 08:13:49" fixdate="2015-11-25 09:09:23" resolution="Fixed">
		<buginformation>
			<summary>VectorMapJoinFastKeyStore has Array OOB errors</summary>
			<description>When combining different sized keys, observing an occasional error in hashtable probes.


Caused by: java.lang.ArrayIndexOutOfBoundsException: 162046429
	at org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastKeyStore.equalKey(VectorMapJoinFastKeyStore.java:150)
	at org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashTable.findReadSlot(VectorMapJoinFastBytesHashTable.java:191)
	at org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashMap.lookup(VectorMapJoinFastBytesHashMap.java:76)
	at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerMultiKeyOperator.process(VectorMapJoinInnerMultiKeyOperator.java:300)
	... 26 more




    // Our reading is positioned to the key.
    writeBuffers.getByteSegmentRefToCurrent(byteSegmentRef, keyLength, readPos);

    byte[] currentBytes = byteSegmentRef.getBytes();
    int currentStart = (int) byteSegmentRef.getOffset();

    for (int i = 0; i &amp;lt; keyLength; i++) {
      if (currentBytes[currentStart + i] != keyBytes[keyStart + i]) {
        // LOG.debug("VectorMapJoinFastKeyStore equalKey no match on bytes");
        return false;
      }
    }


This needs an identical fix to match 


    // Rare case of buffer boundary. Unfortunately we&amp;amp;apos;d have to copy some bytes.

   // Rare case of buffer boundary. Unfortunately we&amp;amp;apos;d have to copy some bytes.
    byte[] bytes = new byte[length];
    int destOffset = 0;
    while (destOffset &amp;lt; length) {
      ponderNextBufferToRead(readPos);

</description>
			<version>1.2.1</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.serde2.WriteBuffers.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastKeyStore.java</file>
		</fixedFiles>
	</bug>
	<bug id="12487" opendate="2015-11-20 20:09:37" fixdate="2015-11-25 18:11:34" resolution="Fixed">
		<buginformation>
			<summary>Fix broken MiniLlap tests</summary>
			<description>Currently MiniLlap tests fail with the following error:


TestMiniLlapCliDriver - did not produce a TEST-*.xml file


Supposedly, it started happening after HIVE-12319.</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">12319</link>
		</links>
	</bug>
	<bug id="12498" opendate="2015-11-23 21:33:00" fixdate="2015-11-25 19:49:44" resolution="Fixed">
		<buginformation>
			<summary>ACID: Setting OrcRecordUpdater.OrcOptions.tableProperties() has no effect</summary>
			<description>OrcRecordUpdater does not honor the  OrcRecordUpdater.OrcOptions.tableProperties()  setting.  
It would need to translate the specified tableProperties (as listed in OrcTableProperties enum)  to the properties that OrcWriter internally understands (listed in HiveConf.ConfVars).
This is needed for multiple clients.. like Streaming API and Compactor.


    Properties orcTblProps = ..   // get Orc Table Properties from MetaStore;
    AcidOutputFormat.Options updaterOptions =   new OrcRecordUpdater.OrcOptions(conf)
                                                     .inspector(..)
                                                     .bucket(..)
                                                     .minimumTransactionId(..)
                                                     .maximumTransactionId(..)
                                                     .tableProperties(orcTblProps); // &amp;lt;&amp;lt;== 
    OrcOutputFormat orcOutput =   new ...
    orcOutput.getRecordUpdater(partitionPath, updaterOptions );


</description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.TestOrcRecordUpdater.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">13563</link>
		</links>
	</bug>
	<bug id="12503" opendate="2015-11-24 03:05:52" fixdate="2015-11-26 19:41:47" resolution="Fixed">
		<buginformation>
			<summary>GBY-Join transpose rule may go in infinite loop</summary>
			<description>This happens when pushing aggregate is not found to be any cheaper. Can be reproduced by running cbo_rp_auto_join1.q with flag turned on.</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveAggregateJoinTransposeRule.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">12508</link>
		</links>
	</bug>
	<bug id="12307" opendate="2015-10-30 19:51:24" fixdate="2015-11-26 20:39:43" resolution="Fixed">
		<buginformation>
			<summary>Streaming API TransactionBatch.close() must abort any remaining transactions in the batch</summary>
			<description>When the client of TransactionBatch API encounters an error it must close() the batch and start a new one.  This prevents attempts to continue writing to a file that may damaged in some way.
The close() should ensure to abort the any txns that still remain in the batch and close (best effort) all the files it&amp;amp;apos;s writing to.  The batch should also put itself into a mode where any future ops on this batch fail.</description>
			<version>0.14.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.hcatalog.streaming.AbstractRecordWriter.java</file>
			<file type="M">org.apache.hive.hcatalog.streaming.HiveEndPoint.java</file>
			<file type="M">org.apache.hive.hcatalog.streaming.ConnectionError.java</file>
			<file type="M">org.apache.hive.hcatalog.streaming.TransactionBatch.java</file>
			<file type="M">org.apache.hive.hcatalog.streaming.StrictJsonWriter.java</file>
			<file type="M">org.apache.hive.hcatalog.streaming.DelimitedInputWriter.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.java</file>
			<file type="M">org.apache.hive.hcatalog.streaming.TransactionError.java</file>
			<file type="M">org.apache.hive.hcatalog.streaming.TestStreaming.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">12440</link>
		</links>
	</bug>
	<bug id="12465" opendate="2015-11-19 11:33:57" fixdate="2015-11-27 09:32:24" resolution="Fixed">
		<buginformation>
			<summary>Hive might produce wrong results when (outer) joins are merged</summary>
			<description>Consider the following query:

select * from
  (select * from tab where tab.key = 0)a
full outer join
  (select * from tab_part where tab_part.key = 98)b
join
  tab_part c
on a.key = b.key and b.key = c.key;


Hive should execute the full outer join operation (without ON clause) and then the join operation (ON a.key = b.key and b.key = c.key). Instead, it merges both joins, generating the following plan:

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: tab
            filterExpr: (key = 0) (type: boolean)
            Statistics: Num rows: 242 Data size: 22748 Basic stats: COMPLETE Column stats: NONE
            Filter Operator
              predicate: (key = 0) (type: boolean)
              Statistics: Num rows: 121 Data size: 11374 Basic stats: COMPLETE Column stats: NONE
              Select Operator
                expressions: 0 (type: int), value (type: string), ds (type: string)
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 121 Data size: 11374 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col0 (type: int)
                  sort order: +
                  Map-reduce partition columns: _col0 (type: int)
                  Statistics: Num rows: 121 Data size: 11374 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col1 (type: string), _col2 (type: string)
          TableScan
            alias: tab_part
            filterExpr: (key = 98) (type: boolean)
            Statistics: Num rows: 500 Data size: 47000 Basic stats: COMPLETE Column stats: NONE
            Filter Operator
              predicate: (key = 98) (type: boolean)
              Statistics: Num rows: 250 Data size: 23500 Basic stats: COMPLETE Column stats: NONE
              Select Operator
                expressions: 98 (type: int), value (type: string), ds (type: string)
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 250 Data size: 23500 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col0 (type: int)
                  sort order: +
                  Map-reduce partition columns: _col0 (type: int)
                  Statistics: Num rows: 250 Data size: 23500 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col1 (type: string), _col2 (type: string)
          TableScan
            alias: c
            Statistics: Num rows: 500 Data size: 47000 Basic stats: COMPLETE Column stats: NONE
            Reduce Output Operator
              key expressions: key (type: int)
              sort order: +
              Map-reduce partition columns: key (type: int)
              Statistics: Num rows: 500 Data size: 47000 Basic stats: COMPLETE Column stats: NONE
              value expressions: value (type: string), ds (type: string)
      Reduce Operator Tree:
        Join Operator
          condition map:
               Outer Join 0 to 1
               Inner Join 1 to 2
          keys:
            0 _col0 (type: int)
            1 _col0 (type: int)
            2 key (type: int)
          outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
          Statistics: Num rows: 1100 Data size: 103400 Basic stats: COMPLETE Column stats: NONE
          File Output Operator
            compressed: false
            Statistics: Num rows: 1100 Data size: 103400 Basic stats: COMPLETE Column stats: NONE
            table:
                input format: org.apache.hadoop.mapred.TextInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink


That plan is equivalent to the following query, which is different than the original one:

select * from
  (select * from tab where tab.key = 0)a
full outer join
  (select * from tab_part where tab_part.key = 98)b
on a.key = b.key
join
  tab_part c
on b.key = c.key;


It seems to be a problem in the recognition of join operations that can be merged into a single multijoin operator.</description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">12017</link>
		</links>
	</bug>
	<bug id="12533" opendate="2015-11-27 02:53:09" fixdate="2015-11-29 22:36:00" resolution="Duplicate">
		<buginformation>
			<summary>Unexpected NULL in map join small table</summary>
			<description>
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.ql.metadata.HiveException: Unexpected NULL in map join small table
        at org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastHashTableLoader.load(VectorMapJoinFastHashTableLoader.java:110)
        at org.apache.hadoop.hive.ql.exec.MapJoinOperator.loadHashTable(MapJoinOperator.java:293)
        at org.apache.hadoop.hive.ql.exec.MapJoinOperator$1.call(MapJoinOperator.java:174)
        at org.apache.hadoop.hive.ql.exec.MapJoinOperator$1.call(MapJoinOperator.java:170)
        at org.apache.hadoop.hive.ql.exec.tez.LlapObjectCache.retrieve(LlapObjectCache.java:104)
        ... 5 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unexpected NULL in map join small table
        at org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastLongHashTable.putRow(VectorMapJoinFastLongHashTable.java:88)
        at org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastTableContainer.putRow(VectorMapJoinFastTableContainer.java:182)
        at org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastHashTableLoader.load(VectorMapJoinFastHashTableLoader.java:97)
        ... 9 more


\cc Gopal V</description>
			<version>2.0.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerLongOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastStringCommon.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyStringOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiLongOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerMultiKeyOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiStringOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastLongHashTable.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyMultiKeyOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyLongOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerStringOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiMultiKeyOperator.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">12399</link>
		</links>
	</bug>
	<bug id="12490" opendate="2015-11-21 12:24:40" fixdate="2015-12-01 02:12:15" resolution="Fixed">
		<buginformation>
			<summary>Metastore: Mysql ANSI_QUOTES is not there for some cases</summary>
			<description>

Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &amp;amp;apos;"PART_COL_STATS" where "DB_NAME" = &amp;amp;apos;tpcds_100&amp;amp;apos; and "TABLE_NAME" =
 &amp;amp;apos;store_sales&amp;amp;apos; at line 1
...
        at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:451) ~[datanucleus-api-jdo-3.2.6.jar:?]
        at org.datanucleus.api.jdo.JDOQuery.executeWithArray(JDOQuery.java:321) ~[datanucleus-api-jdo-3.2.6.jar:?]
        at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.executeWithArray(MetaStoreDirectSql.java:1644) [hive-exec-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.partsFoundForPartitions(MetaStoreDirectSql.java:1227) [hive-exec-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.aggrColStatsForPartitions(MetaStoreDirectSql.java:1157) [hive-exec-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.metastore.ObjectStore$9.getSqlResult(ObjectStore.java:6659) [hive-exec-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.metastore.ObjectStore$9.getSqlResult(ObjectStore.java:6655) [hive-exec-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2493) [hive-exec-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.metastore.ObjectStore.get_aggr_stats_for(ObjectStore.java:6655) [hive-exec-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_40]

</description>
			<version>2.0.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
		</fixedFiles>
	</bug>
	<bug id="12184" opendate="2015-10-15 06:20:48" fixdate="2015-12-01 05:38:38" resolution="Fixed">
		<buginformation>
			<summary>DESCRIBE of fully qualified table fails when db and table name match and non-default database is in use</summary>
			<description>DESCRIBE of fully qualified table fails when db and table name match and non-default database is in use.
Repro:


: jdbc:hive2://localhost:10000/default&amp;gt; create database foo;
No rows affected (0.116 seconds)
0: jdbc:hive2://localhost:10000/default&amp;gt; create table foo.foo(i int);

0: jdbc:hive2://localhost:10000/default&amp;gt; describe foo.foo;
+-----------+------------+----------+--+
| col_name  | data_type  | comment  |
+-----------+------------+----------+--+
| i         | int        |          |
+-----------+------------+----------+--+
1 row selected (0.049 seconds)

0: jdbc:hive2://localhost:10000/default&amp;gt; use foo;

0: jdbc:hive2://localhost:10000/default&amp;gt; describe foo.foo;
Error: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Error in getting fields from serde.Invalid Field foo (state=08S01,code=1)

</description>
			<version>1.2.1</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">11241</link>
			<link type="Duplicate" description="is duplicated by">11261</link>
		</links>
	</bug>
	<bug id="11241" opendate="2015-07-13 21:29:06" fixdate="2015-12-02 00:58:04" resolution="Duplicate">
		<buginformation>
			<summary>Database prefix does not work properly if table has same name</summary>
			<description>If you do the following it will fail: 


0: jdbc:hive2://cdh54-1.test.com:10000/defaul&amp;gt; create database test4; 
No rows affected (0.881 seconds) 
0: jdbc:hive2://cdh54-1.test.com:10000/defaul&amp;gt; use test4; 
No rows affected (0.1 seconds) 
0: jdbc:hive2://cdh54-1.test.com:10000/defaul&amp;gt; create table test4 (c1 char(200)); 
No rows affected (0.306 seconds) 
0: jdbc:hive2://cdh54-1.test.com:10000/defaul&amp;gt; desc test4.test4; 
Error: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. cannot find field test4 from [0:c1] (state=08S01,code=1)

</description>
			<version>1.2.1</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">12184</link>
			<link type="Reference" description="is related to">11261</link>
		</links>
	</bug>
	<bug id="12491" opendate="2015-11-22 06:53:00" fixdate="2015-12-02 17:40:12" resolution="Fixed">
		<buginformation>
			<summary>Improve ndv heuristic for functions</summary>
			<description>The eased out denominator has to detect duplicate row-stats from different attributes.


select account_id from customers c,  customer_activation ca
  where c.customer_id = ca.customer_id
  and year(ca.dt) = year(c.dt) and month(ca.dt) = month(c.dt)
  and year(ca.dt) between year(&amp;amp;apos;2013-12-26&amp;amp;apos;) and year(&amp;amp;apos;2013-12-26&amp;amp;apos;)




  private Long getEasedOutDenominator(List&amp;lt;Long&amp;gt; distinctVals) {
      // Exponential back-off for NDVs.
      // 1) Descending order sort of NDVs
      // 2) denominator = NDV1 * (NDV2 ^ (1/2)) * (NDV3 ^ (1/4))) * ....
      Collections.sort(distinctVals, Collections.reverseOrder());

      long denom = distinctVals.get(0);
      for (int i = 1; i &amp;lt; distinctVals.size(); i++) {
        denom = (long) (denom * Math.pow(distinctVals.get(i), 1.0 / (1 &amp;lt;&amp;lt; i)));
      }

      return denom;
    }


This gets [8007986, 821974390, 821974390], which is actually 3 columns 2 of which are derived from the same column.


        Reduce Output Operator (RS_12)
          key expressions: _col0 (type: bigint), year(_col2) (type: int), month(_col2) (type: int)
          sort order: +++
          Map-reduce partition columns: _col0 (type: bigint), year(_col2) (type: int), month(_col2) (type: int)
          value expressions: _col1 (type: bigint)
          Join Operator (JOIN_13)
            condition map:
                 Inner Join 0 to 1
            keys:
              0 _col0 (type: bigint), year(_col1) (type: int), month(_col1) (type: int)
              1 _col0 (type: bigint), year(_col2) (type: int), month(_col2) (type: int)
            outputColumnNames: _col3


So the eased out denominator is off by a factor of 30,000 or so, causing OOMs in map-joins.</description>
			<version>1.3.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNull.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPGreaterThan.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPLessThan.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFCurrentDate.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqualNS.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFSecond.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFArrayContains.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFCurrentUser.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFMinute.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNotNull.java</file>
			<file type="M">org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqual.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFAddMonths.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqualOrLessThan.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqualOrGreaterThan.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFWeekOfYear.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFBetween.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNotEqual.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPOr.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFDayOfMonth.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFMonth.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFYear.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.UDFCurrentDB.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNot.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFHour.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFCurrentTimestamp.java</file>
		</fixedFiles>
	</bug>
	<bug id="12479" opendate="2015-11-20 10:05:34" fixdate="2015-12-02 23:29:49" resolution="Fixed">
		<buginformation>
			<summary>Vectorization: Vectorized Date UDFs with up-stream Joins</summary>
			<description>The row-counts expected with and without vectorization differ.
The attached small-scale repro case produces 5 rows with vectorized multi-key joins and 53 rows without the vectorized join.</description>
			<version>1.3.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddScalarCol.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColScalar.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffScalarCol.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColScalar.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldLong.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldString.java</file>
		</fixedFiles>
		<links>
			<link type="Supercedes" description="supercedes">12534</link>
		</links>
	</bug>
	<bug id="12508" opendate="2015-11-24 12:48:56" fixdate="2015-12-02 23:54:24" resolution="Duplicate">
		<buginformation>
			<summary>HiveAggregateJoinTransposeRule places a heavy load on the metadata system</summary>
			<description>Finding out whether the input is already unique requires a call to areColumnsUnique that currently (until CALCITE-794 is fixed) places a heavy load on the metadata system. This can lead to long CBO planning.
This is a temporary fix that avoid the call to the method till then.</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveAggregateJoinTransposeRule.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">12503</link>
			<link type="Reference" description="relates to">794</link>
			<link type="Reference" description="relates to">10785</link>
		</links>
	</bug>
	<bug id="12500" opendate="2015-11-24 00:11:37" fixdate="2015-12-03 00:22:34" resolution="Fixed">
		<buginformation>
			<summary>JDBC driver not overlaying params supplied via properties object when reading params from ZK</summary>
			<description>It makes sense to setup the connection info in one place. Right now part of connection configuration happens in Utils#parseURL and part in the HiveConnection constructor.</description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.jdbc.cbo_rp_TestJdbcDriver2.java</file>
			<file type="M">org.apache.hive.jdbc.TestJdbcDriver2.java</file>
			<file type="M">org.apache.hive.jdbc.HiveDriver.java</file>
			<file type="M">org.apache.hive.jdbc.Utils.java</file>
			<file type="M">org.apache.hive.jdbc.HiveConnection.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">11581</link>
		</links>
	</bug>
	<bug id="12537" opendate="2015-11-28 00:33:55" fixdate="2015-12-03 06:59:58" resolution="Fixed">
		<buginformation>
			<summary>RLEv2 doesn&amp;apos;t seem to work</summary>
			<description>Perhaps I&amp;amp;apos;m doing something wrong or is actually working as expected.
Putting 1 million constant int32 values produces an ORC file of 1MB. Surprisingly, 1 million consecutive ints produces a much smaller file.
Code and FileDump attached.


ObjectInspector inspector = ObjectInspectorFactory.getReflectionObjectInspector(
		Integer.class, ObjectInspectorFactory.ObjectInspectorOptions.JAVA);
Writer w = OrcFile.createWriter(new Path("/tmp/my.orc"), 
			OrcFile.writerOptions(new Configuration())
				.compress(CompressionKind.NONE)
				.inspector(inspector)
				.encodingStrategy(OrcFile.EncodingStrategy.COMPRESSION)
				.version(OrcFile.Version.V_0_12)
		);

for (int i = 0; i &amp;lt; 1000000; ++i) {
	w.addRow(123);
}
w.close();


</description>
			<version>0.14.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerWriterV2.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.TestOrcFile.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.TestVectorOrcFile.java</file>
		</fixedFiles>
	</bug>
	<bug id="12575" opendate="2015-12-02 22:00:59" fixdate="2015-12-03 16:04:36" resolution="Duplicate">
		<buginformation>
			<summary>Similar to HIVE-12574, collect_set and count() give incorrect result when partition size is smaller than window size</summary>
			<description>Will  fix these two functions separately since seems the issue and fix would be different from the other functions like max() and last_value(), etc.</description>
			<version>2.0.0</version>
			<fixedVersion></fixedVersion>
			<type>Sub-task</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLastValue.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStreamingEvaluator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFFirstValue.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">12574</link>
		</links>
	</bug>
	<bug id="12532" opendate="2015-11-26 13:39:58" fixdate="2015-12-03 19:46:05" resolution="Fixed">
		<buginformation>
			<summary>LLAP Cache: Uncompressed data cache has NPE</summary>
			<description>

2015-11-26 08:28:45,232 [TezTaskRunner_attempt_1448429572030_0255_2_02_000019_2(attempt_1448429572030_0255_2_02_000019_2)] WARN org.apache.tez.runtime.LogicalIOProcessorRuntimeTask: Ignoring exception when closing input a(cleanup). Exception class=java.io.IOException, message=java.lang.NullPointerException
java.io.IOException: java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat$LlapRecordReader.rethrowErrorIfAny(LlapInputFormat.java:283)
	at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat$LlapRecordReader.close(LlapInputFormat.java:275)
	at org.apache.hadoop.hive.ql.io.HiveRecordReader.doClose(HiveRecordReader.java:50)
	at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.close(HiveContextAwareRecordReader.java:104)
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.close(TezGroupedSplitsInputFormat.java:177)
	at org.apache.tez.mapreduce.lib.MRReaderMapred.close(MRReaderMapred.java:96)
	at org.apache.tez.mapreduce.input.MRInput.close(MRInput.java:559)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.cleanup(LogicalIOProcessorRuntimeTask.java:872)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:104)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:35)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.preReadUncompressedStream(EncodedReaderImpl.java:795)
	at org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.readEncodedColumns(EncodedReaderImpl.java:320)
	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.performDataRead(OrcEncodedDataReader.java:413)
	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$4.run(OrcEncodedDataReader.java:194)
	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$4.run(OrcEncodedDataReader.java:191)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:191)
	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:74)
	... 5 more


Not clear if current.next can set it to null before the continue; 


      assert partOffset &amp;lt;= current.getOffset();
      if (partOffset == current.getOffset() &amp;amp;&amp;amp; current instanceof CacheChunk) {
        // We assume cache chunks would always match the way we read, so check and skip it.
        assert current.getOffset() == partOffset &amp;amp;&amp;amp; current.getEnd() == partEnd;
        lastUncompressed = (CacheChunk)current;
        current = current.next;
        continue;
      }

</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
		</fixedFiles>
	</bug>
	<bug id="12257" opendate="2015-10-24 02:18:28" fixdate="2015-12-03 23:19:24" resolution="Fixed">
		<buginformation>
			<summary>Enhance ORC FileDump utility to handle flush_length files and recovery</summary>
			<description>ORC file dump utility currently does not handle delta directories that contain *_flush_length files. These files contains offsets to footer in the corresponding delta file.</description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.JsonFileDump.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.FileDump.java</file>
			<file type="M">org.apache.hive.hcatalog.streaming.TestStreaming.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.java</file>
		</fixedFiles>
		<links>
			<link type="dependent" description="depends upon">11497</link>
		</links>
	</bug>
	<bug id="12444" opendate="2015-11-17 22:04:51" fixdate="2015-12-04 18:26:39" resolution="Fixed">
		<buginformation>
			<summary>Global Limit optimization on ACID table without base directory may throw exception</summary>
			<description>Steps to reproduce:
set hive.fetch.task.conversion=minimal;
set hive.limit.optimize.enable=true;
create table acidtest1(
 c_custkey int,
 c_name string,
 c_nationkey int,
 c_acctbal double)
clustered by (c_nationkey) into 3 buckets
stored as orc
tblproperties("transactional"="true");
insert into table acidtest1
select c_custkey, c_name, c_nationkey, c_acctbal from tpch_text_10.customer;
select cast (c_nationkey as string) from acidtest.acidtest1 limit 10;


DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0
FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1447362491939_0020_1_00, diagnostics=[Vertex vertex_1447362491939_0020_1_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: acidtest1 initializer failed, vertex=vertex_1447362491939_0020_1_00 [Map 1], java.lang.RuntimeException: serious problem
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1035)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getSplits(OrcInputFormat.java:1062)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:308)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:410)
	at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:155)
	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:246)
	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:240)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:240)
	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:227)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: delta_0000017_0000017 does not start with base_
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1012)
	... 15 more
Caused by: java.lang.IllegalArgumentException: delta_0000017_0000017 does not start with base_
	at org.apache.hadoop.hive.ql.io.AcidUtils.parseBase(AcidUtils.java:144)
	at org.apache.hadoop.hive.ql.io.AcidUtils.parseBaseBucketFilename(AcidUtils.java:172)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$FileGenerator.call(OrcInputFormat.java:667)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$FileGenerator.call(OrcInputFormat.java:625)
	... 4 more
]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0

</description>
			<version>1.2.1</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
		</fixedFiles>
	</bug>
	<bug id="12578" opendate="2015-12-03 00:01:11" fixdate="2015-12-04 18:50:58" resolution="Fixed">
		<buginformation>
			<summary>Hive query failing with error ClassCastException org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc cannot be cast to org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc</summary>
			<description>Two tables:

CREATE TABLE table_1 (boolean_col_1 BOOLEAN, float_col_2 FLOAT, bigint_col_3 BIGINT, varchar0111_col_4 VARCHAR(111), bigint_col_5 BIGINT, float_col_6 FLOAT, boolean_col_7 BOOLEAN, decimal0101_col_8 DECIMAL(1, 1), decimal0904_col_9 DECIMAL(9, 4), char0112_col_10 CHAR(112), double_col_11 DOUBLE, boolean_col_12 BOOLEAN, double_col_13 DOUBLE, varchar0142_col_14 VARCHAR(142), timestamp_col_15 TIMESTAMP, decimal0502_col_16 DECIMAL(5, 2), smallint_col_25 SMALLINT, decimal3222_col_18 DECIMAL(32, 22), boolean_col_19 BOOLEAN, decimal2012_col_20 DECIMAL(20, 12), char0204_col_21 CHAR(204), double_col_61 DOUBLE, timestamp_col_23 TIMESTAMP, int_col_24 INT, float_col_25 FLOAT, smallint_col_26 SMALLINT, double_col_27 DOUBLE, char0180_col_28 CHAR(180), decimal1503_col_29 DECIMAL(15, 3), timestamp_col_30 TIMESTAMP, smallint_col_31 SMALLINT, decimal2020_col_32 DECIMAL(20, 20), timestamp_col_33 TIMESTAMP, boolean_col_34 BOOLEAN, decimal3025_col_35 DECIMAL(30, 25), decimal3117_col_36 DECIMAL(31, 17), timestamp_col_37 TIMESTAMP, varchar0146_col_38 VARCHAR(146), boolean_col_39 BOOLEAN, double_col_40 DOUBLE, float_col_41 FLOAT, timestamp_col_42 TIMESTAMP, double_col_43 DOUBLE, boolean_col_44 BOOLEAN, timestamp_col_45 TIMESTAMP, tinyint_col_8 TINYINT, int_col_47 INT, decimal0401_col_48 DECIMAL(4, 1), varchar0064_col_49 VARCHAR(64), string_col_50 STRING, double_col_51 DOUBLE, string_col_52 STRING, boolean_col_53 BOOLEAN, int_col_54 INT, boolean_col_55 BOOLEAN, string_col_56 STRING, double_col_57 DOUBLE, varchar0131_col_58 VARCHAR(131), boolean_col_59 BOOLEAN, bigint_col_22 BIGINT, char0184_col_61 CHAR(184), varchar0173_col_62 VARCHAR(173), timestamp_col_63 TIMESTAMP, decimal1709_col_26 DECIMAL(20, 5), timestamp_col_65 TIMESTAMP, timestamp_col_66 TIMESTAMP, timestamp_col_67 TIMESTAMP, boolean_col_68 BOOLEAN, decimal1208_col_20 DECIMAL(33, 11), decimal1605_col_70 DECIMAL(16, 5), varchar0010_col_71 VARCHAR(10), tinyint_col_72 TINYINT, timestamp_col_10 TIMESTAMP, decimal2714_col_74 DECIMAL(27, 14), double_col_75 DOUBLE, boolean_col_76 BOOLEAN, double_col_77 DOUBLE, string_col_78 STRING, boolean_col_79 BOOLEAN, boolean_col_80 BOOLEAN, decimal0803_col_81 DECIMAL(8, 3), decimal1303_col_82 DECIMAL(13, 3), tinyint_col_83 TINYINT, decimal3424_col_84 DECIMAL(34, 24), float_col_85 FLOAT, boolean_col_86 BOOLEAN, char0233_col_87 CHAR(233));

CREATE TABLE table_18 (timestamp_col_1 TIMESTAMP, double_col_2 DOUBLE, boolean_col_3 BOOLEAN, timestamp_col_4 TIMESTAMP, decimal2103_col_5 DECIMAL(21, 3), char0221_col_6 CHAR(221), tinyint_col_7 TINYINT, float_col_8 FLOAT, int_col_2 INT, timestamp_col_10 TIMESTAMP, char0228_col_11 CHAR(228), timestamp_col_12 TIMESTAMP, double_col_13 DOUBLE, tinyint_col_6 TINYINT, tinyint_col_33 TINYINT, smallint_col_38 SMALLINT, boolean_col_17 BOOLEAN, double_col_18 DOUBLE, boolean_col_19 BOOLEAN, bigint_col_20 BIGINT, decimal0504_col_37 DECIMAL(37, 34), boolean_col_22 BOOLEAN, double_col_23 DOUBLE, timestamp_col_24 TIMESTAMP, varchar0076_col_25 VARCHAR(76), timestamp_col_18 TIMESTAMP, boolean_col_27 BOOLEAN, decimal1611_col_22 DECIMAL(37, 5), boolean_col_29 BOOLEAN);


Query:

SELECT
COALESCE(498, LEAD(COALESCE(-973, -684, 515)) OVER (PARTITION BY (t2.int_col_2 + t1.smallint_col_25) ORDER BY (t2.int_col_2 + t1.smallint_col_25), FLOOR(t1.double_col_61) DESC), 524) AS int_col,
(t2.int_col_2) + (t1.smallint_col_25) AS int_col_1,
FLOOR(t1.double_col_61) AS float_col,
COALESCE(SUM(COALESCE(62, -380, -435)) OVER (PARTITION BY (t2.int_col_2 + t1.smallint_col_25) ORDER BY (t2.int_col_2 + t1.smallint_col_25) DESC, FLOOR(t1.double_col_61) DESC ROWS BETWEEN UNBOUNDED PRECEDING AND 48 FOLLOWING), 704) AS int_col_2
FROM table_1 t1
INNER JOIN table_18 t2 ON (((t2.tinyint_col_6) = (t1.bigint_col_22)) AND ((t2.decimal0504_col_37) = (t1.decimal1709_col_26))) AND ((t2.tinyint_col_33) = (t1.tinyint_col_8))
WHERE
(t2.smallint_col_38) IN (SELECT
COALESCE(-92, -994) AS int_col
FROM table_1 tt1
INNER JOIN table_18 tt2 ON (tt2.decimal1611_col_22) = (tt1.decimal1208_col_20)
WHERE
(t1.timestamp_col_10) = (tt2.timestamp_col_18));


We get the following error:

ClassCastException org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc cannot be cast to org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc


We need to add support for constants in Select clause of semijoin subquery.</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
		<links>
			<link type="Supercedes" description="supercedes">12296</link>
		</links>
	</bug>
	<bug id="12567" opendate="2015-12-02 16:38:52" fixdate="2015-12-04 19:13:13" resolution="Fixed">
		<buginformation>
			<summary>Enhance TxnHandler retry logic to handle ORA-08176</summary>
			<description>
FAILED: Error in acquiring locks: Error communicating with the metastore
2015-12-01 09:19:32,459 ERROR [HiveServer2-Background-Pool: Thread-55]: ql.Driver (SessionState.java:printError(932)) - FAILED: Error in acquiring locks: Error communicating with the metastore
org.apache.hadoop.hive.ql.lockmgr.LockException: Error communicating with the metastore
        at org.apache.hadoop.hive.ql.lockmgr.DbLockManager.lock(DbLockManager.java:132)
        at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.acquireLocks(DbTxnManager.java:227)
        at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.acquireLocks(DbTxnManager.java:92)
        at org.apache.hadoop.hive.ql.Driver.acquireLocksAndOpenTxn(Driver.java:1029)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1226)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1100)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1095)
        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:154)
        at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:71)
        at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:206)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
        at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:218)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: MetaException(message:Unable to update transaction database java.sql.SQLException: ORA-08176: consistent read failure; rollback data not available

        at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:450)
        at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:399)
        at oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:1059)
        at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:522)
        at oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:257)
        at oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:587)
        at oracle.jdbc.driver.T4CStatement.doOall8(T4CStatement.java:210)
        at oracle.jdbc.driver.T4CStatement.doOall8(T4CStatement.java:30)
        at oracle.jdbc.driver.T4CStatement.executeForDescribe(T4CStatement.java:762)
        at oracle.jdbc.driver.OracleStatement.executeMaybeDescribe(OracleStatement.java:925)
        at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1111)
        at oracle.jdbc.driver.OracleStatement.executeQuery(OracleStatement.java:1309)
        at oracle.jdbc.driver.OracleStatementWrapper.executeQuery(OracleStatementWrapper.java:422)
        at com.jolbox.bonecp.StatementHandle.executeQuery(StatementHandle.java:464)
        at org.apache.hadoop.hive.metastore.txn.TxnHandler.getLockInfoFromLockId(TxnHandler.java:1951)
        at org.apache.hadoop.hive.metastore.txn.TxnHandler.checkLock(TxnHandler.java:1600)
        at org.apache.hadoop.hive.metastore.txn.TxnHandler.lock(TxnHandler.java:1576)
        at org.apache.hadoop.hive.metastore.txn.TxnHandler.lock(TxnHandler.java:480)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.lock(HiveMetaStore.java:5586)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
        at com.sun.proxy.$Proxy8.lock(Unknown Source)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.lock(HiveMetaStoreClient.java:1869)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)
        at com.sun.proxy.$Proxy9.lock(Unknown Source)
        at org.apache.hadoop.hive.ql.lockmgr.DbLockManager.lock(DbLockManager.java:93)
        at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.acquireLocks(DbTxnManager.java:227)
        at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.acquireLocks(DbTxnManager.java:92)
        at org.apache.hadoop.hive.ql.Driver.acquireLocksAndOpenTxn(Driver.java:1029)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1226)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1100)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1095)
        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:154)
        at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:71)
        at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:206)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
        at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:218)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
)
        at org.apache.hadoop.hive.metastore.txn.TxnHandler.lock(TxnHandler.java:485)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.lock(HiveMetaStore.java:5586)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
        at com.sun.proxy.$Proxy8.lock(Unknown Source)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.lock(HiveMetaStoreClient.java:1869)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)
        at com.sun.proxy.$Proxy9.lock(Unknown Source)
        at org.apache.hadoop.hive.ql.lockmgr.DbLockManager.lock(DbLockManager.java:93)
        ... 18 more

</description>
			<version>1.0.0</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">12620</link>
		</links>
	</bug>
	<bug id="12529" opendate="2015-11-26 00:14:12" fixdate="2015-12-04 19:14:10" resolution="Fixed">
		<buginformation>
			<summary>HiveTxnManager.acquireLocks() should not block forever</summary>
			<description>Currently, in DbTxnManager this method will block until all competing locks have gone away.
This is not appropriate for all clients.  There should be a way to specify a max-wait-time.
It will throw an exception on timeout (given how current method signature is written).
</description>
			<version>1.0.0</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager.java</file>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.DbLockManager.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.HiveTxnManager.java</file>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
			<file type="M">org.apache.hadoop.hive.ql.ErrorMsg.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="is blocked by">12544</link>
		</links>
	</bug>
	<bug id="12506" opendate="2015-11-24 06:02:12" fixdate="2015-12-04 19:42:54" resolution="Fixed">
		<buginformation>
			<summary>SHOW CREATE TABLE command creates a table that does not work for RCFile format</summary>
			<description>See the following test case:
1) Create a table with RCFile format:


DROP TABLE IF EXISTS test;
CREATE TABLE test (a int) PARTITIONED BY (p int)
ROW FORMAT DELIMITED FIELDS TERMINATED BY &amp;amp;apos;|&amp;amp;apos; 
STORED AS RCFILE;


2) run "DESC FORMATTED test"


# Storage Information
SerDe Library:      	org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
InputFormat:        	org.apache.hadoop.hive.ql.io.RCFileInputFormat
OutputFormat:       	org.apache.hadoop.hive.ql.io.RCFileOutputFormat


shows that SerDe used is "ColumnarSerDe"
3) run "SHOW CREATE TABLE" and get the output:


CREATE TABLE `test`(
  `a` int)
PARTITIONED BY (
  `p` int)
ROW FORMAT DELIMITED
  FIELDS TERMINATED BY &amp;amp;apos;|&amp;amp;apos;
STORED AS INPUTFORMAT
  &amp;amp;apos;org.apache.hadoop.hive.ql.io.RCFileInputFormat&amp;amp;apos;
OUTPUTFORMAT
  &amp;amp;apos;org.apache.hadoop.hive.ql.io.RCFileOutputFormat&amp;amp;apos;
LOCATION
  &amp;amp;apos;hdfs://node5.lab.cloudera.com:8020/user/hive/warehouse/case_78732.db/test&amp;amp;apos;
TBLPROPERTIES (
  &amp;amp;apos;transient_lastDdlTime&amp;amp;apos;=&amp;amp;apos;1448343875&amp;amp;apos;)


Note that there is no mention of "ColumnarSerDe"
4) Drop the table and then create the table again using the output from 3)
5) Check the output of "DESC FORMATTED test"


# Storage Information
SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
InputFormat:        	org.apache.hadoop.hive.ql.io.RCFileInputFormat
OutputFormat:       	org.apache.hadoop.hive.ql.io.RCFileOutputFormat


The SerDe falls back to "LazySimpleSerDe", which is not correct.
Any further query tries to INSERT or SELECT this table will fail with errors
I suspect that we can&amp;amp;apos;t specify ROW FORMAT DELIMITED with ROW FORMAT SERDE at the same time at table creation, this causes confusion to end users as copy table structure using "SHOW CREATE TABLE" will not work.</description>
			<version>1.1.1</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
		</fixedFiles>
	</bug>
	<bug id="12505" opendate="2015-11-24 04:00:04" fixdate="2015-12-04 19:44:00" resolution="Fixed">
		<buginformation>
			<summary>Insert overwrite in same encrypted zone silently fails to remove some existing files</summary>
			<description>With HDFS Trash enabled but its encryption zone lower than Hive data directory, insert overwrite command silently fails to trash the existing files during overwrite, which could lead to unexpected incorrect results (more rows returned than expected)</description>
			<version>1.2.1</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
			<file type="M">org.apache.hadoop.hive.common.FileUtils.java</file>
		</fixedFiles>
	</bug>
	<bug id="12584" opendate="2015-12-03 21:11:54" fixdate="2015-12-04 20:18:39" resolution="Fixed">
		<buginformation>
			<summary>Vectorized join with partition column of type char does not trim spaces </summary>
			<description>When a table is partitioned on a column of type char and if join is performed on partitioned column then following exception gets thrown from hashtable loader


Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.serde2.SerDeException: Unexpected tag: 52 reserialized to 5
	at org.apache.hadoop.hive.ql.exec.tez.ObjectCache.retrieve(ObjectCache.java:82)
	at org.apache.hadoop.hive.ql.exec.tez.ObjectCache$1.call(ObjectCache.java:92)
	... 4 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.serde2.SerDeException: Unexpected tag: 52 reserialized to 5
	at org.apache.hadoop.hive.ql.exec.tez.HashTableLoader.load(HashTableLoader.java:216)
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.loadHashTable(MapJoinOperator.java:293)
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator$1.call(MapJoinOperator.java:174)
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator$1.call(MapJoinOperator.java:170)
	at org.apache.hadoop.hive.ql.exec.tez.ObjectCache.retrieve(ObjectCache.java:75)
	... 5 more
Caused by: org.apache.hadoop.hive.serde2.SerDeException: Unexpected tag: 52 reserialized to 5
	at org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer$LazyBinaryKvWriter.sanityCheckKeyForTag(MapJoinBytesTableContainer.java:276)
	at org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer$LazyBinaryKvWriter.getHashFromKey(MapJoinBytesTableContainer.java:247)
	at org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.internalPutRow(HybridHashTableContainer.java:451)
	at org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.putRow(HybridHashTableContainer.java:444)
	at org.apache.hadoop.hive.ql.exec.tez.HashTableLoader.load(HashTableLoader.java:210)

</description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.java</file>
		</fixedFiles>
	</bug>
	<bug id="12563" opendate="2015-12-02 08:28:06" fixdate="2015-12-05 00:01:40" resolution="Fixed">
		<buginformation>
			<summary>NullPointerException with 3-way Tez merge join</summary>
			<description>Issue occurs in Tez merge joins with 3 way join (A join B join C), where A is the big table, and A and C end up having 0 rows after table filters.
Was able to repro this issue with using the sample tables in the Hive unit tests:

select
  a.key, b.value, c.value
from
  src a,
  src1 b,
  src c
where
  a.key = b.key and a.key = c.key
  and a.value &amp;gt; &amp;amp;apos;wal_6789&amp;amp;apos;
  and c.value &amp;gt; &amp;amp;apos;wal_6789&amp;amp;apos;



], TaskAttempt 3 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: Hive Runtime Error while closing operators: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=1) {"key":{"reducesinkkey0":"146"},"value":{"_col0":"val_146"}}
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.RuntimeException: Hive Runtime Error while closing operators: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=1) {"key":{"reducesinkkey0":"146"},"value":{"_col0":"val_146"}}
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.close(ReduceRecordProcessor.java:310)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:162)
	... 13 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=1) {"key":{"reducesinkkey0":"146"},"value":{"_col0":"val_146"}}
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinOneGroup(CommonMergeJoinOperator.java:312)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinOneGroup(CommonMergeJoinOperator.java:293)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinFinalLeftData(CommonMergeJoinOperator.java:501)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.closeOp(CommonMergeJoinOperator.java:416)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:617)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.close(ReduceRecordProcessor.java:287)
	... 14 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=1) {"key":{"reducesinkkey0":"146"},"value":{"_col0":"val_146"}}
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.fetchOneRow(CommonMergeJoinOperator.java:439)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.fetchNextGroup(CommonMergeJoinOperator.java:407)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinOneGroup(CommonMergeJoinOperator.java:310)
	... 19 more
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=1) {"key":{"reducesinkkey0":"146"},"value":{"_col0":"val_146"}}
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:302)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.fetchOneRow(CommonMergeJoinOperator.java:431)
	... 21 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=1) {"key":{"reducesinkkey0":"146"},"value":{"_col0":"val_146"}}
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:370)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:292)
	... 22 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.process(CommonMergeJoinOperator.java:288)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:361)
	... 23 more
]], Vertex failed as one or more tasks failed. failedTasks:1, Vertex vertex_1449041190339_0001_1_03 [Reducer 2] killed/failed due to:null]DAG failed due to vertex failure. failedVertices:1 killedVertices:0

</description>
			<version>1.2.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.java</file>
		</fixedFiles>
	</bug>
	<bug id="12566" opendate="2015-12-02 15:54:13" fixdate="2015-12-05 16:55:57" resolution="Fixed">
		<buginformation>
			<summary>Incorrect result returns when using COALESCE in WHERE condition with LEFT JOIN</summary>
			<description>The left join query with on/where clause returns incorrect result (more rows are returned). See the reproducible sample below.
Left table with data:


CREATE TABLE ltable (i int, la int, lk1 string, lk2 string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &amp;amp;apos;,&amp;amp;apos;;
---
1,\N,CD5415192314304,00071
2,\N,CD5415192225530,00071


Right  table with data:


CREATE TABLE rtable (ra int, rk1 string, rk2 string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &amp;amp;apos;,&amp;amp;apos;;
---
1,CD5415192314304,00071
45,CD5415192314304,00072


Query:


SELECT * FROM ltable l LEFT OUTER JOIN rtable r on (l.lk1 = r.rk1 AND l.lk2 = r.rk2) WHERE COALESCE(l.la,&amp;amp;apos;EMPTY&amp;amp;apos;)=COALESCE(r.ra,&amp;amp;apos;EMPTY&amp;amp;apos;);


Result returns:


1	NULL	CD5415192314304	00071	NULL	NULL	NULL
2	NULL	CD5415192225530	00071	NULL	NULL	NULL


The correct result should be


2	NULL	CD5415192225530	00071	NULL	NULL	NULL

</description>
			<version>0.13.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
	</bug>
	<bug id="12583" opendate="2015-12-03 19:28:02" fixdate="2015-12-06 01:45:36" resolution="Fixed">
		<buginformation>
			<summary>HS2 ShutdownHookManager holds extra of Driver instance </summary>
			<description>HIVE-12266 add a shutdown hook for every Driver instance to release the lock th session holds in case Driver does not exist elegantly. However, that holds all Driver instances and HS2 may run out of memory.</description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">12741</link>
			<link type="Reference" description="is related to">12266</link>
		</links>
	</bug>
	<bug id="12477" opendate="2015-11-20 03:35:26" fixdate="2015-12-07 09:57:04" resolution="Fixed">
		<buginformation>
			<summary>Left Semijoins are incompatible with a cross-product</summary>
			<description>with HIVE-12017 in place, a few queries generate left sem-joins without a key.
This is an invalid plan and can be produced by doing.


explain logical select count(1) from store_sales where ss_sold_date_sk in (select d_date_sk from date_dim where d_date_sk = 1);

LOGICAL PLAN:  
$hdt$_0:$hdt$_0:$hdt$_0:store_sales
  TableScan (TS_0)
    alias: store_sales
    filterExpr: (ss_sold_date_sk = 1) (type: boolean)
    Filter Operator (FIL_20)
      predicate: (ss_sold_date_sk = 1) (type: boolean)
      Select Operator (SEL_2)
        Reduce Output Operator (RS_9)
          sort order: 
          Join Operator (JOIN_11)
            condition map:
                 Left Semi Join 0 to 1
            keys:
              0 
              1 
            Group By Operator (GBY_14)
              aggregations: count(1)
              mode: hash


without CBO


sq_1:date_dim
  TableScan (TS_1)
    alias: date_dim
    filterExpr: ((1) IN (RS[6]) and (d_date_sk = 1)) (type: boolean)
    Filter Operator (FIL_21)
      predicate: ((1) IN (RS[6]) and (d_date_sk = 1)) (type: boolean)
      Select Operator (SEL_3)
        expressions: 1 (type: int)
        outputColumnNames: _col0
        Group By Operator (GBY_5)
          keys: _col0 (type: int)
          mode: hash
          outputColumnNames: _col0
          Reduce Output Operator (RS_8)
            key expressions: _col0 (type: int)
            sort order: +
            Map-reduce partition columns: _col0 (type: int)
            Join Operator (JOIN_9)
              condition map:
                   Left Semi Join 0 to 1
              keys:
                0 ss_sold_date_sk (type: int)
                1 _col0 (type: int)
              Group By Operator (GBY_12)
                aggregations: count(1)
                mode: hash

</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">13082</link>
			<link type="Reference" description="is related to">12017</link>
			<link type="dependent" description="is depended upon by">13164</link>
		</links>
	</bug>
	<bug id="12568" opendate="2015-12-02 18:46:02" fixdate="2015-12-07 19:13:39" resolution="Fixed">
		<buginformation>
			<summary>Provide an option to specify network interface used by Spark remote client [Spark Branch]</summary>
			<description>Spark client sends a pair of host name and port number to the remote driver so that the driver can connects back to HS2 where the user session is. Spark client has its own way determining the host name, and pick one network interface if the host happens to have multiple network interfaces. This can be problematic. For that, there is parameter, hive.spark.client.server.address, which user can pick an interface. Unfortunately, this interface isn&amp;amp;apos;t exposed.
Instead of exposing this parameter, we can use the same logic as Hive in determining the host name. Therefore, the remote driver connecting to HS2 using the same network interface as a HS2 client would do.
There might be a case where user may want the remote driver to use a different network. This is rare if at all. Thus, for now it should be sufficient to use the same network interface.</description>
			<version>1.1.0</version>
			<fixedVersion>spark-branch, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
			<file type="M">org.apache.hive.spark.client.rpc.RpcConfiguration.java</file>
			<file type="M">org.apache.hadoop.hive.common.ServerUtils.java</file>
		</fixedFiles>
	</bug>
	<bug id="12601" opendate="2015-12-04 23:56:19" fixdate="2015-12-07 19:24:03" resolution="Fixed">
		<buginformation>
			<summary>HIVE-11985 change does not use partition deserializer</summary>
			<description>As commented in https://reviews.apache.org/r/38862/diff/5?file=1102759#file1102759line786 , the function Hive.getFieldsFromDeserializerForMsStorage is ignoring the deserializer passed to it and it is taking from the table instead.
However, for the call to the function from Partition.java , that is not the right behavior. The partition can potentially have a different deserializer.</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
		<links>
			<link type="Regression" description="is broken by">11985</link>
		</links>
	</bug>
	<bug id="12574" opendate="2015-12-02 21:42:31" fixdate="2015-12-08 01:34:36" resolution="Fixed">
		<buginformation>
			<summary>windowing function returns incorrect result when the window size is larger than the partition size</summary>
			<description>In PTF windowing, when the partition is small and the window size is larger than the partition size, we are seeing incorrect result. It happens for max, min, first_value, last_value and sum functions. 

CREATE TABLE sdy1(
ord int,
type string);


The data is:

2 a
3 a
1 a 


The result is as follows for the query select ord, min(ord) over (partition by type order by ord rows between 1 preceding and 7 following)

1 1
2 1
3 1 


The expected result is:

1 1
2 1
3 2

</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Sub-task</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLastValue.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStreamingEvaluator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFFirstValue.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">12575</link>
		</links>
	</bug>
	<bug id="12603" opendate="2015-12-06 00:07:31" fixdate="2015-12-08 08:20:22" resolution="Duplicate">
		<buginformation>
			<summary>Add config to block queries that scan &gt; N number of partitions </summary>
			<description>Strict mode is useful for blocking queries that load all partitions, but it&amp;amp;apos;s still possible to put significant load on the HMS for queries that scan a large number of partitions. It would be useful to add a config provide a hard limit to the number of partitions scanned by a query.</description>
			<version>2.0.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.MetadataOnlyOptimizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.ErrorMsg.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.TableScanDesc.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">6492</link>
			<link type="Reference" description="is related to">9499</link>
		</links>
	</bug>
	<bug id="12302" opendate="2015-10-30 06:45:30" fixdate="2015-12-08 19:12:51" resolution="Fixed">
		<buginformation>
			<summary>Use KryoPool instead of thread-local caching</summary>
			<description>Kryo 3.x introduces a Pooling mechanism for Kryo
https://github.com/EsotericSoftware/kryo#pooling-kryo-instances


// Build pool with SoftReferences enabled (optional)
KryoPool pool = new KryoPool.Builder(factory).softReferences().build();

</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.QTestUtil.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TestUtilities.java</file>
			<file type="M">org.apache.hive.hcatalog.api.HCatClientHMSImpl.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.spark.SplitOpTreeForDPP.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.parquet.ProjectionPusher.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.KryoSerializer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.stats.fs.FSStatsAggregator.java</file>
			<file type="M">org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.GenSparkSkewJoinProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.mr.MapRedTask.java</file>
			<file type="M">org.apache.hadoop.hive.accumulo.predicate.AccumuloPredicateHandler.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.ObjectContainer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.sarg.TestConvertAstToSearchArg.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.spark.SplitSparkWorkResolver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.sarg.ConvertAstToSearchArg.java</file>
			<file type="M">org.apache.hadoop.hive.accumulo.predicate.TestAccumuloPredicateHandler.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.SortMergeJoinTaskDispatcher.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.GenMRSkewJoinProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TestPlan.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.TestMetastoreExpr.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.TestOrcSplitElimination.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.PTFUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinTaskDispatcher.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ppr.PartitionExpressionForMetastore.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.SerializeFilter.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.parquet.TestParquetRowGroupFilter.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
		<links>
			<link type="Required" description="requires">12175</link>
		</links>
	</bug>
	<bug id="12330" opendate="2015-11-04 01:28:25" fixdate="2015-12-09 00:02:37" resolution="Fixed">
		<buginformation>
			<summary>Fix precommit Spark test part2</summary>
			<description>Regression because of HIVE-11489</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.hbase.HBaseTestSetup.java</file>
		</fixedFiles>
	</bug>
	<bug id="12585" opendate="2015-12-03 21:24:33" fixdate="2015-12-09 01:19:20" resolution="Fixed">
		<buginformation>
			<summary>fix TxnHandler connection leak</summary>
			<description>checkLock(CheckLockRequest rqst) is leaking connection</description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">11948</link>
		</links>
	</bug>
	<bug id="12572" opendate="2015-12-02 21:17:25" fixdate="2015-12-10 20:18:13" resolution="Duplicate">
		<buginformation>
			<summary>select partitioned acid table order by throws java.io.FileNotFoundException</summary>
			<description>Run the below queries:

create table test_acid (a int) partitioned by (b int) clustered by (a) into 2 buckets stored as orc tblproperties (&amp;amp;apos;transactional&amp;amp;apos;=&amp;amp;apos;true&amp;amp;apos;);
insert into table test_acid partition (b=1) values (1), (2), (3), (4);
select * from acid_partitioned order by a;


The above fails with the following error:

15/12/02 21:12:30 INFO SessionState: Map 1: 0(+0,-4)/1	Reducer 2: 0/1
Status: Failed
15/12/02 21:12:30 ERROR SessionState: Status: Failed
Vertex failed, vertexName=Map 1, vertexId=vertex_1449077191499_0023_1_00, diagnostics=[Task failed, taskId=task_1449077191499_0023_1_00_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task: attempt_1449077191499_0023_1_00_000000_0:java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: java.io.FileNotFoundException: Path is not a file: /apps/hive/warehouse/test_acid/b=1
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:75)
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1828)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1799)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1712)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:652)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:365)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2151)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2147)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2145)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:195)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:160)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:348)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:71)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:60)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:60)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:35)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: java.io.FileNotFoundException: Path is not a file: /apps/hive/warehouse/test_acid/b=1
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:75)
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1828)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1799)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1712)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:652)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:365)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2151)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2147)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2145)

	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:74)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:340)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:172)
	... 14 more
Caused by: java.io.IOException: java.io.FileNotFoundException: Path is not a file: /apps/hive/warehouse/test_acid/b=1
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:75)
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1828)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1799)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1712)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:652)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:365)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2151)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2147)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2145)

	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderNextException(HiveIOExceptionHandlerChain.java:121)
	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderNextException(HiveIOExceptionHandlerUtil.java:77)
	at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:355)
	at org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:79)
	at org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:33)
	at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.next(HiveContextAwareRecordReader.java:116)
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.next(TezGroupedSplitsInputFormat.java:141)
	at org.apache.tez.mapreduce.lib.MRReaderMapred.next(MRReaderMapred.java:113)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:62)
	... 16 more
Caused by: java.io.FileNotFoundException: Path is not a file: /apps/hive/warehouse/test_acid/b=1
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:75)
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1828)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1799)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1712)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:652)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:365)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2151)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2147)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2145)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1242)
	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1227)
	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1215)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:303)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:269)
	at org.apache.hadoop.hdfs.DFSInputStream.&amp;lt;init&amp;gt;(DFSInputStream.java:261)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1540)
	at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:303)
	at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:299)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:299)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:767)
	at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.extractMetaInfoFromFooter(ReaderImpl.java:462)
	at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.&amp;lt;init&amp;gt;(ReaderImpl.java:338)
	at org.apache.hadoop.hive.ql.io.orc.encoded.ReaderImpl.&amp;lt;init&amp;gt;(ReaderImpl.java:33)
	at org.apache.hadoop.hive.ql.io.orc.encoded.EncodedOrcFile.createReader(EncodedOrcFile.java:28)
	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.ensureOrcReader(OrcEncodedDataReader.java:580)
	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.getOrReadFileMetadata(OrcEncodedDataReader.java:594)
	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.performDataRead(OrcEncodedDataReader.java:217)
	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$4.run(OrcEncodedDataReader.java:194)
	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$4.run(OrcEncodedDataReader.java:191)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:191)
	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:74)
	... 5 more
Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): Path is not a file: /apps/hive/warehouse/test_acid/b=1
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:75)
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1828)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1799)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1712)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:652)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:365)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2151)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2147)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2145)

	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1358)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy35.getBlockLocations(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:255)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:252)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy36.getBlockLocations(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1240)
	... 30 more

</description>
			<version>2.0.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.MapWork.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcSplit.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
			<file type="M">org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
			<file type="M">org.apache.hadoop.hive.llap.io.decode.OrcColumnVectorProducer.java</file>
			<file type="M">org.apache.hadoop.hive.llap.io.decode.ColumnVectorProducer.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">12632</link>
		</links>
	</bug>
	<bug id="12599" opendate="2015-12-04 22:12:38" fixdate="2015-12-10 20:28:49" resolution="Fixed">
		<buginformation>
			<summary>Add logging to debug rare unexpected refCount error from the LLAP IO layer</summary>
			<description>

java.sql.SQLException: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex fa
  &amp;lt;responseData class="java.lang.String"&amp;gt;Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 5, vertexId=vertex_1449122740455_0665_7_00, d
  at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:195)
  at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:160)
  at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:348)
  at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:71)
  at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:60)
  at java.security.AccessController.doPrivileged(Native Method)
  at javax.security.auth.Subject.doAs(Subject.java:422)
  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
  at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:60)
  at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:35)
  at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
  at java.util.concurrent.FutureTask.run(FutureTask.java:266)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: java.io.IOException: java.lang.AssertionError: Unexpected refCount -1: 0x57c9bd50(-1)
  at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:74)
  at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:352)
  at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:172)
  ... 14 more
Caused by: java.io.IOException: java.io.IOException: java.lang.AssertionError: Unexpected refCount -1: 0x57c9bd50(-1)
  at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderNextException(HiveIOExceptionHandlerChain.java:121)
  at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderNextException(HiveIOExceptionHandlerUtil.java:77)
  at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:355)
  at org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:79)
  at org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:33)
  at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.next(HiveContextAwareRecordReader.java:116)
  at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.next(TezGroupedSplitsInputFormat.java:151)
  at org.apache.tez.mapreduce.lib.MRReaderMapred.next(MRReaderMapred.java:116)
  at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:62)
  ... 16 more
Caused by: java.io.IOException: java.lang.AssertionError: Unexpected refCount -1: 0x57c9bd50(-1)
  at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat$LlapRecordReader.rethrowErrorIfAny(LlapInputFormat.java:292)
  at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat$LlapRecordReader.nextCvb(LlapInputFormat.java:248)
  at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat$LlapRecordReader.next(LlapInputFormat.java:176)
  at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat$LlapRecordReader.next(LlapInputFormat.java:106)
  at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:350)
  ... 22 more
Caused by: java.lang.AssertionError: Unexpected refCount -1: 0x57c9bd50(-1)
  at org.apache.hadoop.hive.llap.cache.LlapDataBuffer.decRef(LlapDataBuffer.java:116)
  at org.apache.hadoop.hive.llap.cache.LowLevelCacheImpl.unlockBuffer(LowLevelCacheImpl.java:349)
  at org.apache.hadoop.hive.llap.cache.LowLevelCacheImpl.releaseBuffer(LowLevelCacheImpl.java:338)
  at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$DataWrapperForOrc.releaseBuffer(OrcEncodedDataReader.java:922)
  at org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.releaseInitialRefcount(EncodedReaderImpl.java:1037)
  at org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.ponderReleaseInitialRefcount(EncodedReaderImpl.java:1026)
  at org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.prepareRangesForCompressedRead(EncodedReaderImpl.java:691)
  at org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.readEncodedStream(EncodedReaderImpl.java:608)
  at org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.readEncodedColumns(EncodedReaderImpl.java:395)
  at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.performDataRead(OrcEncodedDataReader.java:413)
  at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$4.run(OrcEncodedDataReader.java:194)
  at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$4.run(OrcEncodedDataReader.java:191)
  at java.security.AccessController.doPrivileged(Native Method)
  at javax.security.auth.Subject.doAs(Subject.java:422)
  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
  at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:191)
  at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:74)
  ... 5 more


Configured to use the LRFU cache if that is relevant.</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
		</fixedFiles>
	</bug>
	<bug id="12596" opendate="2015-12-04 20:01:16" fixdate="2015-12-11 03:47:19" resolution="Fixed">
		<buginformation>
			<summary>Delete timestamp row throws java.lang.IllegalArgumentException: Timestamp format must be yyyy-mm-dd hh:mm:ss[.fffffffff]</summary>
			<description>Run the below:

create table test_acid( i int, ts timestamp)
                      clustered by (i) into 2 buckets
                      stored as orc
                      tblproperties (&amp;amp;apos;transactional&amp;amp;apos;=&amp;amp;apos;true&amp;amp;apos;);
insert into table test_acid values (1, &amp;amp;apos;2014-09-14 12:34:30&amp;amp;apos;);
delete from test_acid where ts = &amp;amp;apos;2014-15-16 17:18:19.20&amp;amp;apos;;


The below error is thrown:

15/12/04 19:55:49 INFO SessionState: Map 1: -/-	Reducer 2: 0/2
Status: Failed
15/12/04 19:55:49 ERROR SessionState: Status: Failed
Vertex failed, vertexName=Map 1, vertexId=vertex_1447960616881_0022_2_00, diagnostics=[Vertex vertex_1447960616881_0022_2_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: test_acid initializer failed, vertex=vertex_1447960616881_0022_2_00 [Map 1], java.lang.IllegalArgumentException: Timestamp format must be yyyy-mm-dd hh:mm:ss[.fffffffff]
	at java.sql.Timestamp.valueOf(Timestamp.java:237)
	at org.apache.hadoop.hive.ql.io.sarg.ConvertAstToSearchArg.boxLiteral(ConvertAstToSearchArg.java:160)
	at org.apache.hadoop.hive.ql.io.sarg.ConvertAstToSearchArg.findLiteral(ConvertAstToSearchArg.java:191)
	at org.apache.hadoop.hive.ql.io.sarg.ConvertAstToSearchArg.createLeaf(ConvertAstToSearchArg.java:268)
	at org.apache.hadoop.hive.ql.io.sarg.ConvertAstToSearchArg.createLeaf(ConvertAstToSearchArg.java:326)
	at org.apache.hadoop.hive.ql.io.sarg.ConvertAstToSearchArg.parse(ConvertAstToSearchArg.java:377)
	at org.apache.hadoop.hive.ql.io.sarg.ConvertAstToSearchArg.&amp;lt;init&amp;gt;(ConvertAstToSearchArg.java:68)
	at org.apache.hadoop.hive.ql.io.sarg.ConvertAstToSearchArg.create(ConvertAstToSearchArg.java:417)
	at org.apache.hadoop.hive.ql.io.sarg.ConvertAstToSearchArg.createFromConf(ConvertAstToSearchArg.java:436)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$Context.&amp;lt;init&amp;gt;(OrcInputFormat.java:484)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1121)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getSplits(OrcInputFormat.java:1207)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:369)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:481)
	at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:160)
	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:246)
	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:240)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:240)
	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:227)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)


Not sure if this change is intended as the issue is not seen with ver. 1.2</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.sarg.ConvertAstToSearchArg.java</file>
		</fixedFiles>
		<links>
			<link type="Supercedes" description="supercedes">12404</link>
		</links>
	</bug>
	<bug id="12609" opendate="2015-12-07 21:54:35" fixdate="2015-12-11 21:50:54" resolution="Fixed">
		<buginformation>
			<summary>Remove javaXML serialization</summary>
			<description>We use kryo as default serializer and javaXML based serialization is not used in many places and is also not well tested. We should remove javaXML serialization and make kryo as the only serialization option.  </description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.QTestUtil.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.spark.SplitOpTreeForDPP.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.ptf.TableFunctionEvaluator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.AbstractOperatorDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.PTFDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.GenSparkSkewJoinProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.SerializationUtilities.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.mr.MapRedTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.LoadFileDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TestPlan.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Task.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.LoadMultiFilesDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.PTFUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFLeadLag.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.ptf.PTFExpressionDef.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.TableScanDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.ptf.ShapeDetails.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.AggregationDesc.java</file>
		</fixedFiles>
	</bug>
	<bug id="12445" opendate="2015-11-17 23:50:01" fixdate="2015-12-12 00:07:28" resolution="Fixed">
		<buginformation>
			<summary>Tracking of completed dags is a slow memory leak</summary>
			<description>LLAP daemons track completed DAGs, but never clean up these structures. This is primarily to disallow out of order executions. Evaluate whether that can be avoided - otherwise this structure needs to be cleaned up with a delay.</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.llap.daemon.impl.QueryTracker.java</file>
			<file type="D">org.apache.hadoop.hive.llap.daemon.impl.QueryFileCleaner.java</file>
		</fixedFiles>
	</bug>
	<bug id="12662" opendate="2015-12-12 10:46:12" fixdate="2015-12-13 11:44:44" resolution="Fixed">
		<buginformation>
			<summary>StackOverflowError in HiveSortJoinReduceRule when limit=0</summary>
			<description>L96 of HiveSortJoinReduceRule, you will see 

    // Finally, if we do not reduce the input size, we bail out
    if (RexLiteral.intValue(sortLimit.fetch)
            &amp;gt;= RelMetadataQuery.getRowCount(reducedInput)) {
      return false;
    }


It is using  RelMetadataQuery.getRowCount which is always at least 1. This is the problem that we resolved in CALCITE-987.
To confirm this, I just run the q file :

set hive.mapred.mode=nonstrict;
set hive.optimize.limitjointranspose=true;
set hive.optimize.limitjointranspose.reductionpercentage=1f;
set hive.optimize.limitjointranspose.reductiontuples=0;

explain
select *
from src src1 right outer join (
  select *
  from src src2 left outer join src src3
  on src2.value = src3.value) src2
on src1.key = src2.key
limit 0;


  And I got

2015-12-11T10:21:04,435 ERROR [c1efb099-f900-46dc-9f74-97af0944a99d main[]]: parse.CalcitePlanner (CalcitePlanner.java:genOPTree(301)) - CBO failed, skipping CBO.
java.lang.RuntimeException: java.lang.StackOverflowError
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.rethrowCalciteException(CalcitePlanner.java:749) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.getOptimizedAST(CalcitePlanner.java:645) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:264) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10076) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:223) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:237) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:237) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:456) [hive-exec-2.1.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:310) [hive-exec-2.1.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1138) [hive-exec-2.1.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1187) [hive-exec-2.1.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1063) [hive-exec-2.1.0-SNAPSHOT.jar:?]


via Pengcheng Xiong</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveSortJoinReduceRule.java</file>
		</fixedFiles>
		<links>
			<link type="Regression" description="is broken by">11684</link>
		</links>
	</bug>
	<bug id="12620" opendate="2015-12-08 21:11:05" fixdate="2015-12-14 20:27:30" resolution="Fixed">
		<buginformation>
			<summary>Misc improvement to Acid module</summary>
			<description>
DbLockManger.unlock() - if this fails (due to no such lock in turn due to timeout) the lock is not removed from DbLockManger internal tracking
Add logic to DBLockManager to detect if there is attempt to interleave transactions or locks from different statements for read-only auto commit mode
TxnHandler.checkLock() can use 1 connection instead of 2
TxnHandler.timeOutLocks() - refactor so that it can log which locks were expired (simplifies debugging)
TxnHandler#getTxnIdFromLockId() - include lock id if it&amp;amp;apos;s not found
TxnHandler#checkRetryable() - log exception it saw
TxnHandler.lock() - throw new MetaException("Couldn&amp;amp;apos;t find a lock we just created!"); - include lockid

</description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.DbLockManager.java</file>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">12567</link>
		</links>
	</bug>
	<bug id="12473" opendate="2015-11-19 23:33:13" fixdate="2015-12-14 20:59:41" resolution="Fixed">
		<buginformation>
			<summary>DPP: UDFs on the partition column side does not evaluate correctly</summary>
			<description>Related to HIVE-12462



select count(1) from accounts a, transactions t where year(a.dt) = year(t.dt) and account_id = 22;

$hdt$_0:$hdt$_1:a
  TableScan (TS_2)
    alias: a
    filterExpr: (((account_id = 22) and year(dt) is not null) and (year(dt)) IN (RS[6])) (type: boolean)


Ends up being evaluated as year(cast(dt as int)) because the pruner only checks for final type, not the column type.


    ObjectInspector oi =
        PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(TypeInfoFactory
            .getPrimitiveTypeInfo(si.fieldInspector.getTypeName()));

    Converter converter =
        ObjectInspectorConverters.getConverter(
            PrimitiveObjectInspectorFactory.javaStringObjectInspector, oi);

</description>
			<version>1.2.1</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.plan.DynamicPruningEventDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.MapWork.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TestDynamicPartitionPruner.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Table.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.DynamicPartitionPruner.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.DynamicPartitionPruningOptimization.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">12667</link>
		</links>
	</bug>
	<bug id="12030" opendate="2015-10-05 05:08:37" fixdate="2015-12-14 22:38:41" resolution="Duplicate">
		<buginformation>
			<summary>Hive throws NPE with ACID enabled tables</summary>
			<description>
Code based on master: commit 507442319985198466b4f6c2ba18c6b068d8435e Date: Thu Oct 1 
Exception
=========

Caused by: java.io.IOException: java.lang.NullPointerException
        at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97)
        at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57)
        at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:253)
        at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:193)
        ... 25 more
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.io.AcidUtils.deserializeDeltas(AcidUtils.java:371)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getReader(OrcInputFormat.java:1272)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRecordReader(OrcInputFormat.java:1190)
        at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:250)
        ... 26 more


Steps to reproduce the issue:
=============================

--hiveconf hive.support.concurrency=true --hiveconf hive.enforce.bucketing=true --hiveconf hive.exec.dynamic.partition.mode=nonstrict --hiveconf hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager --hiveconf hive.compactor.initiator.on=true --hiveconf hive.compactor.worker.threads=1

DROP TABLE `lineitem_acid_bucket`;

CREATE TABLE `lineitem_acid_bucket`(
  `l_orderkey` bigint,
  `l_partkey` bigint,
  `l_suppkey` bigint,
  `l_linenumber` bigint,
  `l_quantity` double,
  `l_extendedprice` double,
  `l_discount` double,
  `l_tax` double,
  `l_returnflag` string,
  `l_linestatus` string,
  `l_shipdate` string,
  `l_commitdate` string,
  `l_receiptdate` string,
  `l_shipinstruct` string,
  `l_shipmode` string,
  `l_comment` string)
CLUSTERED BY (l_orderkey)
INTO 10 BUCKETS STORED AS ORC TBLPROPERTIES("transactional"="true"); 

INSERT INTO lineitem_acid_bucket SELECT * FROM tpch_flat_orc_1000.lineitem WHERE l_orderkey &amp;gt; 0 AND l_orderkey &amp;lt; 10000000;

INSERT INTO lineitem_acid_bucket SELECT * FROM tpch_flat_orc_1000.lineitem WHERE l_orderkey &amp;gt; 10000001 AND l_orderkey &amp;lt; 20000000;

update lineitem_acid_bucket set l_quantity=1 where l_orderkey=5963520;

ALTER TABLE lineitem_acid_bucket COMPACT &amp;amp;apos;minor&amp;amp;apos;;

update lineitem_acid_bucket set l_quantity=1 where l_orderkey=5963520;

exception thrown here


</description>
			<version>1.3.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.AcidInputFormat.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">12202</link>
		</links>
	</bug>
	<bug id="12538" opendate="2015-11-28 04:47:27" fixdate="2015-12-16 16:54:30" resolution="Fixed">
		<buginformation>
			<summary>After set spark related config, SparkSession never get reused</summary>
			<description>Hive on Spark yarn-cluster mode.
After setting "set spark.yarn.queue=QueueA;" ,
run the query "select count from test"  3 times and you will find  3 different yarn applications.
Two of the yarn applications in FINISHED &amp;amp; SUCCEEDED state,and one in RUNNING &amp;amp; UNDEFINED state waiting for next work.
And if you submit one more "select count from test" ,the third one will be in FINISHED &amp;amp; SUCCEEDED state and a new yarn application will start up.</description>
			<version>1.3.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.conf.TestHiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.SparkUtilities.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
	</bug>
	<bug id="12610" opendate="2015-12-07 22:52:18" fixdate="2015-12-17 00:51:25" resolution="Fixed">
		<buginformation>
			<summary>Hybrid Grace Hash Join should fail task faster if processing first batch fails, instead of continuing processing the rest</summary>
			<description>During processing the spilled partitions, if there&amp;amp;apos;s any fatal error, such as Kryo exception, then we should exit early, instead of moving on to process the rest of spilled partitions.</description>
			<version>1.2.1</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
		</fixedFiles>
	</bug>
	<bug id="12684" opendate="2015-12-16 00:06:08" fixdate="2015-12-17 20:11:03" resolution="Fixed">
		<buginformation>
			<summary>NPE in stats annotation when all values in decimal column are NULLs</summary>
			<description>When all column values are null for a decimal column and when column stats exists. AnnotateWithStatistics optimization can throw NPE. Following is the exception trace


java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.stats.StatsUtils.getColStatistics(StatsUtils.java:712)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.convertColStats(StatsUtils.java:764)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.getTableColumnStats(StatsUtils.java:750)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:197)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:143)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:131)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$TableScanStatsRule.process(StatsRulesProcFactory.java:114)
        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89)
        at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.walk(LevelOrderWalker.java:143)
        at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.startWalking(LevelOrderWalker.java:122)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateWithStatistics.transform(AnnotateWithStatistics.java:78)
        at org.apache.hadoop.hive.ql.optimizer.Optimizer.optimize(Optimizer.java:228)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10156)
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:225)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:237)
        at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:237)


</description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
		</fixedFiles>
	</bug>
	<bug id="12688" opendate="2015-12-16 05:34:29" fixdate="2015-12-18 00:00:15" resolution="Fixed">
		<buginformation>
			<summary>HIVE-11826 makes hive unusable in properly secured cluster</summary>
			<description>HIVE-11826 makes a change to restrict connections to metastore to users who belong to groups under &amp;amp;apos;hadoop.proxyuser.hive.groups&amp;amp;apos;.
That property was only a meant to be a hadoop property, which controls what users the hive user can impersonate. What this change is doing is to enable use of that to also restrict who can connect to metastore server. This is new functionality, not a bug fix. There is value to this functionality.
However, this change makes hive unusable in a properly secured cluster. If &amp;amp;apos;hadoop.proxyuser.hive.hosts&amp;amp;apos; is set to the proper set of hosts that run Metastore and Hiveserver2 (instead of a very open "*"), then users will be able to connect to metastore only from those hosts.
</description>
			<version>1.3.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.thrift.TestHadoopAuthBridge23.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge.java</file>
		</fixedFiles>
		<links>
			<link type="Regression" description="is broken by">11826</link>
		</links>
	</bug>
	<bug id="12435" opendate="2015-11-17 16:51:33" fixdate="2015-12-18 00:59:49" resolution="Fixed">
		<buginformation>
			<summary>SELECT COUNT(CASE WHEN...) GROUPBY returns 1 for &amp;apos;NULL&amp;apos; in a case of ORC and vectorization is enabled.</summary>
			<description>Run the following query:

create table count_case_groupby (key string, bool boolean) STORED AS orc;
insert into table count_case_groupby values (&amp;amp;apos;key1&amp;amp;apos;, true),(&amp;amp;apos;key2&amp;amp;apos;, false),(&amp;amp;apos;key3&amp;amp;apos;, NULL),(&amp;amp;apos;key4&amp;amp;apos;, false),(&amp;amp;apos;key5&amp;amp;apos;,NULL);


The table contains the following:

key1	true
key2	false
key3	NULL
key4	false
key5	NULL


The below query returns:

SELECT key, COUNT(CASE WHEN bool THEN 1 WHEN NOT bool THEN 0 ELSE NULL END) AS cnt_bool0_ok FROM count_case_groupby GROUP BY key;
key1	1
key2	1
key3	1
key4	1
key5	1


while it expects the following results:

key1	1
key2	1
key3	0
key4	1
key5	0


The query works with hive ver 1.2. Also it works when a table is not orc format.
Also even if it&amp;amp;apos;s an orc table, when vectorization is disabled, the query works.</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFArgDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorizedBatchUtil.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">12209</link>
		</links>
	</bug>
	<bug id="12542" opendate="2015-11-30 16:51:52" fixdate="2015-12-18 06:59:28" resolution="Fixed">
		<buginformation>
			<summary>Create HiveRelFactories</summary>
			<description>Calcite 1.5.0 introduced the use of RelFactories to create the operators. In particular, RelFactories contains the factories for all the operators in the system. Although we can still implement old rules by providing each individual factory (the constructor is deprecated, but it won&amp;amp;apos;t be removed till Calcite 2.0.0 is out), new rules will only provide constructors based on RelFactories. Thus, we propose to migrate immediately to the new interface.</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinProjectTransposeRule.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveRelFieldTrimmer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveJoin.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveSortLimit.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveFilter.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveAggregateProjectMergeRule.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveSemiJoin.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveWindowingFixRule.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveAggregate.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveProjectMergeRule.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveFilterJoinRule.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HivePreFilteringRule.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinAddNotNullRule.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveExpandDistinctAggregatesRule.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveUnion.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveAggregateJoinTransposeRule.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinToMultiJoinRule.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveProject.java</file>
		</fixedFiles>
	</bug>
	<bug id="12698" opendate="2015-12-17 01:14:06" fixdate="2015-12-18 20:29:40" resolution="Fixed">
		<buginformation>
			<summary>Remove exposure to internal privilege and principal classes in HiveAuthorizer</summary>
			<description>The changes in HIVE-11179 expose several internal classes to HiveAuthorization implementations. These include PrivilegeObjectDesc, PrivilegeDesc, PrincipalDesc and AuthorizationUtils.
We should avoid exposing that to all Authorization implementations, but also make the ability to customize the mapping of internal classes to the public api classes possible for Apache Sentry (incubating).
</description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.HiveV1Authorizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.AuthorizationUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizerImpl.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">791</link>
			<link type="Reference" description="relates to">12722</link>
			<link type="Reference" description="relates to">11179</link>
			<link type="Regression" description="breaks">997</link>
		</links>
	</bug>
	<bug id="12708" opendate="2015-12-18 18:02:03" fixdate="2015-12-18 22:37:39" resolution="Fixed">
		<buginformation>
			<summary>Hive on Spark doesn&amp;apos;t work with Kerboresed HBase [Spark Branch]</summary>
			<description>Spark application launcher (spark-submit) acquires HBase delegation token on Hive user&amp;amp;apos;s behalf when the application is launched. This mechanism, which doesn&amp;amp;apos;t work for long-running sessions, is not in line with what Hive is doing. Hive actually acquires the token automatically whenever a job needs it. The right approach for Spark should be allowing applications to dynamically add whatever tokens they need to the spark context. While this needs work on Spark side, we provide a workaround solution in Hive.</description>
			<version>1.1.0</version>
			<fixedVersion>spark-branch, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.java</file>
		</fixedFiles>
	</bug>
	<bug id="12633" opendate="2015-12-09 19:48:31" fixdate="2015-12-18 23:37:36" resolution="Fixed">
		<buginformation>
			<summary>LLAP: package included serde jars</summary>
			<description>Some SerDes like JSONSerde are not packaged with LLAP. One cannot localize jars on the daemon (due to security consideration if nothing else), so we should package them.</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.llap.cli.LlapOptionsProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
		</fixedFiles>
	</bug>
	<bug id="12685" opendate="2015-12-16 00:49:06" fixdate="2015-12-19 01:55:34" resolution="Fixed">
		<buginformation>
			<summary>Remove redundant hive-site.xml under common/src/test/resources/</summary>
			<description>Currently there&amp;amp;apos;s such a property as below, which is obviously wrong


&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;javax.jdo.option.ConnectionDriverName&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;hive-site.xml&amp;lt;/value&amp;gt;
  &amp;lt;description&amp;gt;Override ConfVar defined in HiveConf&amp;lt;/description&amp;gt;
&amp;lt;/property&amp;gt;

</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.conf.TestHiveConf.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">12670</link>
			<link type="Reference" description="relates to">12628</link>
		</links>
	</bug>
	<bug id="12644" opendate="2015-12-10 12:31:08" fixdate="2015-12-21 00:52:09" resolution="Fixed">
		<buginformation>
			<summary>Support for offset in HiveSortMergeRule</summary>
			<description>After HIVE-11531 goes in, HiveSortMergeRule needs to be extended to support offset properly when it merges operators that contain Limit. Otherwise, limit pushdown through outer join optimization (introduced in HIVE-11684) will not work properly.</description>
			<version>2.1.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveSortJoinReduceRule.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveSortMergeRule.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">11684</link>
			<link type="Regression" description="is broken by">11531</link>
		</links>
	</bug>
	<bug id="12632" opendate="2015-12-09 19:45:16" fixdate="2015-12-21 21:07:13" resolution="Fixed">
		<buginformation>
			<summary>LLAP: don&amp;apos;t use IO elevator for ACID tables </summary>
			<description>Until HIVE-12631 is fixed, we need to avoid ACID tables in IO elevator. Right now, a FileNotFound error is thrown.</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.MapWork.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcSplit.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
			<file type="M">org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
			<file type="M">org.apache.hadoop.hive.llap.io.decode.OrcColumnVectorProducer.java</file>
			<file type="M">org.apache.hadoop.hive.llap.io.decode.ColumnVectorProducer.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="is blocked by">12648</link>
			<link type="Duplicate" description="is duplicated by">12572</link>
			<link type="Reference" description="relates to">12631</link>
		</links>
	</bug>
	<bug id="12635" opendate="2015-12-09 20:20:16" fixdate="2015-12-21 21:52:47" resolution="Fixed">
		<buginformation>
			<summary>Hive should return the latest hbase cell timestamp as the row timestamp value</summary>
			<description>When hive talks to hbase and maps hbase timestamp field to one hive column,  seems hive returns the first cell timestamp instead of the latest one as the timestamp value. 
Makes sense to return the latest timestamp since adding the latest cell can be  considered an update to the row. </description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.hbase.LazyHBaseRow.java</file>
		</fixedFiles>
	</bug>
	<bug id="12712" opendate="2015-12-19 00:32:39" fixdate="2015-12-22 00:14:42" resolution="Fixed">
		<buginformation>
			<summary>HiveInputFormat may fail to column names to read in some cases</summary>
			<description>The primary issue is when plan is generated pathToAliases map is populated with directory paths to table aliases. pathToAliases.put() uses path.toString() as map key. During probing, path.toUri().toString() is used. This can cause probe misses when path contains spaces in them. path.toUri() will escape the spaces in the path whereas path.toString() does not escape the spaces. As a result, HiveInputFormat can trigger a different code path which can fail to set list of columns to read from the source table. This was causing unexpected NPE in OrcInputFormat (after refactoring HIVE-11705) which removed null check for column names. The resulting exception is 


Caused by: java.lang.RuntimeException: ORC split generation failed with exception: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1288)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getSplits(OrcInputFormat.java:1354)
        at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:367)
        at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:457)
        at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:152)
        at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:246)
        at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:240)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
        at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:240)
        at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:227)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        ... 3 more
Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException
        at java.util.concurrent.FutureTask.report(FutureTask.java:122)
        at java.util.concurrent.FutureTask.get(FutureTask.java:192)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1282)
        ... 15 more
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.extractNeededColNames(OrcInputFormat.java:422)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.extractNeededColNames(OrcInputFormat.java:417)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.access$2000(OrcInputFormat.java:134)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:1072)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:919)
        ... 4 more


</description>
			<version>2.0.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
		</fixedFiles>
	</bug>
	<bug id="12577" opendate="2015-12-02 22:42:10" fixdate="2015-12-23 19:25:44" resolution="Fixed">
		<buginformation>
			<summary>NPE in LlapTaskCommunicator when unregistering containers</summary>
			<description>

2015-12-02 13:29:00,160 [ERROR] [Dispatcher thread {Central}] |common.AsyncDispatcher|: Error in dispatcher thread
java.lang.NullPointerException
        at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator$EntityTracker.unregisterContainer(LlapTaskCommunicator.java:586)
        at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.registerContainerEnd(LlapTaskCommunicator.java:188)
        at org.apache.tez.dag.app.TaskCommunicatorManager.unregisterRunningContainer(TaskCommunicatorManager.java:389)
        at org.apache.tez.dag.app.rm.container.AMContainerImpl.unregisterFromTAListener(AMContainerImpl.java:1121)
        at org.apache.tez.dag.app.rm.container.AMContainerImpl$StopRequestAtLaunchingTransition.transition(AMContainerImpl.java:699)
        at org.apache.tez.dag.app.rm.container.AMContainerImpl$StopRequestAtIdleTransition.transition(AMContainerImpl.java:805)
        at org.apache.tez.dag.app.rm.container.AMContainerImpl$StopRequestAtRunningTransition.transition(AMContainerImpl.java:892)
        at org.apache.tez.dag.app.rm.container.AMContainerImpl$StopRequestAtRunningTransition.transition(AMContainerImpl.java:887)
        at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)
        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)
        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)
        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
        at org.apache.tez.dag.app.rm.container.AMContainerImpl.handle(AMContainerImpl.java:415)
        at org.apache.tez.dag.app.rm.container.AMContainerImpl.handle(AMContainerImpl.java:72)
        at org.apache.tez.dag.app.rm.container.AMContainerMap.handle(AMContainerMap.java:60)
        at org.apache.tez.dag.app.rm.container.AMContainerMap.handle(AMContainerMap.java:36)
        at org.apache.tez.common.AsyncDispatcher.dispatch(AsyncDispatcher.java:183)
        at org.apache.tez.common.AsyncDispatcher$1.run(AsyncDispatcher.java:114)
        at java.lang.Thread.run(Thread.java:745)
2015-12-02 13:29:00,167 [ERROR] [Dispatcher thread {Central}] |common.AsyncDispatcher|: Error in dispatcher thread
java.lang.NullPointerException
        at org.apache.tez.dag.app.TaskCommunicatorManager.unregisterRunningContainer(TaskCommunicatorManager.java:386)
        at org.apache.tez.dag.app.rm.container.AMContainerImpl.unregisterFromTAListener(AMContainerImpl.java:1121)
        at org.apache.tez.dag.app.rm.container.AMContainerImpl$StopRequestAtLaunchingTransition.transition(AMContainerImpl.java:699)
        at org.apache.tez.dag.app.rm.container.AMContainerImpl$StopRequestAtIdleTransition.transition(AMContainerImpl.java:805)
        at org.apache.tez.dag.app.rm.container.AMContainerImpl$StopRequestAtRunningTransition.transition(AMContainerImpl.java:892)
        at org.apache.tez.dag.app.rm.container.AMContainerImpl$StopRequestAtRunningTransition.transition(AMContainerImpl.java:887)
        at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)
        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)
        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)
        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
        at org.apache.tez.dag.app.rm.container.AMContainerImpl.handle(AMContainerImpl.java:415)
        at org.apache.tez.dag.app.rm.container.AMContainerImpl.handle(AMContainerImpl.java:72)
        at org.apache.tez.dag.app.rm.container.AMContainerMap.handle(AMContainerMap.java:60)
        at org.apache.tez.dag.app.rm.container.AMContainerMap.handle(AMContainerMap.java:36)
        at org.apache.tez.common.AsyncDispatcher.dispatch(AsyncDispatcher.java:183)
        at org.apache.tez.common.AsyncDispatcher$1.run(AsyncDispatcher.java:114)
        at java.lang.Thread.run(Thread.java:745)

</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="D">org.apache.hadoop.hive.llap.tezplugins.TestTaskCommunicator.java</file>
			<file type="M">org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
			<file type="D">org.apache.hadoop.hive.llap.tezplugins.TaskCommunicator.java</file>
		</fixedFiles>
	</bug>
	<bug id="12735" opendate="2015-12-23 00:23:34" fixdate="2015-12-24 03:08:12" resolution="Fixed">
		<buginformation>
			<summary>Constant folding for WHEN/CASE expression does not set return type correctly</summary>
			<description>For the following query


SELECT IF ( ( (CASE WHEN bool0 THEN 1 WHEN NOT bool0 THEN 0 END) = (CASE WHEN TRUE THEN 1 WHEN NOT TRUE THEN 0 END) ), key0, IF ( ( (CASE WHEN bool0 THEN 1 WHEN NOT bool0 THEN 0 END) = (CASE WHEN FALSE THEN 1 WHEN NOT FALSE THEN 0 END) ), key1, key2 ) ) FROM src_orc;


the expression gets constant folded to 


if(CASE WHEN (bool0) THEN (true) WHEN ((not bool0)) THEN (false) END, key0, if(CASE WHEN (bool0) THEN (false) WHEN ((not bool0)) THEN (true) END, key1, key2)) (type: string)


however, the GenericUDFWhen and GenericUDFCase expression retain the original return type (int) instead of the folded return type (boolean). This can cause ClassCastException for the above query when vectorization is enabled.
Following is the exception


Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Error evaluating if(CASE WHEN (bool0) THEN (true) WHEN ((not bool0)) THEN (false) END, key0, if(CASE WHEN (bool0) THEN (false) WHEN ((not bool0)) THEN (true) END, key1, key2))
	at org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.process(VectorSelectOperator.java:126)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:852)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:114)
	at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:168)
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:45)
	... 18 more
Caused by: java.lang.ClassCastException: org.apache.hadoop.io.BooleanWritable cannot be cast to org.apache.hadoop.io.IntWritable
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableIntObjectInspector.get(WritableIntObjectInspector.java:36)
	at org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFAdaptor.setOutputCol(VectorUDFAdaptor.java:262)
	at org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFAdaptor.setResult(VectorUDFAdaptor.java:210)
	at org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFAdaptor.evaluate(VectorUDFAdaptor.java:140)
	at org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression.evaluateChildren(VectorExpression.java:121)
	at org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringGroupColumnStringGroupColumn.evaluate(IfExprStringGroupColumnStringGroupColumn.java:54)
	at org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.process(VectorSelectOperator.java:123)
	... 22 more

</description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">9644</link>
		</links>
	</bug>
	<bug id="12728" opendate="2015-12-22 06:51:42" fixdate="2015-12-24 17:43:59" resolution="Fixed">
		<buginformation>
			<summary>Apply DDL restrictions for ORC schema evolution</summary>
			<description>HIVE-11981 added schema evolution for ORC. However, it does not enforce any restrictions in DDL that can break schema evolution. Following changes have to be enforced in DDL to support the assumptions in schema evolution (that columns will only be added).
1) Restrict changing the file format of the table
2) Restrict changing the serde of the table
3) Restrict replacing columns to not drop columns or do unsupported type widening
4) Restrict reordering columns
5) Restrict unsupported type promotions</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.ErrorMsg.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">14406</link>
			<link type="Reference" description="relates to">11981</link>
			<link type="Reference" description="is related to">12625</link>
		</links>
	</bug>
	<bug id="12743" opendate="2015-12-24 03:57:21" fixdate="2015-12-24 19:51:43" resolution="Fixed">
		<buginformation>
			<summary>RCFileInputFormat needs to be registered with kryo</summary>
			<description>Ran into an issue with union distinct query that uses RCFile table with the following exception


Caused by: java.lang.IllegalArgumentException: Unable to create serializer "org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer" for class: org.apache.hadoop.hive.ql.io.RCFileInputFormat
        at org.apache.hive.com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:67) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hive.com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:45) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hive.com.esotericsoftware.kryo.Kryo.newDefaultSerializer(Kryo.java:380) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hive.com.esotericsoftware.kryo.Kryo.getDefaultSerializer(Kryo.java:364) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.registerImplicit(DefaultClassResolver.java:74) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]


</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.SerializationUtilities.java</file>
		</fixedFiles>
	</bug>
	<bug id="12741" opendate="2015-12-24 00:55:18" fixdate="2015-12-24 21:20:08" resolution="Fixed">
		<buginformation>
			<summary>HS2 ShutdownHookManager holds extra of Driver instance in master/branch-2.0</summary>
			<description>HIVE-12187 was meant to fix the described memory leak, however because of interaction with HIVE-12187 in branch-2.0/master, the fix fails to take effect.</description>
			<version>2.0.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hive.common.util.ShutdownHookManager.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">13476</link>
			<link type="Reference" description="is related to">12583</link>
		</links>
	</bug>
	<bug id="12740" opendate="2015-12-23 20:05:29" fixdate="2015-12-27 06:24:01" resolution="Fixed">
		<buginformation>
			<summary>NPE with HS2 when using null input format</summary>
			<description>When we have a query that returns empty rows and when using tez with hs2, we hit NPE:


java.util.concurrent.ExecutionException: java.lang.NullPointerException
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getSplits(CombineHiveInputFormat.java:490)
	at org.apache.tez.mapreduce.hadoop.MRInputHelpers.generateOldSplits(MRInputHelpers.java:447)
	at org.apache.tez.mapreduce.hadoop.MRInputHelpers.writeOldSplits(MRInputHelpers.java:559)
	at org.apache.tez.mapreduce.hadoop.MRInputHelpers.generateInputSplits(MRInputHelpers.java:619)
	at org.apache.tez.mapreduce.hadoop.MRInputHelpers.configureMRInputWithLegacySplitGeneration(MRInputHelpers.java:109)
	at org.apache.hadoop.hive.ql.exec.tez.DagUtils.createVertex(DagUtils.java:617)
	at org.apache.hadoop.hive.ql.exec.tez.DagUtils.createVertex(DagUtils.java:1103)
	at org.apache.hadoop.hive.ql.exec.tez.TezTask.build(TezTask.java:386)
	at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:175)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:156)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1816)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1561)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1338)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1154)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1147)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:181)
	at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:73)
	at org.apache.hive.service.cli.operation.SQLOperation$2$1.run(SQLOperation.java:234)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hive.service.cli.operation.SQLOperation$2.run(SQLOperation.java:247)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.Utilities.isVectorMode(Utilities.java:3241)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.wrapForLlap(HiveInputFormat.java:208)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getInputFormatFromCache(HiveInputFormat.java:267)
	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat$CheckNonCombinablePathCallable.call(CombineHiveInputFormat.java:103)
	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat$CheckNonCombinablePathCallable.call(CombineHiveInputFormat.java:80)
	... 4 more
15/12/17 18:59:06 INFO log.PerfLogger: &amp;lt;/PERFLOG method=getSplits start=1450378746335 end=1450378746433 duration=98 from=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat&amp;gt;
15/12/17 18:59:06 ERROR exec.Task: Failed to execute tez graph.
org.apache.tez.dag.api.TezUncheckedException: Failed to generate InputSplits
	at org.apache.tez.mapreduce.hadoop.MRInputHelpers.configureMRInputWithLegacySplitGeneration(MRInputHelpers.java:124)
	at org.apache.hadoop.hive.ql.exec.tez.DagUtils.createVertex(DagUtils.java:617)
	at org.apache.hadoop.hive.ql.exec.tez.DagUtils.createVertex(DagUtils.java:1103)
	at org.apache.hadoop.hive.ql.exec.tez.TezTask.build(TezTask.java:386)
	at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:175)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:156)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1816)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1561)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1338)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1154)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1147)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:181)
	at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:73)
	at org.apache.hive.service.cli.operation.SQLOperation$2$1.run(SQLOperation.java:234)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hive.service.cli.operation.SQLOperation$2.run(SQLOperation.java:247)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: java.util.concurrent.ExecutionException: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getSplits(CombineHiveInputFormat.java:502)
	at org.apache.tez.mapreduce.hadoop.MRInputHelpers.generateOldSplits(MRInputHelpers.java:447)
	at org.apache.tez.mapreduce.hadoop.MRInputHelpers.writeOldSplits(MRInputHelpers.java:559)
	at org.apache.tez.mapreduce.hadoop.MRInputHelpers.generateInputSplits(MRInputHelpers.java:619)
	at org.apache.tez.mapreduce.hadoop.MRInputHelpers.configureMRInputWithLegacySplitGeneration(MRInputHelpers.java:109)
	... 23 more
Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getSplits(CombineHiveInputFormat.java:490)
	... 27 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.Utilities.isVectorMode(Utilities.java:3241)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.wrapForLlap(HiveInputFormat.java:208)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getInputFormatFromCache(HiveInputFormat.java:267)
	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat$CheckNonCombinablePathCallable.call(CombineHiveInputFormat.java:103)
	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat$CheckNonCombinablePathCallable.call(CombineHiveInputFormat.java:80)
	... 4 more
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask
15/12/17 18:59:06 ERROR ql.Driver: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask
15/12/17 18:59:06 INFO log.PerfLogger: &amp;lt;/PERFLOG method=Driver.execute start=1450378746093 end=1450378746434 duration=341 from=org.apache.hadoop.hive.ql.Driver&amp;gt;
15/12/17 18:59:06 INFO log.PerfLogger: &amp;lt;PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver&amp;gt;
15/12/17 18:59:06 INFO log.PerfLogger: &amp;lt;/PERFLOG method=releaseLocks start=1450378746434 end=1450378746434 duration=0 from=org.apache.hadoop.hive.ql.Driver&amp;gt;
15/12/17 18:59:06 ERROR operation.Operation: Error running hive query:
org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask
	at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:367)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:183)
	at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:73)
	at org.apache.hive.service.cli.operation.SQLOperation$2$1.run(SQLOperation.java:234)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hive.service.cli.operation.SQLOperation$2.run(SQLOperation.java:247)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
		</fixedFiles>
	</bug>
	<bug id="12742" opendate="2015-12-24 01:02:52" fixdate="2015-12-27 19:31:20" resolution="Fixed">
		<buginformation>
			<summary>NULL table comparison within CASE does not work as previous hive versions</summary>
			<description>drop table test_1; 
create table test_1 (id int, id2 int); 
insert into table test_1 values (123, NULL);
SELECT cast(CASE WHEN id = id2 THEN FALSE ELSE TRUE END AS BOOLEAN) AS b 
FROM test_1; 
--NULL
But the output should be true (confirmed with postgres.)</description>
			<version>1.2.1</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">13942</link>
			<link type="Reference" description="relates to">12751</link>
		</links>
	</bug>
	<bug id="12744" opendate="2015-12-24 07:42:21" fixdate="2015-12-29 18:47:23" resolution="Fixed">
		<buginformation>
			<summary>GROUPING__ID failed to be recognized in multiple insert</summary>
			<description>When using multiple insert with multiple group by, grouping__id will failed to be parse.
hive&amp;gt; create temporary table testtable3 (id string, name string);
OK
Time taken: 1.019 seconds
hive&amp;gt; create temporary table testtable2 (id string, name string);
OK
Time taken: 0.069 seconds
hive&amp;gt; create temporary table testtable1 (id string, name string);
OK
Time taken: 0.066 seconds
hive&amp;gt; insert into table testtable1 values ("id", "2333");
...
OK
Time taken: 32.515 seconds
hive&amp;gt; from testtable1
    &amp;gt; insert into table testtable2 select
    &amp;gt;     id, GROUPING__ID
    &amp;gt; group by id, name with cube;
...
OK
Time taken: 42.032 seconds
hive&amp;gt; from testtable1
    &amp;gt; insert into table testtable2 select
    &amp;gt;     id, GROUPING__ID
    &amp;gt; group by id, name with cube
    &amp;gt; insert into table testtable3 select
    &amp;gt;     id, name
    &amp;gt; group by id, name grouping sets ((id), (id, name));
FAILED: SemanticException [Error 10025]: Line 3:8 Expression not in GROUP BY key &amp;amp;apos;GROUPING__ID&amp;amp;apos;</description>
			<version>1.2.1</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.ErrorMsg.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
	</bug>
	<bug id="12717" opendate="2015-12-21 03:53:35" fixdate="2016-01-04 20:13:50" resolution="Fixed">
		<buginformation>
			<summary>Enabled to accept quoting of all character backslash qooting mechanism to json_tuple UDTF</summary>
			<description>Similar to HIVE-11825, we need to enable ALLOW_BACKSLASH_ESCAPING_ANY_CHARACTER property in json_tuple UDTF
For example in HIVE-11825, there are null return in below statement
(https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-json_tuple)


SELECT a.timestamp, b.*
  FROM log a LATERAL VIEW json_tuple(a.appevent, &amp;amp;apos;eventid&amp;amp;apos;, &amp;amp;apos;eventname&amp;amp;apos;) b as f1, f2;

</description>
			<version>0.14.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDTFJSONTuple.java</file>
		</fixedFiles>
	</bug>
	<bug id="6113" opendate="2013-12-27 07:07:00" fixdate="2016-01-04 20:57:05" resolution="Fixed">
		<buginformation>
			<summary>Upgrade DataNucleus [was: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient]</summary>
			<description>CLEAR LIBRARY CACHE
When I exccute SQL "use fdm; desc formatted fdm.tableName;"  in python, throw Error as followed.
but when I tryit again , It will success.
2013-12-25 03:01:32,290 ERROR exec.DDLTask (DDLTask.java:execute(435)) - org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient
	at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1143)
	at org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1128)
	at org.apache.hadoop.hive.ql.exec.DDLTask.switchDatabase(DDLTask.java:3479)
	at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:237)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:151)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:65)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1414)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1192)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1020)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:888)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:260)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:217)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:507)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:875)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:769)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:708)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:197)
Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1217)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&amp;lt;init&amp;gt;(RetryingMetaStoreClient.java:62)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2372)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2383)
	at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1139)
	... 20 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1210)
	... 25 more
Caused by: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: Duplicate entry &amp;amp;apos;default&amp;amp;apos; for key &amp;amp;apos;UNIQUE_DATABASE&amp;amp;apos;
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:451)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:165)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:358)
	at org.apache.hadoop.hive.metastore.ObjectStore.createDatabase(ObjectStore.java:404)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hive.metastore.RetryingRawStore.invoke(RetryingRawStore.java:124)
	at $Proxy9.createDatabase(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB_core(HiveMetaStore.java:422)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:441)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:326)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.&amp;lt;init&amp;gt;(HiveMetaStore.java:286)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&amp;lt;init&amp;gt;(RetryingHMSHandler.java:54)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:59)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newHMSHandler(HiveMetaStore.java:4060)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&amp;lt;init&amp;gt;(HiveMetaStoreClient.java:121)
	... 30 more
Caused by: java.sql.BatchUpdateException: Duplicate entry &amp;amp;apos;default&amp;amp;apos; for key &amp;amp;apos;UNIQUE_DATABASE&amp;amp;apos;
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:2028)
	at com.mysql.jdbc.PreparedStatement.executeBatch(PreparedStatement.java:1451)
	at com.jolbox.bonecp.StatementHandle.executeBatch(StatementHandle.java:469)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:372)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:628)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:596)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:683)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:86)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:454)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:199)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:263)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:98)
	... 46 more
Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Duplicate entry &amp;amp;apos;default&amp;amp;apos; for key &amp;amp;apos;UNIQUE_DATABASE&amp;amp;apos;
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)
	at com.mysql.jdbc.Util.getInstance(Util.java:386)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1039)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3609)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3541)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2002)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2163)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2624)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2127)
	at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2427)
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:1980)
	... 57 more
</description>
			<version>0.12.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.TestMetastoreVersion.java</file>
			<file type="M">org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			<file type="M">org.apache.hive.beeline.cli.TestHiveCli.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">13931</link>
			<link type="Duplicate" description="duplicates">11036</link>
			<link type="Reference" description="relates to">9543</link>
			<link type="Reference" description="relates to">14152</link>
			<link type="Reference" description="relates to">14322</link>
			<link type="Reference" description="relates to">3764</link>
			<link type="Reference" description="relates to">1841</link>
			<link type="Reference" description="relates to">12436</link>
		</links>
	</bug>
	<bug id="12502" opendate="2015-11-24 01:20:52" fixdate="2016-01-05 01:16:06" resolution="Fixed">
		<buginformation>
			<summary>to_date UDF cannot accept NULLs of VOID type</summary>
			<description>The to_date method behaves differently based off the &amp;amp;apos;data type&amp;amp;apos; of null passed in.
hive&amp;gt; select to_date(null);                   
FAILED: SemanticException [Error 10014]: Line 1:7 Wrong arguments &amp;amp;apos;TOK_NULL&amp;amp;apos;: TO_DATE() only takes STRING/TIMESTAMP/DATEWRITABLE types, got VOID
hive&amp;gt; select to_date(cast(null as timestamp));
OK
NULL
Time taken: 0.031 seconds, Fetched: 1 row(s)
This appears to be a regression introduced in HIVE-5731.  The previous version of to_date would not check the type:
https://github.com/apache/hive/commit/09b6553214d6db5ec7049b88bbe8ff640a7fef72#diff-204f5588c0767cf372a5ca7e3fb964afL56</description>
			<version>1.0.0</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFDate.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFDate.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">5731</link>
		</links>
	</bug>
	<bug id="12766" opendate="2015-12-30 23:44:51" fixdate="2016-01-06 18:10:42" resolution="Fixed">
		<buginformation>
			<summary>TezTask does not close DagClient after execution</summary>
			<description>TezTask does not close DagClient after execution, this can result in objects/threads created by Tez/Yarn not getting freed.</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">14210</link>
		</links>
	</bug>
	<bug id="12664" opendate="2015-12-14 03:01:52" fixdate="2016-01-07 16:28:36" resolution="Fixed">
		<buginformation>
			<summary>Bug in reduce deduplication optimization causing ArrayOutOfBoundException</summary>
			<description>The optimisation check for reduce deduplication only checks the first child node for join and the check itself also contains a major bug causing ArrayOutOfBoundException no matter what.
Sample data table form:


time
user
host
path
referer
code
agent
size
method


int
string
string
string
string
bigint
string
bigint
string


Sample query


SELECT 
  t1.host,
  COUNT(DISTINCT t1.`date`) AS login_count,
  MAX(t2.code) AS code,
  unix_timestamp() AS time
FROM (
    SELECT 
      HOST,
      MIN(time) AS DATE
    FROM
      www_access
    WHERE
      HOST IS NOT NULL
    GROUP BY
      HOST
  ) t1
JOIN (
    SELECT 
      HOST,
      MIN(time) AS code
    FROM
      www_access
    WHERE
      HOST IS NOT NULL
    GROUP BY
      HOST
  ) t2
  ON t1.host = t2.host
GROUP BY
  t1.host

</description>
			<version>1.1.1</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.java</file>
		</fixedFiles>
	</bug>
	<bug id="12786" opendate="2016-01-06 01:59:43" fixdate="2016-01-07 22:39:55" resolution="Fixed">
		<buginformation>
			<summary>CBO may fail for recoverable errors</summary>
			<description>In some cases, CBO may generate an error from which it may be possible to recover. </description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
		</fixedFiles>
	</bug>
	<bug id="12762" opendate="2015-12-30 16:53:59" fixdate="2016-01-08 14:56:10" resolution="Fixed">
		<buginformation>
			<summary>Common join on parquet tables returns incorrect result when hive.optimize.index.filter set to true</summary>
			<description>The following query will give incorrect result.

CREATE TABLE tbl1(id INT) STORED AS PARQUET;
INSERT INTO tbl1 VALUES(1), (2);

CREATE TABLE tbl2(id INT, value STRING) STORED AS PARQUET;
INSERT INTO tbl2 VALUES(1, &amp;amp;apos;value1&amp;amp;apos;);
INSERT INTO tbl2 VALUES(1, &amp;amp;apos;value2&amp;amp;apos;);

set hive.optimize.index.filter = true;
set hive.auto.convert.join=false;
select tbl1.id, t1.value, t2.value
FROM tbl1
JOIN (SELECT * FROM tbl2 WHERE value=&amp;amp;apos;value1&amp;amp;apos;) t1 ON tbl1.id=t1.id
JOIN (SELECT * FROM tbl2 WHERE value=&amp;amp;apos;value2&amp;amp;apos;) t2 ON tbl1.id=t2.id;


We are enforcing to use common join and tbl2 will have 2 files after 2 insertions underneath.
the map job contains 3 TableScan operators (2 for tbl2 and 1 for tbl1). When    hive.optimize.index.filter is set to true, we are incorrectly applying the later filtering condition to each block, which causes no data is returned for the subquery SELECT * FROM tbl2 WHERE value=&amp;amp;apos;value1&amp;amp;apos;.</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.sarg.ExpressionTree.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.parquet.ProjectionPusher.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.java</file>
		</fixedFiles>
	</bug>
	<bug id="12792" opendate="2016-01-06 21:21:02" fixdate="2016-01-08 20:04:26" resolution="Fixed">
		<buginformation>
			<summary>HIVE-12075 didn&amp;apos;t update operation type for plugins</summary>
			<description></description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">12781</link>
			<link type="Regression" description="is broken by">12075</link>
		</links>
	</bug>
	<bug id="12800" opendate="2016-01-07 18:00:03" fixdate="2016-01-09 11:52:07" resolution="Fixed">
		<buginformation>
			<summary>HiveFilterSetOpTransposeRule might be executed over non deterministic filter predicates</summary>
			<description></description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveFilterSetOpTransposeRule.java</file>
		</fixedFiles>
	</bug>
	<bug id="12784" opendate="2016-01-05 19:52:54" fixdate="2016-01-09 15:00:09" resolution="Fixed">
		<buginformation>
			<summary>Group by SemanticException: Invalid column reference</summary>
			<description>Some queries work fine in older versions throws SemanticException, the stack trace:

FAILED: SemanticException [Error 10002]: Line 96:1 Invalid column reference &amp;amp;apos;key2&amp;amp;apos;
15/12/21 18:56:44 [main]: ERROR ql.Driver: FAILED: SemanticException [Error 10002]: Line 96:1 Invalid column reference &amp;amp;apos;key2&amp;amp;apos;
org.apache.hadoop.hive.ql.parse.SemanticException: Line 96:1 Invalid column reference &amp;amp;apos;key2&amp;amp;apos;
at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genGroupByPlanGroupByOperator1(SemanticAnalyzer.java:4228)
at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genGroupByPlanMapAggrNoSkew(SemanticAnalyzer.java:5670)
at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:9007)
at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9884)
at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9777)
at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genOPTree(SemanticAnalyzer.java:10250)
at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10261)
at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10141)
at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:222)
at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:421)
at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:305)
at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1110)
at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1158)
at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1047)
at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1037)
at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:207)
at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:159)
at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:370)
at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:305)
at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:403)
at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:419)
at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:708)
at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)
at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:615)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
at org.apache.hadoop.util.RunJar.main(RunJar.java:136)


Reproduce:

create table tlb (key int, key1 int, key2 int);
create table src (key int, value string);
select key, key1, key2 from (select a.key, 0 as key1 , 0 as key2 from tlb a inner join src b on a.key = b.key) a group by key, key1, key2;

</description>
			<version>2.0.0</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">12886</link>
		</links>
	</bug>
	<bug id="12590" opendate="2015-12-04 01:31:39" fixdate="2016-01-10 17:06:49" resolution="Fixed">
		<buginformation>
			<summary>Repeated UDAFs with literals can produce incorrect result</summary>
			<description>Repeated UDAF with literals could produce wrong result.
This is not a common use case, nevertheless a bug.
hive&amp;gt; select max(&amp;amp;apos;pants&amp;amp;apos;), max(&amp;amp;apos;pANTS&amp;amp;apos;) from t1 group by key;
 Total MapReduce CPU Time Spent: 0 msec
OK
pANTS	pANTS
pANTS	pANTS
pANTS	pANTS
pANTS	pANTS
pANTS	pANTS
Time taken: 296.252 seconds, Fetched: 5 row(s)
</description>
			<version>1.0.1</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.ASTNode.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.RowResolver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.TestQBSubQuery.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.positive.TestTransactionStatement.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.TestSQL11ReservedKeyWordsPositive.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.TestIUD.java</file>
		</fixedFiles>
		<links>
			<link type="Supercedes" description="supercedes">11735</link>
		</links>
	</bug>
	<bug id="12795" opendate="2016-01-06 22:40:40" fixdate="2016-01-11 15:30:46" resolution="Fixed">
		<buginformation>
			<summary>Vectorized execution causes ClassCastException</summary>
			<description>In some hive versions, when
set hive.auto.convert.join=false;
set hive.vectorized.execution.enabled = true;
Some join queries fail with ClassCastException:
The stack:

Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyStringObjectInspector cannot be cast to org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableStringObjectInspector
at org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.genVectorExpressionWritable(VectorExpressionWriterFactory.java:419)
at org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.processVectorInspector(VectorExpressionWriterFactory.java:1102)
at org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.initializeOp(VectorReduceSinkOperator.java:55)
at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)
at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:469)
at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:425)
at org.apache.hadoop.hive.ql.exec.TableScanOperator.initializeOp(TableScanOperator.java:193)
at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)
at org.apache.hadoop.hive.ql.exec.MapOperator.initializeOp(MapOperator.java:431)
at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)
at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.configure(ExecMapper.java:126)
... 22 more



It can not be reproduced in hive 2.0 and 1.3 because of different code path. 
Reproduce:


CREATE TABLE test1
 (
   id string)
   PARTITIONED BY (
  cr_year bigint,
  cr_month bigint)
 ROW FORMAT SERDE
  &amp;amp;apos;org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe&amp;amp;apos;
STORED AS INPUTFORMAT
  &amp;amp;apos;org.apache.hadoop.hive.ql.io.RCFileInputFormat&amp;amp;apos;
OUTPUTFORMAT
  &amp;amp;apos;org.apache.hadoop.hive.ql.io.RCFileOutputFormat&amp;amp;apos;
TBLPROPERTIES (
  &amp;amp;apos;serialization.null.format&amp;amp;apos;=&amp;amp;apos;&amp;amp;apos; );
  
  CREATE TABLE test2(
    id string
  )
   PARTITIONED BY (
  cr_year bigint,
  cr_month bigint)
ROW FORMAT SERDE
  &amp;amp;apos;org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe&amp;amp;apos;
STORED AS INPUTFORMAT
  &amp;amp;apos;org.apache.hadoop.hive.ql.io.RCFileInputFormat&amp;amp;apos;
OUTPUTFORMAT
  &amp;amp;apos;org.apache.hadoop.hive.ql.io.RCFileOutputFormat&amp;amp;apos;
TBLPROPERTIES (
  &amp;amp;apos;serialization.null.format&amp;amp;apos;=&amp;amp;apos;&amp;amp;apos;
 );
set hive.auto.convert.join=false;
set hive.vectorized.execution.enabled = true;
 SELECT cr.id1 ,
cr.id2 
FROM
(SELECT t1.id id1,
 t2.id id2
 from
 (select * from test1 ) t1
 left outer join test2  t2
 on t1.id=t2.id) cr;



</description>
			<version>1.1.0</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.java</file>
		</fixedFiles>
	</bug>
	<bug id="12660" opendate="2015-12-11 23:28:32" fixdate="2016-01-11 17:43:02" resolution="Fixed">
		<buginformation>
			<summary>HS2 memory leak with .hiverc file use</summary>
			<description>The Operation objects created to process .hiverc file in HS2 are not closed.
In HiveSessionImpl, GlobalHivercFileProcessor calls executeStatementInternal but ignores the OperationHandle it returns.</description>
			<version>0.14.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
			<file type="M">org.apache.hive.service.cli.session.TestSessionGlobalInitFile.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">5160</link>
		</links>
	</bug>
	<bug id="12824" opendate="2016-01-09 01:45:34" fixdate="2016-01-11 22:09:42" resolution="Fixed">
		<buginformation>
			<summary>CBO doesnt get triggered when aggregate function is used within windowing function </summary>
			<description></description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.translator.PlanModifierForASTConv.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">12750</link>
		</links>
	</bug>
	<bug id="12768" opendate="2015-12-31 02:27:00" fixdate="2016-01-11 23:50:11" resolution="Fixed">
		<buginformation>
			<summary>Thread safety: binary sortable serde decimal deserialization</summary>
			<description>We see thread safety issues due to static decimal buffer in binary sortable serde.</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe.java</file>
		</fixedFiles>
	</bug>
	<bug id="12785" opendate="2016-01-05 21:55:10" fixdate="2016-01-12 18:30:10" resolution="Fixed">
		<buginformation>
			<summary>View with union type and UDF to `cast` the struct is broken</summary>
			<description>Unfortunately HIVE-12156 is breaking the following use case:
I do have a table with a uniontype of struct s, such as:


CREATE TABLE `minimal_sample`(
  `record_type` string,
  `event` uniontype&amp;lt;struct&amp;lt;string_value:string&amp;gt;,struct&amp;lt;int_value:int&amp;gt;&amp;gt;)


In my case, the table comes from an Avro schema which looks like: 

  &amp;amp;apos;avro.schema.literal&amp;amp;apos;=&amp;amp;apos;{\"type\":\"record\",\"name\":\"Minimal\",\"namespace\":\"org.ver.vkanalas.minimalsamp\",\"fields\":[{\"name\":\"record_type\",\"type\":\"string\"},{\"name\":\"event\",\"type\":[{\"type\":\"record\",\"name\":\"a\",\"fields\":[{\"name\":\"string_value\",\"type\":\"string\"}]},{\"type\":\"record\",\"name\":\"b\",\"fields\":[{\"name\":\"int_value\",\"type\":\"int\"}]}]}]}&amp;amp;apos;


I wrote custom UDF (source attached) to cast the union type to one of the struct to access nested elements, such as int_value in my example.


CREATE FUNCTION toSint AS &amp;amp;apos;org.ver.udf.minimal.StructFromUnionMinimalB&amp;amp;apos;;


A simple query with the UDF is working fine. But creating a view with the same select is failing when I&amp;amp;apos;m trying to query it:


CREATE OR REPLACE VIEW minimal_sample_viewB AS SELECT toSint(event).int_value FROM minimal_sample WHERE record_type = &amp;amp;apos;B&amp;amp;apos;;

SELECT * FROM minimal_sample_viewB;


The stack trace is posted below.
I did try to revert (or exclude) HIVE-12156 from the version I&amp;amp;apos;m running and this use case is working fine.


FAILED: SemanticException Line 0:-1 . Operator is only supported on struct or list of struct types &amp;amp;apos;int_value&amp;amp;apos; in definition of VIEW minimal_sample_viewb [
SELECT null.`int_value` FROM `default`.`minimal_sample` WHERE `minimal_sample`.`record_type` = &amp;amp;apos;B&amp;amp;apos;
] used as minimal_sample_viewb at Line 3:14
16/01/05 22:49:41 [main]: ERROR ql.Driver: FAILED: SemanticException Line 0:-1 . Operator is only supported on struct or list of struct types &amp;amp;apos;int_value&amp;amp;apos; in definition of VIEW minimal_sample_viewb [
SELECT null.`int_value` FROM `default`.`minimal_sample` WHERE `minimal_sample`.`record_type` = &amp;amp;apos;B&amp;amp;apos;
] used as minimal_sample_viewb at Line 3:14
org.apache.hadoop.hive.ql.parse.SemanticException: Line 0:-1 . Operator is only supported on struct or list of struct types &amp;amp;apos;int_value&amp;amp;apos; in definition of VIEW minimal_sample_viewb [
SELECT null.`int_value` FROM `default`.`minimal_sample` WHERE `minimal_sample`.`record_type` = &amp;amp;apos;B&amp;amp;apos;
] used as minimal_sample_viewb at Line 3:14
	at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.getXpathOrFuncExprNodeDesc(TypeCheckProcFactory.java:893)
	at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.process(TypeCheckProcFactory.java:1321)
	at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:95)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:79)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:133)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:110)
	at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.genExprNode(TypeCheckProcFactory.java:209)
	at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.genExprNode(TypeCheckProcFactory.java:153)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genAllExprNodeDesc(SemanticAnalyzer.java:10500)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genExprNodeDesc(SemanticAnalyzer.java:10455)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genSelectPlan(SemanticAnalyzer.java:3822)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genSelectPlan(SemanticAnalyzer.java:3601)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPostGroupByBodyPlan(SemanticAnalyzer.java:8943)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:8898)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9743)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9623)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9650)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9636)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genOPTree(SemanticAnalyzer.java:10109)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:329)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10120)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:211)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:454)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:314)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1164)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1212)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1101)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1091)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:216)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:168)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:379)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:314)
	at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:412)
	at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:428)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:717)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:684)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:624)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)

</description>
			<version>1.2.1</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
		<links>
			<link type="Regression" description="is broken by">12156</link>
		</links>
	</bug>
	<bug id="12807" opendate="2016-01-07 23:40:12" fixdate="2016-01-12 19:07:57" resolution="Duplicate">
		<buginformation>
			<summary>Thrift and DB Changes for HIVE-12352</summary>
			<description>This ticket just covers the thrift and DB changes necessary for HIVE-12352</description>
			<version>1.0.0</version>
			<fixedVersion></fixedVersion>
			<type>Sub-task</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.api.OpenTxnRequest.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.LockRequest.java</file>
			<file type="M">org.apache.hive.beeline.HiveSchemaTool.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.CheckLockRequest.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ShowLocksRequest.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.TxnInfo.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">12832</link>
			<link type="dependent" description="is depended upon by">12814</link>
		</links>
	</bug>
	<bug id="12814" opendate="2016-01-08 21:20:07" fixdate="2016-01-12 19:12:59" resolution="Duplicate">
		<buginformation>
			<summary>Make thrift and DB changes for HIVE-11444</summary>
			<description>This JIRA tracks the Thrift and DB schema changes for HIVE-11444.  It depends on HIVE-12807.</description>
			<version>1.0.0</version>
			<fixedVersion></fixedVersion>
			<type>Sub-task</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.api.OpenTxnRequest.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.LockRequest.java</file>
			<file type="M">org.apache.hive.beeline.HiveSchemaTool.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.CheckLockRequest.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ShowLocksRequest.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.TxnInfo.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">12832</link>
			<link type="dependent" description="depends upon">12807</link>
			<link type="dependent" description="is depended upon by">12816</link>
		</links>
	</bug>
	<bug id="12816" opendate="2016-01-08 22:12:42" fixdate="2016-01-12 19:14:45" resolution="Duplicate">
		<buginformation>
			<summary>Thrift and schema changes for HIVE-11685</summary>
			<description>This JIRA depends on HIVE-12814.</description>
			<version>1.0.1</version>
			<fixedVersion></fixedVersion>
			<type>Sub-task</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.api.OpenTxnRequest.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.LockRequest.java</file>
			<file type="M">org.apache.hive.beeline.HiveSchemaTool.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.CheckLockRequest.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ShowLocksRequest.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.TxnInfo.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">12832</link>
			<link type="dependent" description="depends upon">12814</link>
			<link type="dependent" description="is depended upon by">12818</link>
		</links>
	</bug>
	<bug id="12818" opendate="2016-01-08 23:04:22" fixdate="2016-01-12 19:15:47" resolution="Duplicate">
		<buginformation>
			<summary>Schema changes for HIVE-12353</summary>
			<description>This JIRA just covers RDBMS schema changes for HIVE-12353.</description>
			<version>1.0.0</version>
			<fixedVersion></fixedVersion>
			<type>Sub-task</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.api.OpenTxnRequest.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.LockRequest.java</file>
			<file type="M">org.apache.hive.beeline.HiveSchemaTool.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.CheckLockRequest.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ShowLocksRequest.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.TxnInfo.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">12832</link>
			<link type="dependent" description="depends upon">12816</link>
			<link type="dependent" description="is depended upon by">12819</link>
		</links>
	</bug>
	<bug id="12819" opendate="2016-01-08 23:34:56" fixdate="2016-01-12 19:18:32" resolution="Duplicate">
		<buginformation>
			<summary>Thrift and RDBMS schema changes for HIVE-11957</summary>
			<description></description>
			<version>1.0.0</version>
			<fixedVersion></fixedVersion>
			<type>Sub-task</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.api.OpenTxnRequest.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.LockRequest.java</file>
			<file type="M">org.apache.hive.beeline.HiveSchemaTool.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.CheckLockRequest.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ShowLocksRequest.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.TxnInfo.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">12832</link>
			<link type="dependent" description="depends upon">12818</link>
			<link type="dependent" description="is depended upon by">12821</link>
		</links>
	</bug>
	<bug id="12821" opendate="2016-01-09 00:19:59" fixdate="2016-01-12 19:19:56" resolution="Duplicate">
		<buginformation>
			<summary>Thrift and RDBMS schema changes for HIVE-11965</summary>
			<description></description>
			<version>0.14.0</version>
			<fixedVersion></fixedVersion>
			<type>Sub-task</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.api.OpenTxnRequest.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.LockRequest.java</file>
			<file type="M">org.apache.hive.beeline.HiveSchemaTool.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.CheckLockRequest.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ShowLocksRequest.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.TxnInfo.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">12832</link>
			<link type="dependent" description="depends upon">12819</link>
			<link type="dependent" description="is depended upon by">12822</link>
		</links>
	</bug>
	<bug id="12822" opendate="2016-01-09 00:58:20" fixdate="2016-01-12 19:20:46" resolution="Duplicate">
		<buginformation>
			<summary>Thrift and RDBMS schema changes for HIVE-11495</summary>
			<description></description>
			<version>1.0.0</version>
			<fixedVersion></fixedVersion>
			<type>Sub-task</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.api.OpenTxnRequest.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.LockRequest.java</file>
			<file type="M">org.apache.hive.beeline.HiveSchemaTool.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.CheckLockRequest.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ShowLocksRequest.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.TxnInfo.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">12832</link>
			<link type="dependent" description="depends upon">12821</link>
			<link type="dependent" description="is depended upon by">12823</link>
		</links>
	</bug>
	<bug id="12823" opendate="2016-01-09 01:17:18" fixdate="2016-01-12 19:22:12" resolution="Duplicate">
		<buginformation>
			<summary>Thrift and RDBMS schema changes for HIVE-11956</summary>
			<description></description>
			<version>0.14.0</version>
			<fixedVersion></fixedVersion>
			<type>Sub-task</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.api.OpenTxnRequest.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.LockRequest.java</file>
			<file type="M">org.apache.hive.beeline.HiveSchemaTool.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.CheckLockRequest.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ShowLocksRequest.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.TxnInfo.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">12832</link>
			<link type="dependent" description="depends upon">12822</link>
			<link type="dependent" description="is depended upon by">12829</link>
		</links>
	</bug>
	<bug id="12829" opendate="2016-01-09 15:19:18" fixdate="2016-01-12 19:23:02" resolution="Duplicate">
		<buginformation>
			<summary>Thrift changes for HIVE-12686</summary>
			<description></description>
			<version>1.3.0</version>
			<fixedVersion></fixedVersion>
			<type>Sub-task</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.api.OpenTxnRequest.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.LockRequest.java</file>
			<file type="M">org.apache.hive.beeline.HiveSchemaTool.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.CheckLockRequest.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ShowLocksRequest.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.TxnInfo.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">12832</link>
			<link type="dependent" description="depends upon">12823</link>
			<link type="dependent" description="is depended upon by">12830</link>
		</links>
	</bug>
	<bug id="12830" opendate="2016-01-09 15:56:57" fixdate="2016-01-12 19:24:24" resolution="Duplicate">
		<buginformation>
			<summary>Thrift changes for HIVE-11793</summary>
			<description></description>
			<version>1.0.0</version>
			<fixedVersion></fixedVersion>
			<type>Sub-task</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.api.OpenTxnRequest.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.LockRequest.java</file>
			<file type="M">org.apache.hive.beeline.HiveSchemaTool.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.CheckLockRequest.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ShowLocksRequest.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.TxnInfo.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">12832</link>
			<link type="dependent" description="depends upon">12829</link>
			<link type="dependent" description="is depended upon by">12831</link>
		</links>
	</bug>
	<bug id="12831" opendate="2016-01-09 16:26:38" fixdate="2016-01-12 19:25:06" resolution="Duplicate">
		<buginformation>
			<summary>Thrift and RDBMS changes for HIVE-10249</summary>
			<description></description>
			<version>1.0.0</version>
			<fixedVersion></fixedVersion>
			<type>Sub-task</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.api.OpenTxnRequest.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.LockRequest.java</file>
			<file type="M">org.apache.hive.beeline.HiveSchemaTool.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.CheckLockRequest.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ShowLocksRequest.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.TxnInfo.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">12832</link>
			<link type="dependent" description="depends upon">12830</link>
			<link type="dependent" description="is depended upon by">12832</link>
		</links>
	</bug>
	<bug id="12687" opendate="2015-12-16 02:42:45" fixdate="2016-01-12 21:53:18" resolution="Fixed">
		<buginformation>
			<summary>LLAP Workdirs need to default to YARN local</summary>
			<description>

   LLAP_DAEMON_WORK_DIRS("hive.llap.daemon.work.dirs", ""


is a bad default &amp;amp; fails at startup if not overridden.
A better default would be to fall back onto YARN local dirs if this is not configured.</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">11358</link>
			<link type="Reference" description="is related to">14392</link>
		</links>
	</bug>
	<bug id="12788" opendate="2016-01-06 11:19:18" fixdate="2016-01-13 16:09:56" resolution="Fixed">
		<buginformation>
			<summary>Setting hive.optimize.union.remove to TRUE will break UNION ALL with aggregate functions</summary>
			<description>See the test case below:


0: jdbc:hive2://localhost:10000/default&amp;gt; create table test (a int);

0: jdbc:hive2://localhost:10000/default&amp;gt; insert overwrite table test values (1);

0: jdbc:hive2://localhost:10000/default&amp;gt; set hive.optimize.union.remove=true;
No rows affected (0.01 seconds)

0: jdbc:hive2://localhost:10000/default&amp;gt; set hive.mapred.supports.subdirectories=true;
No rows affected (0.007 seconds)

0: jdbc:hive2://localhost:10000/default&amp;gt; SELECT COUNT(1) FROM test UNION ALL SELECT COUNT(1) FROM test;
+----------+--+
| _u1._c0  |
+----------+--+
+----------+--+


UNION ALL without COUNT function will work as expected:


0: jdbc:hive2://localhost:10000/default&amp;gt; select * from test UNION ALL SELECT * FROM test;
+--------+--+
| _u1.a  |
+--------+--+
| 1      |
| 1      |
+--------+--+


Run the same query without setting hive.mapred.supports.subdirectories and hive.optimize.union.remove to true will give correct result:


0: jdbc:hive2://localhost:10000/default&amp;gt; set hive.optimize.union.remove;
+-----------------------------------+--+
|                set                |
+-----------------------------------+--+
| hive.optimize.union.remove=false  |
+-----------------------------------+--+

0: jdbc:hive2://localhost:10000/default&amp;gt; SELECT COUNT(1) FROM test UNION ALL SELECT COUNT(1) FROM test;
+----------+--+
| _u1._c0  |
+----------+--+
| 1        |
| 1        |
+----------+--+


</description>
			<version>1.1.1</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.StatsOptimizer.java</file>
		</fixedFiles>
		<links>
			<link type="dependent" description="is depended upon by">12840</link>
		</links>
	</bug>
	<bug id="12815" opendate="2016-01-08 21:49:58" fixdate="2016-01-13 21:09:32" resolution="Fixed">
		<buginformation>
			<summary>column stats NPE for a query w/o a table</summary>
			<description>I was running something like create table as select 1;
First it logs why it cannot get stats:

2016-01-08T21:46:31,876 ERROR [0883a32c-c789-4695-aec2-ed73bb1cc9ce 0883a32c-c789-4695-aec2-ed73bb1cc9ce main]: stats.StatsUtils (StatsUtils.java:getTableColumnStats(756)) - Failed to retrieve table statistics:
org.apache.hadoop.hive.ql.metadata.HiveException: NoSuchObjectException(message:Specified database/table does not exist : _dummy_database._dummy_table)
        at org.apache.hadoop.hive.ql.metadata.Hive.getTableColumnStatistics(Hive.java:3195) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.stats.StatsUtils.getTableColumnStats(StatsUtils.java:752) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:198) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:144) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:132) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]


and returns null, then it fails with NPE:

2016-01-08T21:46:31,885 ERROR [0883a32c-c789-4695-aec2-ed73bb1cc9ce 0883a32c-c789-4695-aec2-ed73bb1cc9ce main]: ql.Driver (SessionState.java:printError(1010)) - FAILED: NullPointerException null
java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.stats.StatsUtils.getDataSizeFromColumnStats(StatsUtils.java:1450)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:199)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:144)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:132)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$TableScanStatsRule.process(StatsRulesProcFactory.java:114)
        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89)
        at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.walk(LevelOrderWalker.java:143)
        at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.startWalking(LevelOrderWalker.java:122)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateWithStatistics.transform(AnnotateWithStatistics.java:78)


Only "NullPointerException null" is logged to CLI... </description>
			<version>2.0.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
		</fixedFiles>
	</bug>
	<bug id="12832" opendate="2016-01-09 17:46:05" fixdate="2016-01-15 01:20:39" resolution="Fixed">
		<buginformation>
			<summary>RDBMS schema changes for HIVE-11388</summary>
			<description></description>
			<version>1.0.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Sub-task</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.api.OpenTxnRequest.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.LockRequest.java</file>
			<file type="M">org.apache.hive.beeline.HiveSchemaTool.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.CheckLockRequest.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.ShowLocksRequest.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.TxnInfo.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">12352</link>
			<link type="Blocker" description="blocks">12353</link>
			<link type="Duplicate" description="is duplicated by">12807</link>
			<link type="Duplicate" description="is duplicated by">12814</link>
			<link type="Duplicate" description="is duplicated by">12816</link>
			<link type="Duplicate" description="is duplicated by">12818</link>
			<link type="Duplicate" description="is duplicated by">12819</link>
			<link type="Duplicate" description="is duplicated by">12821</link>
			<link type="Duplicate" description="is duplicated by">12822</link>
			<link type="Duplicate" description="is duplicated by">12823</link>
			<link type="Duplicate" description="is duplicated by">12829</link>
			<link type="Duplicate" description="is duplicated by">12831</link>
			<link type="Duplicate" description="is duplicated by">12830</link>
			<link type="dependent" description="depends upon">12831</link>
			<link type="dependent" description="is depended upon by">12686</link>
			<link type="dependent" description="is depended upon by">11444</link>
			<link type="dependent" description="is depended upon by">11495</link>
			<link type="dependent" description="is depended upon by">11685</link>
			<link type="dependent" description="is depended upon by">11965</link>
			<link type="dependent" description="is depended upon by">11957</link>
			<link type="dependent" description="is depended upon by">10249</link>
			<link type="dependent" description="is depended upon by">11956</link>
			<link type="dependent" description="is depended upon by">11793</link>
		</links>
	</bug>
	<bug id="12352" opendate="2015-11-05 21:09:55" fixdate="2016-01-15 03:06:24" resolution="Fixed">
		<buginformation>
			<summary>CompactionTxnHandler.markCleaned() may delete too much</summary>
			<description>   Worker will start with DB in state X (wrt this partition).
   while it&amp;amp;apos;s working more txns will happen, against partition it&amp;amp;apos;s compacting.
   then this will delete state up to X and since then.  There may be new delta files created
   between compaction starting and cleaning.  These will not be compacted until more
   transactions happen.  So this ideally should only delete
   up to TXN_ID that was compacted (i.e. HWM in Worker?)  Then this can also run
   at READ_COMMITTED.  So this means we&amp;amp;apos;d want to store HWM in COMPACTION_QUEUE when
   Worker picks up the job.
Actually the problem is even worse (but also solved using HWM as above):
Suppose some transactions (against same partition) have started and aborted since the time Worker ran compaction job.
That means there are never-compacted delta files with data that belongs to these aborted txns.
Following will pick up these aborted txns.
s = "select txn_id from TXNS, TXN_COMPONENTS where txn_id = tc_txnid and txn_state = &amp;amp;apos;" +
          TXN_ABORTED + "&amp;amp;apos; and tc_database = &amp;amp;apos;" + info.dbname + "&amp;amp;apos; and tc_table = &amp;amp;apos;" +
          info.tableName + "&amp;amp;apos;";
        if (info.partName != null) s += " and tc_partition = &amp;amp;apos;" + info.partName + "&amp;amp;apos;";
The logic after that will delete relevant data from TXN_COMPONENTS and if one of these txns becomes empty, it will be picked up by cleanEmptyAbortedTxns().  At that point any metadata about an Aborted txn is gone and the system will think it&amp;amp;apos;s committed.
HWM in this case would be (in ValidCompactorTxnList)
if(minOpenTxn &amp;gt; 0)
min(highWaterMark, minOpenTxn) 
else 
highWaterMark</description>
			<version>1.0.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.txn.ValidCompactorTxnList.java</file>
			<file type="M">org.apache.hadoop.hive.ql.txn.compactor.Initiator.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnDbUtil.java</file>
			<file type="M">org.apache.hadoop.hive.common.ValidTxnList.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.CompactionInfo.java</file>
			<file type="M">org.apache.hadoop.hive.ql.txn.compactor.Worker.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">12353</link>
			<link type="Blocker" description="is blocked by">12832</link>
			<link type="Reference" description="relates to">12353</link>
			<link type="Reference" description="is related to">11948</link>
		</links>
	</bug>
	<bug id="12724" opendate="2015-12-21 23:54:28" fixdate="2016-01-15 17:12:03" resolution="Fixed">
		<buginformation>
			<summary>ACID: Major compaction fails to include the original bucket files into MR job</summary>
			<description>How the problem happens:

Create a non-ACID table
Before non-ACID to ACID table conversion, we inserted row one
After non-ACID to ACID table conversion, we inserted row two
Both rows can be retrieved before MAJOR compaction
After MAJOR compaction, row one is lost


hive&amp;gt; USE acidtest;
OK
Time taken: 0.77 seconds
hive&amp;gt; CREATE TABLE t1 (nationkey INT, name STRING, regionkey INT, comment STRING)
    &amp;gt; CLUSTERED BY (regionkey) INTO 2 BUCKETS
    &amp;gt; STORED AS ORC;
OK
Time taken: 0.179 seconds
hive&amp;gt; DESC FORMATTED t1;
OK
# col_name            	data_type           	comment

nationkey           	int
name                	string
regionkey           	int
comment             	string

# Detailed Table Information
Database:           	acidtest
Owner:              	wzheng
CreateTime:         	Mon Dec 14 15:50:40 PST 2015
LastAccessTime:     	UNKNOWN
Retention:          	0
Location:           	file:/Users/wzheng/hivetmp/warehouse/acidtest.db/t1
Table Type:         	MANAGED_TABLE
Table Parameters:
	transient_lastDdlTime	1450137040

# Storage Information
SerDe Library:      	org.apache.hadoop.hive.ql.io.orc.OrcSerde
InputFormat:        	org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
OutputFormat:       	org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
Compressed:         	No
Num Buckets:        	2
Bucket Columns:     	[regionkey]
Sort Columns:       	[]
Storage Desc Params:
	serialization.format	1
Time taken: 0.198 seconds, Fetched: 28 row(s)
hive&amp;gt; dfs -ls /Users/wzheng/hivetmp/warehouse/acidtest.db;
Found 1 items
drwxr-xr-x   - wzheng staff         68 2015-12-14 15:50 /Users/wzheng/hivetmp/warehouse/acidtest.db/t1
hive&amp;gt; dfs -ls /Users/wzheng/hivetmp/warehouse/acidtest.db/t1;
hive&amp;gt; INSERT INTO TABLE t1 VALUES (1, &amp;amp;apos;USA&amp;amp;apos;, 1, &amp;amp;apos;united states&amp;amp;apos;);
WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. tez, spark) or using Hive 1.X releases.
Query ID = wzheng_20151214155028_630098c6-605f-4e7e-a797-6b49fb48360d
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=&amp;lt;number&amp;gt;
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=&amp;lt;number&amp;gt;
In order to set a constant number of reducers:
  set mapreduce.job.reduces=&amp;lt;number&amp;gt;
Job running in-process (local Hadoop)
2015-12-14 15:51:58,070 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local73977356_0001
Loading data to table acidtest.t1
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
Time taken: 2.825 seconds
hive&amp;gt; dfs -ls /Users/wzheng/hivetmp/warehouse/acidtest.db/t1;
Found 2 items
-rwxr-xr-x   1 wzheng staff        112 2015-12-14 15:51 /Users/wzheng/hivetmp/warehouse/acidtest.db/t1/000000_0
-rwxr-xr-x   1 wzheng staff        472 2015-12-14 15:51 /Users/wzheng/hivetmp/warehouse/acidtest.db/t1/000001_0
hive&amp;gt; SELECT * FROM t1;
OK
1	USA	1	united states
Time taken: 0.434 seconds, Fetched: 1 row(s)
hive&amp;gt; ALTER TABLE t1 SET TBLPROPERTIES (&amp;amp;apos;transactional&amp;amp;apos; = &amp;amp;apos;true&amp;amp;apos;);
OK
Time taken: 0.071 seconds
hive&amp;gt; DESC FORMATTED t1;
OK
# col_name            	data_type           	comment

nationkey           	int
name                	string
regionkey           	int
comment             	string

# Detailed Table Information
Database:           	acidtest
Owner:              	wzheng
CreateTime:         	Mon Dec 14 15:50:40 PST 2015
LastAccessTime:     	UNKNOWN
Retention:          	0
Location:           	file:/Users/wzheng/hivetmp/warehouse/acidtest.db/t1
Table Type:         	MANAGED_TABLE
Table Parameters:
	COLUMN_STATS_ACCURATE	false
	last_modified_by    	wzheng
	last_modified_time  	1450137141
	numFiles            	2
	numRows             	-1
	rawDataSize         	-1
	totalSize           	584
	transactional       	true
	transient_lastDdlTime	1450137141

# Storage Information
SerDe Library:      	org.apache.hadoop.hive.ql.io.orc.OrcSerde
InputFormat:        	org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
OutputFormat:       	org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
Compressed:         	No
Num Buckets:        	2
Bucket Columns:     	[regionkey]
Sort Columns:       	[]
Storage Desc Params:
	serialization.format	1
Time taken: 0.049 seconds, Fetched: 36 row(s)
hive&amp;gt; set hive.support.concurrency=true;
hive&amp;gt; set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
hive&amp;gt; set hive.compactor.initiator.on=true;
hive&amp;gt; set hive.compactor.worker.threads=5;
hive&amp;gt; set hive.exec.dynamic.partition.mode=nonstrict;
hive&amp;gt; dfs -ls /Users/wzheng/hivetmp/warehouse/acidtest.db/t1;
Found 2 items
-rwxr-xr-x   1 wzheng staff        112 2015-12-14 15:51 /Users/wzheng/hivetmp/warehouse/acidtest.db/t1/000000_0
-rwxr-xr-x   1 wzheng staff        472 2015-12-14 15:51 /Users/wzheng/hivetmp/warehouse/acidtest.db/t1/000001_0
hive&amp;gt; INSERT INTO TABLE t1 VALUES (2, &amp;amp;apos;Canada&amp;amp;apos;, 1, &amp;amp;apos;maple leaf&amp;amp;apos;);
WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. tez, spark) or using Hive 1.X releases.
Query ID = wzheng_20151214155028_630098c6-605f-4e7e-a797-6b49fb48360d
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=&amp;lt;number&amp;gt;
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=&amp;lt;number&amp;gt;
In order to set a constant number of reducers:
  set mapreduce.job.reduces=&amp;lt;number&amp;gt;
Job running in-process (local Hadoop)
2015-12-14 15:54:18,943 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local1674014367_0002
Loading data to table acidtest.t1
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
Time taken: 1.995 seconds
hive&amp;gt; dfs -ls /Users/wzheng/hivetmp/warehouse/acidtest.db/t1;
Found 3 items
-rwxr-xr-x   1 wzheng staff        112 2015-12-14 15:51 /Users/wzheng/hivetmp/warehouse/acidtest.db/t1/000000_0
-rwxr-xr-x   1 wzheng staff        472 2015-12-14 15:51 /Users/wzheng/hivetmp/warehouse/acidtest.db/t1/000001_0
drwxr-xr-x   - wzheng staff        204 2015-12-14 15:54 /Users/wzheng/hivetmp/warehouse/acidtest.db/t1/delta_0000007_0000007_0000
hive&amp;gt; dfs -ls /Users/wzheng/hivetmp/warehouse/acidtest.db/t1/delta_0000007_0000007_0000;
Found 2 items
-rw-r--r--   1 wzheng staff        214 2015-12-14 15:54 /Users/wzheng/hivetmp/warehouse/acidtest.db/t1/delta_0000007_0000007_0000/bucket_00000
-rw-r--r--   1 wzheng staff        797 2015-12-14 15:54 /Users/wzheng/hivetmp/warehouse/acidtest.db/t1/delta_0000007_0000007_0000/bucket_00001
hive&amp;gt; SELECT * FROM t1;
OK
1	USA	1	united states
2	Canada	1	maple leaf
Time taken: 0.1 seconds, Fetched: 2 row(s)
hive&amp;gt; ALTER TABLE t1 COMPACT &amp;amp;apos;MAJOR&amp;amp;apos;;
Compaction enqueued.
OK
Time taken: 0.026 seconds
hive&amp;gt; show compactions;
OK
Database	Table	Partition	Type	State	Worker	Start Time
Time taken: 0.022 seconds, Fetched: 1 row(s)
hive&amp;gt; dfs -ls /Users/wzheng/hivetmp/warehouse/acidtest.db/t1/;
Found 3 items
-rwxr-xr-x   1 wzheng staff        112 2015-12-14 15:51 /Users/wzheng/hivetmp/warehouse/acidtest.db/t1/000000_0
-rwxr-xr-x   1 wzheng staff        472 2015-12-14 15:51 /Users/wzheng/hivetmp/warehouse/acidtest.db/t1/000001_0
drwxr-xr-x   - wzheng staff        204 2015-12-14 15:55 /Users/wzheng/hivetmp/warehouse/acidtest.db/t1/base_0000007
hive&amp;gt; dfs -ls /Users/wzheng/hivetmp/warehouse/acidtest.db/t1/base_0000007;
Found 2 items
-rw-r--r--   1 wzheng staff        222 2015-12-14 15:55 /Users/wzheng/hivetmp/warehouse/acidtest.db/t1/base_0000007/bucket_00000
-rw-r--r--   1 wzheng staff        802 2015-12-14 15:55 /Users/wzheng/hivetmp/warehouse/acidtest.db/t1/base_0000007/bucket_00001
hive&amp;gt; select * from t1;
OK
2	Canada	1	maple leaf
Time taken: 0.396 seconds, Fetched: 1 row(s)
hive&amp;gt; select count(*) from t1;
WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. tez, spark) or using Hive 1.X releases.
Query ID = wzheng_20151214155028_630098c6-605f-4e7e-a797-6b49fb48360d
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=&amp;lt;number&amp;gt;
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=&amp;lt;number&amp;gt;
In order to set a constant number of reducers:
  set mapreduce.job.reduces=&amp;lt;number&amp;gt;
Job running in-process (local Hadoop)
2015-12-14 15:56:20,277 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local1720993786_0003
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
1
Time taken: 1.623 seconds, Fetched: 1 row(s)


Note, the cleanup doesn&amp;amp;apos;t kick in because the compaction fails already. The cleanup itself doesn&amp;amp;apos;t have any problem (at least not that we know of for this case).

</description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.TestAcidUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">13961</link>
			<link type="Reference" description="is related to">14366</link>
		</links>
	</bug>
	<bug id="12661" opendate="2015-12-12 02:14:10" fixdate="2016-01-17 07:01:59" resolution="Fixed">
		<buginformation>
			<summary>StatsSetupConst.COLUMN_STATS_ACCURATE is not used correctly</summary>
			<description>PROBLEM:
Hive stats are autogathered properly till an &amp;amp;apos;analyze table [tablename] compute statistics for columns&amp;amp;apos; is run. Then it does not auto-update the stats till the command is run again. repo:


set hive.stats.autogather=true; 
set hive.stats.atomic=false ; 
set hive.stats.collect.rawdatasize=true ; 
set hive.stats.collect.scancols=false ; 
set hive.stats.collect.tablekeys=false ; 
set hive.stats.fetch.column.stats=true; 
set hive.stats.fetch.partition.stats=true ; 
set hive.stats.reliable=false ; 
set hive.compute.query.using.stats=true; 

CREATE TABLE `default`.`calendar` (`year` int) ROW FORMAT SERDE &amp;amp;apos;org.apache.hadoop.hive.ql.io.orc.OrcSerde&amp;amp;apos; STORED AS INPUTFORMAT &amp;amp;apos;org.apache.hadoop.hive.ql.io.orc.OrcInputFormat&amp;amp;apos; OUTPUTFORMAT &amp;amp;apos;org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat&amp;amp;apos; TBLPROPERTIES ( &amp;amp;apos;orc.compress&amp;amp;apos;=&amp;amp;apos;NONE&amp;amp;apos;) ; 

insert into calendar values (2010), (2011), (2012); 
select * from calendar; 
+----------------+--+ 
| calendar.year | 
+----------------+--+ 
| 2010 | 
| 2011 | 
| 2012 | 
+----------------+--+ 

select max(year) from calendar; 
| 2012 | 

insert into calendar values (2013); 
select * from calendar; 
+----------------+--+ 
| calendar.year | 
+----------------+--+ 
| 2010 | 
| 2011 | 
| 2012 | 
| 2013 | 
+----------------+--+ 

select max(year) from calendar; 
| 2013 | 

insert into calendar values (2014); 
select max(year) from calendar; 
| 2014 |

analyze table calendar compute statistics for columns;

insert into calendar values (2015);
select max(year) from calendar;
| 2014 |

insert into calendar values (2016), (2017), (2018);
select max(year) from calendar;
| 2014  |

analyze table calendar compute statistics for columns;
select max(year) from calendar;
| 2018  |

</description>
			<version>1.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.plan.StatsWork.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
			<file type="M">org.apache.hive.hcatalog.listener.TestDbNotificationListener.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.StatsOptimizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.StatsTask.java</file>
			<file type="M">org.apache.hadoop.hive.common.StatsSetupConst.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Table.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">13147</link>
			<link type="Reference" description="is related to">3917</link>
		</links>
	</bug>
	<bug id="12758" opendate="2015-12-29 23:04:28" fixdate="2016-01-19 02:48:09" resolution="Fixed">
		<buginformation>
			<summary>Parallel compilation: Operator::resetId() is not thread-safe</summary>
			<description>

  private static AtomicInteger seqId;
...

  public Operator() {
    this(String.valueOf(seqId.getAndIncrement()));
  }

  public static void resetId() {
    seqId.set(0);
  }


Potential race-condition.</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.ListSinkOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TestExecDriver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.ProcessAnalyzeTable.java</file>
			<file type="M">org.apache.hadoop.hive.ql.testutil.BaseScalarUdfTest.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.GenTezWork.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.HashTableDummyOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TestOperators.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.mr.HashTableLoader.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TezDummyStoreOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerMultiKeyOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.TestUpdateDeleteSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.UnionOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorFileSinkOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.FetchWork.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.ColumnStatsWork.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiLongOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.DummyStoreOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.merge.MergeFileTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinOuterStringOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyLongOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.OperatorFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.RCFileMergeOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiGenerateResultOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.mr.ExecMapper.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.ppd.PredicateTransitivePropagate.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveGBOpConvUtil.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.MapredWork.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyMultiKeyOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.util.FakeCaptureOutputOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerGenerateResultOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FunctionTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.TestVectorizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinBaseOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinCommonOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.spark.GenSparkWork.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorLimitOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.GenSparkSkewJoinProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.SerializationUtilities.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorAppMasterEventOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TestPlan.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorSparkPartitionPruningSinkOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOuterFilteredOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.spark.SparkReduceSinkMapJoinProc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Task.java</file>
			<file type="M">org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.UDTFOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorDataSourceOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.TestVectorFilterOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.SparkHashTableSinkOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.CollectOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ColumnStatsUpdateTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Operator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.TestVectorSelectOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.LateralViewForwardOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorSMBMapJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.MergeJoinWork.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.PTFOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.MapWork.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.spark.SparkProcessAnalyzeTable.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyStringOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.AppMasterEventOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.SimpleFetchAggregation.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.ReduceWork.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.DynamicPartitionPruningOptimization.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ReduceSinkMapJoinProc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.correlation.QueryPlanTreeTransformation.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.SkewJoinOptimizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TerminalOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.ppd.SyntheticJoinPredicate.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.TestGenTezWork.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerStringOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ColumnStatsTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyGenerateResultOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.TestVectorLimitOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.GenMRSkewJoinProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkMultiKeyOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.LocalMapJoinProcFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.LateralViewJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinTaskDispatcher.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.hooks.ATSHook.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Context.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkCommonOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkLongOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerLongOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.DemuxOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.SparkTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.OrcFileMergeOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinOuterMultiKeyOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkStringOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.GroupByOptimizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.LimitOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ConditionalTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinOuterLongOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MuxOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.AbstractSMBJoinProc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.SelectOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TemporaryHashSinkOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiStringOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.StatsNoJobTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinOuterGenerateResultOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.spark.SparkPartitionPruningSinkOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.UnionWork.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ForwardOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FilterOperator.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.HashTableLoader.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiMultiKeyOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.BaseWork.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorSparkHashTableSinkOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TestFileSinkOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FetchTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.GenMRTableScan1.java</file>
		</fixedFiles>
	</bug>
	<bug id="12879" opendate="2016-01-15 17:52:41" fixdate="2016-01-19 09:20:55" resolution="Fixed">
		<buginformation>
			<summary>RowResolver of Semijoin not updated in CalcitePlanner</summary>
			<description>When we generate a Calcite plan, we might need to cast the column referenced by equality conditions in a Semijoin because Hive works with a more relaxed data type system.
To cast these columns, we introduce a project operators over the Semijoin inputs. However, these columns were not included in the RowResolver of the Semijoin operator (I guess because they couldn&amp;amp;apos;t be referenced beyond the Semijoin). However, if above the Semijoin a Project operator with a windowing function is generated, the RR for the project is taken from the operator below, resulting in a mismatch.
The following query can be used to reproduce the problem (with CBO on):

CREATE TABLE table_1 (int_col_1 INT, decimal3003_col_2 DECIMAL(30, 3), timestamp_col_3 TIMESTAMP, decimal0101_col_4 DECIMAL(1, 1), double_col_5 DOUBLE, boolean_col_6 BOOLEAN, timestamp_col_7 TIMESTAMP, varchar0098_col_8 VARCHAR(98), int_col_9 INT, timestamp_col_10 TIMESTAMP, decimal0903_col_11 DECIMAL(9, 3), int_col_12 INT, bigint_col_13 BIGINT, boolean_col_14 BOOLEAN, char0254_col_15 CHAR(254), boolean_col_16 BOOLEAN, smallint_col_17 SMALLINT, float_col_18 FLOAT, decimal2608_col_19 DECIMAL(26, 8), varchar0216_col_20 VARCHAR(216), string_col_21 STRING, timestamp_col_22 TIMESTAMP, double_col_23 DOUBLE, smallint_col_24 SMALLINT, float_col_25 FLOAT, decimal2016_col_26 DECIMAL(20, 16), string_col_27 STRING, decimal0202_col_28 DECIMAL(2, 2), boolean_col_29 BOOLEAN, decimal2020_col_30 DECIMAL(20, 20), float_col_31 FLOAT, boolean_col_32 BOOLEAN, varchar0148_col_33 VARCHAR(148), decimal2121_col_34 DECIMAL(21, 21), timestamp_col_35 TIMESTAMP, float_col_36 FLOAT, float_col_37 FLOAT, string_col_38 STRING, decimal3420_col_39 DECIMAL(34, 20), smallint_col_40 SMALLINT, decimal1408_col_41 DECIMAL(14, 8), string_col_42 STRING, decimal0902_col_43 DECIMAL(9, 2), varchar0204_col_44 VARCHAR(204), float_col_45 FLOAT, tinyint_col_46 TINYINT, double_col_47 DOUBLE, timestamp_col_48 TIMESTAMP, double_col_49 DOUBLE, timestamp_col_50 TIMESTAMP, decimal0704_col_51 DECIMAL(7, 4), int_col_52 INT, double_col_53 DOUBLE, int_col_54 INT, timestamp_col_55 TIMESTAMP, decimal0505_col_56 DECIMAL(5, 5), char0155_col_57 CHAR(155), double_col_58 DOUBLE, timestamp_col_59 TIMESTAMP, double_col_60 DOUBLE, float_col_61 FLOAT, char0249_col_62 CHAR(249), float_col_63 FLOAT, smallint_col_64 SMALLINT, decimal1309_col_65 DECIMAL(13, 9), timestamp_col_66 TIMESTAMP, boolean_col_67 BOOLEAN, tinyint_col_68 TINYINT, tinyint_col_69 TINYINT, double_col_70 DOUBLE, bigint_col_71 BIGINT, boolean_col_72 BOOLEAN, float_col_73 FLOAT, char0222_col_74 CHAR(222), boolean_col_75 BOOLEAN, string_col_76 STRING, decimal2612_col_77 DECIMAL(26, 12), bigint_col_78 BIGINT, char0128_col_79 CHAR(128), tinyint_col_80 TINYINT, boolean_col_81 BOOLEAN, int_col_82 INT, boolean_col_83 BOOLEAN, decimal2622_col_84 DECIMAL(26, 22), boolean_col_85 BOOLEAN, boolean_col_86 BOOLEAN, decimal0907_col_87 DECIMAL(9, 7))
STORED AS orc;
CREATE TABLE table_18 (float_col_1 FLOAT, double_col_2 DOUBLE, decimal2518_col_3 DECIMAL(25, 18), boolean_col_4 BOOLEAN, bigint_col_5 BIGINT, boolean_col_6 BOOLEAN, boolean_col_7 BOOLEAN, char0035_col_8 CHAR(35), decimal2709_col_9 DECIMAL(27, 9), timestamp_col_10 TIMESTAMP, bigint_col_11 BIGINT, decimal3604_col_12 DECIMAL(36, 4), string_col_13 STRING, timestamp_col_14 TIMESTAMP, timestamp_col_15 TIMESTAMP, decimal1911_col_16 DECIMAL(19, 11), boolean_col_17 BOOLEAN, tinyint_col_18 TINYINT, timestamp_col_19 TIMESTAMP, timestamp_col_20 TIMESTAMP, tinyint_col_21 TINYINT, float_col_22 FLOAT, timestamp_col_23 TIMESTAMP)
STORED AS orc;

explain
SELECT
    COALESCE(498,
      LEAD(COALESCE(-973, -684, 515)) OVER (
        PARTITION BY (t2.tinyint_col_21 + t1.smallint_col_24)
        ORDER BY (t2.tinyint_col_21 + t1.smallint_col_24),
        FLOOR(t1.double_col_60) DESC),
      524) AS int_col
FROM table_1 t1 INNER JOIN table_18 t2
ON (((t2.tinyint_col_18) = (t1.bigint_col_13))
    AND ((t2.decimal2709_col_9) = (t1.decimal1309_col_65)))
    AND ((t2.tinyint_col_21) = (t1.tinyint_col_46))
WHERE (t2.tinyint_col_21) IN (
        SELECT COALESCE(-92, -994) AS int_col_3
        FROM table_1 tt1 INNER JOIN table_18 tt2
        ON (tt2.decimal1911_col_16) = (tt1.decimal1309_col_65)
        WHERE (tt1.timestamp_col_66) = (tt2.timestamp_col_19));

</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
		</fixedFiles>
	</bug>
	<bug id="12820" opendate="2016-01-08 23:48:10" fixdate="2016-01-19 19:12:56" resolution="Fixed">
		<buginformation>
			<summary>Remove the check if carriage return and new line are used for separator or escape character</summary>
			<description>The change in HIVE-11785 doesn&amp;amp;apos;t allow \r or \n to be used as separator or escape character which may break some existing tables which uses \r as separator or escape character e.g..
This case actually can be supported regardless of SERIALIZATION_ESCAPE_CRLF set or not.</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazySerDeParameters.java</file>
		</fixedFiles>
		<links>
			<link type="dependent" description="is depended upon by">11785</link>
		</links>
	</bug>
	<bug id="12657" opendate="2015-12-11 21:42:10" fixdate="2016-01-19 20:04:46" resolution="Fixed">
		<buginformation>
			<summary>selectDistinctStar.q results differ with jdk 1.7 vs jdk 1.8</summary>
			<description>Encountered this issue when analysing test failures of HIVE-12609. selectDistinctStar.q produces the following diff when I ran with java version "1.7.0_55" and java version "1.8.0_60"


&amp;lt; 128   val_128 128     
---
&amp;gt; 128           128     val_128
1770c1770
&amp;lt; 224   val_224 224     
---
&amp;gt; 224           224     val_224
1776c1776
&amp;lt; 369   val_369 369     
---
&amp;gt; 369           369     val_369
1799,1810c1799,1810
&amp;lt; 146   val_146 146     val_146 146     val_146 2008-04-08      11
&amp;lt; 150   val_150 150     val_150 150     val_150 2008-04-08      11
&amp;lt; 213   val_213 213     val_213 213     val_213 2008-04-08      11
&amp;lt; 238   val_238 238     val_238 238     val_238 2008-04-08      11
&amp;lt; 255   val_255 255     val_255 255     val_255 2008-04-08      11
&amp;lt; 273   val_273 273     val_273 273     val_273 2008-04-08      11
&amp;lt; 278   val_278 278     val_278 278     val_278 2008-04-08      11
&amp;lt; 311   val_311 311     val_311 311     val_311 2008-04-08      11
&amp;lt; 401   val_401 401     val_401 401     val_401 2008-04-08      11
&amp;lt; 406   val_406 406     val_406 406     val_406 2008-04-08      11
&amp;lt; 66    val_66  66      val_66  66      val_66  2008-04-08      11
&amp;lt; 98    val_98  98      val_98  98      val_98  2008-04-08      11
---
&amp;gt; 146   val_146 2008-04-08      11      146     val_146 146     val_146
&amp;gt; 150   val_150 2008-04-08      11      150     val_150 150     val_150
&amp;gt; 213   val_213 2008-04-08      11      213     val_213 213     val_213
&amp;gt; 238   val_238 2008-04-08      11      238     val_238 238     val_238
&amp;gt; 255   val_255 2008-04-08      11      255     val_255 255     val_255
&amp;gt; 273   val_273 2008-04-08      11      273     val_273 273     val_273
&amp;gt; 278   val_278 2008-04-08      11      278     val_278 278     val_278
&amp;gt; 311   val_311 2008-04-08      11      311     val_311 311     val_311
&amp;gt; 401   val_401 2008-04-08      11      401     val_401 401     val_401
&amp;gt; 406   val_406 2008-04-08      11      406     val_406 406     val_406
&amp;gt; 66    val_66  2008-04-08      11      66      val_66  66      val_66
&amp;gt; 98    val_98  2008-04-08      11      98      val_98  98      val_98
4212c4212

</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.RowResolver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.translator.JoinCondTypeCheckProcFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
		</fixedFiles>
	</bug>
	<bug id="12682" opendate="2015-12-15 22:13:47" fixdate="2016-01-19 20:27:56" resolution="Fixed">
		<buginformation>
			<summary>Reducers in dynamic partitioning job spend a lot of time running hadoop.conf.Configuration.getOverlay</summary>
			<description>I tested this on Hive 1.2.1 but looks like it&amp;amp;apos;s still applicable to 2.0.
I ran this query:


create table flights (

)
PARTITIONED BY (Year int)
CLUSTERED BY (Month)
SORTED BY (DayofMonth) into 12 buckets
STORED AS ORC
TBLPROPERTIES("orc.bloom.filter.columns"="*")
;


(Taken from here: https://github.com/t3rmin4t0r/all-airlines-data/blob/master/ddl/orc.sql)
I profiled just the reduce phase and noticed something odd, the attached graph shows where time was spent during the reducer phase.

Problem seems to relate to https://github.com/apache/hive/blob/branch-2.0/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java#L903
/cc Gopal V</description>
			<version>1.2.1</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
		</fixedFiles>
	</bug>
	<bug id="12864" opendate="2016-01-13 19:17:54" fixdate="2016-01-21 18:38:50" resolution="Fixed">
		<buginformation>
			<summary>StackOverflowError parsing queries with very large predicates</summary>
			<description>We have seen that queries with very large predicates might fail with the following stacktrace:

016-01-12 05:47:36,516|beaver.machine|INFO|552|5072|Thread-22|Exception in thread "main" java.lang.StackOverflowError

2016-01-12 05:47:36,517|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:145)

2016-01-12 05:47:36,517|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,517|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,517|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,517|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,520|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,520|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,520|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,520|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,520|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,520|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,520|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,523|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,523|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,523|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,523|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,523|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,523|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,525|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,525|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,525|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,525|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,525|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,525|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,634|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:37,582|main|INFO|552|4568|MainThread|TEST "test_WideQuery" FAILED in 10.95 seconds


The problem could be solved by reimplementing some of the parsing methods so they are iterative instead of recursive.</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SubQueryUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.ASTNode.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
	</bug>
	<bug id="12865" opendate="2016-01-13 19:21:33" fixdate="2016-01-21 18:47:49" resolution="Fixed">
		<buginformation>
			<summary>Exchange partition does not show inputs field for post/pre execute hooks</summary>
			<description>The pre/post execute hook interface has fields that indicate which Hive objects were read / written to as a result of running the query. For the exchange partition operation, the read entity field is empty.
This is an important issue as the hook interface may be configured to perform critical warehouse operations.
See
ql/src/test/results/clientpositive/exchange_partition3.q.out


--- a/ql/src/test/results/clientpositive/exchange_partition3.q.out
+++ b/ql/src/test/results/clientpositive/exchange_partition3.q.out
@@ -65,9 +65,17 @@ ds=2013-04-05/hr=2
 PREHOOK: query: -- This will exchange both partitions hr=1 and hr=2
 ALTER TABLE exchange_part_test1 EXCHANGE PARTITION (ds=&amp;amp;apos;2013-04-05&amp;amp;apos;) WITH TABLE exchange_part_test2
 PREHOOK: type: ALTERTABLE_EXCHANGEPARTITION
+PREHOOK: Output: default@exchange_part_test1
+PREHOOK: Output: default@exchange_part_test2
 POSTHOOK: query: -- This will exchange both partitions hr=1 and hr=2
 ALTER TABLE exchange_part_test1 EXCHANGE PARTITION (ds=&amp;amp;apos;2013-04-05&amp;amp;apos;) WITH TABLE exchange_part_test2
 POSTHOOK: type: ALTERTABLE_EXCHANGEPARTITION
+POSTHOOK: Output: default@exchange_part_test1
+POSTHOOK: Output: default@exchange_part_test1@ds=2013-04-05/hr=1
+POSTHOOK: Output: default@exchange_part_test1@ds=2013-04-05/hr=2
+POSTHOOK: Output: default@exchange_part_test2
+POSTHOOK: Output: default@exchange_part_test2@ds=2013-04-05/hr=1
+POSTHOOK: Output: default@exchange_part_test2@ds=2013-04-05/hr=2
 PREHOOK: query: SHOW PARTITIONS exchange_part_test1
 PREHOOK: type: SHOWPARTITIONS
 PREHOOK: Input: default@exchange_part_test1

</description>
			<version>0.12.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
		</fixedFiles>
	</bug>
	<bug id="12353" opendate="2015-11-05 21:29:12" fixdate="2016-01-22 03:02:50" resolution="Fixed">
		<buginformation>
			<summary>When Compactor fails it calls CompactionTxnHandler.markedCleaned().  it should not.</summary>
			<description>One of the things that this method does is delete entries from TXN_COMPONENTS for partition that it was trying to compact.
This causes Aborted transactions in TXNS to become empty according to
CompactionTxnHandler.cleanEmptyAbortedTxns() which means they can now be deleted.  
Once they are deleted, data that belongs to these txns is deemed committed...
We should extend COMPACTION_QUEUE state with &amp;amp;apos;f&amp;amp;apos; and &amp;amp;apos;s&amp;amp;apos; (failed, success) states.  We should also not delete then entry from markedCleaned()
We&amp;amp;apos;ll have separate process that cleans &amp;amp;apos;f&amp;amp;apos; and &amp;amp;apos;s&amp;amp;apos; records after X minutes (or after &amp;gt; N records for a given partition exist).
This allows SHOW COMPACTIONS to show some history info and how many times compaction failed on a given partition (subject to retention interval) so that we don&amp;amp;apos;t have to call markCleaned() on Compactor failures at the same time preventing Compactor to constantly getting stuck on the same bad partition/table.
Ideally we&amp;amp;apos;d want to include END_TIME field.</description>
			<version>1.0.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.txn.compactor.Initiator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.txn.compactor.Worker.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.java</file>
			<file type="M">org.apache.hadoop.hive.ql.txn.AcidHouseKeeperService.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TestCompactionTxnHandler.java</file>
			<file type="M">org.apache.hadoop.hive.ql.txn.compactor.TestWorker.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
			<file type="M">org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
			<file type="M">org.apache.hadoop.hive.ql.txn.compactor.Cleaner.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnDbUtil.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HouseKeeperService.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.CompactionInfo.java</file>
			<file type="M">org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java</file>
			<file type="M">org.apache.hadoop.hive.ql.txn.compactor.TestCleaner.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">11994</link>
			<link type="Blocker" description="is blocked by">12832</link>
			<link type="Blocker" description="is blocked by">12352</link>
			<link type="Reference" description="is related to">12352</link>
			<link type="Reference" description="is related to">13691</link>
			<link type="Reference" description="is related to">11444</link>
			<link type="Required" description="requires">13353</link>
		</links>
	</bug>
	<bug id="12809" opendate="2016-01-08 02:52:10" fixdate="2016-01-22 23:47:56" resolution="Fixed">
		<buginformation>
			<summary>Vectorization: fast-path for coalesce if input.noNulls = true</summary>
			<description>Coalesce can skip processing other columns, if all the input columns are non-null.
Possibly retaining, isRepeating=true.</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.VectorCoalesce.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">12750</link>
		</links>
	</bug>
	<bug id="12911" opendate="2016-01-22 11:24:40" fixdate="2016-01-24 11:12:51" resolution="Fixed">
		<buginformation>
			<summary>PPD might get exercised even when flag is false if CBO is on</summary>
			<description>Introduced in HIVE-11865.</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
		</fixedFiles>
		<links>
			<link type="Regression" description="is broken by">11865</link>
		</links>
	</bug>
	<bug id="12867" opendate="2016-01-13 21:28:40" fixdate="2016-01-25 18:36:08" resolution="Fixed">
		<buginformation>
			<summary>Semantic Exception Error Msg should be with in the range of "10000 to 19999"</summary>
			<description>At many places errors encountered during semantic exception is translated as generic error(GENERIC_ERROR, 40000) msg as opposed to semantic error msg.</description>
			<version>1.2.1</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.EximUtil.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.ExportSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
			<file type="M">org.apache.hadoop.hive.ql.ErrorMsg.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.CreateTableDesc.java</file>
		</fixedFiles>
	</bug>
	<bug id="12797" opendate="2016-01-06 23:01:24" fixdate="2016-01-26 01:00:04" resolution="Fixed">
		<buginformation>
			<summary>Synchronization issues with tez/llap session pool in hs2</summary>
			<description>The changes introduced as part of HIVE-12674 causes issues while shutting down hs2 when session pools are used.


java.util.ConcurrentModificationException
        at java.util.LinkedList$ListItr.checkForComodification(LinkedList.java:966) ~[?:1.8.0_45]
        at java.util.LinkedList$ListItr.remove(LinkedList.java:921) ~[?:1.8.0_45]
        at org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.stop(TezSessionPoolManager.java:288) ~[hive-exec-2.0.0.2.3.5.0-79.jar:2.0.0.2.3.5.0-79]
        at org.apache.hive.service.server.HiveServer2.stop(HiveServer2.java:479) [hive-jdbc-2.0.0.2.3.5.0-79-standalone.jar:2.0.0.2.3.5.0-79]
        at org.apache.hive.service.server.HiveServer2$2.run(HiveServer2.java:183) [hive-jdbc-2.0.0.2.3.5.0-79-standalone.jar:2.0.0.2.3.5.0-79]

</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.java</file>
		</fixedFiles>
		<links>
			<link type="Cloners" description="is cloned by">12926</link>
		</links>
	</bug>
	<bug id="12904" opendate="2016-01-21 19:45:17" fixdate="2016-01-26 03:18:50" resolution="Fixed">
		<buginformation>
			<summary>LLAP: deadlock in task scheduling</summary>
			<description>
Thread 34107: (state = BLOCKED)
 - org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$TaskWrapper.isInWaitQueue() @bci=0, line=690 (Compiled frame)
 - org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.finishableStateUpdated(org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$TaskWrapper, boolean) @bci=8, line=485 (Compiled frame)
 - org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.access$1500(org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService, org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$TaskWrapper, boolean) @bci=3, line=78 (Compiled frame)
 - org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$TaskWrapper.finishableStateUpdated(boolean) @bci=27, line=733 (Compiled frame)
 - org.apache.hadoop.hive.llap.daemon.impl.QueryInfo$FinishableStateTracker.sourceStateUpdated(java.lang.String) @bci=76, line=210 (Compiled frame)
 - org.apache.hadoop.hive.llap.daemon.impl.QueryInfo.sourceStateUpdated(java.lang.String) @bci=5, line=164 (Compiled frame)
 - org.apache.hadoop.hive.llap.daemon.impl.QueryTracker.registerSourceStateChange(java.lang.String, java.lang.String, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SourceStateProto) @bci=34, line=228 (Compiled frame)
 - org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.sourceStateUpdated(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SourceStateUpdatedRequestProto) @bci=47, line=255 (Compiled frame)
 - org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.sourceStateUpdated(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SourceStateUpdatedRequestProto) @bci=5, line=328 (Compiled frame)
 - org.apache.hadoop.hive.llap.daemon.impl.LlapDaemonProtocolServerImpl.sourceStateUpdated(com.google.protobuf.RpcController, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SourceStateUpdatedRequestProto) @bci=5, line=105 (Compiled frame)
 - org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$LlapDaemonProtocol$2.callBlockingMethod(com.google.protobuf.Descriptors$MethodDescriptor, com.google.protobuf.RpcController, com.google.protobuf.Message) @bci=80, line=13067 (Compiled frame)
 - org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(org.apache.hadoop.ipc.RPC$Server, java.lang.String, org.apache.hadoop.io.Writable, long) @bci=246, line=616 (Compiled frame)
 - org.apache.hadoop.ipc.RPC$Server.call(org.apache.hadoop.ipc.RPC$RpcKind, java.lang.String, org.apache.hadoop.io.Writable, long) @bci=9, line=969 (Compiled frame)
 - org.apache.hadoop.ipc.Server$Handler$1.run() @bci=38, line=2151 (Compiled frame)
 - org.apache.hadoop.ipc.Server$Handler$1.run() @bci=1, line=2147 (Compiled frame)
 - java.security.AccessController.doPrivileged(java.security.PrivilegedExceptionAction, java.security.AccessControlContext) @bci=0 (Compiled frame)
 - javax.security.auth.Subject.doAs(javax.security.auth.Subject, java.security.PrivilegedExceptionAction) @bci=42, line=422 (Compiled frame)
 - org.apache.hadoop.security.UserGroupInformation.doAs(java.security.PrivilegedExceptionAction) @bci=14, line=1657 (Compiled frame)
 - org.apache.hadoop.ipc.Server$Handler.run() @bci=315, line=2145 (Interpreted frame)


and 


Thread 34500: (state = BLOCKED)
 - org.apache.hadoop.hive.llap.daemon.impl.QueryInfo$FinishableStateTracker.unregisterForUpdates(org.apache.hadoop.hive.llap.daemon.FinishableStateUpdateHandler) @bci=0, line=195 (Compiled frame)
 - org.apache.hadoop.hive.llap.daemon.impl.QueryInfo.unregisterFinishableStateUpdate(org.apache.hadoop.hive.llap.daemon.FinishableStateUpdateHandler) @bci=5, line=160 (Compiled frame)
 - org.apache.hadoop.hive.llap.daemon.impl.QueryFragmentInfo.unregisterForFinishableStateUpdates(org.apache.hadoop.hive.llap.daemon.FinishableStateUpdateHandler) @bci=5, line=143 (Compiled frame)
 - org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$TaskWrapper.maybeUnregisterForFinishedStateNotifications() @bci=20, line=681 (Compiled frame)
 - org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$InternalCompletionListener.onSuccess(org.apache.tez.runtime.task.TaskRunner2Result) @bci=32, line=548 (Compiled frame)
 - org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$InternalCompletionListener.onSuccess(java.lang.Object) @bci=5, line=535 (Compiled frame)
 - com.google.common.util.concurrent.Futures$4.run() @bci=55, line=1149 (Compiled frame)
 - java.util.concurrent.ThreadPoolExecutor.runWorker(java.util.concurrent.ThreadPoolExecutor$Worker) @bci=95, line=1142 (Compiled frame)
 - java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=5, line=617 (Interpreted frame)
 - java.lang.Thread.run() @bci=11, line=745 (Interpreted frame)

"IPC Server handler 0 on 15001":
  waiting to lock Monitor@0x00007f5d322ecb08 (Object@0x00007f67032cd2c0, a org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorService$TaskWrapper),
  which is held by "ExecutionCompletionThread #0"
"ExecutionCompletionThread #0":
  waiting to lock Monitor@0x00007f6066b9e8c8 (Object@0x00007f66b6570200, a org/apache/hadoop/hive/llap/daemon/impl/QueryInfo$FinishableStateTracker),
  which is held by "IPC Server handler 0 on 15001"

Found a total of 1 deadlock.



Looks like it&amp;amp;apos;s caused by synchronized blocks:

TaskWrapper:
public synchronized void maybeUnregisterForFinishedStateNotifications


Eventually calls 

FinishableStateTracker
synchronized void unregisterForUpdates(FinishableStateUpdateHandler handler) {


and 

FST
 synchronized void sourceStateUpdated(String sourceName) {
   

eventually calls

 public synchronized boolean isInWaitQueue() {


The latter returns the boolean, so it definitely doesn&amp;amp;apos;t need synchronized, however I don&amp;amp;apos;t know if there are other similar issues and what is necessary inside sync blocks, perhaps there&amp;amp;apos;s a better fix.
Overall I&amp;amp;apos;d say synch methods on objects that call any other non-trivial objects should not be used. Perhaps for now it would be good to replace all sync methods by sync blocks that cover entire method, as well as remove the unnecessary ones like the isWait... one. Then the scope of the blocks can be adjusted based on logic in future.</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.llap.daemon.impl.QueryInfo.java</file>
			<file type="M">org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
		</fixedFiles>
	</bug>
	<bug id="12905" opendate="2016-01-21 20:13:40" fixdate="2016-01-26 04:54:50" resolution="Fixed">
		<buginformation>
			<summary>Issue with mapjoin in tez under certain conditions</summary>
			<description>In a specific case where we have an outer join followed by another join on the same key and the non-outer side of the outer join is empty, hive-on-tez produces incorrect results.</description>
			<version>1.0.1</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.MapJoinKey.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastHashTableLoader.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastTableContainer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.HashTableLoader.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.HashTableLoader.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe.java</file>
		</fixedFiles>
	</bug>
	<bug id="12478" opendate="2015-11-20 08:38:11" fixdate="2016-01-27 08:50:21" resolution="Fixed">
		<buginformation>
			<summary>Improve Hive/Calcite Transitive Predicate inference</summary>
			<description>HiveJoinPushTransitivePredicatesRule does not pull up predicates for transitive inference if they contain more than one column.


EXPLAIN select * from srcpart join (select ds as ds, ds as `date` from srcpart where  (ds = &amp;amp;apos;2008-04-08&amp;amp;apos; and value=1)) s on (srcpart.ds = s.ds);

</description>
			<version>2.0.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="D">org.apache.hadoop.hive.ql.optimizer.calcite.HiveVolcanoPlannerContext.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveJoin.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.TestCBORuleFiredOnlyOnce.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.cost.HiveCostModel.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveSemiJoin.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveRulesRegistry.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
			<file type="D">org.apache.hadoop.hive.ql.optimizer.calcite.HiveHepPlannerContext.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.HiveCalciteUtil.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinPushTransitivePredicatesRule.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinAddNotNullRule.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveUnion.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.cost.HiveVolcanoPlanner.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HivePreFilteringRule.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">11110</link>
			<link type="Reference" description="is related to">12543</link>
		</links>
	</bug>
	<bug id="12926" opendate="2016-01-26 00:59:47" fixdate="2016-01-27 22:49:16" resolution="Fixed">
		<buginformation>
			<summary>Another synchronization issue with tez/llap session pool in hs2</summary>
			<description>The changes introduced as part of HIVE-12674 causes issues while shutting down hs2 when session pools are used.


java.util.ConcurrentModificationException
        at java.util.LinkedList$ListItr.checkForComodification(LinkedList.java:966) ~[?:1.8.0_45]
        at java.util.LinkedList$ListItr.remove(LinkedList.java:921) ~[?:1.8.0_45]
        at org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.stop(TezSessionPoolManager.java:288) ~[hive-exec-2.0.0.2.3.5.0-79.jar:2.0.0.2.3.5.0-79]
        at org.apache.hive.service.server.HiveServer2.stop(HiveServer2.java:479) [hive-jdbc-2.0.0.2.3.5.0-79-standalone.jar:2.0.0.2.3.5.0-79]
        at org.apache.hive.service.server.HiveServer2$2.run(HiveServer2.java:183) [hive-jdbc-2.0.0.2.3.5.0-79-standalone.jar:2.0.0.2.3.5.0-79]

</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.java</file>
		</fixedFiles>
		<links>
			<link type="Cloners" description="is a clone of">12797</link>
		</links>
	</bug>
	<bug id="12933" opendate="2016-01-26 18:51:48" fixdate="2016-01-28 19:28:09" resolution="Fixed">
		<buginformation>
			<summary>Beeline will hang when authenticating with PAM when libjpam.so is missing</summary>
			<description>When we setup PAM authentication, we need to have libjpam.so under java.library.path. If it happens to misplace the .so file, rather than giving an exception, the client will hang forever.
Seems we should catch the exception when the lib is missing.</description>
			<version>2.0.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.service.auth.PamAuthenticationProviderImpl.java</file>
		</fixedFiles>
	</bug>
	<bug id="12969" opendate="2016-01-31 01:35:34" fixdate="2016-01-31 20:38:05" resolution="Fixed">
		<buginformation>
			<summary>Fix Javadoc for PredicatePushDown class</summary>
			<description>Fix Javadocs for hive.optimize.ppd - Default Value: true
Added In: Hive 0.4.0 with HIVE-279, default changed to true in Hive 0.4.0 with HIVE-626
NO PRECOMMIT TESTS</description>
			<version>0.4.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.ppd.PredicatePushDown.java</file>
		</fixedFiles>
	</bug>
	<bug id="12893" opendate="2016-01-20 03:05:02" fixdate="2016-01-31 20:43:19" resolution="Fixed">
		<buginformation>
			<summary>Sorted dynamic partition does not work if subset of partition columns are constant folded</summary>
			<description>If all partition columns are constant folded then sorted dynamic partitioning should not be used as it is similar to static partitioning. But if only subset of partition columns are constant folded sorted dynamic partition optimization will be helpful. Currently, this optimization is disabled if atleast one partition column constant folded.</description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.BucketingSortingReduceSinkOptimizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">13546</link>
		</links>
	</bug>
	<bug id="12945" opendate="2016-01-27 19:08:18" fixdate="2016-02-01 18:39:40" resolution="Fixed">
		<buginformation>
			<summary>Bucket pruning: bucketing for -ve hashcodes have historical issues</summary>
			<description>The different ETL pathways differed in reducer choice slightly for -ve hashcodes.


(hashCode &amp;amp; Integer.MAX_VALUE) % numberOfBuckets;
!=
Math.abs(hashCode) % numberOfBuckets


Add a backwards compat option, which can be used to protect against old data left over from 0.13.</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.FixedBucketPruningOptimizer.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
	</bug>
	<bug id="12931" opendate="2016-01-26 16:00:17" fixdate="2016-02-01 19:17:47" resolution="Fixed">
		<buginformation>
			<summary>Shuffle tokens stay around forever in LLAP</summary>
			<description>Shuffle tokens are never cleaned up, resulting in a slow leak.</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
			<file type="M">org.apache.hadoop.hive.llap.daemon.impl.QueryTracker.java</file>
			<file type="M">org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler.java</file>
		</fixedFiles>
	</bug>
	<bug id="12964" opendate="2016-01-29 20:05:00" fixdate="2016-02-01 19:27:00" resolution="Fixed">
		<buginformation>
			<summary>TestOperationLoggingAPIWithMr,TestOperationLoggingAPIWithTez fail on branch-2.0 (with Java 7, at least)</summary>
			<description></description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.service.cli.operation.LogDivertAppender.java</file>
		</fixedFiles>
	</bug>
	<bug id="12947" opendate="2016-01-27 22:23:18" fixdate="2016-02-01 21:04:01" resolution="Fixed">
		<buginformation>
			<summary>SMB join in tez has ClassCastException when container reuse is on</summary>
			<description>SMB join in tez has multiple work items that are connected based on input tag followed by input initialization etc. In case of container re-use, what ends up happening is that we try to reconnect the work items and fail. If we try to work around that issue by recognizing somehow that the cache was in play, we will run into other initialization issues with respect to record readers. So the plan is to disable caching of the SMB work items by clearing out during the close phase.


java.lang.RuntimeException: Map operator initialization failed
        at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:247)
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:147)
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)
        at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:344)
        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)
        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)
        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)
        at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.FileSinkOperator cannot be cast to org.apache.hadoop.hive.ql.exec.DummyStoreOperator
        at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getJoinParentOp(MapRecordProcessor.java:300)
        at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getJoinParentOp(MapRecordProcessor.java:302)
        at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getJoinParentOp(MapRecordProcessor.java:302)
        at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getJoinParentOp(MapRecordProcessor.java:302)
        at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getJoinParentOp(MapRecordProcessor.java:302)
        at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:189)
        ... 15 more

</description>
			<version>2.0.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.java</file>
		</fixedFiles>
	</bug>
	<bug id="12951" opendate="2016-01-28 00:45:31" fixdate="2016-02-04 18:45:45" resolution="Fixed">
		<buginformation>
			<summary>Reduce Spark executor prewarm timeout to 5s</summary>
			<description>Currently it&amp;amp;apos;s set to 30s, which tends to be longer than needed. Reduce it to 5s, only considering jvm startup time. (Eventually, we may want to make this configurable.)</description>
			<version>1.2.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">11363</link>
		</links>
	</bug>
	<bug id="12790" opendate="2016-01-06 17:46:27" fixdate="2016-02-05 14:10:14" resolution="Fixed">
		<buginformation>
			<summary>Metastore connection leaks in HiveServer2</summary>
			<description>HiveServer2 keeps opening new connections to HMS each time it launches a task. These connections do not appear to be closed when the task completes thus causing a HMS connection leak. "lsof" for the HS2 process shows connections to port 9083.


2015-12-03 04:20:56,352 INFO  [HiveServer2-Background-Pool: Thread-424756()]: ql.Driver (SessionState.java:printInfo(558)) - Launching Job 11 out of 41
2015-12-03 04:20:56,354 INFO  [Thread-405728()]: hive.metastore (HiveMetaStoreClient.java:open(311)) - Trying to connect to metastore with URI thrift://&amp;lt;anonymizedURL&amp;gt;:9083
2015-12-03 04:20:56,360 INFO  [Thread-405728()]: hive.metastore (HiveMetaStoreClient.java:open(351)) - Opened a connection to metastore, current connections: 14824
2015-12-03 04:20:56,360 INFO  [Thread-405728()]: hive.metastore (HiveMetaStoreClient.java:open(400)) - Connected to metastore.
....
2015-12-03 04:21:06,355 INFO  [HiveServer2-Background-Pool: Thread-424756()]: ql.Driver (SessionState.java:printInfo(558)) - Launching Job 12 out of 41
2015-12-03 04:21:06,357 INFO  [Thread-405756()]: hive.metastore (HiveMetaStoreClient.java:open(311)) - Trying to connect to metastore with URI thrift://&amp;lt;anonymizedURL&amp;gt;:9083
2015-12-03 04:21:06,362 INFO  [Thread-405756()]: hive.metastore (HiveMetaStoreClient.java:open(351)) - Opened a connection to metastore, current connections: 14825
2015-12-03 04:21:06,362 INFO  [Thread-405756()]: hive.metastore (HiveMetaStoreClient.java:open(400)) - Connected to metastore.
...
2015-12-03 04:21:08,357 INFO  [HiveServer2-Background-Pool: Thread-424756()]: ql.Driver (SessionState.java:printInfo(558)) - Launching Job 13 out of 41
2015-12-03 04:21:08,360 INFO  [Thread-405782()]: hive.metastore (HiveMetaStoreClient.java:open(311)) - Trying to connect to metastore with URI thrift://&amp;lt;anonymizedURL&amp;gt;:9083
2015-12-03 04:21:08,364 INFO  [Thread-405782()]: hive.metastore (HiveMetaStoreClient.java:open(351)) - Opened a connection to metastore, current connections: 14826
2015-12-03 04:21:08,365 INFO  [Thread-405782()]: hive.metastore (HiveMetaStoreClient.java:open(400)) - Connected to metastore.
... 


The TaskRunner thread starts a new SessionState each time, which creates a new connection to the HMS (via Hive.get(conf).getMSC()) that is never closed.
Even SessionState.close(), currently not being called by the TaskRunner thread, does not close this connection.
Attaching a anonymized log snippet where the number of HMS connections reaches north of 25000+ connections.</description>
			<version>1.1.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.session.SessionState.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TaskRunner.java</file>
		</fixedFiles>
	</bug>
	<bug id="12885" opendate="2016-01-18 17:07:50" fixdate="2016-02-05 20:56:04" resolution="Fixed">
		<buginformation>
			<summary>LDAP Authenticator improvements</summary>
			<description>Currently Hive&amp;amp;apos;s LDAP Atn provider assumes certain defaults to keep its configuration simple. 
1) One of the assumptions is the presence of an attribute "distinguishedName". In certain non-standard LDAP implementations, this attribute may not be available. So instead of basing all ldap searches on this attribute, getNameInNamespace() returns the same value. So this API is to be used instead.
2) It also assumes that the "user" value being passed in, will be able to bind to LDAP. However, certain LDAP implementations, by default, only allow the full DN to be used, just short user names are not permitted. We will need to be able to support short names too when hive configuration only has "BaseDN" specified (not userDNPatterns). So instead of hard-coding "uid" or "CN" as keys for the short usernames, it probably better to make this a configurable parameter.</description>
			<version>1.1.0</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.service.auth.LdapAuthenticationProviderImpl.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">7193</link>
		</links>
	</bug>
	<bug id="12990" opendate="2016-02-03 08:37:27" fixdate="2016-02-06 00:10:33" resolution="Fixed">
		<buginformation>
			<summary>LLAP: ORC cache NPE without FileID support</summary>
			<description>

   OrcBatchKey stripeKey = hasFileId ? new OrcBatchKey(fileId, -1, 0) : null;
   ...
          if (hasFileId &amp;amp;&amp;amp; metadataCache != null) {
            stripeKey.stripeIx = stripeIx;
            stripeMetadata = metadataCache.getStripeMetadata(stripeKey);
          }
...
  public void setStripeMetadata(OrcStripeMetadata m) {
    assert stripes != null;
    stripes[m.getStripeIx()] = m;
  }




Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.llap.io.metadata.OrcStripeMetadata.getStripeIx(OrcStripeMetadata.java:106)
        at org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.setStripeMetadata(OrcEncodedDataConsumer.java:70)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.readStripesMetadata(OrcEncodedDataReader.java:685)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.performDataRead(OrcEncodedDataReader.java:283)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$4.run(OrcEncodedDataReader.java:215)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$4.run(OrcEncodedDataReader.java:212)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:212)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:93)
        ... 5 more

</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0, 2.0.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
			<file type="M">org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
		</fixedFiles>
	</bug>
	<bug id="12999" opendate="2016-02-04 03:29:57" fixdate="2016-02-09 02:20:24" resolution="Fixed">
		<buginformation>
			<summary>Tez: Vertex creation reduce NN IPCs</summary>
			<description>Tez vertex building has a decidedly slow path in the code, which is not related to the DAG plan at all.
The total number of RPC calls is not related to the total number of operators, due to a bug in the DagUtils inner loops.


	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1877)
	at org.apache.hadoop.hive.ql.exec.Utilities.createTmpDirs(Utilities.java:3207)
	at org.apache.hadoop.hive.ql.exec.Utilities.createTmpDirs(Utilities.java:3170)
	at org.apache.hadoop.hive.ql.exec.tez.DagUtils.createVertex(DagUtils.java:548)
	at org.apache.hadoop.hive.ql.exec.tez.DagUtils.createVertex(DagUtils.java:1151)
	at org.apache.hadoop.hive.ql.exec.tez.TezTask.build(TezTask.java:388)
	at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:175)

</description>
			<version>1.2.0</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
		</fixedFiles>
	</bug>
	<bug id="13016" opendate="2016-02-05 21:32:32" fixdate="2016-02-09 19:39:23" resolution="Fixed">
		<buginformation>
			<summary>ORC FileDump recovery utility fails in Windows</summary>
			<description>org.apache.hive.hcatalog.streaming.TestStreaming.testFileDumpCorruptDataFiles
org.apache.hive.hcatalog.streaming.TestStreaming.testFileDumpCorruptSideFiles
java.io.IOException: Unable to move file:/E:/hive/hcatalog/streaming/target/tmp/junit4129594478393496260/testing3.db/dimensionTable/delta_0000001_0000002/bucket_00000 to E:/hive/hcatalog/streaming/target/tmp/E:/hive/hcatalog/streaming/target/tmp/junit4129594478393496260/testing3.db/dimensionTable/delta_0000001_0000002/bucket_00000
        at org.apache.hadoop.hive.ql.io.orc.FileDump.moveFiles(FileDump.java:546)^M
        at org.apache.hadoop.hive.ql.io.orc.FileDump.recoverFile(FileDump.java:513)^M
        at org.apache.hadoop.hive.ql.io.orc.FileDump.recoverFiles(FileDump.java:428)^M
        at org.apache.hadoop.hive.ql.io.orc.FileDump.main(FileDump.java:125)^M
        at org.apache.hive.hcatalog.streaming.TestStreaming.testFileDumpCorruptSideFiles(TestStreaming.java:1523)^M
Note that FileDump appends the full source path to the backup path when trying to recover files (see "E:" in the middle of the destination path).</description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.FileDump.java</file>
		</fixedFiles>
	</bug>
	<bug id="1608" opendate="2010-08-31 18:52:57" fixdate="2016-02-09 19:58:04" resolution="Fixed">
		<buginformation>
			<summary>use sequencefile as the default for storing intermediate results</summary>
			<description>The only argument for having a text file for storing intermediate results seems to be better debuggability.
But, tailing a sequence file is possible, and it should be more space efficient</description>
			<version>0.7.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
		<links>
			<link type="Container" description="Is contained by">8591</link>
			<link type="Reference" description="is related to">3065</link>
			<link type="Reference" description="is related to">1598</link>
		</links>
	</bug>
	<bug id="12993" opendate="2016-02-03 19:46:17" fixdate="2016-02-09 20:55:47" resolution="Fixed">
		<buginformation>
			<summary>user and password supplied from URL is overwritten by the empty user and password of the JDBC connection string when it&amp;apos;s calling from beeline</summary>
			<description>When we make the call beeline -u "jdbc:hive2://localhost:10000/;user=aaa;password=bbb", the user and password are overwritten by the blank ones since internally it constructs a "connect &amp;lt;url&amp;gt; &amp;amp;apos;&amp;amp;apos; &amp;amp;apos;&amp;amp;apos; &amp;lt;driver&amp;gt;" call with empty user and password. </description>
			<version>2.0.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.beeline.DatabaseConnection.java</file>
			<file type="M">org.apache.hive.jdbc.Utils.java</file>
		</fixedFiles>
	</bug>
	<bug id="12941" opendate="2016-01-27 07:14:44" fixdate="2016-02-11 14:24:21" resolution="Fixed">
		<buginformation>
			<summary>Unexpected result when using MIN() on struct with NULL in first field</summary>
			<description>Using MIN() on struct with NULL in first field of a row yields NULL as result.
Example:
select min(a) FROM (select 1 as a union all select 2 as a union all select cast(null as int) as a) tmp;
OK
_c0
1
As expected. But if we wrap it in a struct:
select min(a) FROM (select named_struct("field",1) as a union all select named_struct("field",2) as a union all select named_struct("field",cast(null as int)) as a) tmp;
OK
_c0
NULL
Using MAX() works as expected for structs.</description>
			<version>1.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin.java</file>
		</fixedFiles>
	</bug>
	<bug id="12441" opendate="2015-11-17 19:57:07" fixdate="2016-02-11 18:52:51" resolution="Fixed">
		<buginformation>
			<summary>Driver.acquireLocksAndOpenTxn() should only call recordValidTxns() when needed</summary>
			<description>recordValidTxns() is only needed if ACID tables are part of the query.  Otherwise it&amp;amp;apos;s just overhead.</description>
			<version>1.0.0</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">11716</link>
		</links>
	</bug>
	<bug id="10026" opendate="2015-03-20 00:43:57" fixdate="2016-02-11 21:55:54" resolution="Duplicate">
		<buginformation>
			<summary>LLAP: AM should get notifications on daemons going down or restarting</summary>
			<description>There&amp;amp;apos;s lost state otherwise, which can cause queries to hang.</description>
			<version>2.0.0</version>
			<fixedVersion>llap</fixedVersion>
			<type>Sub-task</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.llap.registry.ServiceInstanceSet.java</file>
			<file type="M">org.apache.hadoop.hive.llap.registry.ServiceRegistry.java</file>
			<file type="M">org.apache.hadoop.hive.llap.security.LlapSecurityHelper.java</file>
			<file type="M">org.apache.hadoop.hive.llap.registry.impl.LlapRegistryService.java</file>
			<file type="M">org.apache.hadoop.hive.llap.registry.impl.LlapFixedRegistryImpl.java</file>
			<file type="D">org.apache.hadoop.hive.llap.registry.impl.LlapYarnRegistryImpl.java</file>
			<file type="M">org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">12935</link>
		</links>
	</bug>
	<bug id="13020" opendate="2016-02-08 04:48:57" fixdate="2016-02-11 23:55:00" resolution="Fixed">
		<buginformation>
			<summary>Hive Metastore and HiveServer2 to Zookeeper fails with IBM JDK</summary>
			<description>HiveServer2 and Hive Metastore Zookeeper component is hardcoded to only support the Oracle/Open JDK. I was performing testing of Hadoop running on the IBM JDK and discovered this issue and have since drawn up the attached patch. This looks to resolve the issue in a similar manner as how the Hadoop core folks handle the IBM JDK.</description>
			<version>1.2.0</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.shims.Utils.java</file>
		</fixedFiles>
	</bug>
	<bug id="13036" opendate="2016-02-10 20:38:57" fixdate="2016-02-12 18:32:02" resolution="Fixed">
		<buginformation>
			<summary>Split hive.root.logger separately to make it compatible with log4j1.x (for remaining services)</summary>
			<description>Similar to HIVE-12402 but for HS2 and metastore this time.</description>
			<version>2.0.0</version>
			<fixedVersion>2.1.0, 2.0.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.common.cli.CommonCliOptions.java</file>
			<file type="M">org.apache.hive.service.server.HiveServer2.java</file>
			<file type="M">org.apache.hadoop.hive.cli.OptionsProcessor.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">12402</link>
		</links>
	</bug>
	<bug id="11752" opendate="2015-09-08 01:55:19" fixdate="2016-02-12 18:57:52" resolution="Fixed">
		<buginformation>
			<summary>Pre-materializing complex CTE queries</summary>
			<description>Currently, hive regards CTE clauses as a simple alias to the query block, which makes redundant works if it&amp;amp;apos;s used multiple times in a query. This introduces a reference threshold for pre-materializing the CTE clause as a volatile table (which is not exists in any form of metastore and just accessible from QB).</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.GenTezProcContext.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.CreateTableDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
			<file type="M">org.apache.hadoop.hive.ql.QueryPlan.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Table.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Task.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.FileSinkDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Context.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.TestGenTezWork.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">11049</link>
		</links>
	</bug>
	<bug id="13017" opendate="2016-02-05 23:40:09" fixdate="2016-02-12 19:08:25" resolution="Fixed">
		<buginformation>
			<summary>Child process of HiveServer2 fails to get delegation token from non default FileSystem</summary>
			<description>The following query fails, when Azure Filesystem is used as default file system, and HDFS is used for intermediate data.

&amp;gt;&amp;gt;&amp;gt;  create temporary table s10k stored as orc as select * from studenttab10k;
&amp;gt;&amp;gt;&amp;gt;  create temporary table v10k as select * from votertab10k;
&amp;gt;&amp;gt;&amp;gt;  select registration 
from s10k s join v10k v 
on (s.name = v.name) join studentparttab30k p 
on (p.name = v.name) 
where s.age &amp;lt; 25 and v.age &amp;lt; 25 and p.age &amp;lt; 25;
ERROR : Execution failed with exit status: 2
ERROR : Obtaining error information
ERROR : 
Task failed!
Task ID:
  Stage-5

Logs:

ERROR : /var/log/hive/hiveServer2.log
Error: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask (state=08S01,code=2)
Aborting command set because "force" is false and command failed: "select registration 
from s10k s join v10k v 
on (s.name = v.name) join studentparttab30k p 
on (p.name = v.name) 
where s.age &amp;lt; 25 and v.age &amp;lt; 25 and p.age &amp;lt; 25;"
Closing: 0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;principal=hive/_HOST@HDINSIGHT.NET;transportMode=http;httpPath=cliservice
hiveServer2.log shows:
2016-02-02 18:04:34,182 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - &amp;lt;PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver&amp;gt;
2016-02-02 18:04:34,199 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - &amp;lt;PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver&amp;gt;
2016-02-02 18:04:34,212 INFO  [HiveServer2-HttpHandler-Pool: Thread-55]: thrift.ThriftHttpServlet (ThriftHttpServlet.java:doPost(127)) - Could not validate cookie sent, will try to generate a new cookie
2016-02-02 18:04:34,213 INFO  [HiveServer2-Background-Pool: Thread-517]: ql.Driver (Driver.java:checkConcurrency(168)) - Concurrency mode is disabled, not creating a lock manager
2016-02-02 18:04:34,219 INFO  [HiveServer2-HttpHandler-Pool: Thread-55]: thrift.ThriftHttpServlet (ThriftHttpServlet.java:doKerberosAuth(352)) - Failed to authenticate with http/_HOST kerberos principal, trying with hive/_HOST kerberos principal
2016-02-02 18:04:34,219 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - &amp;lt;PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver&amp;gt;
2016-02-02 18:04:34,225 INFO  [HiveServer2-Background-Pool: Thread-517]: ql.Driver (Driver.java:execute(1390)) - Setting caller context to query id hive_20160202180429_7ffff6ab-64d6-4c89-88b0-6355cc5acbd0
2016-02-02 18:04:34,226 INFO  [HiveServer2-Background-Pool: Thread-517]: ql.Driver (Driver.java:execute(1393)) - Starting command(queryId=hive_20160202180429_7ffff6ab-64d6-4c89-88b0-6355cc5acbd0): select registration
from s10k s join v10k v
on (s.name = v.name) join studentparttab30k p
on (p.name = v.name)
where s.age &amp;lt; 25 and v.age &amp;lt; 25 and p.age &amp;lt; 25
2016-02-02 18:04:34,228 INFO  [HiveServer2-Background-Pool: Thread-517]: hooks.ATSHook (ATSHook.java:&amp;lt;init&amp;gt;(90)) - Created ATS Hook
2016-02-02 18:04:34,229 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - &amp;lt;PERFLOG method=PreHook.org.apache.hadoop.hive.ql.hooks.ATSHook from=org.apache.hadoop.hive.ql.Driver&amp;gt;
2016-02-02 18:04:34,237 INFO  [HiveServer2-HttpHandler-Pool: Thread-55]: thrift.ThriftHttpServlet (ThriftHttpServlet.java:doPost(169)) - Cookie added for clientUserName hrt_qa
2016-02-02 18:04:34,238 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogEnd(162)) - &amp;lt;/PERFLOG method=PreHook.org.apache.hadoop.hive.ql.hooks.ATSHook start=1454436274229 end=1454436274238 duration=9 from=org.apache.hadoop.hive.ql.Driver&amp;gt;
2016-02-02 18:04:34,239 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - &amp;lt;PERFLOG method=PreHook.org.apache.hadoop.hive.ql.security.authorization.plugin.DisallowTransformHook from=org.apache.hadoop.hive.ql.Driver&amp;gt;
2016-02-02 18:04:34,240 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogEnd(162)) - &amp;lt;/PERFLOG method=PreHook.org.apache.hadoop.hive.ql.security.authorization.plugin.DisallowTransformHook start=1454436274239 end=1454436274240 duration=1 from=org.apache.hadoop.hive.ql.Driver&amp;gt;
Query ID = hive_20160202180429_7ffff6ab-64d6-4c89-88b0-6355cc5acbd0
2016-02-02 18:04:34,242 INFO  [HiveServer2-Background-Pool: Thread-517]: ql.Driver (SessionState.java:printInfo(923)) - Query ID = hive_20160202180429_7ffff6ab-64d6-4c89-88b0-6355cc5acbd0
Total jobs = 1
2016-02-02 18:04:34,243 INFO  [HiveServer2-Background-Pool: Thread-517]: ql.Driver (SessionState.java:printInfo(923)) - Total jobs = 1
2016-02-02 18:04:34,245 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogEnd(162)) - &amp;lt;/PERFLOG method=TimeToSubmit start=1454436274199 end=1454436274245 duration=46 from=org.apache.hadoop.hive.ql.Driver&amp;gt;
2016-02-02 18:04:34,246 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - &amp;lt;PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver&amp;gt;
2016-02-02 18:04:34,247 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - &amp;lt;PERFLOG method=task.MAPREDLOCAL.Stage-5 from=org.apache.hadoop.hive.ql.Driver&amp;gt;
2016-02-02 18:04:34,258 INFO  [HiveServer2-Background-Pool: Thread-517]: ql.Driver (Driver.java:launchTask(1718)) - Starting task [Stage-5:MAPREDLOCAL] in serial mode
2016-02-02 18:04:34,280 INFO  [HiveServer2-Background-Pool: Thread-517]: mr.MapredLocalTask (MapredLocalTask.java:executeInChildVM(158)) - Generating plan file file:/tmp/hive/916e3dbb-a10d-4888-a063-52fb058ea421/hive_2016-02-02_18-04-29_153_625340340820843828-4/-local-10006/plan.xml
2016-02-02 18:04:34,288 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - &amp;lt;PERFLOG method=serializePlan from=org.apache.hadoop.hive.ql.exec.Utilities&amp;gt;
2016-02-02 18:04:34,289 INFO  [HiveServer2-Background-Pool: Thread-517]: exec.Utilities (Utilities.java:serializePlan(1028)) - Serializing MapredLocalWork via kryo
2016-02-02 18:04:34,290 INFO  [ATS Logger 0]: hooks.ATSHook (ATSHook.java:createPreHookEvent(158)) - Received pre-hook notification for :hive_20160202180429_7ffff6ab-64d6-4c89-88b0-6355cc5acbd0
2016-02-02 18:04:34,358 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogEnd(162)) - &amp;lt;/PERFLOG method=serializePlan start=1454436274288 end=1454436274358 duration=70 from=org.apache.hadoop.hive.ql.exec.Utilities&amp;gt;
2016-02-02 18:04:34,737 INFO  [HiveServer2-Background-Pool: Thread-517]: mr.MapredLocalTask (MapredLocalTask.java:executeInChildVM(287)) - Executing: /usr/hdp/2.4.1.0-170/hadoop/bin/hadoop jar /usr/hdp/2.4.1.0-170/hive/lib/hive-common-1.2.1000.2.4.1.0-170.jar org.apache.hadoop.hive.ql.exec.mr.ExecDriver -localtask -plan file:/tmp/hive/916e3dbb-a10d-4888-a063-52fb058ea421/hive_2016-02-02_18-04-29_153_625340340820843828-4/-local-10006/plan.xml   -jobconffile file:/tmp/hive/916e3dbb-a10d-4888-a063-52fb058ea421/hive_2016-02-02_18-04-29_153_625340340820843828-4/-local-10007/jobconf.xml
WARNING: Use "yarn jar" to launch YARN applications.
2016-02-02 18:04:37,450 INFO  [org.apache.ranger.audit.queue.AuditBatchQueue0]: provider.BaseAuditHandler (BaseAuditHandler.java:logStatus(312)) - Audit Status Log: name=hiveServer2.async.summary.batch.solr, interval=01:21.012 minutes, events=2, succcessCount=2, totalEvents=4, totalSuccessCount=4
Execution log at: /tmp/hive/hive_20160202180429_7ffff6ab-64d6-4c89-88b0-6355cc5acbd0.log
2016-02-02 18:04:39,248 INFO  [HiveServer2-HttpHandler-Pool: Thread-55]: thrift.ThriftHttpServlet (ThriftHttpServlet.java:doPost(127)) - Could not validate cookie sent, will try to generate a new cookie
2016-02-02 18:04:39,254 INFO  [HiveServer2-HttpHandler-Pool: Thread-55]: thrift.ThriftHttpServlet (ThriftHttpServlet.java:doKerberosAuth(352)) - Failed to authenticate with http/_HOST kerberos principal, trying with hive/_HOST kerberos principal
2016-02-02 18:04:39,261 INFO  [HiveServer2-HttpHandler-Pool: Thread-55]: thrift.ThriftHttpServlet (ThriftHttpServlet.java:doPost(169)) - Cookie added for clientUserName hrt_qa
2016-02-02 18:04:40     Starting to launch local task to process map join;      maximum memory = 477102080
Execution failed with exit status: 2
2016-02-02 18:04:43,728 ERROR [HiveServer2-Background-Pool: Thread-517]: exec.Task (SessionState.java:printError(932)) - Execution failed with exit status: 2
Obtaining error information
2016-02-02 18:04:43,730 ERROR [HiveServer2-Background-Pool: Thread-517]: exec.Task (SessionState.java:printError(932)) - Obtaining error information

Task failed!
Task ID:
  Stage-5

Logs:

2016-02-02 18:04:43,730 ERROR [HiveServer2-Background-Pool: Thread-517]: exec.Task (SessionState.java:printError(932)) -
Task failed!
Task ID:
  Stage-5

Logs:

/var/log/hive/hiveServer2.log
2016-02-02 18:04:43,732 ERROR [HiveServer2-Background-Pool: Thread-517]: exec.Task (SessionState.java:printError(932)) - /var/log/hive/hiveServer2.log
2016-02-02 18:04:43,732 ERROR [HiveServer2-Background-Pool: Thread-517]: mr.MapredLocalTask (MapredLocalTask.java:executeInChildVM(307)) - Execution failed with exit status: 2
2016-02-02 18:04:43,733 INFO  [HiveServer2-Background-Pool: Thread-517]: hooks.ATSHook (ATSHook.java:&amp;lt;init&amp;gt;(90)) - Created ATS Hook
2016-02-02 18:04:43,734 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - &amp;lt;PERFLOG method=FailureHook.org.apache.hadoop.hive.ql.hooks.ATSHook from=org.apache.hadoop.hive.ql.Driver&amp;gt;
2016-02-02 18:04:43,736 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogEnd(162)) - &amp;lt;/PERFLOG method=FailureHook.org.apache.hadoop.hive.ql.hooks.ATSHook start=1454436283734 end=1454436283736 duration=2 from=org.apache.hadoop.hive.ql.Driver&amp;gt;
2016-02-02 18:04:43,736 INFO  [ATS Logger 0]: hooks.ATSHook (ATSHook.java:createPostHookEvent(193)) - Received post-hook notification for :hive_20160202180429_7ffff6ab-64d6-4c89-88b0-6355cc5acbd0
FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask
2016-02-02 18:04:43,757 ERROR [HiveServer2-Background-Pool: Thread-517]: ql.Driver (SessionState.java:printError(932)) - FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask
2016-02-02 18:04:43,758 INFO  [HiveServer2-Background-Pool: Thread-517]: ql.Driver (Driver.java:execute(1621)) - Resetting the caller context to HIVE_SSN_ID:916e3dbb-a10d-4888-a063-52fb058ea421
2016-02-02 18:04:43,759 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogEnd(162)) - &amp;lt;/PERFLOG method=Driver.execute start=1454436274219 end=1454436283759 duration=9540 from=org.apache.hadoop.hive.ql.Driver&amp;gt;
2016-02-02 18:04:43,760 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - &amp;lt;PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver&amp;gt;
2016-02-02 18:04:43,761 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogEnd(162)) - &amp;lt;/PERFLOG method=releaseLocks start=1454436283760 end=1454436283761 duration=1 from=org.apache.hadoop.hive.ql.Driver&amp;gt;
2016-02-02 18:04:43,766 ERROR [HiveServer2-Background-Pool: Thread-517]: operation.Operation (SQLOperation.java:run(209)) - Error running hive query:
org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask
        at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:315)
        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:156)
        at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:71)
        at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:206)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
        at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:218)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
hive Configs can be viewed from http://qelog.hortonworks.com/log/hs21-hbs24-1454382123/artifacts/tmpModifyConfDir_1454394513592/
Attachments
Drop files to attach, or browse.
Add LinkIssue Links
relates to
Bug - A problem which impairs or prevents the functions of the product. HIVE-739 webhcat tests failing in HDInsight secure cluster throwing NullPointerException	 Blocker - Blocks development and/or testing work, production could not run. RESOLVED
Activity
All
Comments
Work Log
History
Activity
Ascending order - Click to sort in descending order
Permalink Edit Delete 
tsaito Takahiko Saito added a comment - 3 days ago
The test passes via hive CLI and explain shows:
0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181&amp;gt; explain select registration
0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181&amp;gt; from s10k s join v10k v
0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181&amp;gt; on (s.name = v.name) join studentparttab30k p
0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181&amp;gt; on (p.name = v.name)
0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181&amp;gt; where s.age &amp;lt; 25 and v.age &amp;lt; 25 and p.age &amp;lt; 25;
+--------------------------------------------------------------------------------------------------------------+--+
|                                                   Explain                                                    |
+--------------------------------------------------------------------------------------------------------------+--+
| STAGE DEPENDENCIES:                                                                                          |
|   Stage-5 is a root stage                                                                                    |
|   Stage-4 depends on stages: Stage-5                                                                         |
|   Stage-0 depends on stages: Stage-4                                                                         |
|                                                                                                              |
| STAGE PLANS:                                                                                                 |
|   Stage: Stage-5                                                                                             |
|     Map Reduce Local Work                                                                                    |
|       Alias -&amp;gt; Map Local Tables:                                                                             |
|         s                                                                                                    |
|           Fetch Operator                                                                                     |
|             limit: -1                                                                                        |
|         v                                                                                                    |
|           Fetch Operator                                                                                     |
|             limit: -1                                                                                        |
|       Alias -&amp;gt; Map Local Operator Tree:                                                                      |
|         s                                                                                                    |
|           TableScan                                                                                          |
|             alias: s                                                                                         |
|             filterExpr: (name is not null and (age &amp;lt; 25)) (type: boolean)                                    |
|             Statistics: Num rows: 459 Data size: 47777 Basic stats: COMPLETE Column stats: NONE              |
|             Filter Operator                                                                                  |
|               predicate: (name is not null and (age &amp;lt; 25)) (type: boolean)                                   |
|               Statistics: Num rows: 76 Data size: 7910 Basic stats: COMPLETE Column stats: NONE              |
|               HashTable Sink Operator                                                                        |
|                 keys:                                                                                        |
|                   0 name (type: string)                                                                      |
|                   1 name (type: string)                                                                      |
|                   2 name (type: string)                                                                      |
|         v                                                                                                    |
|           TableScan                                                                                          |
|             alias: v                                                                                         |
|             filterExpr: (name is not null and (age &amp;lt; 25)) (type: boolean)                                    |
|             Statistics: Num rows: 1653 Data size: 337233 Basic stats: COMPLETE Column stats: NONE            |
|             Filter Operator                                                                                  |
|               predicate: (name is not null and (age &amp;lt; 25)) (type: boolean)                                   |
|               Statistics: Num rows: 275 Data size: 56103 Basic stats: COMPLETE Column stats: NONE            |
|               HashTable Sink Operator                                                                        |
|                 keys:                                                                                        |
|                   0 name (type: string)                                                                      |
|                   1 name (type: string)                                                                      |
|                   2 name (type: string)                                                                      |
|                                                                                                              |
|   Stage: Stage-4                                                                                             |
|     Map Reduce                                                                                               |
|       Map Operator Tree:                                                                                     |
|           TableScan                                                                                          |
|             alias: p                                                                                         |
|             filterExpr: (name is not null and (age &amp;lt; 25)) (type: boolean)                                    |
|             Statistics: Num rows: 30000 Data size: 627520 Basic stats: COMPLETE Column stats: COMPLETE       |
|             Filter Operator                                                                                  |
|               predicate: (name is not null and (age &amp;lt; 25)) (type: boolean)                                   |
|               Statistics: Num rows: 10000 Data size: 1010000 Basic stats: COMPLETE Column stats: COMPLETE    |
|               Map Join Operator                                                                              |
|                 condition map:                                                                               |
|                      Inner Join 0 to 1                                                                       |
|                      Inner Join 1 to 2                                                                       |
|                 keys:                                                                                        |
|                   0 name (type: string)                                                                      |
|                   1 name (type: string)                                                                      |
|                   2 name (type: string)                                                                      |
|                 outputColumnNames: _col8                                                                     |
|                 Statistics: Num rows: 22000 Data size: 2222000 Basic stats: COMPLETE Column stats: NONE      |
|                 Select Operator                                                                              |
|                   expressions: _col8 (type: string)                                                          |
|                   outputColumnNames: _col0                                                                   |
|                   Statistics: Num rows: 22000 Data size: 2222000 Basic stats: COMPLETE Column stats: NONE    |
|                   File Output Operator                                                                       |
|                     compressed: false                                                                        |
|                     Statistics: Num rows: 22000 Data size: 2222000 Basic stats: COMPLETE Column stats: NONE  |
|                     table:                                                                                   |
|                         input format: org.apache.hadoop.mapred.TextInputFormat                               |
|                         output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat            |
|                         serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe                            |
|       Local Work:                                                                                            |
|         Map Reduce Local Work                                                                                |
|                                                                                                              |
|   Stage: Stage-0                                                                                             |
|     Fetch Operator                                                                                           |
|       limit: -1                                                                                              |
|       Processor Tree:                                                                                        |
|         ListSink                                                                                             |
|                                                                                                              |
+--------------------------------------------------------------------------------------------------------------+--+
83 rows selected (2.473 seconds)
Permalink Edit Delete 
tsaito Takahiko Saito added a comment - 3 days ago
emptablemisc_8 also fails with the same error:
&amp;gt;&amp;gt;&amp;gt;  create temporary table temp1 as select * from votertab10k;
&amp;gt;&amp;gt;&amp;gt;  select * 
from studenttab10k s 
where s.name not in 
(select name from temp1);
INFO  : Number of reduce tasks determined at compile time: 1
INFO  : In order to change the average load for a reducer (in bytes):
INFO  :   set hive.exec.reducers.bytes.per.reducer=&amp;lt;number&amp;gt;
INFO  : In order to limit the maximum number of reducers:
INFO  :   set hive.exec.reducers.max=&amp;lt;number&amp;gt;
INFO  : In order to set a constant number of reducers:
INFO  :   set mapreduce.job.reduces=&amp;lt;number&amp;gt;
INFO  : number of splits:1
INFO  : Submitting tokens for job: job_1454394534358_0164
INFO  : Kind: HDFS_DELEGATION_TOKEN, Service: 10.0.0.36:8020, Ident: (HDFS_DELEGATION_TOKEN token 246 for hrt_qa)
INFO  : The url to track the job: http://hn0-hs21-h.hdinsight.net:8088/proxy/application_1454394534358_0164/
INFO  : Starting Job = job_1454394534358_0164, Tracking URL = http://hn0-hs21-h.hdinsight.net:8088/proxy/application_1454394534358_0164/
INFO  : Kill Command = /usr/hdp/2.4.1.0-170/hadoop/bin/hadoop job  -kill job_1454394534358_0164
INFO  : Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
INFO  : 2016-02-02 10:11:02,367 Stage-4 map = 0%,  reduce = 0%
INFO  : 2016-02-02 10:11:26,060 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 5.48 sec
INFO  : 2016-02-02 10:11:39,024 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 11.18 sec
INFO  : MapReduce Total cumulative CPU time: 11 seconds 180 msec
INFO  : Ended Job = job_1454394534358_0164
INFO  : Stage-9 is selected by condition resolver.
INFO  : Stage-1 is filtered out by condition resolver.
ERROR : Execution failed with exit status: 2
ERROR : Obtaining error information
ERROR : 
Task failed!
Task ID:
  Stage-9

Logs:

ERROR : /var/log/hive/hiveServer2.log
INFO  : Number of reduce tasks determined at compile time: 1
INFO  : In order to change the average load for a reducer (in bytes):
INFO  :   set hive.exec.reducers.bytes.per.reducer=&amp;lt;number&amp;gt;
INFO  : In order to limit the maximum number of reducers:
INFO  :   set hive.exec.reducers.max=&amp;lt;number&amp;gt;
INFO  : In order to set a constant number of reducers:
INFO  :   set mapreduce.job.reduces=&amp;lt;number&amp;gt;
INFO  : number of splits:2
INFO  : Submitting tokens for job: job_1454394534358_0169
INFO  : Kind: HDFS_DELEGATION_TOKEN, Service: 10.0.0.36:8020, Ident: (HDFS_DELEGATION_TOKEN token 252 for hrt_qa)
INFO  : The url to track the job: http://hn0-hs21-h.hdinsight.net:8088/proxy/application_1454394534358_0169/
INFO  : Starting Job = job_1454394534358_0169, Tracking URL = http://hn0-hs21-h.hdinsight.net:8088/proxy/application_1454394534358_0169/
INFO  : Kill Command = /usr/hdp/2.4.1.0-170/hadoop/bin/hadoop job  -kill job_1454394534358_0169
INFO  : Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
INFO  : 2016-02-02 10:16:38,027 Stage-1 map = 0%,  reduce = 0%
INFO  : 2016-02-02 10:16:52,498 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 3.8 sec
INFO  : 2016-02-02 10:16:53,566 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 9.94 sec
INFO  : 2016-02-02 10:17:16,202 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 17.04 sec
INFO  : MapReduce Total cumulative CPU time: 17 seconds 40 msec
INFO  : Ended Job = job_1454394534358_0169
ERROR : Execution failed with exit status: 2
ERROR : Obtaining error information
ERROR : 
Task failed!
Task ID:
  Stage-8

Logs:

ERROR : /var/log/hive/hiveServer2.log
Error: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask (state=08S01,code=2)
Aborting command set because "force" is false and command failed: "select * 
from studenttab10k s 
where s.name not in 
(select name from temp1);"
Closing: 0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;principal=hive/_HOST@HDINSIGHT.NET;transportMode=http;httpPath=cliservice
Its app log can be viewed at http://qelog.hortonworks.com/log/hs21-hbs24-1454382123/app-logs/application_1454394534358_0169.log
Permalink Edit Delete 
tsaito Takahiko Saito added a comment - 3 days ago
The query works without &amp;amp;apos;temporary table&amp;amp;apos;:
0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181&amp;gt; select registration from studenttab10k s join votertab10k v
0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181&amp;gt; on (s.name = v.name) join studentparttab30k p
0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181&amp;gt; on (p.name = v.name)
0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181&amp;gt; where s.age &amp;lt; 25 and v.age &amp;lt; 25 and p.age &amp;lt; 25;
Permalink Edit Delete 
tsaito Takahiko Saito added a comment - 3 days ago
More info about temporary table:
0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181&amp;gt; describe formatted s10k;
+-------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+--+
|           col_name            |                                                                  data_type                                                                  |        comment        |
+-------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+--+
| # col_name                    | data_type                                                                                                                                   | comment               |
|                               | NULL                                                                                                                                        | NULL                  |
| name                          | string                                                                                                                                      |                       |
| age                           | int                                                                                                                                         |                       |
| gpa                           | double                                                                                                                                      |                       |
|                               | NULL                                                                                                                                        | NULL                  |
| # Detailed Table Information  | NULL                                                                                                                                        | NULL                  |
| Database:                     | default                                                                                                                                     | NULL                  |
| Owner:                        | hrt_qa                                                                                                                                      | NULL                  |
| CreateTime:                   | Tue Feb 02 23:02:31 UTC 2016                                                                                                                | NULL                  |
| LastAccessTime:               | UNKNOWN                                                                                                                                     | NULL                  |
| Protect Mode:                 | None                                                                                                                                        | NULL                  |
| Retention:                    | 0                                                                                                                                           | NULL                  |
| Location:                     | hdfs://hn0-hs21-h.hdinsight.net:8020/tmp/hive/hive/de2667ea-5bc1-4548-a4c4-97e46d6081f8/_tmp_space.db/8e8482c9-a2a4-4e8c-ad0f-2cd6e8fcdb4c  | NULL                  |
| Table Type:                   | MANAGED_TABLE                                                                                                                               | NULL                  |
|                               | NULL                                                                                                                                        | NULL                  |
| # Storage Information         | NULL                                                                                                                                        | NULL                  |
| SerDe Library:                | org.apache.hadoop.hive.ql.io.orc.OrcSerde                                                                                                   | NULL                  |
| InputFormat:                  | org.apache.hadoop.hive.ql.io.orc.OrcInputFormat                                                                                             | NULL                  |
| OutputFormat:                 | org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat                                                                                            | NULL                  |
| Compressed:                   | No                                                                                                                                          | NULL                  |
| Num Buckets:                  | -1                                                                                                                                          | NULL                  |
| Bucket Columns:               | []                                                                                                                                          | NULL                  |
| Sort Columns:                 | []                                                                                                                                          | NULL                  |
| Storage Desc Params:          | NULL                                                                                                                                        | NULL                  |
|                               | serialization.format                                                                                                                        | 1                     |
+-------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+--+
26 rows selected (0.22 seconds)
0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181&amp;gt; set hive.server2.enable.doAs;
+---------------------------------+--+
|               set               |
+---------------------------------+--+
| hive.server2.enable.doAs=false  |
+---------------------------------+--+
Permalink Edit Delete 
tsaito Takahiko Saito added a comment - 3 days ago
hdfs dir of temporary table is owned by hive as expected since hive.server2.enable.doAs=false:
hdfs@hn0-hs21-h:~$ hdfs dfs -ls hdfs://hn0-hs21-h.hdinsight.net:8020/tmp/hive/hive/de2667ea-5bc1-4548-a4c4-97e46d6081f8/_tmp_space.db/8e8482c9-a2a4-4e8c-ad0f-2cd6e8fcdb4c
Found 1 items
-rwx------   3 hive hdfs      47777 2016-02-02 23:02 hdfs://hn0-hs21-h.hdinsight.net:8020/tmp/hive/hive/de2667ea-5bc1-4548-a4c4-97e46d6081f8/_tmp_space.db/8e8482c9-a2a4-4e8c-ad0f-2cd6e8fcdb4c/000000_0
Not sure if it&amp;amp;apos;s related to the JIRA, but one thing I noticed was that dfs cmd throws error via beeline:
0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181&amp;gt; dfs -ls hdfs://hn0-hs21-h.hdinsight.net:8020/tmp/hive/hive/de2667ea-5bc1-4548-a4c4-97e46d6081f8/_tmp_space.db/8e8482c9-a2a4-4e8c-ad0f-2cd6e8fcdb4c;
Error: Error while processing statement: Permission denied: user [hrt_qa] does not have privilege for [DFS] command (state=,code=1)
Permalink Edit 
thejas Thejas Nair added a comment - 5 hours ago - edited
Takahiko Saito 
The above DFS error would be from Ranger of SQL standard authorization. It is not related to the test failure.
But you are right that it looks like a permission issue.
The issue seen in HIVE-739 also exists in this part of Hive. Hive in MR mode launches a new child process to process the small table and create a hash table. This child process needs credentials from HDFS. However, in this setup, azure fs is the default file system. We should also get delegation token from all FS URIs listed under mapreduce.job.hdfs-servers config.
There are hueristics around when map-join gets used. That is why you don&amp;amp;apos;t see it unless temp table is used (that uses ORC format and might have stats as well, while the original table is probaly text format).
https://github.com/hortonworks/hive/blob/2.4-maint/ql/src/java/org/apache/hadoop/hive/ql/exec/SecureCmdDoAs.java needs change similar to webhcat change in HIVE-739
Permalink Edit 
sush Sushanth Sowmyan added a comment - 7 minutes ago
Takahiko Saito, we should file an apache jira for this as well. Would you like to do so, so the bug report has attribution to you?
Permalink Edit Delete 
tsaito Takahiko Saito added a comment - 4 minutes ago
Sushanth Sowmyan I will file one and update with that JIRA.
 Comment	
People
Assignee:	 sush Sushanth Sowmyan
Assign to me
Reporter:	 tsaito Takahiko Saito
QEAssignee:	Takahiko Saito
Votes:	0
Watchers:	3 Stop watching this issue 
Dates
Created:	3 days ago
Updated:	4 minutes ago
Who&amp;amp;apos;s Looking?

Agile
View on Board


hivesever2 log shows:

2016-02-02 18:04:43,766 ERROR [HiveServer2-Background-Pool: Thread-517]: operation.Operation (SQLOperation.java:run(209)) - Error running hive query:
org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask
        at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:315)
        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:156)
        at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:71)
        at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:206)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
        at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:218)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)

</description>
			<version>1.2.1</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.SecureCmdDoAs.java</file>
		</fixedFiles>
	</bug>
	<bug id="13042" opendate="2016-02-11 16:34:28" fixdate="2016-02-13 00:07:54" resolution="Fixed">
		<buginformation>
			<summary>OrcFiledump runs into an ArrayIndexOutOfBoundsException when running against old versions of ORC files</summary>
			<description>
Exception in thread "main" java.lang.IndexOutOfBoundsException: Index: 0
at java.util.Collections$EmptyList.get(Collections.java:3212)
at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getFileVersion(ReaderImpl.java:194)
at org.apache.hadoop.hive.ql.io.orc.FileDump.printMetaDataImpl(FileDump.java:289)
at org.apache.hadoop.hive.ql.io.orc.FileDump.printMetaData(FileDump.java:261)
at org.apache.hadoop.hive.ql.io.orc.FileDump.main(FileDump.java:127)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
at org.apache.hadoop.util.RunJar.main(RunJar.java:136)


\cc Prasanth Jayachandran, Siddharth Seth</description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
		</fixedFiles>
	</bug>
	<bug id="13039" opendate="2016-02-10 21:06:05" fixdate="2016-02-16 16:26:48" resolution="Fixed">
		<buginformation>
			<summary>BETWEEN predicate is not functioning correctly with predicate pushdown on Parquet table</summary>
			<description>BETWEEN becomes exclusive in parquet table when predicate pushdown is on (as it is by default in newer Hive versions). To reproduce(in a cluster, not local setup):
CREATE TABLE parquet_tbl(
  key int,
  ldate string)
 PARTITIONED BY (
 lyear string )
 ROW FORMAT SERDE
 &amp;amp;apos;org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe&amp;amp;apos;
 STORED AS INPUTFORMAT
 &amp;amp;apos;org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat&amp;amp;apos;
 OUTPUTFORMAT
 &amp;amp;apos;org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat&amp;amp;apos;;
insert overwrite table parquet_tbl partition (lyear=&amp;amp;apos;2016&amp;amp;apos;) select
  1,
  &amp;amp;apos;2016-02-03&amp;amp;apos; from src limit 1;
set hive.optimize.ppd.storage = true;
set hive.optimize.ppd = true;
select * from parquet_tbl where ldate between &amp;amp;apos;2016-02-03&amp;amp;apos; and &amp;amp;apos;2016-02-03&amp;amp;apos;;
No row will be returned in a cluster.
But if you turn off hive.optimize.ppd, one row will be returned.</description>
			<version>1.2.1</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.parquet.TestParquetRecordReaderWrapper.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.sarg.TestConvertAstToSearchArg.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.parquet.read.TestParquetFilterPredicate.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.parquet.FilterPredicateLeafBuilder.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">12678</link>
		</links>
	</bug>
	<bug id="13056" opendate="2016-02-13 01:11:52" fixdate="2016-02-17 17:44:43" resolution="Fixed">
		<buginformation>
			<summary>delegation tokens do not work with HS2 when used with http transport and kerberos</summary>
			<description>We&amp;amp;apos;re getting a HiveSQLException on secure windows clusters.


2016-02-08 13:48:09,535|beaver.machine|INFO|6114|140264674350912|MainThread|Job ID : 0000000-160208134528402-oozie-oozi-W
2016-02-08 13:48:09,536|beaver.machine|INFO|6114|140264674350912|MainThread|------------------------------------------------------------------------------------------------------------------------------------
2016-02-08 13:48:09,536|beaver.machine|INFO|6114|140264674350912|MainThread|Workflow Name : hive2-wf
2016-02-08 13:48:09,536|beaver.machine|INFO|6114|140264674350912|MainThread|App Path      : wasb://oozie1-hbs24@humbtestings5jp.blob.core.windows.net/user/hrt_qa/test_hiveserver2
2016-02-08 13:48:09,536|beaver.machine|INFO|6114|140264674350912|MainThread|Status        : KILLED
2016-02-08 13:48:09,537|beaver.machine|INFO|6114|140264674350912|MainThread|Run           : 0
2016-02-08 13:48:09,537|beaver.machine|INFO|6114|140264674350912|MainThread|User          : hrt_qa
2016-02-08 13:48:09,537|beaver.machine|INFO|6114|140264674350912|MainThread|Group         : -
2016-02-08 13:48:09,547|beaver.machine|INFO|6114|140264674350912|MainThread|Created       : 2016-02-08 13:47 GMT
2016-02-08 13:48:09,548|beaver.machine|INFO|6114|140264674350912|MainThread|Started       : 2016-02-08 13:47 GMT
2016-02-08 13:48:09,552|beaver.machine|INFO|6114|140264674350912|MainThread|Last Modified : 2016-02-08 13:48 GMT
2016-02-08 13:48:09,553|beaver.machine|INFO|6114|140264674350912|MainThread|Ended         : 2016-02-08 13:48 GMT
2016-02-08 13:48:09,553|beaver.machine|INFO|6114|140264674350912|MainThread|CoordAction ID: -
2016-02-08 13:48:09,566|beaver.machine|INFO|6114|140264674350912|MainThread|
2016-02-08 13:48:09,566|beaver.machine|INFO|6114|140264674350912|MainThread|Actions
2016-02-08 13:48:09,567|beaver.machine|INFO|6114|140264674350912|MainThread|------------------------------------------------------------------------------------------------------------------------------------
2016-02-08 13:48:09,567|beaver.machine|INFO|6114|140264674350912|MainThread|ID                                                                            Status    Ext ID                 Ext Status Err Code
2016-02-08 13:48:09,567|beaver.machine|INFO|6114|140264674350912|MainThread|------------------------------------------------------------------------------------------------------------------------------------
2016-02-08 13:48:09,571|beaver.machine|INFO|6114|140264674350912|MainThread|0000000-160208134528402-oozie-oozi-W@:start:                                  OK        -                      OK         -
2016-02-08 13:48:09,572|beaver.machine|INFO|6114|140264674350912|MainThread|------------------------------------------------------------------------------------------------------------------------------------
2016-02-08 13:48:09,572|beaver.machine|INFO|6114|140264674350912|MainThread|0000000-160208134528402-oozie-oozi-W@hive-node                                ERROR     -                      ERROR      HiveSQLException
2016-02-08 13:48:09,572|beaver.machine|INFO|6114|140264674350912|MainThread|------------------------------------------------------------------------------------------------------------------------------------
2016-02-08 13:48:09,572|beaver.machine|INFO|6114|140264674350912|MainThread|0000000-160208134528402-oozie-oozi-W@fail                                     OK        -                      OK         E0729
2016-02-08 13:48:09,572|beaver.machine|INFO|6114|140264674350912|MainThread|------------------------------------------------------------------------------------------------------------------------------------

</description>
			<version>1.2.1</version>
			<fixedVersion>1.3.0, 2.1.0, 2.0.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.service.auth.HiveAuthFactory.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">13169</link>
		</links>
	</bug>
	<bug id="12981" opendate="2016-02-02 12:01:15" fixdate="2016-02-18 03:44:11" resolution="Fixed">
		<buginformation>
			<summary>ThriftCLIService uses incompatible getShortName() implementation</summary>
			<description>ThriftCLIService has a local implementation getShortName() that assumes a short name is always the part before "@" and "/". This is not always the case as Kerberos Rules (from Hadoop&amp;amp;apos;s KerberosName) might actually transform a name to something else.
Considering a pending change to getShortName() (#HADOOP-12751) and the normal use of KerberosName in other parts of Hive it only seems logical to use the standard implementation.</description>
			<version>1.2.1</version>
			<fixedVersion>1.3.0, 1.2.2, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">12751</link>
			<link type="Reference" description="is related to">15174</link>
			<link type="Regression" description="breaks">13590</link>
		</links>
	</bug>
	<bug id="12927" opendate="2016-01-26 02:31:44" fixdate="2016-02-18 17:59:52" resolution="Fixed">
		<buginformation>
			<summary>HBase metastore: sequences should be one per row, not all in one row</summary>
			<description>
  long getNextSequence(byte[] sequence) throws IOException {


Is not safe in presence of any concurrency. It should use HBase increment API.</description>
			<version>2.0.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.hbase.TestHBaseSchemaTool.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.hbase.HBaseReadWrite.java</file>
		</fixedFiles>
	</bug>
	<bug id="13065" opendate="2016-02-16 21:11:31" fixdate="2016-02-18 18:58:40" resolution="Fixed">
		<buginformation>
			<summary>Hive throws NPE when writing map type data to a HBase backed table</summary>
			<description>Hive throws NPE when writing data to a HBase backed table with below conditions:

There is a map type column
The map type column has NULL in its values

Below are the reproduce steps:
1) Create a HBase backed Hive table


create table hbase_test (id bigint, data map&amp;lt;string, string&amp;gt;)
stored by &amp;amp;apos;org.apache.hadoop.hive.hbase.HBaseStorageHandler&amp;amp;apos;
with serdeproperties ("hbase.columns.mapping" = ":key,cf:map_col")
tblproperties ("hbase.table.name" = "hive_test");


2) insert data into above table


insert overwrite table hbase_test select 1 as id, map(&amp;amp;apos;abcd&amp;amp;apos;, null) as data from src limit 1;


The mapreduce job for insert query fails. Error messages are as below:

2016-02-15 02:26:33,225 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {"key":{},"value":{"_col0":1,"_col1":{"abcd":null}}}
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:265)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:444)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {"key":{},"value":{"_col0":1,"_col1":{"abcd":null}}}
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:253)
	... 7 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.serde2.SerDeException: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:731)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)
	at org.apache.hadoop.hive.ql.exec.LimitOperator.processOp(LimitOperator.java:51)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:244)
	... 7 more
Caused by: org.apache.hadoop.hive.serde2.SerDeException: java.lang.NullPointerException
	at org.apache.hadoop.hive.hbase.HBaseSerDe.serialize(HBaseSerDe.java:286)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:666)
	... 14 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(LazyUtils.java:221)
	at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:236)
	at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:275)
	at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:222)
	at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serializeField(HBaseRowSerializer.java:194)
	at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:118)
	at org.apache.hadoop.hive.hbase.HBaseSerDe.serialize(HBaseSerDe.java:282)
	... 15 more

</description>
			<version>1.1.0</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.hbase.HBaseRowSerializer.java</file>
		</fixedFiles>
	</bug>
	<bug id="13075" opendate="2016-02-17 15:12:52" fixdate="2016-02-19 03:03:17" resolution="Duplicate">
		<buginformation>
			<summary>Metastore shuts down when no delegation token is found in ZooKeeper</summary>
			<description>ZooKeeperTokenStore looks as follows:


@Override
public DelegationTokenInformation getToken(DelegationTokenIdentifier tokenIdentifier) {
  byte[] tokenBytes = zkGetData(getTokenPath(tokenIdentifier));
  try {
    return HiveDelegationTokenSupport.decodeDelegationTokenInformation(tokenBytes);
  } catch (Exception ex) {
    throw new TokenStoreException("Failed to decode token", ex);
  }
}


which is slightly different from DBTokenStore implementation that is protected against tokenBytes==null because nullable tokenBytes causes NPE to be thrown in HiveDelegationTokenSupport#decodeDelegationTokenInformation
Furthermore, NPE thrown here causes TokenStoreDelegationTokenSecretManager.ExpiredTokenRemover to catch it and exits MetaStore.
null from zkGetData() is possible during ZooKeeper failure or (and that was our case) when another metastore instance removes tokens during ExpiredTokenRemover run. There were two solutions of this problem:

distributed lock in ZooKeeper acquired during one metastore instance&amp;amp;apos;s ExpiredTokenRemover run,
simple null check

I think null check is sufficient if it is in DBTokenStore.
Patch will be attached.
Sorry for an edit but I think worth mentioning is a fact that possible workaround for this issue is setting hive.cluster.delegation.key.update-interval, hive.cluster.delegation.token.renew-interval and hive.cluster.delegation.token.max-lifetime to one year as described here. But in my opinion it is not an engineer-way of doing things </description>
			<version>1.2.1</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.TestZooKeeperTokenStore.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">13090</link>
		</links>
	</bug>
	<bug id="13079" opendate="2016-02-18 01:18:31" fixdate="2016-02-19 07:01:46" resolution="Fixed">
		<buginformation>
			<summary>LLAP: Allow reading log4j properties from default JAR resources</summary>
			<description>If the log4j2 configuration is not overriden by the user, the Slider pkg creation fails since the config is generated from a URL.
Allow for the .properties file to be created from default JAR resources if user provides no overrides.</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0, 2.0.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
		</fixedFiles>
	</bug>
	<bug id="13077" opendate="2016-02-17 20:37:46" fixdate="2016-02-19 07:03:08" resolution="Fixed">
		<buginformation>
			<summary>LLAP: Scrub daemon-site.xml from client configs</summary>
			<description>


 if (llapMode) {
      // add configs for llap-daemon-site.xml + localize llap jars
      // they cannot be referred to directly as it would be a circular depedency
      conf.addResource("llap-daemon-site.xml");



</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
		</fixedFiles>
		<links>
			<link type="dependent" description="depends upon">12967</link>
		</links>
	</bug>
	<bug id="13086" opendate="2016-02-18 18:47:38" fixdate="2016-02-20 02:05:37" resolution="Fixed">
		<buginformation>
			<summary>LLAP: Programmatically initialize log4j2 to print out the properties location</summary>
			<description>In some cases, llap daemon gets initialized with different log4j2.properties than the expected llap-daemon-log4j2.properties. It will be easier if programmatically configure log4j2 so that we can print out the location of properties file that is used for initialization. </description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0, 2.0.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
			<file type="M">org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
		</fixedFiles>
	</bug>
	<bug id="13021" opendate="2016-02-08 08:28:31" fixdate="2016-02-20 05:16:03" resolution="Fixed">
		<buginformation>
			<summary>GenericUDAFEvaluator.isEstimable(agg) always returns false</summary>
			<description>GenericUDAFEvaluator.isEstimable(agg) always returns false, because annotation AggregationType has default RetentionPolicy.CLASS and cannot be retained by the VM at run time.
As result estimate method will never be executed.</description>
			<version>1.2.1</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.NumDistinctValueEstimator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">13109</link>
			<link type="Reference" description="is related to">8188</link>
			<link type="Supercedes" description="supercedes">13109</link>
		</links>
	</bug>
	<bug id="13089" opendate="2016-02-18 20:55:32" fixdate="2016-02-22 09:48:28" resolution="Fixed">
		<buginformation>
			<summary>Rounding in Stats for equality expressions</summary>
			<description>Currently we divide numRows(long) by countDistinct(long), thus ignoring the decimals. We should do proper rounding.
This is specially useful for equality expressions over columns whose values are unique. As NDV estimates allow for a certain error, if countDistinct &amp;gt; numRows, we end up with 0 rows in the estimate for the expression.</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
		</fixedFiles>
	</bug>
	<bug id="13093" opendate="2016-02-19 07:05:00" fixdate="2016-02-22 16:52:51" resolution="Fixed">
		<buginformation>
			<summary>hive metastore does not exit on start failure</summary>
			<description>If metastore startup fails for some reason, such as not being able to access the database, it fails to exit. Instead the process continues to be up in a bad state.
This is happening because of a non daemon thread.</description>
			<version>0.13.1</version>
			<fixedVersion>1.3.0, 2.1.0, 2.0.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
		</fixedFiles>
		<links>
			<link type="Regression" description="is broken by">6319</link>
		</links>
	</bug>
	<bug id="13090" opendate="2016-02-18 21:58:48" fixdate="2016-02-22 17:11:56" resolution="Fixed">
		<buginformation>
			<summary>Hive metastore crashes on NPE with ZooKeeperTokenStore</summary>
			<description>Observed that hive metastore shutdown with NPE from ZookeeperTokenStore.


INFO  [pool-5-thread-192]: metastore.HiveMetaStore (HiveMetaStore.java:logInfo(714)) - 191: Metastore shutdown complete.
 INFO  [pool-5-thread-192]: HiveMetaStore.audit (HiveMetaStore.java:logAuditEvent(340)) - ugi=cvdpqap	ip=/19.1.2.129	cmd=Metastore shutdown complete.	
 ERROR [Thread[Thread-6,5,main]]: thrift.TokenStoreDelegationTokenSecretManager (TokenStoreDelegationTokenSecretManager.java:run(331)) - ExpiredTokenRemover thread received unexpected exception. org.apache.hadoop.hive.thrift.DelegationTokenStore$TokenStoreException: Failed to decode token
org.apache.hadoop.hive.thrift.DelegationTokenStore$TokenStoreException: Failed to decode token
	at org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.getToken(ZooKeeperTokenStore.java:401)
	at org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.removeExpiredTokens(TokenStoreDelegationTokenSecretManager.java:256)
	at org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager$ExpiredTokenRemover.run(TokenStoreDelegationTokenSecretManager.java:319)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.NullPointerException
	at java.io.ByteArrayInputStream.&amp;lt;init&amp;gt;(ByteArrayInputStream.java:106)
	at org.apache.hadoop.security.token.delegation.HiveDelegationTokenSupport.decodeDelegationTokenInformation(HiveDelegationTokenSupport.java:53)
	at org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.getToken(ZooKeeperTokenStore.java:399)
	... 3 more
 INFO  [Thread-3]: metastore.HiveMetaStore (HiveMetaStore.java:run(5639)) - Shutting down hive metastore.

</description>
			<version>1.0.0</version>
			<fixedVersion>1.3.0, 2.1.0, 2.0.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.TestZooKeeperTokenStore.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">13075</link>
			<link type="Regression" description="breaks">15090</link>
		</links>
	</bug>
	<bug id="13105" opendate="2016-02-20 01:47:43" fixdate="2016-02-22 21:52:47" resolution="Fixed">
		<buginformation>
			<summary>LLAP token hashCode and equals methods are incorrect</summary>
			<description>I had wrong assumptions about object vs functional equality. This would need to go to 2.0.1 (target version field is AWOL)
"Luckily" the implications are spurious access denied errors, and not the other way around.</description>
			<version>2.0.0</version>
			<fixedVersion>2.1.0, 2.0.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.llap.security.LlapTokenIdentifier.java</file>
		</fixedFiles>
	</bug>
	<bug id="13082" opendate="2016-02-18 05:22:32" fixdate="2016-02-23 00:08:53" resolution="Fixed">
		<buginformation>
			<summary>Enable constant propagation optimization in query with left semi join</summary>
			<description>Currently constant folding is only allowed for inner or unique join, I think it is also applicable and allowed for left semi join. Otherwise the query like following having multiple joins with left semi joins will fail:

 
select table1.id, table1.val, table2.val2 from table1 inner join table2 on table1.val = &amp;amp;apos;t1val01&amp;amp;apos; and table1.id = table2.id left semi join table3 on table1.dimid = table3.id;


with errors:


java.lang.Exception: java.lang.RuntimeException: Error in configuring object
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462) ~[hadoop-mapreduce-client-common-2.6.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522) [hadoop-mapreduce-client-common-2.6.0.jar:?]
Caused by: java.lang.RuntimeException: Error in configuring object
	at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109) ~[hadoop-common-2.6.0.jar:?]
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75) ~[hadoop-common-2.6.0.jar:?]
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) ~[hadoop-common-2.6.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:446) ~[hadoop-mapreduce-client-core-2.6.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343) ~[hadoop-mapreduce-client-core-2.6.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243) ~[hadoop-mapreduce-client-common-2.6.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[?:1.7.0_45]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[?:1.7.0_45]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[?:1.7.0_45]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[?:1.7.0_45]
	at java.lang.Thread.run(Thread.java:744) ~[?:1.7.0_45]
...
Caused by: java.lang.IndexOutOfBoundsException: Index: 3, Size: 3
	at java.util.ArrayList.rangeCheck(ArrayList.java:635) ~[?:1.7.0_45]
	at java.util.ArrayList.get(ArrayList.java:411) ~[?:1.7.0_45]
	at org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.init(StandardStructObjectInspector.java:118) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
	at org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.&amp;lt;init&amp;gt;(StandardStructObjectInspector.java:109) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector(ObjectInspectorFactory.java:326) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector(ObjectInspectorFactory.java:311) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.getJoinOutputObjectInspector(CommonJoinOperator.java:181) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.initializeOp(CommonJoinOperator.java:319) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator.initializeOp(AbstractMapJoinOperator.java:78) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.initializeOp(MapJoinOperator.java:138) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:355) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:504) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]

</description>
			<version>2.0.0</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">12477</link>
			<link type="dependent" description="is depended upon by">13164</link>
		</links>
	</bug>
	<bug id="13114" opendate="2016-02-22 20:39:11" fixdate="2016-02-23 17:31:32" resolution="Duplicate">
		<buginformation>
			<summary>parquet filter fails for type float when hive.optimize.index.filter=true</summary>
			<description>Hive fails when selecting from a table &amp;amp;apos;stored as parquet&amp;amp;apos; with a row filter based on a float column and hive.optimize.index.filter=true.
The following example fails in hive 1.2.1 (HDP 2.3.2), but works fine in hive 1.1.0 (CDH 5.5.0):


create table p(f float)stored as parquet;
insert into table p values (1), (2), (3);

select * from p where f &amp;gt;= 2;

set hive.optimize.index.filter=true;
select * from p where f &amp;gt;= 2;


The first select query works fine, the second fails with:


Failed with exception java.io.IOException:java.lang.IllegalArgumentException: FilterPredicate column: f&amp;amp;apos;s declared type (java.lang.Double) does not match the schema found in file metadata. Column f is of type: FullTypeDescriptor(PrimitiveType: FLOAT, OriginalType: null)
Valid types for this column are: [class java.lang.Float]


Here&amp;amp;apos;s the stack trace from log4j:


2016-02-22 12:18:30,691 ERROR [main]: CliDriver (SessionState.java:printError(960)) - Failed with exception java.io.IOException:java.lang.IllegalArgumentException: FilterPredicate column: f&amp;amp;apos;s declared type (java.lang.Double) does not match the schema found in file metadata. Column f is of type: FullTypeDescriptor(PrimitiveType: FLOAT, OriginalType: null)
Valid types for this column are: [class java.lang.Float]
java.io.IOException: java.lang.IllegalArgumentException: FilterPredicate column: f&amp;amp;apos;s declared type (java.lang.Double) does not match the schema found in file metadata. Column f is of type: FullTypeDescriptor(PrimitiveType: FLOAT, OriginalType: null)
Valid types for this column are: [class java.lang.Float]
	at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:508)
	at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:415)
	at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:140)
	at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1672)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:736)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.lang.IllegalArgumentException: FilterPredicate column: f&amp;amp;apos;s declared type (java.lang.Double) does not match the schema found in file metadata. Column f is of type: FullTypeDescriptor(PrimitiveType: FLOAT, OriginalType: null)
Valid types for this column are: [class java.lang.Float]
	at parquet.filter2.predicate.ValidTypeMap.assertTypeValid(ValidTypeMap.java:132)
	at parquet.filter2.predicate.SchemaCompatibilityValidator.validateColumn(SchemaCompatibilityValidator.java:185)
	at parquet.filter2.predicate.SchemaCompatibilityValidator.validateColumnFilterPredicate(SchemaCompatibilityValidator.java:160)
	at parquet.filter2.predicate.SchemaCompatibilityValidator.visit(SchemaCompatibilityValidator.java:124)
	at parquet.filter2.predicate.SchemaCompatibilityValidator.visit(SchemaCompatibilityValidator.java:59)
	at parquet.filter2.predicate.Operators$GtEq.accept(Operators.java:248)
	at parquet.filter2.predicate.SchemaCompatibilityValidator.validate(SchemaCompatibilityValidator.java:64)
	at parquet.filter2.compat.RowGroupFilter.visit(RowGroupFilter.java:59)
	at parquet.filter2.compat.RowGroupFilter.visit(RowGroupFilter.java:40)
	at parquet.filter2.compat.FilterCompat$FilterPredicateCompat.accept(FilterCompat.java:126)
	at parquet.filter2.compat.RowGroupFilter.filterRowGroups(RowGroupFilter.java:46)
	at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.getSplit(ParquetRecordReaderWrapper.java:275)
	at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.&amp;lt;init&amp;gt;(ParquetRecordReaderWrapper.java:99)
	at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.&amp;lt;init&amp;gt;(ParquetRecordReaderWrapper.java:85)
	at org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat.getRecordReader(MapredParquetInputFormat.java:72)
	at org.apache.hadoop.hive.ql.exec.FetchOperator$FetchInputFormatSplit.getRecordReader(FetchOperator.java:674)
	at org.apache.hadoop.hive.ql.exec.FetchOperator.getRecordReader(FetchOperator.java:324)
	at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:446)
	... 15 more


</description>
			<version>1.2.1</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.parquet.read.TestParquetFilterPredicate.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.parquet.LeafFilterFactory.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">11504</link>
		</links>
	</bug>
	<bug id="13110" opendate="2016-02-20 06:34:32" fixdate="2016-02-23 23:46:51" resolution="Fixed">
		<buginformation>
			<summary>LLAP: Package log4j2 jars into Slider pkg</summary>
			<description>This forms the alternative path for HIVE-13015 (reverted), so that HIVE-13027 can pick up the right logger impl always.</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0, 2.0.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
		</fixedFiles>
		<links>
			<link type="Supercedes" description="supercedes">13100</link>
			<link type="dependent" description="is depended upon by">13027</link>
		</links>
	</bug>
	<bug id="13094" opendate="2016-02-19 08:33:18" fixdate="2016-02-24 05:03:40" resolution="Fixed">
		<buginformation>
			<summary>CBO: Assertion error  in Case expression</summary>
			<description>Triggered by a trap case in the case evaluation


CASE WHEN (-2) &amp;gt;= 0  THEN SUBSTRING(str0, 1,CAST((-2) AS INT)) ELSE NULL




Exception in thread "b367ad08-d900-4672-8e75-a4e90a52141b b367ad08-d900-4672-8e75-a4e90a52141b main" java.lang.AssertionError: Internal error: Cannot add expression of different type to set:
set type is RecordType(VARCHAR(2147483647) CHARACTER SET "ISO-8859-1" COLLATE "ISO-8859-1$en_US$primary" $f0, VARCHAR(2147483647) CHARACTER SET "ISO-8859-1" COLLATE "ISO-8859-1$en_US$primary" $f1, VARCL
expression type is RecordType(VARCHAR(2147483647) CHARACTER SET "ISO-8859-1" COLLATE "ISO-8859-1$en_US$primary" $f0, VARCHAR(2147483647) CHARACTER SET "ISO-8859-1" COLLATE "ISO-8859-1$en_US$primary" $fL
set is rel#12408:HiveProject.HIVE.[](input=HepRelVertex#12407,$f0=$0,$f1=$6,$f2=CASE(&amp;gt;=(-(2), 0), substring($6, 1, -(2)), null))
expression is HiveProject#12414
        at org.apache.calcite.util.Util.newInternal(Util.java:774)
        at org.apache.calcite.plan.RelOptUtil.verifyTypeEquivalence(RelOptUtil.java:317)
        at org.apache.calcite.plan.hep.HepRuleCall.transformTo(HepRuleCall.java:57)
        at org.apache.calcite.plan.RelOptRuleCall.transformTo(RelOptRuleCall.java:224)
        at org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveReduceExpressionsRule$ProjectReduceExpressionsRule.onMatch(HiveReduceExpressionsRule.java:208)
        at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:318)
        at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:514)
        at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:392)
        at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:285)
        at org.apache.calcite.plan.hep.HepInstruction$RuleCollection.execute(HepInstruction.java:72)
        at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:207)
        at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:194)
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.hepPlan(CalcitePlanner.java:1265)
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.applyPreJoinOrderingTransforms(CalcitePlanner.java:1125)
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.apply(CalcitePlanner.java:938)
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.apply(CalcitePlanner.java:878)
        at org.apache.calcite.tools.Frameworks$1.apply(Frameworks.java:113)
        at org.apache.calcite.prepare.CalcitePrepareImpl.perform(CalcitePrepareImpl.java:969)

</description>
			<version>2.0.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveReduceExpressionsRule.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.HiveRexUtil.java</file>
		</fixedFiles>
	</bug>
	<bug id="13126" opendate="2016-02-23 19:03:03" fixdate="2016-02-24 18:06:15" resolution="Fixed">
		<buginformation>
			<summary>Clean up MapJoinOperator properly to avoid object cache reuse with unintentional states</summary>
			<description>For a given job, one task may reuse other task&amp;amp;apos;s object cache (plan cache) such as MapJoinOperator. This is fine. But if we have some dirty states left over, it may cause issue like wrong results.</description>
			<version>2.0.0</version>
			<fixedVersion>2.1.0, 2.0.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
		</fixedFiles>
	</bug>
	<bug id="13128" opendate="2016-02-23 21:30:54" fixdate="2016-02-24 18:32:28" resolution="Fixed">
		<buginformation>
			<summary>NullScan fails on a secure setup</summary>
			<description>Nullscan provides uris of the form nullscan://null/ - which are added to the list of FileSystems for which Tez should obtain tokens.


2016-02-19T02:48:04,481 ERROR [main]: exec.Task (TezTask.java:execute(219)) - Failed to execute tez graph.
java.lang.IllegalArgumentException: java.net.UnknownHostException: null
  at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:406) ~[hadoop-common-2.7.1.2.3.5.1-26.jar:?]
  at org.apache.hadoop.security.SecurityUtil.buildDTServiceName(SecurityUtil.java:291) ~[hadoop-common-2.7.1.2.3.5.1-26.jar:?]
  at org.apache.hadoop.fs.FileSystem.getCanonicalServiceName(FileSystem.java:302) ~[hadoop-common-2.7.1.2.3.5.1-26.jar:?]
  at org.apache.hadoop.fs.FileSystem.collectDelegationTokens(FileSystem.java:524) ~[hadoop-common-2.7.1.2.3.5.1-26.jar:?]
  at org.apache.hadoop.fs.FileSystem.addDelegationTokens(FileSystem.java:508) ~[hadoop-common-2.7.1.2.3.5.1-26.jar:?]
  at org.apache.tez.common.security.TokenCache.obtainTokensForFileSystemsInternal(TokenCache.java:107) ~[tez-api-0.8.2.2.3.5.1-26.jar:0.8.2.2.3.5.1-26]
  at org.apache.tez.common.security.TokenCache.obtainTokensForFileSystemsInternal(TokenCache.java:86) ~[tez-api-0.8.2.2.3.5.1-26.jar:0.8.2.2.3.5.1-26]
  at org.apache.tez.common.security.TokenCache.obtainTokensForFileSystems(TokenCache.java:76) ~[tez-api-0.8.2.2.3.5.1-26.jar:0.8.2.2.3.5.1-26]
  at org.apache.tez.client.TezClientUtils.addFileSystemCredentialsFromURIs(TezClientUtils.java:338) ~[tez-api-0.8.2.2.3.5.1-26.jar:0.8.2.2.3.5.1-26]
  at org.apache.tez.client.TezClientUtils.setupDAGCredentials(TezClientUtils.java:369) ~[tez-api-0.8.2.2.3.5.1-26.jar:0.8.2.2.3.5.1-26]
  at org.apache.tez.client.TezClientUtils.prepareAndCreateDAGPlan(TezClientUtils.java:704) ~[tez-api-0.8.2.2.3.5.1-26.jar:0.8.2.2.3.5.1-26]
  at org.apache.tez.client.TezClient.submitDAGSession(TezClient.java:522) ~[tez-api-0.8.2.2.3.5.1-26.jar:0.8.2.2.3.5.1-26]
  at org.apache.tez.client.TezClient.submitDAG(TezClient.java:468) ~[tez-api-0.8.2.2.3.5.1-26.jar:0.8.2.2.3.5.1-26]
  at org.apache.hadoop.hive.ql.exec.tez.TezTask.submit(TezTask.java:466) ~[hive-exec-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]
  at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:187) [hive-exec-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]
  at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:158) [hive-exec-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]
  at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89) [hive-exec-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]
  at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1858) [hive-exec-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]
  at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1600) [hive-exec-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]
  at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1373) [hive-exec-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]
  at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1196) [hive-exec-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]
  at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1184) [hive-exec-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]
  at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:228) [hive-cli-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]
  at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:180) [hive-cli-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]
  at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:395) [hive-cli-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]
  at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:331) [hive-cli-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]
  at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:428) [hive-cli-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]
  at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:444) [hive-cli-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]
  at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:744) [hive-cli-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]
  at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:711) [hive-cli-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]
  at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:640) [hive-cli-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_45]
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_45]
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_45]
  at java.lang.reflect.Method.invoke(Method.java:497) ~[?:1.8.0_45]
  at org.apache.hadoop.util.RunJar.run(RunJar.java:221) [hadoop-common-2.7.1.2.3.5.1-26.jar:?]
  at org.apache.hadoop.util.RunJar.main(RunJar.java:136) [hadoop-common-2.7.1.2.3.5.1-26.jar:?]
Caused by: java.net.UnknownHostException: null
  ... 37 more


This is while trying to obtain tokens for 

2016-02-23T02:49:41,782 DEBUG [main]: tez.DagUtils (DagUtils.java:addCredentials(164)) - Marking URI as needing credentials: nullscan://null/default.studenttab10k/part_

</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.NullScanFileSystem.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
		</fixedFiles>
	</bug>
	<bug id="12808" opendate="2016-01-07 23:42:08" fixdate="2016-02-24 19:52:27" resolution="Fixed">
		<buginformation>
			<summary>Logical PPD: Push filter clauses through PTF(Windowing) into TS</summary>
			<description>Simplified repro case of HCC #8880, with the slow query showing the push-down miss. 
And the manually rewritten query to indicate the expected one.
Part of the problem could be the window range not being split apart for PPD, but the FIL is not pushed down even if the rownum filter is removed.


create temporary table positions (regionid string, id bigint, deviceid string, ts string);

insert into positions values(&amp;amp;apos;1d6a0be1-6366-4692-9597-ebd5cd0f01d1&amp;amp;apos;, 1422792010, &amp;amp;apos;6c5d1a30-2331-448b-a726-a380d6b3a432&amp;amp;apos;, &amp;amp;apos;2016-01-01&amp;amp;apos;),
(&amp;amp;apos;1d6a0be1-6366-4692-9597-ebd5cd0f01d1&amp;amp;apos;, 1422792010, &amp;amp;apos;6c5d1a30-2331-448b-a726-a380d6b3a432&amp;amp;apos;, &amp;amp;apos;2016-01-01&amp;amp;apos;),
(&amp;amp;apos;1d6a0be1-6366-4692-9597-ebd5cd0f01d1&amp;amp;apos;, 1422792010, &amp;amp;apos;6c5d1a30-2331-448b-a726-a380d6b3a432&amp;amp;apos;, &amp;amp;apos;2016-01-02&amp;amp;apos;),
(&amp;amp;apos;1d6a0be1-6366-4692-9597-ebd5cd0f01d1&amp;amp;apos;, 1422792010, &amp;amp;apos;6c5d1a30-2331-448b-a726-a380d6b3a432&amp;amp;apos;, &amp;amp;apos;2016-01-02&amp;amp;apos;);


-- slow query
explain
WITH t1 AS 
( 
         SELECT   *, 
                  Row_number() over ( PARTITION BY regionid, id, deviceid ORDER BY ts DESC) AS rownos
         FROM     positions ), 
latestposition as ( 
       SELECT * 
       FROM   t1 
       WHERE  rownos = 1) 
SELECT * 
FROM   latestposition 
WHERE  regionid=&amp;amp;apos;1d6a0be1-6366-4692-9597-ebd5cd0f01d1&amp;amp;apos; 
AND    id=1422792010 
AND    deviceid=&amp;amp;apos;6c5d1a30-2331-448b-a726-a380d6b3a432&amp;amp;apos;;

-- fast query
explain
WITH t1 AS 
( 
         SELECT   *, 
                  Row_number() over ( PARTITION BY regionid, id, deviceid ORDER BY ts DESC) AS rownos
         FROM     positions 
         WHERE  regionid=&amp;amp;apos;1d6a0be1-6366-4692-9597-ebd5cd0f01d1&amp;amp;apos; 
         AND    id=1422792010 
         AND    deviceid=&amp;amp;apos;6c5d1a30-2331-448b-a726-a380d6b3a432&amp;amp;apos;
),latestposition as ( 
       SELECT * 
       FROM   t1 
       WHERE  rownos = 1) 
SELECT * 
FROM   latestposition 
;

</description>
			<version>1.2.1</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.HiveCalciteUtil.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveFilterProjectTransposeRule.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
	</bug>
	<bug id="13147" opendate="2016-02-24 18:31:37" fixdate="2016-02-25 07:01:02" resolution="Duplicate">
		<buginformation>
			<summary>COLUMN_STATS_ACCURATE is not accurate</summary>
			<description>Often we see on a described table:


Table Parameters:	NULL	NULL
18		COLUMN_STATS_ACCURATE	true                
19		numFiles            	1                   
20		numRows             	0                   
21		rawDataSize         	0                   
22		totalSize           	46069               
23		transient_lastDdlTime	1448930216          


Notice


20		numRows             	0                   
21		rawDataSize         	0       


are wrong.
After doing an analyze we get:


Table Parameters:	NULL	NULL
18		COLUMN_STATS_ACCURATE	true                
19		numFiles            	1                   
20		numRows             	823                 
21		rawDataSize         	45246               
22		totalSize           	46069               
23		transient_lastDdlTime	1456338426 

</description>
			<version>1.2.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.plan.StatsWork.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
			<file type="M">org.apache.hive.hcatalog.listener.TestDbNotificationListener.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.StatsOptimizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.StatsTask.java</file>
			<file type="M">org.apache.hadoop.hive.common.StatsSetupConst.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Table.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">12661</link>
		</links>
	</bug>
	<bug id="3335" opendate="2012-08-06 01:07:21" fixdate="2016-02-26 09:13:34" resolution="Duplicate">
		<buginformation>
			<summary>Thousand of CLOSE_WAIT socket when we using SymbolicInputFormat</summary>
			<description>Procedure for reproduction:
 1. Set up hadoop
 2. Prepare data file and link.txt:
    data:
      $ hadoop fs -cat /path/to/data/2012-07-01/20120701.csv
      1, 20120701 00:00:00
      2, 20120701 00:00:01
      3, 20120701 01:12:45
    link.txt
      $ cat link.txt
       /path/to/data/2012-07-01//*
 2. On hive, create table like below:
   CREATE TABLE user_logs(id INT, created_at STRING)
   row format delimited fields terminated by &amp;amp;apos;,&amp;amp;apos; lines terminated by &amp;amp;apos;\n&amp;amp;apos;
   stored as inputformat &amp;amp;apos;org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat&amp;amp;apos;
   outputformat &amp;amp;apos;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&amp;amp;apos;;
 3. Put link.txt to /user/hive/warehouse/user_logs
   $ sudo -u hdfs hadoop fs -put link.txt  /user/hive/warehouse/user_logs
 4. Open another session(A session), and watch socket,
   $ netstat -a | grep CLOSE_WAIT
    tcp        1      0 localhost:48121             localhost:50010
         CLOSE_WAIT
    tcp        1      0 localhost:48124             localhost:50010
         CLOSE_WAIT
   $
 5. Return to hive session, execute this,
   $ select * from user_logs;
 6. Return to A session, watch socket again,
   $ netstat -a | grep CLOSE_WAIT
   tcp        1      0 localhost:48121             localhost:50010
        CLOSE_WAIT
   tcp        1      0 localhost:48124             localhost:50010
        CLOSE_WAIT
   tcp        1      0 localhost:48166             localhost:50010
        CLOSE_WAIT
 If you makes any partitions, you&amp;amp;apos;ll watch unclosed socket whose count
equals partitions by once.
I think that this problem maybe is caused by this point:
  At https://github.com/apache/hive/blob/trunk/ql/src/java/org/apache/hadoop/hive/ql/io/SymbolicInputFormat.java,
  line 66. BufferedReader was opened, but it doesn&amp;amp;apos;t closed.</description>
			<version>0.8.1</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.SymbolicInputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.TestSymlinkTextInputFormat.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">3480</link>
		</links>
	</bug>
	<bug id="13135" opendate="2016-02-24 03:27:50" fixdate="2016-02-26 09:58:24" resolution="Fixed">
		<buginformation>
			<summary>LLAP: HTTPS Webservices needs trusted keystore configs</summary>
			<description>ssl-server.xml is not picked up internally by the hive-common HttpServer impl, unlike the default configs.</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="D">org.apache.hadoop.hive.llap.daemon.services.impl.LlapWebApp.java</file>
			<file type="M">org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
			<file type="M">org.apache.hadoop.hive.llap.configuration.LlapDaemonConfiguration.java</file>
			<file type="M">org.apache.hadoop.hive.llap.daemon.services.impl.LlapWebServices.java</file>
		</fixedFiles>
	</bug>
	<bug id="12935" opendate="2016-01-26 22:14:09" fixdate="2016-02-26 21:20:03" resolution="Fixed">
		<buginformation>
			<summary>LLAP: Replace Yarn registry with Zookeeper registry</summary>
			<description>Existing YARN registry service for cluster membership has to depend on refresh intervals to get the list of instances/daemons that are running in the cluster. Better approach would be replace it with zookeeper based registry service so that custom listeners can be added to update healthiness of daemons in the cluster.  </description>
			<version>2.0.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.llap.registry.ServiceInstanceSet.java</file>
			<file type="M">org.apache.hadoop.hive.llap.registry.ServiceRegistry.java</file>
			<file type="M">org.apache.hadoop.hive.llap.security.LlapSecurityHelper.java</file>
			<file type="M">org.apache.hadoop.hive.llap.registry.impl.LlapRegistryService.java</file>
			<file type="M">org.apache.hadoop.hive.llap.registry.impl.LlapFixedRegistryImpl.java</file>
			<file type="D">org.apache.hadoop.hive.llap.registry.impl.LlapYarnRegistryImpl.java</file>
			<file type="M">org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">10026</link>
			<link type="Supercedes" description="supercedes">10004</link>
			<link type="Supercedes" description="supercedes">10648</link>
			<link type="dependent" description="is depended upon by">13168</link>
			<link type="dependent" description="is depended upon by">13167</link>
			<link type="dependent" description="is depended upon by">12959</link>
		</links>
	</bug>
	<bug id="13013" opendate="2016-02-05 18:03:51" fixdate="2016-02-29 19:34:07" resolution="Fixed">
		<buginformation>
			<summary>Further Improve concurrency in TxnHandler</summary>
			<description>There are still a few operations in TxnHandler that run at Serializable isolation.
Most or all of them can be dropped to READ_COMMITTED now that we have SELECT ... FOR UPDATE support.  This will reduce number of deadlocks in the DBs.</description>
			<version>1.0.0</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TestTxnHandlerNegative.java</file>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="is blocked by">11948</link>
			<link type="Blocker" description="is blocked by">11388</link>
		</links>
	</bug>
	<bug id="13160" opendate="2016-02-26 00:02:11" fixdate="2016-03-01 14:30:23" resolution="Fixed">
		<buginformation>
			<summary>HS2 unable to load UDFs on startup when HMS is not ready</summary>
			<description>The error looks like this:


2016-02-18 14:43:54,251 INFO  hive.metastore: [main]: Trying to connect to metastore with URI thrift://host-10-17-81-201.coe.cloudera.com:9083
2016-02-18 14:48:54,692 WARN  hive.metastore: [main]: Failed to connect to the MetaStore Server...
2016-02-18 14:48:54,692 INFO  hive.metastore: [main]: Waiting 1 seconds before next connection attempt.
2016-02-18 14:48:55,692 INFO  hive.metastore: [main]: Trying to connect to metastore with URI thrift://host-10-17-81-201.coe.cloudera.com:9083
2016-02-18 14:53:55,800 WARN  hive.metastore: [main]: Failed to connect to the MetaStore Server...
2016-02-18 14:53:55,800 INFO  hive.metastore: [main]: Waiting 1 seconds before next connection attempt.
2016-02-18 14:53:56,801 INFO  hive.metastore: [main]: Trying to connect to metastore with URI thrift://host-10-17-81-201.coe.cloudera.com:9083
2016-02-18 14:58:56,967 WARN  hive.metastore: [main]: Failed to connect to the MetaStore Server...
2016-02-18 14:58:56,967 INFO  hive.metastore: [main]: Waiting 1 seconds before next connection attempt.
2016-02-18 14:58:57,994 WARN  hive.ql.metadata.Hive: [main]: Failed to register all functions.
java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
        at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1492)
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&amp;lt;init&amp;gt;(RetryingMetaStoreClient.java:64)
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:74)
        at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2915)
.......
016-02-18 14:58:57,997 INFO  hive.metastore: [main]: Trying to connect to metastore with URI thrift://host-10-17-81-201.coe.cloudera.com:9083
2016-02-18 15:03:58,094 WARN  hive.metastore: [main]: Failed to connect to the MetaStore Server...
2016-02-18 15:03:58,095 INFO  hive.metastore: [main]: Waiting 1 seconds before next connection attempt.
2016-02-18 15:03:59,095 INFO  hive.metastore: [main]: Trying to connect to metastore with URI thrift://host-10-17-81-201.coe.cloudera.com:9083
2016-02-18 15:08:59,203 WARN  hive.metastore: [main]: Failed to connect to the MetaStore Server...
2016-02-18 15:08:59,203 INFO  hive.metastore: [main]: Waiting 1 seconds before next connection attempt.
2016-02-18 15:09:00,203 INFO  hive.metastore: [main]: Trying to connect to metastore with URI thrift://host-10-17-81-201.coe.cloudera.com:9083
2016-02-18 15:14:00,304 WARN  hive.metastore: [main]: Failed to connect to the MetaStore Server...
2016-02-18 15:14:00,304 INFO  hive.metastore: [main]: Waiting 1 seconds before next connection attempt.
2016-02-18 15:14:01,306 INFO  org.apache.hive.service.server.HiveServer2: [main]: Shutting down HiveServer2
2016-02-18 15:14:01,308 INFO  org.apache.hive.service.server.HiveServer2: [main]: Exception caught when calling stop of HiveServer2 before retrying start
java.lang.NullPointerException
        at org.apache.hive.service.server.HiveServer2.stop(HiveServer2.java:283)
        at org.apache.hive.service.server.HiveServer2.startHiveServer2(HiveServer2.java:351)
        at org.apache.hive.service.server.HiveServer2.access$400(HiveServer2.java:69)
        at org.apache.hive.service.server.HiveServer2$StartOptionExecutor.execute(HiveServer2.java:545)


And then none of the functions will be available for use as HS2 does not re-register them after HMS is up and ready.
This is not desired behaviour, we shouldn&amp;amp;apos;t allow HS2 to be in a servicing state if function list is not ready. Or, maybe instead of initialize the function list when HS2 starts, try to load the function list when each Hive session is created. Of course we can have a cache of function list somewhere for better performance, but we would better decouple it from class Hive.</description>
			<version>1.2.1</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
	</bug>
	<bug id="13146" opendate="2016-02-24 16:01:58" fixdate="2016-03-01 14:48:59" resolution="Fixed">
		<buginformation>
			<summary>OrcFile table property values are case sensitive</summary>
			<description>In Hive v1.2.1.2.3, with Tez , create an external table with compression SNAPPY value marked as lower case.  Table is created successfully.  Insert data into table fails with no enum constant error.
CREATE EXTERNAL TABLE mydb.mytable 
(id int)
  PARTITIONED BY (business_date date)
STORED AS ORC
LOCATION
  &amp;amp;apos;/data/mydb/mytable&amp;amp;apos;
TBLPROPERTIES (
  &amp;amp;apos;orc.compress&amp;amp;apos;=&amp;amp;apos;snappy&amp;amp;apos;);
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
INSERT OVERWRITE mydb.mytable PARTITION (business_date)
SELECT * from mydb.sourcetable;
Caused by: java.lang.IllegalArgumentException: No enum constant org.apache.hadoop.hive.ql.io.orc.CompressionKind.snappy
	at java.lang.Enum.valueOf(Enum.java:238)
	at org.apache.hadoop.hive.ql.io.orc.CompressionKind.valueOf(CompressionKind.java:25)
Constant SNAPPY needs to be uppercase in definition to fix.  Case should be agnostic or throw error on creation of table.</description>
			<version>1.2.1</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
			<file type="M">org.apache.orc.OrcFile.java</file>
		</fixedFiles>
	</bug>
	<bug id="12757" opendate="2015-12-29 21:44:00" fixdate="2016-03-01 20:08:56" resolution="Fixed">
		<buginformation>
			<summary>Fix TestCodahaleMetrics#testFileReporting</summary>
			<description>Codahale Metrics file reporter is time based, hence test is as well.  On slow machines, sometimes the file is not written fast enough to be read.</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.common.metrics.MetricsTestUtils.java</file>
			<file type="M">org.apache.hadoop.hive.common.metrics.metrics2.TestCodahaleMetrics.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">12628</link>
		</links>
	</bug>
	<bug id="13174" opendate="2016-02-26 23:34:36" fixdate="2016-03-01 22:45:32" resolution="Fixed">
		<buginformation>
			<summary>Remove Vectorizer noise in logs</summary>
			<description>If you have a table with a bin column you&amp;amp;apos;re hs2/client logs are full of the stack traces below. These should either be made debug or we just log the message not the trace.


2015-10-12 12:34:23,922 INFO  [main]: physical.Vectorizer (Vectorizer.java:validateExprNodeDesc(1249)) - Failed to vectorize
org.apache.hadoop.hive.ql.metadata.HiveException: No vector argument type for type name binary
	at org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getConstantVectorExpression(VectorizationContext.java:872)
	at org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getVectorExpression(VectorizationContext.java:443)
	at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateExprNodeDesc(Vectorizer.java:1243)
	at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateExprNodeDesc(Vectorizer.java:1234)
	at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateSelectOperator(Vectorizer.java:1100)
	at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateMapWorkOperator(Vectorizer.java:911)
	at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$MapWorkValidationNodeProcessor.process(Vectorizer.java:581)
	at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:95)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:79)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:133)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:110)
	at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.validateMapWork(Vectorizer.java:412)
	at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.convertMapWork(Vectorizer.java:355)
	at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.dispatch(Vectorizer.java:330)
	at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.dispatch(TaskGraphWalker.java:111)
	at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.walk(TaskGraphWalker.java:180)
	at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.startWalking(TaskGraphWalker.java:125)
	at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.resolve(Vectorizer.java:890)
	at org.apache.hadoop.hive.ql.parse.TezCompiler.optimizeTaskPlan(TezCompiler.java:469)
	at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:227)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10188)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:211)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:424)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:308)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1122)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1170)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311)
	at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:409)
	at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:425)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:714)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)

</description>
			<version>2.0.0</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
		</fixedFiles>
	</bug>
	<bug id="12749" opendate="2015-12-25 13:04:29" fixdate="2016-03-02 12:39:10" resolution="Fixed">
		<buginformation>
			<summary>Constant propagate returns string values in incorrect format</summary>
			<description>STEP 1. Create and upload test data
Execute in command line:

nano stest.data


Add to file:

000126,000777
000126,000778
000126,000779
000474,000888
000468,000889
000272,000880



hadoop fs -put stest.data /



hive&amp;gt; create table stest(x STRING, y STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY &amp;amp;apos;,&amp;amp;apos;;
hive&amp;gt; LOAD DATA  INPATH &amp;amp;apos;/stest.data&amp;amp;apos; OVERWRITE INTO TABLE stest;


STEP 2. Execute test query (with cast for x)

select x from stest where cast(x as int) = 126;


EXPECTED RESULT:

000126
000126
000126


ACTUAL RESULT:

126
126
126


STEP 3. Execute test query (no cast for x)

hive&amp;gt; select x from stest where  x = 126; 


EXPECTED RESULT:

000126
000126
000126


ACTUAL RESULT:

126
126
126


In steps #2, #3 I expected &amp;amp;apos;000126&amp;amp;apos; because the origin type of x is STRING in stest table.
Note, setting hive.optimize.constant.propagation=false fixes the issue.

hive&amp;gt; set hive.optimize.constant.propagation=false;
hive&amp;gt; select x from stest where  x = 126;
OK
000126
000126
000126


Related to HIVE-11104, HIVE-8555</description>
			<version>1.0.0</version>
			<fixedVersion>2.1.0, 2.0.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
		</fixedFiles>
		<links>
			<link type="Incorporates" description="incorporates">13197</link>
			<link type="Reference" description="is related to">15106</link>
		</links>
	</bug>
	<bug id="13163" opendate="2016-02-26 01:26:33" fixdate="2016-03-03 06:54:36" resolution="Fixed">
		<buginformation>
			<summary>ORC MemoryManager thread checks are fatal, should WARN </summary>
			<description>The MemoryManager is tied to a WriterOptions on create, which can occur in a different thread from the writer calls.
This is unexpected, but safe and needs a warning not a fatal.


  /**
   * Light weight thread-safety check for multi-threaded access patterns
   */
  private void checkOwner() {
    Preconditions.checkArgument(ownerLock.isHeldByCurrentThread(),
        "Owner thread expected %s, got %s",
        ownerLock.getOwner(),
        Thread.currentThread());
  }


</description>
			<version>2.0.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.orc.impl.MemoryManager.java</file>
		</fixedFiles>
	</bug>
	<bug id="13108" opendate="2016-02-20 02:52:45" fixdate="2016-03-03 06:55:20" resolution="Fixed">
		<buginformation>
			<summary>Operators: SORT BY randomness is not safe with network partitions</summary>
			<description>SORT BY relies on a transient Random object, which is initialized once per deserialize operation.
This results in complications during a network partition and when Tez/Spark reuses a cached plan.</description>
			<version>1.2.1</version>
			<fixedVersion>1.3.0, 2.1.0, 2.0.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
		</fixedFiles>
	</bug>
	<bug id="13186" opendate="2016-02-29 22:53:39" fixdate="2016-03-03 20:51:59" resolution="Fixed">
		<buginformation>
			<summary>ALTER TABLE RENAME should lowercase table name and hdfs location</summary>
			<description></description>
			<version>2.0.0</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.hcatalog.cli.TestSemanticAnalysis.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
		</fixedFiles>
	</bug>
	<bug id="13169" opendate="2016-02-26 06:56:55" fixdate="2016-03-04 20:51:38" resolution="Fixed">
		<buginformation>
			<summary>HiveServer2: Support delegation token based connection when using http transport</summary>
			<description>HIVE-5155 introduced support for delegation token based connection. However, it was intended for tcp transport mode. We need to have similar mechanisms for http transport.</description>
			<version>1.2.1</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.jdbc.HiveConnection.java</file>
			<file type="M">org.apache.hive.service.auth.HiveAuthFactory.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.ThriftHttpServlet.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.TestZooKeeperTokenStore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.DelegationTokenSecretManager.java</file>
			<file type="M">org.apache.hive.service.cli.CLIService.java</file>
			<file type="M">org.apache.hadoop.hive.thrift.TestHadoopAuthBridge23.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.ThriftHttpCLIService.java</file>
			<file type="M">org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
			<file type="M">org.apache.hive.service.cli.session.HiveSessionImplwithUGI.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">5155</link>
			<link type="Reference" description="is related to">13056</link>
			<link type="Regression" description="breaks">13209</link>
		</links>
	</bug>
	<bug id="13200" opendate="2016-03-03 16:29:07" fixdate="2016-03-05 16:12:07" resolution="Fixed">
		<buginformation>
			<summary>Aggregation functions returning empty rows on partitioned columns</summary>
			<description>Running aggregation functions like MAX, MIN, DISTINCT against partitioned columns will return empty rows if table has property: &amp;amp;apos;skip.header.line.count&amp;amp;apos;=&amp;amp;apos;1&amp;amp;apos;
Reproduce:

DROP TABLE IF EXISTS test;

CREATE TABLE test (a int) 
PARTITIONED BY (b int) 
ROW FORMAT DELIMITED FIELDS TERMINATED BY &amp;amp;apos;|&amp;amp;apos; 
TBLPROPERTIES(&amp;amp;apos;skip.header.line.count&amp;amp;apos;=&amp;amp;apos;1&amp;amp;apos;);

INSERT OVERWRITE TABLE test PARTITION (b = 1) VALUES (1), (2), (3), (4);
INSERT OVERWRITE TABLE test PARTITION (b = 2) VALUES (1), (2), (3), (4);

SELECT * FROM test;

SELECT DISTINCT b FROM test;
SELECT MAX(b) FROM test;
SELECT DISTINCT a FROM test;


The output:

0: jdbc:hive2://localhost:10000/default&amp;gt; SELECT * FROM test;
+---------+---------+--+
| test.a  | test.b  |
+---------+---------+--+
| 2       | 1       |
| 3       | 1       |
| 4       | 1       |
| 2       | 2       |
| 3       | 2       |
| 4       | 2       |
+---------+---------+--+
6 rows selected (0.631 seconds)

0: jdbc:hive2://localhost:10000/default&amp;gt; SELECT DISTINCT b FROM test;
+----+--+
| b  |
+----+--+
+----+--+
No rows selected (47.229 seconds)

0: jdbc:hive2://localhost:10000/default&amp;gt; SELECT MAX(b) FROM test;
+-------+--+
|  _c0  |
+-------+--+
| NULL  |
+-------+--+
1 row selected (49.508 seconds)

0: jdbc:hive2://localhost:10000/default&amp;gt; SELECT DISTINCT a FROM test;
+----+--+
| a  |
+----+--+
| 2  |
| 3  |
| 4  |
+----+--+
3 rows selected (46.859 seconds)

</description>
			<version>1.0.0</version>
			<fixedVersion>1.3.0, 2.1.0, 2.0.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.MetadataOnlyOptimizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.TableScanDesc.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">12950</link>
		</links>
	</bug>
	<bug id="13199" opendate="2016-03-03 00:50:45" fixdate="2016-03-06 07:01:54" resolution="Fixed">
		<buginformation>
			<summary>NDC stopped working in LLAP logging</summary>
			<description>NDC context were missing from the log lines. Reason for it is NDC class is part of log4j-1.2-api (bridge jar). This is added as compile time dependency. Due to the absence of this jar in llap daemons, the NDC context failed to initialize. Log4j2 replaced NDC with ThreadContext. Hence we need the bridge jar.</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0, 2.0.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
		</fixedFiles>
	</bug>
	<bug id="13063" opendate="2016-02-16 00:13:40" fixdate="2016-03-07 02:41:18" resolution="Fixed">
		<buginformation>
			<summary>Create UDFs for CHR and REPLACE </summary>
			<description>Create UDFS for these functions.
CHR: convert n where n : [0, 256) into the ascii equivalent as a varchar. If n is less than 0 or greater than 255, return the empty string. If n is 0, return null.
REPLACE: replace all substrings of &amp;amp;apos;str&amp;amp;apos; that match &amp;amp;apos;search&amp;amp;apos; with &amp;amp;apos;rep&amp;amp;apos;.
Example. SELECT REPLACE(&amp;amp;apos;Hack and Hue&amp;amp;apos;, &amp;amp;apos;H&amp;amp;apos;, &amp;amp;apos;BL&amp;amp;apos;);
Equals &amp;amp;apos;BLack and BLue&amp;amp;apos;"</description>
			<version>1.2.0</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">14581</link>
			<link type="Duplicate" description="is duplicated by">14581</link>
		</links>
	</bug>
	<bug id="13096" opendate="2016-02-19 14:41:50" fixdate="2016-03-07 08:49:16" resolution="Fixed">
		<buginformation>
			<summary>Cost to choose side table in MapJoin conversion based on cumulative cardinality</summary>
			<description>HIVE-11954 changed the logic to choose the side table in the MapJoin conversion algorithm. Initial heuristic for the cost was based on number of heavyweight operators.
This extends that work so the heuristic is based on accumulate cardinality. In the future, we should choose the side based on total latency for the input.</description>
			<version>2.0.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">14307</link>
			<link type="Supercedes" description="supercedes">11954</link>
		</links>
	</bug>
	<bug id="13209" opendate="2016-03-04 21:39:50" fixdate="2016-03-07 17:04:33" resolution="Fixed">
		<buginformation>
			<summary>metastore get_delegation_token fails with null ip address</summary>
			<description>After changes in HIVE-13169, metastore get_delegation_token fails with null ip address.


2016-03-03 07:45:31,055 ERROR [pool-6-thread-22]: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(159)) - MetaException(message:Unauthorized connection for super-user: HTTP/&amp;lt;hostname@realm&amp;gt; from IP null)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_delegation_token(HiveMetaStore.java:5290)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at com.sun.proxy.$Proxy16.get_delegation_token(Unknown Source)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_delegation_token.getResult(ThriftHiveMetastore.java:11492)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_delegation_token.getResult(ThriftHiveMetastore.java:11476)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:551)
	at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:546)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:546)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)


</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.IpAddressListener.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.TSetIpAddressProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
		</fixedFiles>
		<links>
			<link type="Regression" description="is broken by">13169</link>
		</links>
	</bug>
	<bug id="13083" opendate="2016-02-18 06:33:01" fixdate="2016-03-07 21:09:36" resolution="Fixed">
		<buginformation>
			<summary>Writing HiveDecimal to ORC can wrongly suppress present stream</summary>
			<description>HIVE-3976 can cause ORC file to be unreadable. The changes introduced in HIVE-3976 for DecimalTreeWriter can create null values after updating the isPresent stream. https://github.com/apache/hive/blob/branch-0.13/ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java#L1337
As result of the above return statement, isPresent stream state can become wrong. The isPresent stream thinks all values are non-null and hence suppressed. But the data stream will be of 0 length. When reading such files we will get the following exception


Caused by: java.io.EOFException: Reading BigInteger past EOF from compressed stream Stream for column 3 kind DATA position: 0 length: 0 range: 0 offset: 0 limit: 0
        at org.apache.hadoop.hive.ql.io.orc.SerializationUtils.readBigInteger(SerializationUtils.java:176)
        at org.apache.hadoop.hive.ql.io.orc.TreeReaderFactory$DecimalTreeReader.next(TreeReaderFactory.java:1264)
        at org.apache.hadoop.hive.ql.io.orc.TreeReaderFactory$StructTreeReader.next(TreeReaderFactory.java:2004)
        at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.next(RecordReaderImpl.java:1039)
        ... 24 more

</description>
			<version>0.13.0</version>
			<fixedVersion>1.3.0, 2.1.0, 2.0.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.orc.OrcProto.java</file>
			<file type="M">org.apache.orc.OrcFile.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.TestOrcFile.java</file>
		</fixedFiles>
		<links>
			<link type="dependent" description="is depended upon by">13220</link>
		</links>
	</bug>
	<bug id="13210" opendate="2016-03-04 22:47:06" fixdate="2016-03-08 18:11:28" resolution="Fixed">
		<buginformation>
			<summary>Revert changes in HIVE-12994 related to metastore</summary>
			<description>As we do not control what is written in the physical layer and thus we cannot ensure NULLS ORDER (and even if we did, currently we do not take advantage of it), it seems exposing the NULLS ORDER property at metastore level does not make much sense. We will revert that part of patch HIVE-12994.</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.hbase.HBaseUtils.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.hbase.TestHBaseStore.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.AbstractSMBJoinProc.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.hbase.HbaseMetastoreProto.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.hbase.TestSharedStorageDescriptor.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.hbase.TestHBaseStoreBitVector.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.RelOptHiveTable.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.txn.compactor.TestWorker.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.Order.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingCtx.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.model.MOrder.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.BucketingSortingReduceSinkOptimizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingOpProcFactory.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">11160</link>
			<link type="Incorporates" description="is part of">12994</link>
		</links>
	</bug>
	<bug id="13227" opendate="2016-03-08 08:29:02" fixdate="2016-03-08 18:30:39" resolution="Fixed">
		<buginformation>
			<summary>LLAP: Change daemon initialization logs from INFO to WARN</summary>
			<description>In production LLAP is typically run with WARN log level. It will be useful to print the llap daemon initialization configs at WARN level instead of INFO level so that we can verify if daemon configs are propagated properly. 
NO PRECOMMIT TESTS</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
		</fixedFiles>
	</bug>
	<bug id="13153" opendate="2016-02-25 02:50:36" fixdate="2016-03-08 21:27:01" resolution="Fixed">
		<buginformation>
			<summary>SessionID is appended to thread name twice</summary>
			<description>HIVE-12249 added sessionId to thread name. In some cases the sessionId could be appended twice. Example log line


DEBUG [6432ec22-9f66-4fa5-8770-488a9d3f0b61 6432ec22-9f66-4fa5-8770-488a9d3f0b61 main]

 </description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0, 2.0.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.session.SessionState.java</file>
			<file type="M">org.apache.hadoop.hive.cli.CliDriver.java</file>
			<file type="M">org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">13327</link>
			<link type="Reference" description="is related to">13485</link>
		</links>
	</bug>
	<bug id="13216" opendate="2016-03-07 05:51:37" fixdate="2016-03-09 19:17:33" resolution="Fixed">
		<buginformation>
			<summary>ORC Reader will leave file open until GC when opening a malformed ORC file</summary>
			<description>In ORC extractMetaInfoFromFooter method of ReaderImpl.java:
A new input stream is open without try-catch-finally to enforce closing.
Once the footer parse has some exception, the stream close will miss. 
Until GC happen to close the stream.
private static FileMetaInfo extractMetaInfoFromFooter(FileSystem fs,
                                                        Path path,
                                                        long maxFileLength
                                                        ) throws IOException 
{
    FSDataInputStream file = fs.open(path);

    ...
    file.close();

    return new FileMetaInfo(
        ps.getCompression().toString(),
        (int) ps.getCompressionBlockSize(),
        (int) ps.getMetadataLength(),
        buffer,
        ps.getVersionList(),
        writerVersion
        );
  }</description>
			<version>1.2.0</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
		</fixedFiles>
	</bug>
	<bug id="13247" opendate="2016-03-09 20:17:48" fixdate="2016-03-09 21:39:37" resolution="Duplicate">
		<buginformation>
			<summary>HIVE-13040 broke spark tests</summary>
			<description>I confirmed that spark tests are getting stuck due to HIVE-13040. join_empty is an example test; it gets stuck on master presently, presumably because 0 splits are generated. When I reverted  HIVE-13040 locally, it passed for me. We should fix this or revert HIVE-13040</description>
			<version>2.1.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.spark.client.RemoteDriver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">13223</link>
			<link type="Regression" description="is broken by">13040</link>
		</links>
	</bug>
	<bug id="12039" opendate="2015-10-05 23:38:13" fixdate="2016-03-10 00:29:19" resolution="Fixed">
		<buginformation>
			<summary>Temporarily disable TestSSL#testSSLVersion </summary>
			<description>Looks like it&amp;amp;apos;s only run on Linux and failing after HIVE-11720.</description>
			<version>1.3.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.jdbc.TestSSL.java</file>
		</fixedFiles>
		<links>
			<link type="Container" description="Is contained by">12773</link>
			<link type="Duplicate" description="is duplicated by">12214</link>
			<link type="Reference" description="is related to">13253</link>
		</links>
	</bug>
	<bug id="13144" opendate="2016-02-24 14:04:46" fixdate="2016-03-10 18:19:41" resolution="Fixed">
		<buginformation>
			<summary>HS2 can leak ZK ACL objects when curator retries to create the persistent ephemeral node</summary>
			<description>When the node gets deleted from ZK due to connection loss and curator tries to recreate the node, it might leak ZK ACL.</description>
			<version>1.2.1</version>
			<fixedVersion>1.3.0, 2.1.0, 2.0.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.service.server.HiveServer2.java</file>
		</fixedFiles>
	</bug>
	<bug id="13175" opendate="2016-02-27 01:26:11" fixdate="2016-03-10 22:16:08" resolution="Fixed">
		<buginformation>
			<summary>Disallow making external tables transactional</summary>
			<description>The fact that compactor rewrites contents of ACID tables is in conflict with what is expected of external tables.
Conversely, end user can write to External table which certainly not what is expected of ACID table.
So we should explicitly disallow making an external table ACID.</description>
			<version>2.0.0</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.TransactionalValidationListener.java</file>
		</fixedFiles>
	</bug>
	<bug id="13236" opendate="2016-03-08 21:51:36" fixdate="2016-03-11 03:43:13" resolution="Fixed">
		<buginformation>
			<summary>LLAP: token renewal interval needs to be set</summary>
			<description></description>
			<version>2.0.0</version>
			<fixedVersion>2.1.0, 2.0.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.llap.security.SecretManager.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">12659</link>
		</links>
	</bug>
	<bug id="13251" opendate="2016-03-09 22:29:59" fixdate="2016-03-14 15:10:33" resolution="Fixed">
		<buginformation>
			<summary>hive can&amp;apos;t read the decimal in AVRO file generated from previous version</summary>
			<description>HIVE-7174 makes the avro schema change to match avro definition, while it breaks the compatibility if the file is generated from the previous Hive although the file schema from the file for such decimal is not correct based on avro definition. We should allow to read old file format "precision" : "4", "scale": "8", but when we write, we should write in the new format.</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.serde2.avro.AvroDeserializer.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">7174</link>
		</links>
	</bug>
	<bug id="13201" opendate="2016-03-03 19:22:15" fixdate="2016-03-14 21:47:53" resolution="Fixed">
		<buginformation>
			<summary>Compaction shouldn&amp;apos;t be allowed on non-ACID table</summary>
			<description>Looks like compaction is allowed on non-ACID table, although that&amp;amp;apos;s of no sense and does nothing. Moreover the compaction request will be enqueued into COMPACTION_QUEUE metastore table, which brings unnecessary overhead.
We should prevent compaction commands being allowed on non-ACID tables.</description>
			<version>2.0.0</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.ErrorMsg.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
		</fixedFiles>
	</bug>
	<bug id="13185" opendate="2016-02-29 21:44:04" fixdate="2016-03-15 00:00:48" resolution="Fixed">
		<buginformation>
			<summary>orc.ReaderImp.ensureOrcFooter() method fails on small text files with IndexOutOfBoundsException</summary>
			<description>Steps to reproduce:
1. Create a Text source table with one line of data:


create table src (id int);
insert overwrite table src values (1);


2. Create a target table:


create table trg (id int);


3. Try to load small text file to the target table:


load data inpath &amp;amp;apos;user/hive/warehouse/src/000000_0&amp;amp;apos; into table trg;


Error message:

FAILED: SemanticException Unable to load data to destination table. Error: java.lang.IndexOutOfBoundsException
Stack trace:

org.apache.hadoop.hive.ql.parse.SemanticException: Unable to load data to destination table. Error: java.lang.IndexOutOfBoundsException
	at org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.ensureFileFormatsMatch(LoadSemanticAnalyzer.java:340)
	at org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.analyzeInternal(LoadSemanticAnalyzer.java:224)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:242)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:481)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:317)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1190)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1285)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1116)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1104)
...

</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">14556</link>
		</links>
	</bug>
	<bug id="13233" opendate="2016-03-08 19:18:00" fixdate="2016-03-15 09:44:24" resolution="Fixed">
		<buginformation>
			<summary>Use min and max values to estimate better stats for comparison operators</summary>
			<description>We should benefit from the min/max values for each column to calculate more precisely the number of rows produced by expressions with comparison operators</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
		</fixedFiles>
	</bug>
	<bug id="13226" opendate="2016-03-08 08:00:11" fixdate="2016-03-15 18:17:42" resolution="Fixed">
		<buginformation>
			<summary>Improve tez print summary to print query execution breakdown</summary>
			<description>When tez print summary is enabled, methods summary is printed which are difficult to correlate with the actual execution time. We can improve that to print  the execution times in the sequence of operations that happens behind the scenes.
Instead of printing the methods name it will be useful to print something like below
1) Query Compilation time
2) Query Submit to DAG Submit time
3) DAG Submit to DAG Accept time
4) DAG Accept to DAG Start time
5) DAG Start to DAG End time
With this it will be easier to find out where the actual time is spent. </description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
			<file type="M">org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">13256</link>
			<link type="dependent" description="depends upon">12558</link>
		</links>
	</bug>
	<bug id="12995" opendate="2016-02-03 22:42:17" fixdate="2016-03-15 20:51:06" resolution="Fixed">
		<buginformation>
			<summary>LLAP: Synthetic file ids need collision checks</summary>
			<description>LLAP synthetic file ids do not have any way of checking whether a collision occurs other than a data-error.
Synthetic file-ids have only been used with unit tests so far - but they will be needed to add cache mechanisms to non-HDFS filesystems.
In case of Synthetic file-ids, it is recommended that we track the full-tuple (path, mtime, len) in the cache so that a cache-hit for the synthetic file-id can be compared against the parameters &amp;amp; only accepted if those match.</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.java</file>
			<file type="D">org.apache.hadoop.hive.llap.cache.Cache.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.encoded.Reader.java</file>
			<file type="M">org.apache.hadoop.hive.llap.io.metadata.OrcFileMetadata.java</file>
			<file type="M">org.apache.hadoop.hive.llap.io.api.impl.LlapIoImpl.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcSplit.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.encoded.ReaderImpl.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.HdfsUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.encoded.OrcBatchKey.java</file>
			<file type="M">org.apache.hadoop.hive.common.io.DataCache.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.encoded.StreamUtils.java</file>
			<file type="M">org.apache.hadoop.hive.llap.IncrementalObjectSizeEstimator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
			<file type="M">org.apache.hadoop.hive.llap.io.decode.EncodedDataConsumer.java</file>
			<file type="M">org.apache.hadoop.hive.llap.io.metadata.OrcMetadataCache.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.llap.cache.LowLevelCache.java</file>
			<file type="D">org.apache.hadoop.hive.llap.cache.NoopCache.java</file>
			<file type="M">org.apache.hadoop.hive.llap.io.decode.OrcColumnVectorProducer.java</file>
			<file type="M">org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
			<file type="M">org.apache.hadoop.hive.llap.io.metadata.OrcStripeMetadata.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.common.io.encoded.EncodedColumnBatch.java</file>
			<file type="M">org.apache.orc.FileMetadata.java</file>
			<file type="D">org.apache.hadoop.hive.ql.io.orc.encoded.OrcCacheKey.java</file>
			<file type="M">org.apache.hadoop.hive.llap.cache.LowLevelCacheImpl.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">13225</link>
		</links>
	</bug>
	<bug id="13260" opendate="2016-03-10 21:12:33" fixdate="2016-03-16 17:08:48" resolution="Fixed">
		<buginformation>
			<summary>ReduceSinkDeDuplication throws exception when pRS key is empty</summary>
			<description>Steps to reproduce:


set hive.mapred.mode=nonstrict;
set hive.cbo.enable=false;

set hive.map.aggr=false;

set hive.groupby.skewindata=false;
set mapred.reduce.tasks=31;

select compute_stats(a,16),compute_stats(b,16),compute_stats(c,16),compute_stats(d,16)
from
(
select
  avg(DISTINCT substr(src.value,5)) as a,
  max(substr(src.value,5)) as b,
  variance(substr(src.value,5)) as c,
  var_samp(substr(src.value,5)) as d
 from src)subq;

</description>
			<version>2.0.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">11160</link>
		</links>
	</bug>
	<bug id="13285" opendate="2016-03-15 00:29:48" fixdate="2016-03-16 21:09:16" resolution="Fixed">
		<buginformation>
			<summary>Orc concatenation may drop old files from moving to final path</summary>
			<description>ORC concatenation uses combine hive input format for merging files. Under specific case where all files within a combine split are incompatible for merge (old files without stripe statistics) then these files are added to incompatible file set. But this file set is not processed as closeOp() will not be called (no output file writer will exist which will skip super.closeOp()). As a result, these incompatible files are not moved to final path.</description>
			<version>0.14.0</version>
			<fixedVersion>1.3.0, 2.1.0, 2.0.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.OrcFileMergeOperator.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">7509</link>
		</links>
	</bug>
	<bug id="13242" opendate="2016-03-09 16:28:04" fixdate="2016-03-18 12:17:09" resolution="Fixed">
		<buginformation>
			<summary>DISTINCT keyword is dropped by the parser for windowing</summary>
			<description>To reproduce, the following query can be used:

select distinct first_value(t) over ( partition by si order by i, b ) from over10k limit 100;


The distinct keyword is ignored and duplicates are produced.</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0, 2.0.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveAggregate.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelFactories.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">14959</link>
			<link type="Reference" description="is related to">4662</link>
		</links>
	</bug>
	<bug id="13299" opendate="2016-03-17 18:26:45" fixdate="2016-03-19 16:21:47" resolution="Fixed">
		<buginformation>
			<summary>Column Names trimmed of leading and trailing spaces</summary>
			<description>PROBLEM:
As per the Hive Language DDL: 
https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL
In Hive 0.12 and earlier, only alphanumeric and underscore characters are allowed in table and column names.
In Hive 0.13 and later, column names can contain any Unicode character (see HIVE-6013). Any column name that is specified within backticks (`) is treated literally.
However column names


` left` resulted in `left`
` middle ` resulted in `middle`
`right ` resulted in `right`
`middle space` resulted in `middle space`
` middle space ` resulted in `middle space`

</description>
			<version>2.0.0</version>
			<fixedVersion>2.1.0, 2.0.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.hbase.HBaseStore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">13618</link>
		</links>
	</bug>
	<bug id="13125" opendate="2016-02-23 15:29:20" fixdate="2016-03-20 17:54:16" resolution="Fixed">
		<buginformation>
			<summary>Support masking and filtering of rows/columns</summary>
			<description>Traditionally, access control at the row and column level is implemented through views. Using views as an access control method works well only when access rules, restrictions, and conditions are monolithic and simple. It however becomes ineffective when view definitions become too complex because of the complexity and granularity of privacy and security policies. It also becomes costly when a large number of views must be manually updated and maintained. In addition, the ability to update views proves to be challenging. As privacy and security policies evolve, required updates to views may negatively affect the security logic particularly when database applications reference the views directly by name. HIVE row and column access control helps resolve all these problems.</description>
			<version>2.0.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>New Feature</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.HiveV1Authorizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizationValidator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidatorForTest.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizerImpl.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.DummyHiveAuthorizationValidator.java</file>
		</fixedFiles>
		<links>
			<link type="Dependent" description="Dependent">895</link>
			<link type="Duplicate" description="is duplicated by">4046</link>
		</links>
	</bug>
	<bug id="13141" opendate="2016-02-24 07:54:22" fixdate="2016-03-21 15:02:06" resolution="Fixed">
		<buginformation>
			<summary>Hive on Spark over HBase should accept parameters starting with "zookeeper.znode"</summary>
			<description>HBase related paramters has been added by HIVE-12708.
Following the same way,parameters starting with "zookeeper.znode" should be add too,which are also HBase related paramters .
Refering to http://blog.cloudera.com/blog/2013/10/what-are-hbase-znodes/
I have seen a failure with Hive on Spark over HBase  due to customize zookeeper.znode.parent.</description>
			<version>1.2.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.java</file>
		</fixedFiles>
	</bug>
	<bug id="13291" opendate="2016-03-16 03:32:56" fixdate="2016-03-21 17:56:36" resolution="Fixed">
		<buginformation>
			<summary>ORC BI Split strategy should consider block size instead of file size</summary>
			<description>When we force split strategy to use "BI" (using hive.exec.orc.split.strategy), entire file is considered as single split. This might be inefficient when the files are large. Instead, BI should consider splitting at block boundary. </description>
			<version>2.1.0</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
		</fixedFiles>
	</bug>
	<bug id="13327" opendate="2016-03-22 04:43:50" fixdate="2016-03-22 04:49:46" resolution="Fixed">
		<buginformation>
			<summary>SessionID added to HS2 threadname does not trim spaces</summary>
			<description>HIVE-13153 introduced off-by-one in appending spaces to thread names. 
NO PRECOMMIT TESTS</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0, 2.0.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.session.SessionState.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">13153</link>
		</links>
	</bug>
	<bug id="13286" opendate="2016-03-15 01:06:33" fixdate="2016-03-22 21:05:57" resolution="Fixed">
		<buginformation>
			<summary>Query ID is being reused across queries</summary>
			<description>Aihua Xu I see this commit made via HIVE-11488. I see that query id is being reused across queries. This defeats the purpose of a query id. I am not sure what the purpose of the change in that jira is but it breaks the assumption about a query id being unique for each query. Please take a look into this at the earliest.</description>
			<version>2.0.0</version>
			<fixedVersion>2.1.0, 2.0.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.cli.CliDriver.java</file>
			<file type="M">org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
			<file type="M">org.apache.hive.service.cli.session.TestHiveSessionImpl.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">13329</link>
			<link type="Required" description="is required by">11488</link>
		</links>
	</bug>
	<bug id="13246" opendate="2016-03-09 19:39:15" fixdate="2016-03-23 18:27:05" resolution="Fixed">
		<buginformation>
			<summary>Add log line to ORC writer to print out the file path</summary>
			<description>Currently ORC writer does not log anything making it difficult find where the destination path is. 
NO PRECOMMIT TESTS</description>
			<version>2.1.0</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.orc.impl.WriterImpl.java</file>
		</fixedFiles>
	</bug>
	<bug id="13261" opendate="2016-03-10 22:58:04" fixdate="2016-03-24 00:36:54" resolution="Fixed">
		<buginformation>
			<summary>Can not compute column stats for partition when schema evolves</summary>
			<description>To repro


CREATE TABLE partitioned1(a INT, b STRING) PARTITIONED BY(part INT) STORED AS TEXTFILE;

insert into table partitioned1 partition(part=1) values(1, &amp;amp;apos;original&amp;amp;apos;),(2, &amp;amp;apos;original&amp;amp;apos;), (3, &amp;amp;apos;original&amp;amp;apos;),(4, &amp;amp;apos;original&amp;amp;apos;);

-- Table-Non-Cascade ADD COLUMNS ...
alter table partitioned1 add columns(c int, d string);

insert into table partitioned1 partition(part=2) values(1, &amp;amp;apos;new&amp;amp;apos;, 10, &amp;amp;apos;ten&amp;amp;apos;),(2, &amp;amp;apos;new&amp;amp;apos;, 20, &amp;amp;apos;twenty&amp;amp;apos;), (3, &amp;amp;apos;new&amp;amp;apos;, 30, &amp;amp;apos;thirty&amp;amp;apos;),(4, &amp;amp;apos;new&amp;amp;apos;, 40, &amp;amp;apos;forty&amp;amp;apos;);

insert into table partitioned1 partition(part=1) values(5, &amp;amp;apos;new&amp;amp;apos;, 100, &amp;amp;apos;hundred&amp;amp;apos;),(6, &amp;amp;apos;new&amp;amp;apos;, 200, &amp;amp;apos;two hundred&amp;amp;apos;);

analyze table partitioned1 compute statistics for columns;


Error msg:


2016-03-10T14:55:43,205 ERROR [abc3eb8d-7432-47ae-b76f-54c8d7020312 main[]]: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invokeInternal(177)) - NoSuchObjectException(message:Column c for which stats gathering is requested doesn&amp;amp;apos;t exist.)
        at org.apache.hadoop.hive.metastore.ObjectStore.writeMPartitionColumnStatistics(ObjectStore.java:6492)
        at org.apache.hadoop.hive.metastore.ObjectStore.updatePartitionColumnStatistics(ObjectStore.java:6574)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)

</description>
			<version>1.0.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">11160</link>
		</links>
	</bug>
	<bug id="13217" opendate="2016-03-07 18:50:59" fixdate="2016-03-24 08:39:20" resolution="Fixed">
		<buginformation>
			<summary>Replication for HoS mapjoin small file needs to respect dfs.replication.max</summary>
			<description>Currently Hive on Spark Mapjoin replicates small table file to a hard-coded value of 10.  See SparkHashTableSinkOperator.MIN_REPLICATION. 
When dfs.replication.max is less than 10, HoS query fails.  This constant should cap at dfs.replication.max.
Normally dfs.replication.max seems set at 512.</description>
			<version>1.2.1</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.SparkHashTableSinkOperator.java</file>
		</fixedFiles>
	</bug>
	<bug id="13300" opendate="2016-03-17 20:37:57" fixdate="2016-03-24 18:11:09" resolution="Fixed">
		<buginformation>
			<summary>Hive on spark throws exception for multi-insert with join</summary>
			<description>For certain multi-insert queries, Hive on Spark throws a deserialization error.

create table status_updates(userid int,status string,ds string);
create table profiles(userid int,school string,gender int);
drop table school_summary; create table school_summary(school string,cnt int) partitioned by (ds string);
drop table gender_summary; create table gender_summary(gender int,cnt int) partitioned by (ds string);

insert into status_updates values (1, "status_1", "2016-03-16");
insert into profiles values (1, "school_1", 0);

set hive.auto.convert.join=false;
set hive.execution.engine=spark;

FROM (SELECT a.status, b.school, b.gender
FROM status_updates a JOIN profiles b
ON (a.userid = b.userid and
a.ds=&amp;amp;apos;2009-03-20&amp;amp;apos; )
) subq1
INSERT OVERWRITE TABLE gender_summary
PARTITION(ds=&amp;amp;apos;2009-03-20&amp;amp;apos;)
SELECT subq1.gender, COUNT(1) GROUP BY subq1.gender
INSERT OVERWRITE TABLE school_summary
PARTITION(ds=&amp;amp;apos;2009-03-20&amp;amp;apos;)
SELECT subq1.school, COUNT(1) GROUP BY subq1.school


Error:

16/03/17 13:29:00 [task-result-getter-3]: WARN scheduler.TaskSetManager: Lost task 0.0 in stage 2.0 (TID 3, localhost): java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error: Unable to deserialize reduce input key from x1x128x0x0 with properties {serialization.sort.order.null=a, columns=reducesinkkey0, serialization.lib=org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe, serialization.sort.order=+, columns.types=int}
	at org.apache.hadoop.hive.ql.exec.spark.SparkReduceRecordHandler.processRow(SparkReduceRecordHandler.java:279)
	at org.apache.hadoop.hive.ql.exec.spark.HiveReduceFunctionResultList.processNextRecord(HiveReduceFunctionResultList.java:49)
	at org.apache.hadoop.hive.ql.exec.spark.HiveReduceFunctionResultList.processNextRecord(HiveReduceFunctionResultList.java:28)
	at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList$ResultIterator.hasNext(HiveBaseFunctionResultList.java:95)
	at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:41)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error: Unable to deserialize reduce input key from x1x128x0x0 with properties {serialization.sort.order.null=a, columns=reducesinkkey0, serialization.lib=org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe, serialization.sort.order=+, columns.types=int}
	at org.apache.hadoop.hive.ql.exec.spark.SparkReduceRecordHandler.processRow(SparkReduceRecordHandler.java:251)
	... 12 more
Caused by: org.apache.hadoop.hive.serde2.SerDeException: java.io.EOFException
	at org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe.deserialize(BinarySortableSerDe.java:241)
	at org.apache.hadoop.hive.ql.exec.spark.SparkReduceRecordHandler.processRow(SparkReduceRecordHandler.java:249)
	... 12 more
Caused by: java.io.EOFException
	at org.apache.hadoop.hive.serde2.binarysortable.InputByteBuffer.read(InputByteBuffer.java:54)
	at org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe.deserializeInt(BinarySortableSerDe.java:597)
	at org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe.deserialize(BinarySortableSerDe.java:288)
	at org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe.deserialize(BinarySortableSerDe.java:237)
	... 13 more

</description>
			<version>2.0.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.SparkReduceRecordHandler.java</file>
		</fixedFiles>
	</bug>
	<bug id="12616" opendate="2015-12-08 09:39:56" fixdate="2016-03-24 18:14:02" resolution="Fixed">
		<buginformation>
			<summary>NullPointerException when spark session is reused to run a mapjoin</summary>
			<description>The way to reproduce:

set hive.execution.engine=spark;
create table if not exists test(id int);
create table if not exists test1(id int);
insert into test values(1);
insert into test1 values(1);
select max(a.id) from test a ,test1 b
where a.id = b.id;

</description>
			<version>1.3.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">13314</link>
		</links>
	</bug>
	<bug id="13325" opendate="2016-03-21 23:07:59" fixdate="2016-03-24 19:20:49" resolution="Fixed">
		<buginformation>
			<summary>Excessive logging when ORC PPD fails type conversions</summary>
			<description>Timestamp was specified as "YYYY-MM-DD HH:MM:SS": 2016-01-23 00:00:00


2016-02-10 02:15:43,175 [WARN] [TezChild] |orc.RecordReaderImpl|: Exception when evaluating predicate. Skipping ORC PPD. Exception: java.lang.IllegalArgumentException: ORC SARGS could not convert from String to TIMESTAMP
        at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.getBaseObjectForComparison(RecordReaderImpl.java:659)
        at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.evaluatePredicateRange(RecordReaderImpl.java:373)
        at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.evaluatePredicateProto(RecordReaderImpl.java:338)
        at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl$SargApplier.pickRowGroups(RecordReaderImpl.java:710)
        at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.pickRowGroups(RecordReaderImpl.java:751)
        at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.readStripe(RecordReaderImpl.java:777)
        at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.advanceStripe(RecordReaderImpl.java:986)
        at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.advanceToNextRow(RecordReaderImpl.java:1019)
        at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.&amp;lt;init&amp;gt;(RecordReaderImpl.java:205)
        at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.rowsOptions(ReaderImpl.java:598)
        at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger$ReaderPair.&amp;lt;init&amp;gt;(OrcRawRecordMerger.java:183)
        at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger$OriginalReaderPair.&amp;lt;init&amp;gt;(OrcRawRecordMerger.java:226)
        at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.&amp;lt;init&amp;gt;(OrcRawRecordMerger.java:437)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getReader(OrcInputFormat.java:1269)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRecordReader(OrcInputFormat.java:1151)
        at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:249)
        at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:193)
        at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.&amp;lt;init&amp;gt;(TezGroupedSplitsInputFormat.java:135)
        at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat.getRecordReader(TezGroupedSplitsInputFormat.java:101)
        at org.apache.tez.mapreduce.lib.MRReaderMapred.setupOldRecordReader(MRReaderMapred.java:149)
        at org.apache.tez.mapreduce.lib.MRReaderMapred.setSplit(MRReaderMapred.java:80)
        at org.apache.tez.mapreduce.input.MRInput.initFromEventInternal(MRInput.java:650)
        at org.apache.tez.mapreduce.input.MRInput.initFromEvent(MRInput.java:621)
        at org.apache.tez.mapreduce.input.MRInputLegacy.checkAndAwaitRecordReaderInitialization(MRInputLegacy.java:145)
        at org.apache.tez.mapreduce.input.MRInputLegacy.init(MRInputLegacy.java:109)
        at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getMRInput(MapRecordProcessor.java:406)
        at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:128)
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:149)
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:139)
        at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:344)
        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:181)
        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:172)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:172)
        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:168)
        at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)


Timestamp specified as unix_ts (seconds): 1453507200


2016-02-10 02:20:06,588 [WARN] [TezChild] |orc.RecordReaderImpl|: Exception when evaluating predicate. Skipping ORC PPD. Exception: java.lang.IllegalArgumentException: ORC SARGS could not convert from Integer to TIMESTAMP
        at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.getBaseObjectForComparison(RecordReaderImpl.java:659)
        at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.evaluatePredicateRange(RecordReaderImpl.java:373)
        at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.evaluatePredicateProto(RecordReaderImpl.java:338)
        at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl$SargApplier.pickRowGroups(RecordReaderImpl.java:710)
        at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.pickRowGroups(RecordReaderImpl.java:751)
        at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.readStripe(RecordReaderImpl.java:777)
        at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.advanceStripe(RecordReaderImpl.java:986)
        at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.advanceToNextRow(RecordReaderImpl.java:1019)
        at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.&amp;lt;init&amp;gt;(RecordReaderImpl.java:205)
        at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.rowsOptions(ReaderImpl.java:598)
        at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger$ReaderPair.&amp;lt;init&amp;gt;(OrcRawRecordMerger.java:183)
        at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger$OriginalReaderPair.&amp;lt;init&amp;gt;(OrcRawRecordMerger.java:226)
        at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.&amp;lt;init&amp;gt;(OrcRawRecordMerger.java:437)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getReader(OrcInputFormat.java:1269)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRecordReader(OrcInputFormat.java:1151)
        at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:249)
        at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:193)
        at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.&amp;lt;init&amp;gt;(TezGroupedSplitsInputFormat.java:135)
        at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat.getRecordReader(TezGroupedSplitsInputFormat.java:101)
        at org.apache.tez.mapreduce.lib.MRReaderMapred.setupOldRecordReader(MRReaderMapred.java:149)
        at org.apache.tez.mapreduce.lib.MRReaderMapred.setSplit(MRReaderMapred.java:80)
        at org.apache.tez.mapreduce.input.MRInput.initFromEventInternal(MRInput.java:650)
        at org.apache.tez.mapreduce.input.MRInput.initFromEvent(MRInput.java:621)
        at org.apache.tez.mapreduce.input.MRInputLegacy.checkAndAwaitRecordReaderInitialization(MRInputLegacy.java:145)
        at org.apache.tez.mapreduce.input.MRInputLegacy.init(MRInputLegacy.java:109)
        at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getMRInput(MapRecordProcessor.java:406)
        at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:128)
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:149)
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:139)
        at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:344)
        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:181)
        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:172)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:172)
        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:168)
        at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)


These messages are logged for each and every row group processed in tasks. </description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
		</fixedFiles>
	</bug>
	<bug id="13314" opendate="2016-03-19 00:33:29" fixdate="2016-03-24 19:25:15" resolution="Duplicate">
		<buginformation>
			<summary>Hive on spark mapjoin errors if spark.master is not set</summary>
			<description>There are some errors that happen if spark.master is not set.
This is despite the code defaulting to yarn-cluster if spark.master is not set by user or on the config files: https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveSparkClientFactory.java#L51
The funny thing is that while it works the first time due to this default, subsequent tries will fail as the hiveConf is refreshed without that default being set.
https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/RemoteHiveSparkClient.java#L180
Exception is follows:

Job aborted due to stage failure: Task 40 in stage 1.0 failed 4 times, most recent failure: Lost task 40.3 in stage 1.0 (TID 22, d2409.halxg.cloudera.com): java.lang.RuntimeException: Error processing row: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.processRow(SparkMapRecordHandler.java:154)
	at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:48)
	at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:27)
	at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList$ResultIterator.hasNext(HiveBaseFunctionResultList.java:95)
	at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:41)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$15.apply(AsyncRDDActions.scala:120)
	at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$15.apply(AsyncRDDActions.scala:120)
	at org.apache.spark.SparkContext$$anonfun$38.apply(SparkContext.scala:2003)
	at org.apache.spark.SparkContext$$anonfun$38.apply(SparkContext.scala:2003)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.spark.HashTableLoader.load(HashTableLoader.java:117)
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.loadHashTable(MapJoinOperator.java:197)
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.cleanUpInputFileChangedOp(MapJoinOperator.java:223)
	at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1051)
	at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1055)
	at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1055)
	at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1055)
	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:490)
	at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.processRow(SparkMapRecordHandler.java:141)
	... 16 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.spark.SparkUtilities.isDedicatedCluster(SparkUtilities.java:108)
	at org.apache.hadoop.hive.ql.exec.spark.HashTableLoader.load(HashTableLoader.java:124)
	at org.apache.hadoop.hive.ql.exec.spark.HashTableLoader.load(HashTableLoader.java:114)
	... 24 more

Driver stacktrace:

</description>
			<version>1.3.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">12616</link>
		</links>
	</bug>
	<bug id="13008" opendate="2016-02-05 01:08:05" fixdate="2016-03-25 01:04:12" resolution="Fixed">
		<buginformation>
			<summary>WebHcat DDL commands in secure mode NPE when default FileSystem doesn&amp;apos;t support delegation tokens</summary>
			<description>
ERROR | 11 Jan 2016 20:19:02,781 | org.apache.hive.hcatalog.templeton.CatchallExceptionMapper |
java.lang.NullPointerException
        at org.apache.hive.hcatalog.templeton.SecureProxySupport$2.run(SecureProxySupport.java:171)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
        at org.apache.hive.hcatalog.templeton.SecureProxySupport.writeProxyDelegationTokens(SecureProxySupport.java:168)
        at org.apache.hive.hcatalog.templeton.SecureProxySupport.open(SecureProxySupport.java:95)
        at org.apache.hive.hcatalog.templeton.HcatDelegator.run(HcatDelegator.java:63)
        at org.apache.hive.hcatalog.templeton.Server.ddl(Server.java:217)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)
        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1480)
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1411)
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1360)
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1350)
        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:538)
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:716)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
        at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:565)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1360)
        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:615)
        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:574)
        at org.apache.hadoop.hdfs.web.AuthFilter.doFilter(AuthFilter.java:88)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1331)
        at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:477)
        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1031)
        at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:406)
        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:965)
        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:117)
        at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:47)

</description>
			<version>1.0.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.hcatalog.templeton.SecureProxySupport.java</file>
		</fixedFiles>
	</bug>
	<bug id="13262" opendate="2016-03-11 00:57:54" fixdate="2016-03-25 01:53:31" resolution="Fixed">
		<buginformation>
			<summary>LLAP: Remove log levels from DebugUtils</summary>
			<description>DebugUtils has many hardcoded log levels. To enable logging we need to recompile code with desired value. Instead configure add loggers for these classes with log levels via log4j properties. Also use parametrized logging in IO elevator. </description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReader.java</file>
			<file type="M">org.apache.hadoop.hive.llap.cache.LowLevelFifoCachePolicy.java</file>
			<file type="M">org.apache.hadoop.hive.llap.DebugUtils.java</file>
			<file type="D">org.apache.hadoop.hive.llap.old.ChunkPool.java</file>
			<file type="D">org.apache.hadoop.hive.llap.old.BufferPool.java</file>
			<file type="M">org.apache.hadoop.hive.llap.cache.LowLevelCacheMemoryManager.java</file>
			<file type="M">org.apache.hadoop.hive.llap.cache.SimpleBufferManager.java</file>
			<file type="M">org.apache.hadoop.hive.llap.cache.LlapDataBuffer.java</file>
			<file type="M">org.apache.hadoop.hive.llap.io.api.impl.LlapIoImpl.java</file>
			<file type="M">org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.java</file>
			<file type="M">org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
			<file type="D">org.apache.hadoop.hive.llap.old.BufferInProgress.java</file>
			<file type="M">org.apache.hadoop.hive.llap.io.decode.OrcColumnVectorProducer.java</file>
			<file type="D">org.apache.hadoop.hive.llap.old.CachePolicy.java</file>
			<file type="M">org.apache.hadoop.hive.llap.IncrementalObjectSizeEstimator.java</file>
			<file type="D">org.apache.hadoop.hive.llap.LogLevels.java</file>
			<file type="M">org.apache.hadoop.hive.llap.cache.LowLevelCacheImpl.java</file>
			<file type="M">org.apache.hadoop.hive.llap.cache.LowLevelLrfuCachePolicy.java</file>
		</fixedFiles>
	</bug>
	<bug id="12367" opendate="2015-11-09 04:52:48" fixdate="2016-03-25 02:23:23" resolution="Fixed">
		<buginformation>
			<summary>Lock/unlock database should add current database to inputs and outputs of authz hook</summary>
			<description></description>
			<version>1.2.1</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
		</fixedFiles>
		<links>
			<link type="Regression" description="breaks">12495</link>
		</links>
	</bug>
	<bug id="13151" opendate="2016-02-24 23:04:20" fixdate="2016-03-25 05:39:28" resolution="Fixed">
		<buginformation>
			<summary>Clean up UGI objects in FileSystem cache for transactions</summary>
			<description>One issue with FileSystem.CACHE is that it does not clean itself. The key in that cache includes UGI object. When new UGI objects are created and used with the FileSystem api, new entries get added to the cache.
We need to manually clean up those UGI objects once they are no longer in use.</description>
			<version>2.0.0</version>
			<fixedVersion>1.3.0, 2.1.0, 2.0.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.txn.compactor.Initiator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.txn.compactor.Worker.java</file>
			<file type="M">org.apache.hadoop.hive.ql.txn.compactor.Cleaner.java</file>
			<file type="M">org.apache.hive.hcatalog.streaming.HiveEndPoint.java</file>
			<file type="M">org.apache.hadoop.hive.ql.txn.compactor.CompactorThread.java</file>
			<file type="M">org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
		</fixedFiles>
	</bug>
	<bug id="13115" opendate="2016-02-22 21:43:32" fixdate="2016-03-27 23:17:30" resolution="Fixed">
		<buginformation>
			<summary>MetaStore Direct SQL getPartitions call fail when the columns schemas for a partition are null</summary>
			<description>We are seeing the following exception in our MetaStore logs

2016-02-11 00:00:19,002 DEBUG metastore.MetaStoreDirectSql (MetaStoreDirectSql.java:timingTrace(602)) - Direct SQL query in 5.842372ms + 1.066728ms, the query is [select "PARTITIONS"."PART_ID" from "PARTITIONS"  inner join "TBLS" on "PART
ITIONS"."TBL_ID" = "TBLS"."TBL_ID"     and "TBLS"."TBL_NAME" = ?   inner join "DBS" on "TBLS"."DB_ID" = "DBS"."DB_ID"      and "DBS"."NAME" = ?  order by "PART_NAME" asc]
2016-02-11 00:00:19,021 ERROR metastore.ObjectStore (ObjectStore.java:handleDirectSqlError(2243)) - Direct SQL failed, falling back to ORM
MetaException(message:Unexpected null for one of the IDs, SD 6437, column null, serde 6437 for a non- view)
        at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilterInternal(MetaStoreDirectSql.java:360)
        at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitions(MetaStoreDirectSql.java:224)
        at org.apache.hadoop.hive.metastore.ObjectStore$1.getSqlResult(ObjectStore.java:1563)
        at org.apache.hadoop.hive.metastore.ObjectStore$1.getSqlResult(ObjectStore.java:1559)
        at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2208)
        at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsInternal(ObjectStore.java:1570)
        at org.apache.hadoop.hive.metastore.ObjectStore.getPartitions(ObjectStore.java:1553)
        at sun.reflect.GeneratedMethodAccessor43.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:483)
        at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:108)
        at com.sun.proxy.$Proxy5.getPartitions(Unknown Source)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_partitions(HiveMetaStore.java:2526)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_partitions.getResult(ThriftHiveMetastore.java:8747)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_partitions.getResult(ThriftHiveMetastore.java:8731)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
        at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge20S.java:617)
        at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge20S.java:613)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1591)
        at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge20S.java:613)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)



This direct SQL call fails for every getPartitions call and then falls back to ORM.
The query which fails is


select 
  PARTITIONS.PART_ID, SDS.SD_ID, SDS.CD_ID,
  SERDES.SERDE_ID, PARTITIONS.CREATE_TIME,
  PARTITIONS.LAST_ACCESS_TIME, SDS.INPUT_FORMAT, SDS.IS_COMPRESSED,
  SDS.IS_STOREDASSUBDIRECTORIES, SDS.LOCATION, SDS.NUM_BUCKETS,
  SDS.OUTPUT_FORMAT, SERDES.NAME, SERDES.SLIB 
from PARTITIONS
  left outer join SDS on PARTITIONS.SD_ID = SDS.SD_ID 
  left outer join SERDES on SDS.SERDE_ID = SERDES.SERDE_ID 
  where PART_ID in (  ?  ) order by PART_NAME asc;


By looking at the source MetaStoreDirectSql.java, the third column in the query ( SDS.CD_ID), the column descriptor ID, is null, which triggers the exception. This exception is not thrown from the ORM layer since it is more forgiving to the null column descriptor. See ObjectStore.java:1197


 List&amp;lt;MFieldSchema&amp;gt; mFieldSchemas = msd.getCD() == null ? null : msd.getCD().getCols();


I verified that this exception gets triggered in the first place when we add a new partition without setting column level schemas for the partition, using the MetaStoreClient API. This exception does not occur when adding partitions using the CLI
I see two ways to solve the issue.
1. Make the MetaStoreClient API more strict and not allow creating partition without having column level schemas set. (This could break clients which use the MetaStoreclient API)
2. Make the Direct SQL code path and the ORM code path more consistent, where the Direct SQL does not fail on null column descriptor ID.
I feel 2 is more safer and easier to fix.</description>
			<version>1.2.1</version>
			<fixedVersion>1.3.0, 1.2.2, 2.1.0, 2.0.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
		</fixedFiles>
	</bug>
	<bug id="13256" opendate="2016-03-10 02:35:49" fixdate="2016-03-27 23:40:26" resolution="Duplicate">
		<buginformation>
			<summary>LLAP: RowGroup counter is wrong</summary>
			<description>Log line from LlapIOCounter


ROWS_EMITTED=23528469, SELECTED_ROWGROUPS=87


If rowgroups contain 10K rows by default then expected count is 235 for the above case.</description>
			<version>2.1.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
			<file type="M">org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">13226</link>
		</links>
	</bug>
	<bug id="12992" opendate="2016-02-03 19:23:12" fixdate="2016-03-28 18:54:10" resolution="Fixed">
		<buginformation>
			<summary>Hive on tez: Bucket map join plan is incorrect</summary>
			<description>TPCH Query 9 fails when bucket map join is enabled:


FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Reducer 5, vertexId=vertex_1450634494433_0007_2_06, diagnostics=[Exception in EdgeManager, vertex=vertex_1450634494433_0007_2_06 [Reducer 5], Fail to sendTezEventToDestinationTasks, event:DataMovementEvent [sourceIndex=0, targetIndex=-1, version=0], sourceInfo:{ producerConsumerType=OUTPUT, taskVertexName=Map 1, edgeVertexName=Reducer 5, taskAttemptId=attempt_1450634494433_0007_2_05_000000_0 }, destinationInfo:null, EdgeInfo: sourceVertexName=Map 1, destinationVertexName=Reducer 5, java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.tez.CustomPartitionEdge.routeDataMovementEventToDestination(CustomPartitionEdge.java:88)
	at org.apache.tez.dag.app.dag.impl.Edge.sendTezEventToDestinationTasks(Edge.java:458)
	at org.apache.tez.dag.app.dag.impl.Edge.handleCompositeDataMovementEvent(Edge.java:386)
	at org.apache.tez.dag.app.dag.impl.Edge.sendTezEventToDestinationTasks(Edge.java:439)
	at org.apache.tez.dag.app.dag.impl.VertexImpl.handleRoutedTezEvents(VertexImpl.java:4382)
	at org.apache.tez.dag.app.dag.impl.VertexImpl.access$4000(VertexImpl.java:202)
	at org.apache.tez.dag.app.dag.impl.VertexImpl$RouteEventTransition.transition(VertexImpl.java:4172)
	at org.apache.tez.dag.app.dag.impl.VertexImpl$RouteEventTransition.transition(VertexImpl.java:4164)

</description>
			<version>1.2.1</version>
			<fixedVersion>1.2.2, 2.1.0, 2.0.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.OperatorUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ReduceSinkMapJoinProc.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">13619</link>
		</links>
	</bug>
	<bug id="11424" opendate="2015-07-31 14:30:30" fixdate="2016-03-29 18:20:14" resolution="Fixed">
		<buginformation>
			<summary>Rule to transform OR clauses into IN clauses in CBO</summary>
			<description>We create a rule that will transform OR clauses into IN clauses (when possible).</description>
			<version>1.0.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">14194</link>
			<link type="Incorporates" description="is part of">11315</link>
			<link type="Reference" description="relates to">9069</link>
			<link type="Reference" description="relates to">11461</link>
			<link type="Regression" description="breaks">14652</link>
			<link type="Required" description="requires">11602</link>
		</links>
	</bug>
	<bug id="12937" opendate="2016-01-26 23:19:00" fixdate="2016-03-29 18:37:45" resolution="Fixed">
		<buginformation>
			<summary>DbNotificationListener unable to clean up old notification events</summary>
			<description>There is a bug in ObjectStore, where we use pm.deletePersistent instead of pm.deletePersistentAll, which causes the persistenceManager to try and drop a org.datanucleus.store.rdbms.query.ForwardQueryResult instead of the appropriate associated org.apache.hadoop.hive.metastore.model.MNotificationLog.
This results in an error that looks like this:

Exception in thread "CleanerThread" org.datanucleus.api.jdo.exceptions.ClassNotPersistenceCapableException: The class "org.datanucleus.store.rdbms.query.ForwardQueryResult" is not persistable. This means that it either hasnt been enhanced, or that the enhanced version of the file is not in the CLASSPATH (or is hidden by an unenhanced version), or the Meta-Data/annotations for the class are not found.
at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:380)
at org.datanucleus.api.jdo.JDOPersistenceManager.jdoDeletePersistent(JDOPersistenceManager.java:807)
at org.datanucleus.api.jdo.JDOPersistenceManager.deletePersistent(JDOPersistenceManager.java:820)
at org.apache.hadoop.hive.metastore.ObjectStore.cleanNotificationEvents(ObjectStore.java:7149)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:114)
at com.sun.proxy.$Proxy0.cleanNotificationEvents(Unknown Source)
at org.apache.hive.hcatalog.listener.DbNotificationListener$CleanerThread.run(DbNotificationListener.java:277)
NestedThrowablesStackTrace:
The class "org.datanucleus.store.rdbms.query.ForwardQueryResult" is not persistable. This means that it either hasnt been enhanced, or that the enhanced version of the file is not in the CLASSPATH (or is hidden by an unenhanced version), or the Meta-Data/annotations for the class are not found.
org.datanucleus.exceptions.ClassNotPersistableException: The class "org.datanucleus.store.rdbms.query.ForwardQueryResult" is not persistable. This means that it either hasnt been enhanced, or that the enhanced version of the file is not in the CLASSPATH (or is hidden by an unenhanced version), or the Meta-Data/annotations for the class are not found.
at org.datanucleus.ExecutionContextImpl.assertClassPersistable(ExecutionContextImpl.java:5698)
at org.datanucleus.ExecutionContextImpl.deleteObjectInternal(ExecutionContextImpl.java:2495)
at org.datanucleus.ExecutionContextImpl.deleteObjectWork(ExecutionContextImpl.java:2466)
at org.datanucleus.ExecutionContextImpl.deleteObject(ExecutionContextImpl.java:2417)
at org.datanucleus.ExecutionContextThreadedImpl.deleteObject(ExecutionContextThreadedImpl.java:245)
at org.datanucleus.api.jdo.JDOPersistenceManager.jdoDeletePersistent(JDOPersistenceManager.java:802)
at org.datanucleus.api.jdo.JDOPersistenceManager.deletePersistent(JDOPersistenceManager.java:820)
at org.apache.hadoop.hive.metastore.ObjectStore.cleanNotificationEvents(ObjectStore.java:7149)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:114)
at com.sun.proxy.$Proxy0.cleanNotificationEvents(Unknown Source)
at org.apache.hive.hcatalog.listener.DbNotificationListener$CleanerThread.run(DbNotificationListener.java:277)


The end result of this bug is that users of DbNotificationListener will have an evergrowing number of notification events that are not cleaned up as they age. This is an easy enough fix, but shows that we have a lack of test coverage here.</description>
			<version>1.2.1</version>
			<fixedVersion>1.3.0, 1.2.2, 2.1.0, 2.0.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.hcatalog.listener.TestDbNotificationListener.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
		</fixedFiles>
	</bug>
	<bug id="13326" opendate="2016-03-21 23:18:56" fixdate="2016-03-29 20:03:50" resolution="Fixed">
		<buginformation>
			<summary>HiveServer2: Make ZK config publishing configurable</summary>
			<description>We should revert to older behaviour when config publishing is disabled.</description>
			<version>2.0.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.jdbc.authorization.TestJdbcWithSQLAuthorization.java</file>
			<file type="M">org.apache.hive.jdbc.authorization.TestHS2AuthzContext.java</file>
			<file type="M">org.apache.hive.jdbc.TestMultiSessionsHS2WithLocalClusterSpark.java</file>
			<file type="M">org.apache.hive.jdbc.TestJdbcWithLocalClusterSpark.java</file>
			<file type="M">org.apache.hive.service.server.HiveServer2.java</file>
			<file type="M">org.apache.hive.jdbc.authorization.TestJdbcMetadataApiAuth.java</file>
			<file type="M">org.apache.hive.jdbc.ZooKeeperHiveClientHelper.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hive.jdbc.TestJdbcWithMiniMr.java</file>
			<file type="M">org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">11581</link>
		</links>
	</bug>
	<bug id="13361" opendate="2016-03-24 22:00:45" fixdate="2016-03-30 06:19:05" resolution="Fixed">
		<buginformation>
			<summary>Orc concatenation should enforce the compression buffer size</summary>
			<description>With HIVE-11807 buffer size estimation happens by default. This can have undesired effect wrt file concatenation. Consider the following table with files


testtable
  -- 000000_0 (created before HIVE-11807 which has buffer size 256KB)
  -- 000001_0 (created before HIVE-11807 which has buffer size 256KB)
  -- 000002_0 (created after HIVE-11807 with buffer size chosen as 128KB)
  -- 000003_0 (created after HIVE-11807 with buffer size chosen as 128KB)


If we perform ALTER TABLE .. CONCATENATE on the above table with HIVE-11807, then depending on the split arrangement 000000_0 and 000001_0 will be concatenated together to new merged file. But this new merged file will have 128KB buffer size (estimated buffer size and not requested buffer size). Since new ORC writer size does not honor the requested buffer size the new merged files will have smaller buffers than the required 256KB making the file unreadable. Following exception will be thrown when reading the table after concatenation


2016-03-24T16:26:33,974 ERROR [a9e27a9a-37cb-411d-9708-6c58a4ce34f2 main]: CliDriver (SessionState.java:printError(1049)) - Failed with exception java.io.IOException:java.lang.IllegalArgumentException: Buffer size too small. size = 131072 needed = 153187
java.io.IOException: java.lang.IllegalArgumentException: Buffer size too small. size = 131072 needed = 153187
        at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:513)
        at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:420)
        at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:145)
        at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1848)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:256)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:187)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403)
        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:782)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:721)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:648)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)

</description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.1.0, 2.0.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.orc.OrcFile.java</file>
			<file type="M">org.apache.orc.impl.WriterImpl.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.OrcFileMergeOperator.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="is blocked by">13362</link>
		</links>
	</bug>
	<bug id="12619" opendate="2015-12-08 19:56:09" fixdate="2016-03-30 15:00:11" resolution="Fixed">
		<buginformation>
			<summary>(Parquet) Switching the field order within an array of structs causes the query to fail</summary>
			<description>Switching the field order within an array of structs causes the query to fail or return the wrong data for the fields, but switching the field order within just a struct works.
How to reproduce:
Case1 if the two fields have the same type, query will return wrong data for the fields
drop table if exists schema_test;
create table schema_test (msg array&amp;lt;struct&amp;lt;f1: string, f2: string&amp;gt;&amp;gt;) stored as parquet;
insert into table schema_test select stack(2, array(named_struct(&amp;amp;apos;f1&amp;amp;apos;, &amp;amp;apos;abc&amp;amp;apos;, &amp;amp;apos;f2&amp;amp;apos;, &amp;amp;apos;abc2&amp;amp;apos;)), array(named_struct(&amp;amp;apos;f1&amp;amp;apos;, &amp;amp;apos;efg&amp;amp;apos;, &amp;amp;apos;f2&amp;amp;apos;, &amp;amp;apos;efg2&amp;amp;apos;))) from one limit 2;
select * from schema_test;
--returns
--[
{"f1":"efg","f2":"efg2"}
]
--[
{"f1":"abc","f2":"abc2"}
]
alter table schema_test change msg msg array&amp;lt;struct&amp;lt;f2: string, f1: string&amp;gt;&amp;gt;;
select * from schema_test;
--returns
--[
{"f2":"efg","f1":"efg2"}
]
--[
{"f2":"abc","f1":"abc2"}
]
Case2: if the two fields have different type, the query will fail
drop table if exists schema_test;
create table schema_test (msg array&amp;lt;struct&amp;lt;f1: string, f2: int&amp;gt;&amp;gt;) stored as parquet;
insert into table schema_test select stack(2, array(named_struct(&amp;amp;apos;f1&amp;amp;apos;, &amp;amp;apos;abc&amp;amp;apos;, &amp;amp;apos;f2&amp;amp;apos;, 1)), array(named_struct(&amp;amp;apos;f1&amp;amp;apos;, &amp;amp;apos;efg&amp;amp;apos;, &amp;amp;apos;f2&amp;amp;apos;, 2))) from one limit 2;
select * from schema_test;
--returns
--[
{"f1":"efg","f2":2}
]
--[
{"f1":"abc","f2":1}
]
alter table schema_test change msg msg array&amp;lt;struct&amp;lt;f2: int, f1: string&amp;gt;&amp;gt;;
select * from schema_test;
Failed with exception java.io.IOException:org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: org.apache.hadoop.io.Text cannot be cast to org.apache.hadoop.io.IntWritable</description>
			<version>1.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.parquet.convert.HiveSchemaConverter.java</file>
		</fixedFiles>
	</bug>
	<bug id="12569" opendate="2015-12-02 19:05:47" fixdate="2016-03-31 02:38:00" resolution="Duplicate">
		<buginformation>
			<summary>Excessive console message from SparkClientImpl [Spark Branch]</summary>
			<description>

15/12/02 11:00:46 INFO client.SparkClientImpl: 15/12/02 11:00:46 INFO Client: Application report for application_1442517343449_0038 (state: RUNNING)
15/12/02 11:00:47 INFO client.SparkClientImpl: 15/12/02 11:00:47 INFO Client: Application report for application_1442517343449_0038 (state: RUNNING)
15/12/02 11:00:48 INFO client.SparkClientImpl: 15/12/02 11:00:48 INFO Client: Application report for application_1442517343449_0038 (state: RUNNING)
15/12/02 11:00:49 INFO client.SparkClientImpl: 15/12/02 11:00:49 INFO Client: Application report for application_1442517343449_0038 (state: RUNNING)
15/12/02 11:00:50 INFO client.SparkClientImpl: 15/12/02 11:00:50 INFO Client: Application report for application_1442517343449_0038 (state: RUNNING)


I see this using Hive CLI after a spark job is launched and it goes non-stopping even if the job is finished.</description>
			<version>2.0.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">13376</link>
		</links>
	</bug>
	<bug id="13372" opendate="2016-03-28 22:52:19" fixdate="2016-03-31 04:35:30" resolution="Fixed">
		<buginformation>
			<summary>Hive Macro overwritten when multiple macros are used in one column</summary>
			<description>When multiple macros are used in one column, results of the later ones are over written by that of the first.
For example:
Suppose we have created a table called macro_test with single column x in STRING type, and with data as:
"a"
"bb"
"ccc"
We also create three macros:


CREATE TEMPORARY MACRO STRING_LEN(x string) length(x);
CREATE TEMPORARY MACRO STRING_LEN_PLUS_ONE(x string) length(x)+1;
CREATE TEMPORARY MACRO STRING_LEN_PLUS_TWO(x string) length(x)+2;


When we ran the following query, 


SELECT
    CONCAT(STRING_LEN(x), ":", STRING_LEN_PLUS_ONE(x), ":", STRING_LEN_PLUS_TWO(x)) a
FROM macro_test
SORT BY a DESC;


We get result:
3:3:3
2:2:2
1:1:1
instead of expected:
3:4:5
2:3:4
1:2:3
Currently we are using Hive 1.2.1, and have applied both HIVE-11432 and HIVE-12277 patches.</description>
			<version>1.2.1</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">2655</link>
		</links>
	</bug>
	<bug id="13255" opendate="2016-03-10 01:40:46" fixdate="2016-03-31 09:54:29" resolution="Fixed">
		<buginformation>
			<summary>FloatTreeReader.nextVector is expensive </summary>
			<description>Some TPCDS queries on 1TB scale shows FloatTreeReader on profile samples. It is most likely because of multiple branching and polymorphic dispatch in FloatTreeReader.nextVector() implementation. See attached image for sampling profile output.</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0, 2.0.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.orc.impl.RunLengthIntegerReader.java</file>
			<file type="M">org.apache.orc.impl.SerializationUtils.java</file>
			<file type="M">org.apache.orc.impl.TestSerializationUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
			<file type="M">org.apache.orc.impl.RunLengthIntegerReaderV2.java</file>
			<file type="M">org.apache.orc.impl.IntegerReader.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.TreeReaderFactory.java</file>
		</fixedFiles>
	</bug>
	<bug id="12650" opendate="2015-12-11 03:58:44" fixdate="2016-04-01 06:42:41" resolution="Fixed">
		<buginformation>
			<summary>Improve error messages for Hive on Spark in case the cluster has no resources available</summary>
			<description>I think hive.spark.client.server.connect.timeout should be set greater than spark.yarn.am.waitTime. The default value for 
spark.yarn.am.waitTime is 100s, and the default value for hive.spark.client.server.connect.timeout is 90s, which is not good. We can increase it to a larger value such as 120s.</description>
			<version>1.1.1</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.spark.client.SparkClientImpl.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.status.RemoteSparkJobMonitor.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.SparkTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.status.LocalSparkJobMonitor.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.java</file>
		</fixedFiles>
	</bug>
	<bug id="12612" opendate="2015-12-08 05:20:47" fixdate="2016-04-01 15:41:10" resolution="Fixed">
		<buginformation>
			<summary>beeline always exits with 0 status when reading query from standard input</summary>
			<description>Similar to what was reported on HIVE-6978, but now it only happens when the query is read from the standard input. For example, the following fails as expected:


bash$ if beeline -u "jdbc:hive2://..." -e "boo;" ; then echo "Ok?!" ; else echo "Failed!" ; fi
Connecting to jdbc:hive2://...
Connected to: Apache Hive (version 1.1.0-cdh5.5.0)
Driver: Hive JDBC (version 1.1.0-cdh5.5.0)
Transaction isolation: TRANSACTION_REPEATABLE_READ
Error: Error while compiling statement: FAILED: ParseException line 1:0 cannot recognize input near &amp;amp;apos;boo&amp;amp;apos; &amp;amp;apos;&amp;lt;EOF&amp;gt;&amp;amp;apos; &amp;amp;apos;&amp;lt;EOF&amp;gt;&amp;amp;apos; (state=42000,code=40000)
Closing: 0: jdbc:hive2://...
Failed!


But the following does not:


bash$ if echo "boo;"|beeline -u "jdbc:hive2://..." ; then echo "Ok?!" ; else echo "Failed!" ; fi
Connecting to jdbc:hive2://...
Connected to: Apache Hive (version 1.1.0-cdh5.5.0)
Driver: Hive JDBC (version 1.1.0-cdh5.5.0)
Transaction isolation: TRANSACTION_REPEATABLE_READ
Beeline version 1.1.0-cdh5.5.0 by Apache Hive
0: jdbc:hive2://...:8&amp;gt; Error: Error while compiling statement: FAILED: ParseException line 1:0 cannot recognize input near &amp;amp;apos;boo&amp;amp;apos; &amp;amp;apos;&amp;lt;EOF&amp;gt;&amp;amp;apos; &amp;amp;apos;&amp;lt;EOF&amp;gt;&amp;amp;apos; (state=42000,code=40000)
0: jdbc:hive2://...:8&amp;gt; Closing: 0: jdbc:hive2://...
Ok?!


This was misleading our batch scripts to always believe that the execution of the queries succeded, when sometimes that was not the case. 
Workaround
We found we can work around the issue by always using the -e or the -f parameters, and even reading the standard input through the /dev/stdin device (this was useful because a lot of the scripts fed the queries from here documents), like this:
some-script.sh

#!/bin/sh

set -o nounset -o errexit -o pipefail

# As beeline is failing to report an error status if reading the query
# to be executed from STDIN, check whether no -f or -e option is used
# and, in that case, pretend it has to read the query from a regular
# file using -f to read from /dev/stdin
function beeline_workaround_exit_status () {
    for arg in "$@"
    do if [ "$arg" = "-f" -o "$arg" = "-e" ]
       then beeline -u "..." "$@"
            return
       fi
    done
    beeline -u "..." "$@" -f /dev/stdin
}

beeline_workaround_exit_status &amp;lt;&amp;lt;EOF
boo;
EOF

</description>
			<version>1.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.beeline.BeeLine.java</file>
			<file type="M">org.apache.hive.beeline.cli.TestHiveCli.java</file>
		</fixedFiles>
		<links>
			<link type="Regression" description="breaks">13845</link>
		</links>
	</bug>
	<bug id="13376" opendate="2016-03-29 17:42:09" fixdate="2016-04-01 18:49:52" resolution="Fixed">
		<buginformation>
			<summary>HoS emits too many logs with application state</summary>
			<description>The logs get flooded with something like:
&amp;gt; Mar 28, 3:12:21.851 PM        INFO    org.apache.hive.spark.client.SparkClientImpl
&amp;gt; [stderr-redir-1]: 16/03/28 15:12:21 INFO yarn.Client: Application report for application_1458679386200_0161 (state: RUNNING)
&amp;gt; Mar 28, 3:12:21.912 PM        INFO    org.apache.hive.spark.client.SparkClientImpl
&amp;gt; [stderr-redir-1]: 16/03/28 15:12:21 INFO yarn.Client: Application report for application_1458679386200_0149 (state: RUNNING)
&amp;gt; Mar 28, 3:12:22.853 PM        INFO    org.apache.hive.spark.client.SparkClientImpl
&amp;gt; [stderr-redir-1]: 16/03/28 15:12:22 INFO yarn.Client: Application report for application_1458679386200_0161 (state: RUNNING)
&amp;gt; Mar 28, 3:12:22.913 PM        INFO    org.apache.hive.spark.client.SparkClientImpl
&amp;gt; [stderr-redir-1]: 16/03/28 15:12:22 INFO yarn.Client: Application report for application_1458679386200_0149 (state: RUNNING)
&amp;gt; Mar 28, 3:12:23.855 PM        INFO    org.apache.hive.spark.client.SparkClientImpl
&amp;gt; [stderr-redir-1]: 16/03/28 15:12:23 INFO yarn.Client: Application report for application_1458679386200_0161 (state: RUNNING)
While this is good information, it is a bit much.
Seems like SparkJobMonitor hard-codes its interval to 1 second.  It should be higher and perhaps made configurable.</description>
			<version>2.0.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">12569</link>
		</links>
	</bug>
	<bug id="13381" opendate="2016-03-30 02:01:12" fixdate="2016-04-04 20:12:59" resolution="Fixed">
		<buginformation>
			<summary>Timestamp &amp; date should have precedence in type hierarchy than string group</summary>
			<description>Both sql server &amp;amp; oracle treats date/timestamp higher in hierarchy than varchars</description>
			<version>1.0.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
		</fixedFiles>
		<links>
			<link type="Regression" description="breaks">15291</link>
		</links>
	</bug>
	<bug id="13396" opendate="2016-03-31 01:14:26" fixdate="2016-04-05 00:15:34" resolution="Fixed">
		<buginformation>
			<summary>LLAP: Include hadoop-metrics2.properties file LlapServiceDriver</summary>
			<description>LlapServiceDriver should package hadoop-metrics2.properties file to support dumping llap daemon metrics to hadoop metric sinks. </description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0, 2.0.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.llap.metrics.LlapDaemonCacheMetrics.java</file>
			<file type="M">org.apache.hadoop.hive.llap.metrics.LlapDaemonExecutorMetrics.java</file>
			<file type="M">org.apache.hadoop.hive.llap.metrics.MetricsUtils.java</file>
			<file type="M">org.apache.hadoop.hive.llap.metrics.LlapDaemonQueueMetrics.java</file>
			<file type="M">org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
			<file type="M">org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
		</fixedFiles>
	</bug>
	<bug id="13330" opendate="2016-03-22 20:29:39" fixdate="2016-04-05 00:41:21" resolution="Fixed">
		<buginformation>
			<summary>ORC vectorized string dictionary reader does not differentiate null vs empty string dictionary</summary>
			<description>Vectorized string dictionary reader cannot differentiate between the case where all dictionary entries are null vs single entry with empty string. This causes wrong results when reading data out of such files. 
Vectorization On

SET hive.vectorized.execution.enabled=true;
SET hive.fetch.task.conversion=none;
select vcol from testnullorc3 limit 1;

OK
NULL


Vectorization Off

SET hive.vectorized.execution.enabled=false;
SET hive.fetch.task.conversion=none;
select vcol from testnullorc3 limit 1;

OK



The input table testnullorc3 contains a varchar column vcol with few empty strings and few nulls. For this table, non vectorized reader returns empty as first row but vectorized reader returns NULL. </description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.1.0, 2.0.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.TreeReaderFactory.java</file>
		</fixedFiles>
	</bug>
	<bug id="13373" opendate="2016-03-29 01:35:56" fixdate="2016-04-05 14:46:19" resolution="Fixed">
		<buginformation>
			<summary>Use most specific type for numerical constants</summary>
			<description>tinyint &amp;amp; shortint are currently inferred as ints, if they are without postfix.</description>
			<version>1.0.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
		</fixedFiles>
	</bug>
	<bug id="13394" opendate="2016-03-31 00:54:42" fixdate="2016-04-05 18:47:01" resolution="Fixed">
		<buginformation>
			<summary>Analyze table fails in tez on empty partitions/files/tables</summary>
			<description>

        at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:352)
        at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:237)
        at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:252)
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:150)
        ... 14 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ArrayIndexOutOfBoundsException: 0
        at org.apache.hadoop.hive.ql.exec.GroupByOperator.process(GroupByOperator.java:766)
        at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:343)
        ... 17 more
Caused by: java.lang.ArrayIndexOutOfBoundsException: 0
        at org.apache.hadoop.hive.ql.udf.generic.NumDistinctValueEstimator.deserialize(NumDistinctValueEstimator.java:219)
        at org.apache.hadoop.hive.ql.udf.generic.NumDistinctValueEstimator.&amp;lt;init&amp;gt;(NumDistinctValueEstimator.java:112)
        at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats$GenericUDAFNumericStatsEvaluator.merge(GenericUDAFComputeStats.java:556)
        at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.aggregate(GenericUDAFEvaluator.java:188)
        at org.apache.hadoop.hive.ql.exec.GroupByOperator.updateAggregations(GroupByOperator.java:612)
        at org.apache.hadoop.hive.ql.exec.GroupByOperator.processAggr(GroupByOperator.java:851)
        at org.apache.hadoop.hive.ql.exec.GroupByOperator.processKey(GroupByOperator.java:695)
        at org.apache.hadoop.hive.ql.exec.GroupByOperator.process(GroupByOperator.java:761)
        ... 18 more
]], Vertex did not succeed due to OWN_TASK_FAILURE, failedTasks:1 killedTasks:0, Vertex vertex_1455918888034_27748_1_01 [Reducer 2] killed/failed due to:OWN_TASK_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0

</description>
			<version>1.2.1</version>
			<fixedVersion>1.2.2, 2.1.0, 2.0.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.java</file>
		</fixedFiles>
	</bug>
	<bug id="13333" opendate="2016-03-23 00:34:22" fixdate="2016-04-08 00:10:16" resolution="Fixed">
		<buginformation>
			<summary>StatsOptimizer throws ClassCastException</summary>
			<description>mvn test -Dtest=TestCliDriver -Dtest.output.overwrite=true -Dqfile=cbo_rp_udf_udaf.q -Dhive.compute.query.using.stats=true repros the issue.
In StatsOptimizer with return path on, we may have aggr($f0), aggr($f1) in GBY
and then select aggr($f1), aggr($f0) in SEL.
Thus we need to use colExp to find out which position is
corresponding to which position.</description>
			<version>2.0.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.StatsOptimizer.java</file>
		</fixedFiles>
	</bug>
	<bug id="13428" opendate="2016-04-05 20:53:49" fixdate="2016-04-08 18:53:56" resolution="Fixed">
		<buginformation>
			<summary>ZK SM in LLAP should have unique paths per cluster</summary>
			<description>Noticed this while working on some other patch</description>
			<version>2.0.0</version>
			<fixedVersion>2.1.0, 2.0.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.llap.security.SecretManager.java</file>
		</fixedFiles>
	</bug>
	<bug id="13320" opendate="2016-03-21 19:14:34" fixdate="2016-04-09 22:33:06" resolution="Fixed">
		<buginformation>
			<summary>Apply HIVE-11544 to explicit conversions as well as implicit ones</summary>
			<description>Parsing 1 million blank values through cast(x as int) is 3x slower than parsing a valid single digit.</description>
			<version>1.2.1</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFToByte.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFToLong.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFToInteger.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFToFloat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFToDouble.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFToShort.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">11544</link>
		</links>
	</bug>
	<bug id="13405" opendate="2016-04-01 17:16:57" fixdate="2016-04-10 00:18:32" resolution="Fixed">
		<buginformation>
			<summary>Fix Connection Leak in OrcRawRecordMerger</summary>
			<description>In OrcRawRecordMerger.getLastFlushLength, if the opened stream throws an IOException on .available() or on .readLong(), the function will exit without closing the stream.
This patch adds a try-with-resources to fix this.</description>
			<version>2.0.0</version>
			<fixedVersion>1.3.0, 2.1.0, 2.0.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.java</file>
		</fixedFiles>
	</bug>
	<bug id="13434" opendate="2016-04-06 14:13:17" fixdate="2016-04-10 00:38:20" resolution="Fixed">
		<buginformation>
			<summary>BaseSemanticAnalyzer.unescapeSQLString doesn&amp;apos;t unescape \u0000 style character literals.</summary>
			<description>BaseSemanticAnalyzer.unescapeSQLString method may have a fault. When "\u0061" style character literals are passed to the method, it&amp;amp;apos;s not unescaped successfully.
In Spark SQL project, we referenced the unescaping logic and noticed this issue (SPARK-14426)</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.TestSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">13412</link>
		</links>
	</bug>
	<bug id="13439" opendate="2016-04-06 21:20:28" fixdate="2016-04-12 00:56:24" resolution="Fixed">
		<buginformation>
			<summary>JDBC: provide a way to retrieve GUID to query Yarn ATS</summary>
			<description>HIVE-9673 added support for passing base64 encoded operation handles to ATS. We should a method on client side to retrieve that.</description>
			<version>1.2.1</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.jdbc.TestJdbcDriver2.java</file>
			<file type="M">org.apache.hive.jdbc.HiveStatement.java</file>
		</fixedFiles>
	</bug>
	<bug id="13410" opendate="2016-04-02 01:22:57" fixdate="2016-04-13 18:34:34" resolution="Fixed">
		<buginformation>
			<summary>PerfLog metrics scopes not closed if there are exceptions on HS2</summary>
			<description>If there are errors, the HS2 PerfLog api scopes are not closed.  Then there are sometimes messages like &amp;amp;apos;java.io.IOException: Scope named api_parse is not closed, cannot be opened.&amp;amp;apos;
I had simply forgetting to close the dangling scopes if there is an exception.  Doing so now.</description>
			<version>2.0.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.log.PerfLogger.java</file>
			<file type="M">org.apache.hive.jdbc.miniHS2.TestHs2Metrics.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">14521</link>
			<link type="Reference" description="relates to">14521</link>
		</links>
	</bug>
	<bug id="13400" opendate="2016-03-31 13:17:43" fixdate="2016-04-14 21:22:43" resolution="Fixed">
		<buginformation>
			<summary>Following up HIVE-12481, add retry for Zookeeper service discovery</summary>
			<description></description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.jdbc.HiveConnection.java</file>
			<file type="M">org.apache.hive.jdbc.Utils.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">14164</link>
			<link type="Reference" description="is related to">12481</link>
		</links>
	</bug>
	<bug id="13287" opendate="2016-03-15 09:56:57" fixdate="2016-04-15 11:03:48" resolution="Fixed">
		<buginformation>
			<summary>Add logic to estimate stats for IN operator</summary>
			<description>Currently, IN operator is considered in the default case: reduces the input rows to the half. This may lead to wrong estimates for the number of rows produced by Filter operators.</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">14018</link>
		</links>
	</bug>
	<bug id="13415" opendate="2016-04-04 09:15:38" fixdate="2016-04-15 11:50:35" resolution="Fixed">
		<buginformation>
			<summary>Decouple Sessions from thrift binary transport</summary>
			<description>Current behaviour is:

Open a thrift binary transport
create a session
close the transport

Then the session gets closed. Consequently, all the operations running in the session also get killed.
Whereas, if you open an HTTP transport, and close, the enclosing sessions are not closed. 
This seems like a bad design, having transport and sessions tightly coupled. I&amp;amp;apos;d like to fix this. 
The issue that introduced it is HIVE-9601 Relevant discussions at here, here and mentioned links on those comments. 
Another thing that seems like a slightly bad design is this line of code in ThriftBinaryCLIService:

server.setServerEventHandler(serverEventHandler);


Whereas serverEventHandler is defined by the base class, with no users except one sub-class(ThriftBinaryCLIService), violating the separation of concerns. </description>
			<version>1.2.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.service.cli.TestRetryingThriftCLIServiceClient.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">518</link>
			<link type="Reference" description="relates to">9601</link>
			<link type="Reference" description="is related to">14227</link>
			<link type="dependent" description="is depended upon by">1150</link>
		</links>
	</bug>
	<bug id="13523" opendate="2016-04-14 23:07:23" fixdate="2016-04-18 21:09:07" resolution="Fixed">
		<buginformation>
			<summary>Fix connection leak in ORC RecordReader and refactor for unit testing</summary>
			<description>In RecordReaderImpl, a MetadataReaderImpl object was being created (opening a file), but never closed, causing a leak. This change closes the Metadata object in RecordReaderImpl, and does substantial refactoring to make RecordReaderImpl testable:

Created DataReaderFactory and MetadataReaderFactory (plus default implementations) so that the create() methods can be mocked to verify that the objects are actually closed in RecordReaderImpl.close()
Created MetadataReaderProperties and DataReaderProperties to clean up argument lists, making code more readable
Created a builder() for RecordReaderImpl to make the code more readable
DataReader and MetadataReader now extend closeable (there was no reason for them not to in the first place) so I can use the guava Closer interface: http://docs.guava-libraries.googlecode.com/git/javadoc/com/google/common/io/Closer.html
Use the Closer interface to guarantee that regardless of if either close() call fails, both will be attempted (preventing further potential leaks)
Create builders for MetadataReaderProperties, DataReaderProperties, and RecordReaderImpl to help with code readability

</description>
			<version>2.0.0</version>
			<fixedVersion>2.1.0, 2.0.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.orc.DataReader.java</file>
			<file type="M">org.apache.orc.impl.MetadataReader.java</file>
			<file type="M">org.apache.orc.impl.MetadataReaderImpl.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.RecordReaderUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
		</fixedFiles>
	</bug>
	<bug id="13489" opendate="2016-04-12 01:27:09" fixdate="2016-04-18 23:55:02" resolution="Duplicate">
		<buginformation>
			<summary>TransactionBatchImpl.getCurrentTxnId() ArrayIndexOutOfBounds</summary>
			<description>
2016-04-06 20:04:14.430 o.a.s.h.t.HiveState [WARN] unable to close hive connections. 
java.lang.IndexOutOfBoundsException: Index: 10, Size: 10
	at java.util.ArrayList.rangeCheck(ArrayList.java:653) ~[?:1.8.0_45]
	at java.util.ArrayList.get(ArrayList.java:429) ~[?:1.8.0_45]
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.getCurrentTxnId(HiveEndPoint.java:647) ~[stormjar.jar:?]
	at org.apache.storm.hive.common.HiveWriter.abortTxn(HiveWriter.java:304) ~[stormjar.jar:?]
	at org.apache.storm.hive.common.HiveWriter.abort(HiveWriter.java:294) ~[stormjar.jar:?]
	at org.apache.storm.hive.trident.HiveState.abortAllWriters(HiveState.java:148) ~[stormjar.jar:?]
	at org.apache.storm.hive.trident.HiveState.abortAndCloseWriters(HiveState.java:136) [stormjar.jar:?]
	at org.apache.storm.hive.trident.HiveState.updateState(HiveState.java:112) [stormjar.jar:?]
	at org.apache.storm.hive.trident.HiveUpdater.updateState(HiveUpdater.java:12) [stormjar.jar:?]
	at org.apache.storm.hive.trident.HiveUpdater.updateState(HiveUpdater.java:9) [stormjar.jar:?]
	at 


This can happen after all transaction in the batch have been processed (i.e. there is no current txn).  Also, id there is an error and the Batch automatically closes all remaining errors.  This is a problem since Storm is written to call getCurrentTxnId() in a lot of its logging/error handling routines which ends up hiding the original error.</description>
			<version>1.3.0</version>
			<fixedVersion></fixedVersion>
			<type>New Feature</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.hcatalog.streaming.HiveEndPoint.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.java</file>
			<file type="M">org.apache.hive.hcatalog.streaming.TestStreaming.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">13493</link>
		</links>
	</bug>
	<bug id="13487" opendate="2016-04-12 01:12:39" fixdate="2016-04-21 20:50:09" resolution="Fixed">
		<buginformation>
			<summary>Finish time is wrong when perflog is missing SUBMIT_TO_RUNNING</summary>
			<description>Sometimes PerfLog misses SUBMIT_TO_RUNNING end time which makes finish time to be wrong. Like below


Compile Query                           0.60s
Prepare Plan                            0.44s
Submit Plan                             0.83s
Start                                   0.00s
Finish                              1460423234.44s


NO PRECOMMIT TESTS</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">14070</link>
		</links>
	</bug>
	<bug id="13488" opendate="2016-04-12 01:14:16" fixdate="2016-04-21 20:58:37" resolution="Fixed">
		<buginformation>
			<summary>Restore dag summary when tez exec print summary enabled and in-place updates disabled</summary>
			<description>Restore the old way of printing methods summary when file redirection is enabled. This may be used by some tools which will break because of the change introduced by HIVE-13226
NO PRECOMMIT TESTS</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
		</fixedFiles>
	</bug>
	<bug id="13240" opendate="2016-03-09 02:29:28" fixdate="2016-04-22 18:19:19" resolution="Fixed">
		<buginformation>
			<summary>GroupByOperator: Drop the hash aggregates when closing operator</summary>
			<description>GroupByOperator holds onto the Hash aggregates accumulated when the plan is cached.
Drop the hashAggregates in case of error during forwarding to the next operator.
Added for PTF, TopN and all GroupBy cases.</description>
			<version>1.2.1</version>
			<fixedVersion>1.3.0, 2.1.0, 2.0.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
		</fixedFiles>
		<links>
			<link type="dependent" description="is depended upon by">13238</link>
		</links>
	</bug>
	<bug id="13553" opendate="2016-04-20 01:04:20" fixdate="2016-04-24 00:01:22" resolution="Fixed">
		<buginformation>
			<summary>CTE with upperCase alias throws exception</summary>
			<description></description>
			<version>2.0.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
	</bug>
	<bug id="13527" opendate="2016-04-15 19:05:32" fixdate="2016-04-24 00:52:32" resolution="Fixed">
		<buginformation>
			<summary>Using deprecated APIs in HBase client causes zookeeper connection leaks.</summary>
			<description>When running queries against hbase-backed hive tables, the following log messages are seen in the HS2 log.


2016-04-11 07:25:23,657 WARN org.apache.hadoop.hbase.mapreduce.TableInputFormatBase: You are using an HTable instance that relies on an HBase-managed Connection. This is usually due to directly creating an HTable, which is deprecated. Instead, you should create a Connection object and then request a Table instance from it. If you don&amp;amp;apos;t need the Table instance for your own use, you should instead use the TableInputFormatBase.initalizeTable method directly.
2016-04-11 07:25:23,658 INFO org.apache.hadoop.hbase.mapreduce.TableInputFormatBase: Creating an additional unmanaged connection because user provided one can&amp;amp;apos;t be used for administrative actions. We&amp;amp;apos;ll close it when we close out the table.


In a HS2 log file, there are 1366 zookeeper connections established but only a small fraction of them were closed. So lsof would show 1300+ open TCP connections to Zookeeper.
grep "org.apache.zookeeper.ClientCnxn: Session establishment complete on server" * |wc -l
1366
grep "INFO org.apache.zookeeper.ZooKeeper: Session:" * |grep closed |wc -l
54
According to the comments in TableInputFormatBase, the recommended means for subclasses like HiveHBaseTableInputFormat is to call initializeTable() instead of setHTable() that it currently uses.
"
Subclasses MUST ensure initializeTable(Connection, TableName) is called for an instance to function properly. Each of the entry points to this class used by the MapReduce framework, 
{@link #createRecordReader(InputSplit, TaskAttemptContext)}
 and 
{@link #getSplits(JobContext)}
, will call 
{@link #initialize(JobContext)}
 as a convenient centralized location to handle retrieving the necessary configuration information. If your subclass overrides either of these methods, either call the parent version or call initialize yourself.
"
Currently setHTable() also creates an additional Admin connection, even though it is not needed.
So the use of deprecated APIs are to be replaced.</description>
			<version>1.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.java</file>
		</fixedFiles>
	</bug>
	<bug id="13494" opendate="2016-04-12 20:27:21" fixdate="2016-04-24 07:34:07" resolution="Fixed">
		<buginformation>
			<summary>LLAP: Some metrics from daemon are not exposed to hadoop-metrics2</summary>
			<description>LlapDaemonInfo is exposed via JMX but not sent to hadoop metrics.
Async IO metrics also seems incorrect.</description>
			<version>2.0.1</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
			<file type="M">org.apache.hadoop.hive.llap.metrics.LlapDaemonCacheInfo.java</file>
			<file type="M">org.apache.hadoop.hive.llap.io.decode.EncodedDataConsumer.java</file>
			<file type="M">org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
			<file type="M">org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.java</file>
			<file type="D">org.apache.hadoop.hive.llap.metrics.LlapDaemonQueueInfo.java</file>
			<file type="M">org.apache.hadoop.hive.llap.cache.BuddyAllocator.java</file>
			<file type="M">org.apache.hadoop.hive.llap.metrics.LlapDaemonCacheMetrics.java</file>
			<file type="D">org.apache.hadoop.hive.llap.metrics.MetricsUtils.java</file>
			<file type="D">org.apache.hadoop.hive.llap.metrics.LlapDaemonQueueMetrics.java</file>
			<file type="M">org.apache.hive.common.util.FixedSizedObjectPool.java</file>
			<file type="M">org.apache.hadoop.hive.common.Pool.java</file>
			<file type="M">org.apache.hadoop.hive.llap.io.api.impl.LlapIoImpl.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
			<file type="M">org.apache.hadoop.hive.llap.io.decode.OrcColumnVectorProducer.java</file>
			<file type="M">org.apache.hadoop.hive.llap.metrics.LlapDaemonExecutorInfo.java</file>
			<file type="D">org.apache.hadoop.hive.llap.metrics.LlapMetricsSystem.java</file>
			<file type="M">org.apache.hadoop.hive.llap.metrics.LlapDaemonExecutorMetrics.java</file>
		</fixedFiles>
		<links>
			<link type="dependent" description="is depended upon by">13536</link>
		</links>
	</bug>
	<bug id="13570" opendate="2016-04-21 00:51:01" fixdate="2016-04-24 23:59:34" resolution="Fixed">
		<buginformation>
			<summary>Some queries with Union all fail when CBO is off</summary>
			<description>Some queries with union all throws IndexOutOfBoundsException
when:
set hive.cbo.enable=false;
set hive.ppd.remove.duplicatefilters=true;
The stack is as:

java.lang.IndexOutOfBoundsException: Index: 67, Size: 67 
        at java.util.ArrayList.rangeCheck(ArrayList.java:635) 
        at java.util.ArrayList.get(ArrayList.java:411) 
        at org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcCtx.genColLists(ColumnPrunerProcCtx.java:161) 
        at org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcCtx.handleFilterUnionChildren(ColumnPrunerProcCtx.java:273) 
        at org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory$ColumnPrunerFilterProc.process(ColumnPrunerProcFactory.java:108) 
        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90) 
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:94) 
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:78) 
        at org.apache.hadoop.hive.ql.optimizer.ColumnPruner$ColumnPrunerWalker.walk(ColumnPruner.java:172) 
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:109) 
        at org.apache.hadoop.hive.ql.optimizer.ColumnPruner.transform(ColumnPruner.java:135) 
        at org.apache.hadoop.hive.ql.optimizer.Optimizer.optimize(Optimizer.java:198) 
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10327) 
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:192) 
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:222) 
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:432) 
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:305) 
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1119) 
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1167) 
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1055) 
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1045) 
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:207) 
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:159) 
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:370) 
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:305) 
        at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:403) 
        at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:419) 
        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:708) 
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675) 
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:615) 

</description>
			<version>2.0.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcCtx.java</file>
		</fixedFiles>
	</bug>
	<bug id="13533" opendate="2016-04-18 09:15:11" fixdate="2016-04-25 15:06:10" resolution="Fixed">
		<buginformation>
			<summary>Remove AST dump</summary>
			<description>For very large queries, dumping the AST can lead to OOM errors. Currently there are two places where we dump the AST:

CalcitePlanner if we are running in DEBUG mode (line 300).
ExplainTask if we use extended explain (line 179).

I guess the original reason to add the dump was to check whether the AST conversion from CBO was working properly, but I think we are past that stage now.
We will remove the logic to dump the AST in explain extended. For debug mode in CalcitePlanner, we will lower the level to LOG.TRACE.</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.plan.ExplainWork.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
			<file type="M">org.apache.hadoop.hive.ql.hooks.ATSHook.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.TestUpdateDeleteSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
		</fixedFiles>
	</bug>
	<bug id="13463" opendate="2016-04-08 20:04:34" fixdate="2016-04-26 22:38:21" resolution="Fixed">
		<buginformation>
			<summary>Fix ImportSemanticAnalyzer to allow for different src/dst filesystems</summary>
			<description>In ImportSemanticAnalyzer, there is an assumption that the src filesystem for import and the final location are on the same filesystem. Therefore the check for emptiness and getExternalTmpLocation will be looking on the wrong filesystem and will cause an error. The output path should be fed into getExternalTmpLocation to get a temporary file on the correct filesystem. The check for emptiness should use the output filesystem.</description>
			<version>2.0.0</version>
			<fixedVersion>2.1.0, 2.0.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
		</fixedFiles>
	</bug>
	<bug id="13440" opendate="2016-04-06 22:37:58" fixdate="2016-04-27 22:24:36" resolution="Fixed">
		<buginformation>
			<summary>remove hiveserver1 scripts and thrift generated files</summary>
			<description>HIVE-6977 deleted hiveserver1, however the scripts remain under bin/ext/-
ls bin/ext/hiveserver.*
bin/ext/hiveserver.cmd bin/ext/hiveserver.sh
The should be removed as well.</description>
			<version>1.2.1</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="D">org.apache.hadoop.hive.service.HiveClusterStatus.java</file>
			<file type="D">org.apache.hadoop.hive.service.ThriftHive.java</file>
			<file type="D">org.apache.hadoop.hive.service.JobTrackerState.java</file>
			<file type="D">org.apache.hadoop.hive.service.HiveServerException.java</file>
		</fixedFiles>
	</bug>
	<bug id="13493" opendate="2016-04-12 18:54:43" fixdate="2016-04-27 23:28:12" resolution="Fixed">
		<buginformation>
			<summary>Fix TransactionBatchImpl.getCurrentTxnId() and mis logging fixes</summary>
			<description>sort list of transaction IDs deleted by performTimeouts
sort list of "empty aborted"
log the list of lock id removed due to timeout
fix TransactionBatchImpl.getCurrentTxnId() not to look past end of array (see HIVE-13489)
beginNextTransactionImpl()
if ( currentTxnIndex &amp;gt;= txnIds.size() )//todo: this condition is bogus should check currentTxnIndex + 1</description>
			<version>1.2.0</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.hcatalog.streaming.HiveEndPoint.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.java</file>
			<file type="M">org.apache.hive.hcatalog.streaming.TestStreaming.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">13489</link>
		</links>
	</bug>
	<bug id="13572" opendate="2016-04-21 03:20:41" fixdate="2016-04-29 02:23:37" resolution="Fixed">
		<buginformation>
			<summary>Redundant setting full file status in Hive::copyFiles</summary>
			<description>We set full file status in each copy-file thread. I think it&amp;amp;apos;s redundant and hurts performance when we have multiple files to copy.


            if (inheritPerms) {
              ShimLoader.getHadoopShims().setFullFileStatus(conf, fullDestStatus, destFs, destf);
            }

</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.shims.ShimLoader.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
	</bug>
	<bug id="13510" opendate="2016-04-13 23:20:00" fixdate="2016-04-29 10:38:14" resolution="Fixed">
		<buginformation>
			<summary>Dynamic partitioning doesnt work when remote metastore is used</summary>
			<description>Steps to reproduce:

Configure remote metastore (hive.metastore.uris)
Create table t1 (a string);
Create table t2 (a string) partitioned by (b string);
set hive.exec.dynamic.partition.mode=nonstrict;
Insert overwrite table t2 partition (b) select a,a from t1;

Result:

FAILED: SemanticException org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.thrift.TApplicationException: getMetaConf failed: unknown result
16/04/13 15:04:51 [c679e424-2501-4347-8146-cf1b1cae217c main]: ERROR ql.Driver: FAILED: SemanticException org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.thrift.TApplicationException: getMetaConf failed: unknown result
org.apache.hadoop.hive.ql.parse.SemanticException: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.thrift.TApplicationException: getMetaConf failed: unknown result
        at org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx.&amp;lt;init&amp;gt;(DynamicPartitionCtx.java:84)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genFileSinkPlan(SemanticAnalyzer.java:6550)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPostGroupByBodyPlan(SemanticAnalyzer.java:9315)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:9204)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:10071)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9949)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genOPTree(SemanticAnalyzer.java:10607)
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:358)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10618)
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:233)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:245)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:476)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:318)
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1192)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1287)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1118)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1106)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:236)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:187)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:339)
        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:748)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:721)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:648)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.thrift.TApplicationException: getMetaConf failed: unknown result
        at org.apache.hadoop.hive.ql.metadata.Hive.getMetaConf(Hive.java:3493)
        at org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx.&amp;lt;init&amp;gt;(DynamicPartitionCtx.java:82)
        ... 29 more
Caused by: org.apache.thrift.TApplicationException: getMetaConf failed: unknown result
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_getMetaConf(ThriftHiveMetastore.java:666)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.getMetaConf(ThriftHiveMetastore.java:646)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getMetaConf(HiveMetaStoreClient.java:550)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:153)
        at com.sun.proxy.$Proxy20.getMetaConf(Unknown Source)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2153)
        at com.sun.proxy.$Proxy20.getMetaConf(Unknown Source)
        at org.apache.hadoop.hive.ql.metadata.Hive.getMetaConf(Hive.java:3491)
        ... 30 more


During construction of DynamicPartitionCtx it tries to read hive.metastore.partition.name.whitelist.pattern from the metastore. If no value is configured in hive-site.xml then NULL will be returned. Thrift considers NULL as invalid result and fails with an exception.</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">12897</link>
		</links>
	</bug>
	<bug id="12963" opendate="2016-01-29 18:30:13" fixdate="2016-04-29 19:14:50" resolution="Fixed">
		<buginformation>
			<summary>LIMIT statement with SORT BY creates additional MR job with hardcoded only one reducer</summary>
			<description>I execute query:
hive&amp;gt; select age from test1 sort by age.age  limit 10;                      
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
When I have a large number of rows then the last stage of the job takes a long time. I think we could allow to user choose number of reducers of last job or refuse extra MR job.
The same behavior I observed with querie:
hive&amp;gt; create table new_test as select age from test1 group by age.age  limit 10;
</description>
			<version>0.13</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
	</bug>
	<bug id="13512" opendate="2016-04-14 02:54:48" fixdate="2016-05-02 01:20:25" resolution="Fixed">
		<buginformation>
			<summary>Make initializing dag ids in TezWork thread safe for parallel compilation</summary>
			<description>When parallel query compilation is enabled, it is possible for concurrent running threads to create TezWork objects that have the same dag id. This is because the counter used to obtain the next dag id is not thread safe. The counter should be an AtomicInteger rather than an int.


  private static int counter;
  ...
  public TezWork(String queryId, Configuration conf) {
    this.dagId = queryId + ":" + (++counter);
    ...
  }

</description>
			<version>2.0.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.plan.TezWork.java</file>
		</fixedFiles>
	</bug>
	<bug id="13485" opendate="2016-04-11 22:04:53" fixdate="2016-05-02 01:24:03" resolution="Fixed">
		<buginformation>
			<summary>Session id appended to thread name multiple times.</summary>
			<description>HIVE-13153 addressed a portion of this issue. Follow up from there.</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.cli.CliDriver.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">13153</link>
			<link type="Reference" description="relates to">13885</link>
		</links>
	</bug>
	<bug id="6712" opendate="2014-03-20 20:18:43" fixdate="2016-05-02 15:19:45" resolution="Duplicate">
		<buginformation>
			<summary>HS2 JDBC driver is inconsistent w.r.t. auto commit</summary>
			<description>I see an inconsistency in HS2 JDBC driver code:


  @Override
  public void setAutoCommit(boolean autoCommit) throws SQLException {
    if (autoCommit) {
      throw new SQLException("enabling autocommit is not supported");
    }
  }


From above, it seems that auto commit is not supported. However, 


  @Override
  public boolean getAutoCommit() throws SQLException {
    return true;
  }

</description>
			<version>0.13.0</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.jdbc.HiveConnection.java</file>
			<file type="M">org.apache.hive.jdbc.TestJdbcDriver2.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">6705</link>
			<link type="Duplicate" description="duplicates">11293</link>
		</links>
	</bug>
	<bug id="13645" opendate="2016-04-28 19:42:56" fixdate="2016-05-02 19:12:51" resolution="Fixed">
		<buginformation>
			<summary>Beeline needs null-guard around hiveVars and hiveConfVars read</summary>
			<description>Beeline has a bug wherein if a user does a !save ever, then on next load, if beeline.hiveVariables or beeline.hiveconfvariables are empty, i.e. {} or unspecified, then it loads it as null, and then, on next connect, there is no null-check on these variables leading to an NPE.  </description>
			<version>2.1.0</version>
			<fixedVersion>1.3.0, 1.2.2, 2.1.0, 2.0.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.beeline.DatabaseConnection.java</file>
		</fixedFiles>
	</bug>
	<bug id="13390" opendate="2016-03-30 22:07:27" fixdate="2016-05-02 20:36:36" resolution="Fixed">
		<buginformation>
			<summary>HiveServer2: Add more test to ZK service discovery using MiniHS2</summary>
			<description></description>
			<version>1.2.1</version>
			<fixedVersion>1.3.0, 1.2.2, 2.1.0, 2.0.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.jdbc.HiveConnection.java</file>
			<file type="M">org.apache.hive.service.auth.HiveAuthFactory.java</file>
			<file type="M">org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
			<file type="M">org.apache.hive.jdbc.TestSSL.java</file>
		</fixedFiles>
	</bug>
	<bug id="13213" opendate="2016-03-05 06:01:27" fixdate="2016-05-03 22:26:48" resolution="Fixed">
		<buginformation>
			<summary>make DbLockManger work for non-acid resources</summary>
			<description>for example,
insert into T values(...)
if T is an ACID table we acquire Read lock
but for non-acid table it should acquire Exclusive lock</description>
			<version>1.0.0</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.java</file>
			<file type="M">org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
		</fixedFiles>
	</bug>
	<bug id="13646" opendate="2016-04-28 20:34:09" fixdate="2016-05-04 00:12:46" resolution="Fixed">
		<buginformation>
			<summary>make hive.optimize.sort.dynamic.partition compatible with ACID tables</summary>
			<description>HIVE-8875 disabled hive.optimize.sort.dynamic.partition for ACID queries.
dynamic inserts are common in ACID and this leaves users with few options if they are seeing OutOfMemory errors due to too many writers.
hive.optimize.sort.dynamic.partition sorts data by partition col/bucket col/sort col to ensure each reducer only needs 1 writer.
Acid requires data in each bucket file to be sorted by ROW__ID and thus doesn&amp;amp;apos;t allow end user to determine sorting.
So we should be able to support hive.optimize.sort.dynamic.partition with
sort on partition col/bucket col/ROW__ID </description>
			<version>1.0.0</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">11550</link>
			<link type="Blocker" description="is blocked by">13773</link>
		</links>
	</bug>
	<bug id="12579" opendate="2015-12-03 00:39:48" fixdate="2016-05-05 00:00:39" resolution="Fixed">
		<buginformation>
			<summary>add support for datanucleus.connectionPoolingType=None in TxnHandler.setupJdbcConnectionPool()</summary>
			<description>"None" is a valid option for datanucleus.connectionPoolingType
http://www.datanucleus.org/products/accessplatform_2_2/rdbms/connection_pooling.html#Manual.
TxnHandler.setupJdbcConnectionPool() doesn&amp;amp;apos;t support it.
If nothing else, this is useful for debugging.</description>
			<version>1.0.0</version>
			<fixedVersion></fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">13159</link>
		</links>
	</bug>
	<bug id="12837" opendate="2016-01-11 18:45:41" fixdate="2016-05-05 06:11:42" resolution="Fixed">
		<buginformation>
			<summary>Better memory estimation/allocation for hybrid grace hash join during hash table loading</summary>
			<description>This is to avoid an edge case when the memory available is very little (less than a single write buffer size), and we start loading the hash table. Since the write buffer is lazily allocated, we will easily run out of memory before even checking if we should spill any hash partition.
e.g.
Total memory available: 210 MB
Size of ref array of BytesBytesMultiHashMap for each hash partition: ~16 MB
Size of write buffer: 8 MB (lazy allocation)
Number of hash partitions: 16
Number of hash partitions created in memory: 13
Number of hash partitions created on disk: 3
Available memory left after HybridHashTableContainer initialization: 210-16*13=2MB
Now let&amp;amp;apos;s say a row is to be loaded into a hash partition in memory, it will try to allocate an 8MB write buffer for it, but we only have 2MB, thus OOM.
Solution is to perform the check for possible spilling earlier so we can spill partitions if memory is about to be full, to avoid OOM.</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.KeyValueContainer.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">13730</link>
		</links>
	</bug>
	<bug id="13632" opendate="2016-04-27 23:20:12" fixdate="2016-05-05 15:10:45" resolution="Fixed">
		<buginformation>
			<summary>Hive failing on insert empty array into parquet table</summary>
			<description>The insert will fail with following stack:

by: parquet.io.ParquetEncodingException: empty fields are illegal, the field should be ommited completely instead
	at parquet.io.MessageColumnIO$MessageColumnIORecordConsumer.endField(MessageColumnIO.java:271)
	at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter$ListDataWriter.write(DataWritableWriter.java:271)
	at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter$GroupDataWriter.write(DataWritableWriter.java:199)
	at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter$MessageDataWriter.write(DataWritableWriter.java:215)
	at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter.write(DataWritableWriter.java:88)
	at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriteSupport.write(DataWritableWriteSupport.java:59)
	at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriteSupport.write(DataWritableWriteSupport.java:31)
	at parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:116)
	at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:123)
	at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:42)
	at org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper.write(ParquetRecordWriterWrapper.java:111)
	at org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper.write(ParquetRecordWriterWrapper.java:124)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:697)


Reproduce:

create table test_small (
key string,
arrayValues array&amp;lt;string&amp;gt;)
stored as parquet;
insert into table test_small select &amp;amp;apos;abcd&amp;amp;apos;, array() from src limit 1;

</description>
			<version>1.1.0</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveArrayInspector.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.parquet.serde.TestParquetHiveArrayInspector.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.parquet.serde.AbstractParquetMapInspector.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.parquet.serde.TestAbstractParquetMapInspector.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.parquet.TestDataWritableWriter.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">596</link>
		</links>
	</bug>
	<bug id="13619" opendate="2016-04-26 22:48:02" fixdate="2016-05-05 21:37:15" resolution="Fixed">
		<buginformation>
			<summary>Bucket map join plan is incorrect</summary>
			<description>Same as HIVE-12992. Missed a single line check. TPCDS query 4 with bucketing can produce this issue.</description>
			<version>2.0.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.OperatorUtils.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">12992</link>
		</links>
	</bug>
	<bug id="13395" opendate="2016-03-31 00:57:49" fixdate="2016-05-05 22:29:55" resolution="Fixed">
		<buginformation>
			<summary>Lost Update problem in ACID</summary>
			<description>ACID users can run into Lost Update problem.
In Hive 1.2, Driver.recordValidTxns() (which records the snapshot to use for the query) is called in Driver.compile().
Now suppose to concurrent "update T set x = x + 1" are executed.  (for simplicity assume there is exactly 1 row in T)
What can happen is that both compile at the same time (more precisely before acquireLocksAndOpenTxn() in runInternal() is called) and thus will lock in the same snapshot, say the value of x = 7 in this snapshot.
Now 1 will get the lock on the row, the second will block.  
Now 1, makes x = 8 and commits.
Now 2 proceeds and makes x = 8 again since in it&amp;amp;apos;s snapshot x is still 7.
This specific issue is solved in Hive 1.3/2.0 (HIVE-11077 which is a large patch that deals with multi-statement txns) by moving recordValidTxns() after locks are acquired which reduces the likelihood of this but doesn&amp;amp;apos;t eliminate the problem.

Even in 1.3 version of the code, you could have the same issue.  Assume the same 2 queries:
Both start a txn, say txnid 9 and 10.  Say 10 gets the lock first, 9 blocks.
10 updates the row (so x = 8) and thus ReaderKey.currentTransactionId=10.
10 commits.
Now 9 can proceed and it will get a snapshot that includes 10, i.e. it will see x = 8 and it will write x = 9, but it will set ReaderKey.currentTransactionId = 9.  Thus when merge logic runs, it will see x = 8 is the later version of this row, i.e. lost update.
The problem is that locks alone are insufficient for MVCC architecture.  

At lower level Row ID has (originalTransactionId, rowid, bucket id, currentTransactionId) and since on update/delete we do a table scan, we could check that we are about to write a row with currentTransactionId &amp;lt; (currentTransactionId of row we&amp;amp;apos;ve read) and fail the query.  Currently, currentTransactionId is not surfaced at higher level where this check can be made.
This would not work (efficiently) longer term where we want to support fast update on user defined PK vis streaming ingest.
Also, this would not work with multi statement txns since in that case we&amp;amp;apos;d lock in the snapshot at the start of the txn, but then 2nd, 3rd etc queries would use the same snapshot and the locks for these queries would be acquired after the snapshot is locked in so this would be the same situation as pre HIVE-11077.

A more robust solution (commonly used with MVCC) is to keep track of start and commit time (logical counter) or each transaction to detect if two txns overlap.  The 2nd part is to keep track of write-set, i.e. which data (rows, partitions, whatever appropriate level of granularity is) were modified by any txn and if 2 txns overlap in time and wrote the same element, abort later one.  This is called first-committer-wins rule.  This requires a MS DB schema change
It would be most convenient to use the same sequence for txnId, start and commit time (in which case txnid=start time).  In this case we&amp;amp;apos;d need to add 1 filed to TXNS table.  The complication here is that we&amp;amp;apos;ll be using elements of the sequence faster and they are used as part of file name of delta and base dir and currently limited to 7 digits which can be exceeded.  So this would require some thought to handling upgrade/migration.
Also, write-set tracking requires either additional metastore table or keeping info in HIVE_LOCKS around longer with new state.

In the short term, on SQL side of things we could (in auto commit mode only)
acquire the locks first and then open the txn AND update these locks with txn id.
This implies another Thrift change to pass in lockId to openTxn.
The same would not work for Streaming API since it opens several txns at once and then acquires locks for each.
(Not sure if that&amp;amp;apos;s is an issue or not since Streaming only does Insert).
Either way this feels hacky.

Here is one simple example why we need Write-Set tracking for multi-statement txns
Consider transactions T 1 and T 2:
T 1: r 1[x] -&amp;gt; w 1[y] -&amp;gt; c 1 
T 2: w 2[x] -&amp;gt; w 2[y] -&amp;gt; c 2 
Suppose the order of operations is r 1[x] w 2[x].... then a conventional R/W lock manager w/o MVCSS will block the write from T 2 
With MVCC we don&amp;amp;apos;t want readers to interfere with writers and so the following schedule is possible (because Hive&amp;amp;apos;s semi-shared (write) don&amp;amp;apos;t conflict with shared (read) locks) in Hive&amp;amp;apos;s current implementation.
r 1[x] w 2[x] w 2[y] c 2 w 1[y] c 1
By the time w 1[y] happens, T 2 has committed and released it&amp;amp;apos;s locks.  But this is a lost update if c 1 is allowed to commit.  That&amp;amp;apos;s where write-set tracking comes in.</description>
			<version>1.2.0</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.txn.compactor.HouseKeeperServiceBase.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TestTxnHandler.java</file>
			<file type="M">org.apache.hadoop.hive.ql.txn.compactor.Worker.java</file>
			<file type="M">org.apache.hadoop.hive.ql.txn.compactor.TestCleaner.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TestCompactionTxnHandler.java</file>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.DbLockManager.java</file>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.TestHiveMetaStoreTxns.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnStore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnDbUtil.java</file>
			<file type="M">org.apache.hadoop.hive.ql.txn.compactor.Initiator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.TestAcidUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.ErrorMsg.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">13622</link>
			<link type="Blocker" description="blocks">13354</link>
			<link type="Blocker" description="is blocked by">13664</link>
			<link type="Reference" description="relates to">14047</link>
			<link type="Reference" description="is related to">11077</link>
		</links>
	</bug>
	<bug id="13701" opendate="2016-05-06 00:52:08" fixdate="2016-05-06 02:44:15" resolution="Fixed">
		<buginformation>
			<summary>LLAP: Use different prefix for llap task scheduler metrics</summary>
			<description>LLAP task scheduler runs inside AM and typically runs on different hosts than llap daemons. Using the same prefix "llapdaemon" for daemon metrics and task scheduler metrics will cause conflicts when these metrics are published with hadoop-metrics2.
NO PRECOMMIT TESTS</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.llap.tezplugins.metrics.LlapTaskSchedulerMetrics.java</file>
			<file type="M">org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
		</fixedFiles>
	</bug>
	<bug id="13542" opendate="2016-04-18 23:10:21" fixdate="2016-05-06 20:56:17" resolution="Fixed">
		<buginformation>
			<summary>Missing stats for tables in TPCDS performance regression suite</summary>
			<description>These are the tables whose stats are missing in data/files/tpcds-perf/metastore_export/csv/TAB_COL_STATS.txt:

catalog_returns
catalog_sales
inventory
store_returns
store_sales
web_returns
web_sales

Thanks to Jesus Camacho Rodriguez for discovering this issue.</description>
			<version>2.0.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.QTestUtil.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">13269</link>
			<link type="Reference" description="relates to">13601</link>
		</links>
	</bug>
	<bug id="13525" opendate="2016-04-15 11:46:56" fixdate="2016-05-09 01:53:27" resolution="Fixed">
		<buginformation>
			<summary>HoS hangs when job is empty</summary>
			<description>Observed in local tests. This should be the cause of HIVE-13402.</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.spark.client.RemoteDriver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">13223</link>
			<link type="Reference" description="relates to">13402</link>
			<link type="Reference" description="relates to">13843</link>
			<link type="Reference" description="is related to">14958</link>
		</links>
	</bug>
	<bug id="13618" opendate="2016-04-26 22:43:51" fixdate="2016-05-09 05:42:28" resolution="Fixed">
		<buginformation>
			<summary>Trailing spaces in partition column will be treated differently</summary>
			<description>We store the partition spec value in the metastore. In mysql (and derby i think), the trailing space is ignored. That is, if you have a partition column "col" (type varchar or string) with value "a " and then select from the table where col = "a", it will return. However, in postgres and Oracle, the trailing space is not ignored. </description>
			<version>2.0.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="is blocked by">13608</link>
			<link type="Reference" description="is related to">13299</link>
		</links>
	</bug>
	<bug id="13712" opendate="2016-05-07 05:36:53" fixdate="2016-05-09 18:32:26" resolution="Fixed">
		<buginformation>
			<summary>LLAP: LlapServiceDriver should package hadoop-metrics2-llapdaemon.properties when available</summary>
			<description>HIVE-13701 renamed hadoop-metrics2.properties to hadoop-metrics2-llapdaemon.properties to avoid conflicts in classpath lookup. MetricsSystem first looks for hadoop-metrics2-llapdaemon.properties file first before falling back to hadoop-metrics2.properties. Make LlapServiceDriver package hadoop-metrics2-llapdaemon.properties first and fallback to hadoop-metrics2.properties.
NO PRECOMMIT TESTS</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
			<file type="M">org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
		</fixedFiles>
	</bug>
	<bug id="13700" opendate="2016-05-05 22:18:01" fixdate="2016-05-09 23:21:39" resolution="Fixed">
		<buginformation>
			<summary>TestHiveOperationType is failing on master</summary>
			<description>Presumably be broken by HIVE-13351</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
		</fixedFiles>
	</bug>
	<bug id="13676" opendate="2016-05-03 08:15:28" fixdate="2016-05-10 16:49:07" resolution="Fixed">
		<buginformation>
			<summary>Tests failing because metastore doesn&amp;apos;t come up</summary>
			<description>In 5-6 test classes, metastore is required to be up for tests to run. The metastore is started in setup Phase asynchronously. But there&amp;amp;apos;s no logic to wait till the metastore comes up. Hence, sometimes tests run even when metastore isn&amp;amp;apos;t up and fail. </description>
			<version>1.2.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.hcatalog.api.TestHCatClient.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.TestMarkPartitionRemote.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.TestHiveRemote.java</file>
			<file type="M">org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.TestHiveMetaStoreGetMetaConf.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.TestHiveMetaStorePartitionSpecs.java</file>
		</fixedFiles>
	</bug>
	<bug id="13342" opendate="2016-03-23 22:33:21" fixdate="2016-05-10 21:04:58" resolution="Fixed">
		<buginformation>
			<summary>Improve logging in llap decider and throw exception in case llap mode is all but we cannot run in llap.</summary>
			<description>Currently we do not log our decisions with respect to llap. Are we running everything in llap mode or only parts of the plan. We need more logging. Also, if llap mode is all but for some reason, we cannot run the work in llap mode, fail and throw an exception advise the user to change the mode to auto.</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.LlapDecider.java</file>
		</fixedFiles>
	</bug>
	<bug id="12996" opendate="2016-02-03 23:08:28" fixdate="2016-05-11 18:21:43" resolution="Fixed">
		<buginformation>
			<summary>Temp tables shouldn&amp;apos;t be locked</summary>
			<description>Internally, INSERT INTO ... VALUES statements use temp table to accomplish its functionality. But temp tables shouldn&amp;amp;apos;t be stored in the metastore tables for ACID, because they are by definition only visible inside the session that created them, and we don&amp;amp;apos;t allow multiple threads inside a session. If a temp table is used in a query, it should be ignored by lock manager.


mysql&amp;gt; select * from COMPLETED_TXN_COMPONENTS;
+-----------+--------------+-----------------------+------------------+
| CTC_TXNID | CTC_DATABASE | CTC_TABLE             | CTC_PARTITION    |
+-----------+--------------+-----------------------+------------------+
|         1 | acid         | t1                    | NULL             |
|         1 | acid         | values__tmp__table__1 | NULL             |
|         2 | acid         | t1                    | NULL             |
|         2 | acid         | values__tmp__table__2 | NULL             |
|         3 | acid         | values__tmp__table__3 | NULL             |
|         3 | acid         | t1                    | NULL             |
|         4 | acid         | values__tmp__table__1 | NULL             |
|         4 | acid         | t2p                   | ds=today         |
|         5 | acid         | values__tmp__table__1 | NULL             |
|         5 | acid         | t3p                   | ds=today/hour=12 |
+-----------+--------------+-----------------------+------------------+

</description>
			<version>2.0.0</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.java</file>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">10632</link>
		</links>
	</bug>
	<bug id="13458" opendate="2016-04-07 21:51:18" fixdate="2016-05-11 23:55:04" resolution="Fixed">
		<buginformation>
			<summary>Heartbeater doesn&amp;apos;t fail query when heartbeat fails</summary>
			<description>When a heartbeat fails to locate a lock, it should fail the current query. That doesn&amp;amp;apos;t happen, which is a bug.
Another thing is, we need to make sure stopHeartbeat really stops the heartbeat, i.e. no additional heartbeat will be sent, since that will break the assumption and cause the query to fail.</description>
			<version>2.1.0</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.merge.MergeFileTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Context.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">12366</link>
			<link type="Reference" description="is related to">12634</link>
		</links>
	</bug>
	<bug id="13621" opendate="2016-04-27 00:45:30" fixdate="2016-05-12 17:22:50" resolution="Fixed">
		<buginformation>
			<summary>compute stats in certain cases fails with NPE</summary>
			<description>

FAILED: NullPointerException null
java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.stats.StatsUtils.getColStatistics(StatsUtils.java:693)
	at org.apache.hadoop.hive.ql.stats.StatsUtils.convertColStats(StatsUtils.java:739)
	at org.apache.hadoop.hive.ql.stats.StatsUtils.getTableColumnStats(StatsUtils.java:728)
	at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:183)
	at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:136)
	at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:124)
</description>
			<version>2.0.1</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.hbase.HBaseUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Operator.java</file>
		</fixedFiles>
	</bug>
	<bug id="13728" opendate="2016-05-10 16:35:42" fixdate="2016-05-13 15:49:27" resolution="Fixed">
		<buginformation>
			<summary>TestHBaseSchemaTool fails on master</summary>
			<description>Presumably because of HIVE-13597</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.hbase.TestHBaseSchemaTool.java</file>
		</fixedFiles>
	</bug>
	<bug id="13602" opendate="2016-04-25 04:07:28" fixdate="2016-05-13 17:13:09" resolution="Fixed">
		<buginformation>
			<summary>TPCH q16 return wrong result when CBO is on</summary>
			<description>Running tpch with factor 2, 
q16 returns 1,160 rows when CBO is on,
while returns 24,581 rows when CBO is off.
See attachment for detail .</description>
			<version>1.2.2</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.metadata.VirtualColumn.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcCtx.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">13733</link>
			<link type="Reference" description="is related to">14705</link>
		</links>
	</bug>
	<bug id="13743" opendate="2016-05-12 00:06:38" fixdate="2016-05-14 15:22:11" resolution="Fixed">
		<buginformation>
			<summary>Data move codepath is broken with hive (2.1.0-SNAPSHOT)</summary>
			<description>Data move codepath is broken with hive 2.1.0-SNAPSHOT with hadoop 2.8.0-snapshot.

Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): Path not found: /apps/hive/warehouse/tpcds_bin_partitioned_orc_10000.db/date_dim1
        at org.apache.hadoop.hdfs.server.namenode.FSDirEncryptionZoneOp.getEZForPath(FSDirEncryptionZoneOp.java:178)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getEZForPath(FSNamesystem.java:7336)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getEZForPath(NameNodeRpcServer.java:1973)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getEZForPath(ClientNamenodeProtocolServerSideTranslatorPB.java:1376)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:645)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2339)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2335)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1711)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2333)

        at org.apache.hadoop.ipc.Client.call(Client.java:1448)
        at org.apache.hadoop.ipc.Client.call(Client.java:1385)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
        at com.sun.proxy.$Proxy30.getEZForPath(Unknown Source)/apps/hive/warehouse/tpcds_bin_partitioned_orc_200.db/

...
...
...
2016-05-11T09:40:43,760 ERROR [main]: ql.Driver (:()) - FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask. Unable to move source hdfs://xyz:8020/apps/hive/warehouse/tpcds_bin_partitioned_orc_10000.db/.hive-staging_hive_2016-05-11_09-40-42_489_5056654133706433454-1/-ext-10002 to destination hdfs://xyz:8020/apps/hive/warehouse/tpcds_bin_partitioned_orc_10000.db/date_dim1


https://github.com/apache/hive/blob/26b5c7b56a4f28ce3eabc0207566cce46b29b558/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java#L2836
hdfsEncryptionShim.isPathEncrypted(destf) in Hive could end up throwing FileNotFoundException as the destf is not present yet.  This causes moveFile to fail.</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
		</fixedFiles>
	</bug>
	<bug id="13686" opendate="2016-05-04 10:35:15" fixdate="2016-05-14 16:10:39" resolution="Fixed">
		<buginformation>
			<summary>TestRecordReaderImpl is deleting target/tmp causing all the tests after it to fail</summary>
			<description>The issue was introduced in HIVE-12159 (https://github.com/apache/hive/blame/master/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestRecordReaderImpl.java). This test deletes target/tmp directory. Because of this, the subsequent tests don&amp;amp;apos;t get to read target/tmp/conf/hive-site.xml, which contains test-specific configurations. Also, target/tmp has metastore db directory, which also gets deleted causing subsequent tests that use metastore db to fail too. 
I&amp;amp;apos;m surprised this issue wasn&amp;amp;apos;t caught in pre-commit builds. Sergio Pea I see that even the latest pre-commit jobs aren&amp;amp;apos;t reporting any errors, while building on local and running hive-exec test cases causes a bunch of tests to fail. </description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.java</file>
		</fixedFiles>
		<links>
			<link type="Regression" description="is broken by">12159</link>
		</links>
	</bug>
	<bug id="13753" opendate="2016-05-12 22:48:00" fixdate="2016-05-16 17:27:19" resolution="Fixed">
		<buginformation>
			<summary>Make metastore client thread safe in DbTxnManager</summary>
			<description>The fact that multiple threads sharing the same metastore client which is used for RPC to Thrift is not thread safe.
Race condition can happen when one sees "out of sequence response" error message from Thrift server. That means the response from the Thrift server is for a different request (by a different thread).
Solution will be to synchronize methods from the client side.</description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.DbLockManager.java</file>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
		</fixedFiles>
	</bug>
	<bug id="13293" opendate="2016-03-16 06:17:44" fixdate="2016-05-17 08:17:23" resolution="Fixed">
		<buginformation>
			<summary>Query occurs performance degradation after enabling parallel order by for Hive on Spark</summary>
			<description>I use TPCx-BB to do some performance test on Hive on Spark engine. And found query 10 has performance degradation when enabling parallel order by.
It seems that sampling cost much time before running the real query.</description>
			<version>2.0.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.SortByShuffler.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.java</file>
		</fixedFiles>
	</bug>
	<bug id="13767" opendate="2016-05-16 17:31:25" fixdate="2016-05-17 16:13:24" resolution="Fixed">
		<buginformation>
			<summary>Wrong type inferred in Semijoin condition leads to AssertionError</summary>
			<description>Following query fails to run:

SELECT
    COALESCE(498, LEAD(COALESCE(-973, -684, 515)) OVER (PARTITION BY (t2.int_col_10 + t1.smallint_col_50) ORDER BY (t2.int_col_10 + t1.smallint_col_50), FLOOR(t1.double_col_16) DESC), 524) AS int_col,
    (t2.int_col_10) + (t1.smallint_col_50) AS int_col_1,
    FLOOR(t1.double_col_16) AS float_col,
    COALESCE(SUM(COALESCE(62, -380, -435)) OVER (PARTITION BY (t2.int_col_10 + t1.smallint_col_50) ORDER BY (t2.int_col_10 + t1.smallint_col_50) DESC, FLOOR(t1.double_col_16) DESC ROWS BETWEEN UNBOUNDED PRECEDING AND 48 FOLLOWING), 704) AS int_col_2
FROM table_1 t1
INNER JOIN table_18 t2 ON (((t2.tinyint_col_15) = (t1.bigint_col_7)) AND
                           ((t2.decimal2709_col_9) = (t1.decimal2016_col_26))) AND
                           ((t2.tinyint_col_20) = (t1.tinyint_col_3))
WHERE (t2.smallint_col_19) IN (SELECT
    COALESCE(-92, -994) AS int_col
    FROM table_1 tt1
    INNER JOIN table_18 tt2 ON (tt2.decimal1911_col_16) = (tt1.decimal2612_col_77)
    WHERE (t1.timestamp_col_9) = (tt2.timestamp_col_18));


Following error is seen in the logs:

2016-04-27T04:32:09,605 WARN  [...2a24 HiveServer2-Handler-Pool: Thread-211]: thrift.ThriftCLIService (ThriftCLIService.java:ExecuteStatement(501)) - Error executing statement:
org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.AssertionError: mismatched type $8 TIMESTAMP(9)
        at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:178) ~[hive-service-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]
        at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:216) ~[hive-service-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]
        at org.apache.hive.service.cli.operation.Operation.run(Operation.java:327) ~[hive-service-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]
        at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:458) ~[hive-service-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]
        at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:435) ~[hive-service-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]
        at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:272) ~[hive-service-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]
        at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:492) [hive-service-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]
        at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1317) [hive-service-rpc-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]
        at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1302) [hive-service-rpc-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) [hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) [hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]
        at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56) [hive-service-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) [hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_77]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_77]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_77]
Caused by: java.lang.AssertionError: mismatched type $8 TIMESTAMP(9)
        at org.apache.calcite.rex.RexUtil$FixNullabilityShuttle.visitInputRef(RexUtil.java:2042) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]
        at org.apache.calcite.rex.RexUtil$FixNullabilityShuttle.visitInputRef(RexUtil.java:2020) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]
        at org.apache.calcite.rex.RexInputRef.accept(RexInputRef.java:112) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]
        at org.apache.calcite.rex.RexShuttle.visitList(RexShuttle.java:144) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]
        at org.apache.calcite.rex.RexShuttle.visitCall(RexShuttle.java:93) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]
        at org.apache.calcite.rex.RexShuttle.visitCall(RexShuttle.java:36) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]
        at org.apache.calcite.rex.RexCall.accept(RexCall.java:108) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]
        at org.apache.calcite.rex.RexShuttle.apply(RexShuttle.java:275) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]
        at org.apache.calcite.rex.RexShuttle.mutate(RexShuttle.java:234) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]
        at org.apache.calcite.rex.RexShuttle.apply(RexShuttle.java:252) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]
        at org.apache.calcite.rex.RexUtil.fixUp(RexUtil.java:1239) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]
        at org.apache.calcite.rel.rules.FilterJoinRule.perform(FilterJoinRule.java:232) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]
        at org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveFilterJoinRule$HiveFilterJoinMergeRule.onMatch(HiveFilterJoinRule.java:78) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]
        at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:318) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]
        at org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveFilterJoinRule$HiveFilterJoinMergeRule.onMatch(HiveFilterJoinRule.java:78) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]
        at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:318) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]
        at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:514) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]
        at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:392) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]
        at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:285) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]
        at org.apache.calcite.plan.hep.HepInstruction$RuleCollection.execute(HepInstruction.java:72) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]
        at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:207) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]
        at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:194) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.hepPlan(CalcitePlanner.java:1293) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.applyPreJoinOrderingTransforms(CalcitePlanner.java:1166) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.apply(CalcitePlanner.java:956) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.apply(CalcitePlanner.java:887) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]
        at org.apache.calcite.tools.Frameworks$1.apply(Frameworks.java:113) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]
        at org.apache.calcite.prepare.CalcitePrepareImpl.perform(CalcitePrepareImpl.java:969) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]
        at org.apache.calcite.tools.Frameworks.withPrepare(Frameworks.java:149) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]
        at org.apache.calcite.tools.Frameworks.withPlanner(Frameworks.java:106) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.getOptimizedAST(CalcitePlanner.java:706) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:274) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10642) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:233) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:245) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]
        at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:245) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:476) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:318) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1191) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]
        ... 15 more


Hive DDL for supporting tables are:

CREATE TABLE table_1 (timestamp_col_1 TIMESTAMP, decimal3003_col_2 DECIMAL(30, 3), tinyint_col_3 TINYINT, decimal0101_col_4 DECIMAL(1, 1), boolean_col_5 BOOLEAN, float_col_6 FLOAT, bigint_col_7 BIGINT, varchar0098_col_8 VARCHAR(98), timestamp_col_9 TIMESTAMP, bigint_col_10 BIGINT, decimal0903_col_11 DECIMAL(9, 3), timestamp_col_12 TIMESTAMP, timestamp_col_13 TIMESTAMP, float_col_14 FLOAT, char0254_col_15 CHAR(254), double_col_16 DOUBLE, timestamp_col_17 TIMESTAMP, boolean_col_18 BOOLEAN, decimal2608_col_19 DECIMAL(26, 8), varchar0216_col_20 VARCHAR(216), string_col_21 STRING, bigint_col_22 BIGINT, boolean_col_23 BOOLEAN, timestamp_col_24 TIMESTAMP, boolean_col_25 BOOLEAN, decimal2016_col_26 DECIMAL(20, 16), string_col_27 STRING, decimal0202_col_28 DECIMAL(2, 2), float_col_29 FLOAT, decimal2020_col_30 DECIMAL(20, 20), boolean_col_31 BOOLEAN, double_col_32 DOUBLE, varchar0148_col_33 VARCHAR(148), decimal2121_col_34 DECIMAL(21, 21), tinyint_col_35 TINYINT, boolean_col_36 BOOLEAN, boolean_col_37 BOOLEAN, string_col_38 STRING, decimal3420_col_39 DECIMAL(34, 20), timestamp_col_40 TIMESTAMP, decimal1408_col_41 DECIMAL(14, 8), string_col_42 STRING, decimal0902_col_43 DECIMAL(9, 2), varchar0204_col_44 VARCHAR(204), boolean_col_45 BOOLEAN, timestamp_col_46 TIMESTAMP, boolean_col_47 BOOLEAN, bigint_col_48 BIGINT, boolean_col_49 BOOLEAN, smallint_col_50 SMALLINT, decimal0704_col_51 DECIMAL(7, 4), timestamp_col_52 TIMESTAMP, boolean_col_53 BOOLEAN, timestamp_col_54 TIMESTAMP, int_col_55 INT, decimal0505_col_56 DECIMAL(5, 5), char0155_col_57 CHAR(155), boolean_col_58 BOOLEAN, bigint_col_59 BIGINT, boolean_col_60 BOOLEAN, boolean_col_61 BOOLEAN, char0249_col_62 CHAR(249), boolean_col_63 BOOLEAN, timestamp_col_64 TIMESTAMP, decimal1309_col_65 DECIMAL(13, 9), int_col_66 INT, float_col_67 FLOAT, timestamp_col_68 TIMESTAMP, timestamp_col_69 TIMESTAMP, boolean_col_70 BOOLEAN, timestamp_col_71 TIMESTAMP, double_col_72 DOUBLE, boolean_col_73 BOOLEAN, char0222_col_74 CHAR(222), float_col_75 FLOAT, string_col_76 STRING, decimal2612_col_77 DECIMAL(26, 12), timestamp_col_78 TIMESTAMP, char0128_col_79 CHAR(128), timestamp_col_80 TIMESTAMP, double_col_81 DOUBLE, timestamp_col_82 TIMESTAMP, float_col_83 FLOAT, decimal2622_col_84 DECIMAL(26, 22), double_col_85 DOUBLE, float_col_86 FLOAT, decimal0907_col_87 DECIMAL(9, 7)) STORED AS orc;

CREATE TABLE table_18 (boolean_col_1 BOOLEAN, boolean_col_2 BOOLEAN, decimal2518_col_3 DECIMAL(25, 18), float_col_4 FLOAT, timestamp_col_5 TIMESTAMP, double_col_6 DOUBLE, double_col_7 DOUBLE, char0035_col_8 CHAR(35), decimal2709_col_9 DECIMAL(27, 9), int_col_10 INT, timestamp_col_11 TIMESTAMP, decimal3604_col_12 DECIMAL(36, 4), string_col_13 STRING, int_col_14 INT, tinyint_col_15 TINYINT, decimal1911_col_16 DECIMAL(19, 11), float_col_17 FLOAT, timestamp_col_18 TIMESTAMP, smallint_col_19 SMALLINT, tinyint_col_20 TINYINT, timestamp_col_21 TIMESTAMP, boolean_col_22 BOOLEAN, int_col_23 INT) STORED AS orc;


The problem is that the reference indices in the condition (and thus, their type) are inferred incorrectly.</description>
			<version>2.1.0</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.HiveCalciteUtil.java</file>
		</fixedFiles>
	</bug>
	<bug id="13608" opendate="2016-04-25 21:12:18" fixdate="2016-05-17 20:43:33" resolution="Fixed">
		<buginformation>
			<summary>We should provide better error message while constraints with duplicate names are created</summary>
			<description>

PREHOOK: query: create table t1(x int, constraint pk1 primary key (x) disable novalidate)
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@t1
POSTHOOK: query: create table t1(x int, constraint pk1 primary key (x) disable novalidate)
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@t1
PREHOOK: query: create table t2(x int, constraint pk1 primary key (x) disable novalidate)
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@t2
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:For direct MetaStore DB connections, we don&amp;amp;apos;t support retries at the client level.)


In the above case, it seems like useful error message is lost. It looks like a  generic problem with metastore server/client exception handling and message propagation. Seems like exception parsing logic of RetryingMetaStoreClient::invoke() needs to be updated.</description>
			<version>2.0.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.QTestUtil.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">13618</link>
			<link type="Duplicate" description="is duplicated by">11918</link>
			<link type="Reference" description="is related to">13290</link>
		</links>
	</bug>
	<bug id="13730" opendate="2016-05-10 19:52:38" fixdate="2016-05-18 16:52:36" resolution="Fixed">
		<buginformation>
			<summary>Avoid double spilling the same partition when memory threshold is set very low</summary>
			<description>I am seeing hybridgrace_hashjoin_1.q getting stuck on master.</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">13755</link>
			<link type="Reference" description="relates to">12837</link>
		</links>
	</bug>
	<bug id="13691" opendate="2016-05-04 23:40:03" fixdate="2016-05-18 17:49:32" resolution="Fixed">
		<buginformation>
			<summary>No record with CQ_ID=0 found in COMPACTION_QUEUE</summary>
			<description>
2016-04-29 18:49:31,594 ERROR [Thread-11]: compactor.Initiator (Initiator.java:run(141)) - Caught exception while trying to determine if we should compact id:0,dbname:default,tableName:service_logs_v2,par
tName:ds=2016-04-21,state:^@,type:null,runAs:null,tooManyAborts:false,highestTxnId:0.  Marking clean to avoid repeated failures, MetaException(message:Timeout when executing method: getTable)
        at org.apache.hadoop.hive.metastore.Deadline.newMetaException(Deadline.java:187)
        at org.apache.hadoop.hive.metastore.Deadline.check(Deadline.java:177)
        at org.apache.hadoop.hive.metastore.Deadline.checkTimeout(Deadline.java:160)
        at org.apache.hadoop.hive.metastore.ObjectStore.convertToParts(ObjectStore.java:1839)
        at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsViaOrmFilter(ObjectStore.java:2255)
        at org.apache.hadoop.hive.metastore.ObjectStore.access$300(ObjectStore.java:165)
        at org.apache.hadoop.hive.metastore.ObjectStore$3.getJdoResult(ObjectStore.java:2051)
        at org.apache.hadoop.hive.metastore.ObjectStore$3.getJdoResult(ObjectStore.java:2043)
        at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2400)
        at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByNamesInternal(ObjectStore.java:2043)
        at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByNames(ObjectStore.java:2037)
        at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:114)
        at com.sun.proxy.$Proxy0.getPartitionsByNames(Unknown Source)
        at org.apache.hadoop.hive.ql.txn.compactor.CompactorThread.resolvePartition(CompactorThread.java:111)
        at org.apache.hadoop.hive.ql.txn.compactor.Initiator.run(Initiator.java:129)
Caused by: org.apache.hadoop.hive.metastore.DeadlineException: Timeout when executing method: getTable
        at org.apache.hadoop.hive.metastore.Deadline.check(Deadline.java:174)
        ... 16 more

2016-04-29 18:49:31,595 ERROR [Thread-11]: compactor.Initiator (Initiator.java:run(154)) - Initiator loop caught unexpected exception this time through the loop: java.lang.IllegalStateException: No record with CQ_ID=0 found in COMPACTION_QUEUE
        at org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.markFailed(CompactionTxnHandler.java:861)
        at org.apache.hadoop.hive.ql.txn.compactor.Initiator.run(Initiator.java:144)




2016-04-29 18:49:31,595 ERROR [Thread-11]: compactor.Initiator (Initiator.java:run(154)) - Initiator loop caught unexpected exception this time through the loop: java.lang.IllegalStateException: No record with CQ_ID=0 found in COMPACTION_QUEUE
        at org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.markFailed(CompactionTxnHandler.java:861)
        at org.apache.hadoop.hive.ql.txn.compactor.Initiator.run(Initiator.java:144)


is triggered by DeadlineException: Timeout when executing method but is nonetheless an issue.
We should be able to record an entry in completed_compaction_queue to represent a failed compaction even if an entry in compaction_queue was never made, as is the case here.</description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.txn.compactor.Initiator.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.java</file>
			<file type="M">org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">12353</link>
		</links>
	</bug>
	<bug id="13343" opendate="2016-03-23 23:08:39" fixdate="2016-05-18 18:06:51" resolution="Fixed">
		<buginformation>
			<summary>Need to disable hybrid grace hash join in llap mode except for dynamically partitioned hash join</summary>
			<description>Due to performance reasons, we should disable use of hybrid grace hash join in llap when dynamic partition hash join is not used. With dynamic partition hash join, we need hybrid grace hash join due to the possibility of skews.</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.LlapDecider.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
	</bug>
	<bug id="11049" opendate="2015-06-18 23:31:20" fixdate="2016-05-19 11:13:49" resolution="Duplicate">
		<buginformation>
			<summary>With Clause should cache data &amp; reuse </summary>
			<description>Hive supports with clause. However Hive don&amp;amp;apos;t cache the result set of with clause and reuse it.
Instead we inline the query definition of the with clause. This results in re execution of with clause query every where it is referenced.</description>
			<version>2.1.0</version>
			<fixedVersion></fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.GenTezProcContext.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.CreateTableDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
			<file type="M">org.apache.hadoop.hive.ql.QueryPlan.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Table.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Task.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.FileSinkDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Context.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.TestGenTezWork.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">11752</link>
		</links>
	</bug>
	<bug id="13786" opendate="2016-05-18 21:21:31" fixdate="2016-05-19 15:49:36" resolution="Fixed">
		<buginformation>
			<summary>Fix the unit test failure org.apache.hive.service.cli.session.TestHiveSessionImpl.testLeakOperationHandle</summary>
			<description></description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Sub-task</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.service.cli.session.TestHiveSessionImpl.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">13702</link>
			<link type="Duplicate" description="is duplicated by">13791</link>
			<link type="Required" description="is required by">4924</link>
		</links>
	</bug>
	<bug id="13791" opendate="2016-05-19 12:40:49" fixdate="2016-05-19 16:06:08" resolution="Duplicate">
		<buginformation>
			<summary>Fix  failure Unit Test TestHiveSessionImpl.testLeakOperationHandle</summary>
			<description></description>
			<version>2.1.0</version>
			<fixedVersion></fixedVersion>
			<type>Test</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.service.cli.session.TestHiveSessionImpl.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">13786</link>
		</links>
	</bug>
	<bug id="13622" opendate="2016-04-27 01:01:30" fixdate="2016-05-19 20:04:08" resolution="Fixed">
		<buginformation>
			<summary>WriteSet tracking optimizations</summary>
			<description>HIVE-13395 solves the the lost update problem with some inefficiencies.
1. TxhHandler.OperationType is currently derived from LockType.  This doesn&amp;amp;apos;t  distinguish between Update and Delete but would be useful.  See comments in TxnHandler.  Should be able to pass in Insert/Update/Delete info from client into TxnHandler.
2. TxnHandler.addDynamicPartitions() should know the OperationType as well from the client.  It currently extrapolates it from TXN_COMPONENTS.  This works but requires extra SQL statements and is thus less performant.  It will not work multi-stmt txns.  See comments in the code.
3. TxnHandler.checkLock() see more comments around "isPartOfDynamicPartitionInsert".  If TxnHandler knew whether it is being called as part of an op running with dynamic partitions, it could be more efficient.  In that case we don&amp;amp;apos;t have to write to TXN_COMPONENTS at all during lock acquisition.  Conversely, if not running with DynPart then, we can kill current txn on lock grant rather than wait until commit time.
4. TxnHandler.addDynamicPartitions() - the insert stmt here should combing multiple rows into single SQL stmt (but with a limit for extreme cases)
5. TxnHandler.enqueueLockWithRetry() - this currently adds components that are only being read to TXN_COMPONENTS.   This is useless at best since read op don&amp;amp;apos;t generate anything to compact.  For example, delete from T where t1 in (select c1 from C) - no reason to add C to txn_components but we do.
All of these require some Thrift changes
Once done, re-enable TestDbTxnHandler2.testWriteSetTracking11()
Also see comments in here</description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.txn.compactor.TestInitiator.java</file>
			<file type="M">org.apache.hive.hcatalog.streaming.HiveEndPoint.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TestTxnHandler.java</file>
			<file type="M">org.apache.hadoop.hive.ql.txn.compactor.TestCleaner.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
			<file type="M">org.apache.hive.hcatalog.streaming.mutate.client.lock.TestLock.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.LockRequestBuilder.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.TestHiveMetaStoreTxns.java</file>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.java</file>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TestCompactionTxnHandler.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
			<file type="M">org.apache.hive.hcatalog.streaming.mutate.client.lock.Lock.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.AddDynamicPartitions.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.LockComponent.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.LockComponentBuilder.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">9675</link>
			<link type="Blocker" description="is blocked by">13395</link>
			<link type="Reference" description="relates to">13795</link>
			<link type="Reference" description="is related to">13741</link>
		</links>
	</bug>
	<bug id="13702" opendate="2016-05-06 01:10:44" fixdate="2016-05-21 00:53:18" resolution="Duplicate">
		<buginformation>
			<summary>TestHiveSessionImpl fails on master</summary>
			<description>Presumably broken by HIVE-4924</description>
			<version>2.1.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.service.cli.session.TestHiveSessionImpl.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">13786</link>
		</links>
	</bug>
	<bug id="13223" opendate="2016-03-08 00:13:38" fixdate="2016-05-21 00:53:20" resolution="Fixed">
		<buginformation>
			<summary>HoS  may hang for queries that run on 0 splits </summary>
			<description>Can be seen on all timed out tests after HIVE-13040 went in</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.spark.client.RemoteDriver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">13525</link>
			<link type="Duplicate" description="is duplicated by">13355</link>
			<link type="Duplicate" description="is duplicated by">13247</link>
		</links>
	</bug>
	<bug id="13699" opendate="2016-05-05 22:14:30" fixdate="2016-05-21 01:16:14" resolution="Fixed">
		<buginformation>
			<summary>Make JavaDataModel#get thread safe for parallel compilation</summary>
			<description>The class JavaDataModel has a static method, #get, that is not thread safe. This may be an issue when parallel query compilation is enabled because two threads may attempt to call JavaDataModel#get at the same time, etc.</description>
			<version>2.0.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.util.JavaDataModel.java</file>
		</fixedFiles>
	</bug>
	<bug id="13502" opendate="2016-04-13 16:02:01" fixdate="2016-05-23 14:26:21" resolution="Fixed">
		<buginformation>
			<summary>Beeline doesnt support session parameters in JDBC URL as documentation states.</summary>
			<description>https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients#HiveServer2Clients-ConnectionURLs
documents that sessions variables like credentials etc are accepted as part of the URL. However, Beeline does not support such URLs today.</description>
			<version>1.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.beeline.BeeLine.java</file>
			<file type="M">org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
			<file type="M">org.apache.hive.beeline.Commands.java</file>
			<file type="M">org.apache.hive.jdbc.Utils.java</file>
		</fixedFiles>
		<links>
			<link type="Container" description="contains">9144</link>
		</links>
	</bug>
	<bug id="13628" opendate="2016-04-27 17:17:21" fixdate="2016-05-23 17:51:28" resolution="Fixed">
		<buginformation>
			<summary>Support for permanent functions - error handling if no restart</summary>
			<description>Support for permanent functions - error handling if no restart</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Operator.java</file>
		</fixedFiles>
	</bug>
	<bug id="12643" opendate="2015-12-10 01:23:47" fixdate="2016-05-23 23:52:03" resolution="Fixed">
		<buginformation>
			<summary>For self describing InputFormat don&amp;apos;t replicate schema information in partitions</summary>
			<description>Since self describing Input Formats don&amp;amp;apos;t use individual partition schemas for schema resolution, there is no need to send that info to tasks.
Doing this should cut down plan size.</description>
			<version>2.0.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.PartitionDesc.java</file>
		</fixedFiles>
	</bug>
	<bug id="13832" opendate="2016-05-24 15:35:04" fixdate="2016-05-25 08:31:31" resolution="Fixed">
		<buginformation>
			<summary>Add missing license header to files</summary>
			<description>Preparing to cut the branch for 2.1.0.</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.hbase.stats.IExtrapolatePartStatus.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.model.MConstraint.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.TestTezWorkConcurrency.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.status.impl.SparkJobUtils.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.thrift.ThriftFormatter.java</file>
			<file type="M">org.apache.hadoop.hive.ql.txn.AcidWriteSetService.java</file>
			<file type="M">org.apache.orc.impl.TestDataReaderProperties.java</file>
			<file type="M">org.apache.hadoop.hive.ql.util.JavaDataModelTest.java</file>
			<file type="M">org.apache.orc.impl.DataReaderProperties.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">13438</link>
		</links>
	</bug>
	<bug id="13720" opendate="2016-05-09 17:31:53" fixdate="2016-05-25 21:59:56" resolution="Fixed">
		<buginformation>
			<summary>TestLlapTaskCommunicator fails on master</summary>
			<description>Can be reproduced locally as well</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.java</file>
		</fixedFiles>
	</bug>
	<bug id="13267" opendate="2016-03-11 09:50:10" fixdate="2016-05-26 00:13:43" resolution="Fixed">
		<buginformation>
			<summary>Vectorization: Add SelectLikeStringColScalar for non-filter operations</summary>
			<description>FilterStringColLikeStringScalar only applies to the values within filter clauses.
Borrow the Checker impls and extend to the value generation - for cases like
select col is null or col like &amp;amp;apos;%(null)%&amp;amp;apos; </description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.AbstractFilterStringColLikeStringScalar.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFLike.java</file>
		</fixedFiles>
	</bug>
	<bug id="13821" opendate="2016-05-23 18:28:21" fixdate="2016-05-26 01:25:11" resolution="Fixed">
		<buginformation>
			<summary>OrcSplit groups all delta files together into a single split</summary>
			<description>HIVE-7428 had fix for worst case column projection size estimate. It was removed in HIVE-10397 to return file length but for ACID strategy file length is passed as 0. In worst case, this always return 0 and all files ends up in single split. </description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.ColumnarSplitSizeEstimator.java</file>
		</fixedFiles>
	</bug>
	<bug id="13269" opendate="2016-03-11 20:31:12" fixdate="2016-05-26 09:11:07" resolution="Fixed">
		<buginformation>
			<summary>Simplify comparison expressions using column stats</summary>
			<description></description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.HiveRexUtil.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="is blocked by">13822</link>
			<link type="Blocker" description="is blocked by">13542</link>
		</links>
	</bug>
	<bug id="13561" opendate="2016-04-20 21:41:42" fixdate="2016-05-27 07:24:48" resolution="Fixed">
		<buginformation>
			<summary>HiveServer2 is leaking ClassLoaders when add jar / temporary functions are used</summary>
			<description>I can repo this on branch-1.2 and branch-2.0.
It looks to be the same issues as: HIVE-11408
The patch from HIVE-11408 looks to fix the issue as well.
I&amp;amp;apos;ve updated the patch from HIVE-11408 to be aligned with branch-1.2 and master
</description>
			<version>1.2.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Registry.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">11408</link>
		</links>
	</bug>
	<bug id="13841" opendate="2016-05-25 02:04:25" fixdate="2016-05-27 23:44:07" resolution="Fixed">
		<buginformation>
			<summary>Orc split generation returns different strategies with cache enabled vs disabled</summary>
			<description>Split strategy chosen by OrcInputFormat should not change when enabling or disabling footer cache. Currently if footer cache is disabled minSplits in OrcInputFormat.Context will be set to -1 which is used during determination of split strategies. minSplits should be set to requested value or some default instead of cache size</description>
			<version>2.1.0</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
		</fixedFiles>
	</bug>
	<bug id="13861" opendate="2016-05-26 10:03:54" fixdate="2016-05-30 14:47:17" resolution="Fixed">
		<buginformation>
			<summary>Fix up nullability issue that might be created by pull up constants rules</summary>
			<description>When we pull up constants through Union or Sort operators, we might end up rewriting the original expression into an expression whose schema has different nullability properties for some of its columns.
This results in AssertionError of the following kind:

...
org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.AssertionError: Internal error: Cannot add expression of different type to set:
...

</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveUnionPullUpConstantsRule.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveSortLimitPullUpConstantsRule.java</file>
		</fixedFiles>
		<links>
			<link type="Required" description="is required by">13807</link>
		</links>
	</bug>
	<bug id="13849" opendate="2016-05-25 16:58:26" fixdate="2016-05-30 23:08:42" resolution="Fixed">
		<buginformation>
			<summary>Wrong plan for hive.optimize.sort.dynamic.partition=true</summary>
			<description>To reproduce:

set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.optimize.sort.dynamic.partition=true;

CREATE TABLE non_acid(key string, value string) PARTITIONED BY(ds string, hr int) CLUSTERED BY(key) INTO 2 BUCKETS STORED AS ORC;

explain insert into table non_acid partition(ds,hr) select * from srcpart sort by value;


CC&amp;amp;apos;ed Ashutosh Chauhan, Eugene Koifman</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.java</file>
		</fixedFiles>
	</bug>
	<bug id="13818" opendate="2016-05-23 07:32:24" fixdate="2016-05-30 23:53:26" resolution="Fixed">
		<buginformation>
			<summary>Fast Vector MapJoin Long hashtable has to handle all integral types</summary>
			<description>Changes for HIVE-13682 did fix a bug in Fast Hash Tables, but evidently not this issue according to Gopal/Rajesh/Nita.</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.plan.VectorMapJoinDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.optimized.VectorMapJoinOptimizedLongCommon.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastLongHashTable.java</file>
		</fixedFiles>
		<links>
			<link type="Incorporates" description="is part of">13874</link>
		</links>
	</bug>
	<bug id="13837" opendate="2016-05-24 23:13:33" fixdate="2016-05-31 05:31:42" resolution="Fixed">
		<buginformation>
			<summary>current_timestamp() output format is different in some cases</summary>
			<description>As Jason Dere reports:


current_timestamp() udf returns result with different format in some cases.

select current_timestamp() returns result with decimal precision:
{noformat}
hive&amp;gt; select current_timestamp();
OK
2016-04-14 18:26:58.875
Time taken: 0.077 seconds, Fetched: 1 row(s)
{noformat}

But output format is different for select current_timestamp() from all100k union select current_timestamp() from over100k limit 5; 
{noformat}
hive&amp;gt; select current_timestamp() from all100k union select current_timestamp() from over100k limit 5;
Query ID = hrt_qa_20160414182956_c4ed48f2-9913-4b3b-8f09-668ebf55b3e3
Total jobs = 1
Launching Job 1 out of 1
Tez session was closed. Reopening...
Session re-established.


Status: Running (Executing on YARN cluster with App id application_1460611908643_0624)

----------------------------------------------------------------------------------------------
        VERTICES      MODE        STATUS  TOTAL  COMPLETED  RUNNING  PENDING  FAILED  KILLED  
----------------------------------------------------------------------------------------------
Map 1 ..........      llap     SUCCEEDED      1          1        0        0       0       0  
Map 4 ..........      llap     SUCCEEDED      1          1        0        0       0       0  
Reducer 3 ......      llap     SUCCEEDED      1          1        0        0       0       0  
----------------------------------------------------------------------------------------------
VERTICES: 03/03  [==========================&amp;gt;&amp;gt;] 100%  ELAPSED TIME: 0.92 s     
----------------------------------------------------------------------------------------------
OK
2016-04-14 18:29:56
Time taken: 10.558 seconds, Fetched: 1 row(s)
{noformat}

explain plan for select current_timestamp();
{noformat}
hive&amp;gt; explain extended select current_timestamp();
OK
ABSTRACT SYNTAX TREE:
  
TOK_QUERY
   TOK_INSERT
      TOK_DESTINATION
         TOK_DIR
            TOK_TMP_FILE
      TOK_SELECT
         TOK_SELEXPR
            TOK_FUNCTION
               current_timestamp


STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: _dummy_table
          Row Limit Per Split: 1
          GatherStats: false
          Select Operator
            expressions: 2016-04-14 18:30:57.206 (type: timestamp)
            outputColumnNames: _col0
            ListSink

Time taken: 0.062 seconds, Fetched: 30 row(s)
{noformat}

explain plan for select current_timestamp() from all100k union select current_timestamp() from over100k limit 5;
{noformat}
hive&amp;gt; explain extended select current_timestamp() from all100k union select current_timestamp() from over100k limit 5;
OK
ABSTRACT SYNTAX TREE:
  
TOK_QUERY
   TOK_FROM
      TOK_SUBQUERY
         TOK_QUERY
            TOK_FROM
               TOK_SUBQUERY
                  TOK_UNIONALL
                     TOK_QUERY
                        TOK_FROM
                           TOK_TABREF
                              TOK_TABNAME
                                 all100k
                        TOK_INSERT
                           TOK_DESTINATION
                              TOK_DIR
                                 TOK_TMP_FILE
                           TOK_SELECT
                              TOK_SELEXPR
                                 TOK_FUNCTION
                                    current_timestamp
                     TOK_QUERY
                        TOK_FROM
                           TOK_TABREF
                              TOK_TABNAME
                                 over100k
                        TOK_INSERT
                           TOK_DESTINATION
                              TOK_DIR
                                 TOK_TMP_FILE
                           TOK_SELECT
                              TOK_SELEXPR
                                 TOK_FUNCTION
                                    current_timestamp
                  _u1
            TOK_INSERT
               TOK_DESTINATION
                  TOK_DIR
                     TOK_TMP_FILE
               TOK_SELECTDI
                  TOK_SELEXPR
                     TOK_ALLCOLREF
         _u2
   TOK_INSERT
      TOK_DESTINATION
         TOK_DIR
            TOK_TMP_FILE
      TOK_SELECT
         TOK_SELEXPR
            TOK_ALLCOLREF
      TOK_LIMIT
         5


STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
      DagId: hrt_qa_20160414183119_ec8e109e-8975-4799-a142-4a2289f85910:7
      Edges:
        Map 1 &amp;lt;- Union 2 (CONTAINS)
        Map 4 &amp;lt;- Union 2 (CONTAINS)
        Reducer 3 &amp;lt;- Union 2 (SIMPLE_EDGE)
      DagName: 
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: all100k
                  Statistics: Num rows: 100000 Data size: 15801336 Basic stats: COMPLETE Column stats: COMPLETE
                  GatherStats: false
                  Select Operator
                    Statistics: Num rows: 100000 Data size: 4000000 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: 2016-04-14 18:31:19.0 (type: timestamp)
                      outputColumnNames: _col0
                      Statistics: Num rows: 200000 Data size: 8000000 Basic stats: COMPLETE Column stats: COMPLETE
                      Group By Operator
                        keys: _col0 (type: timestamp)
                        mode: hash
                        outputColumnNames: _col0
                        Statistics: Num rows: 1 Data size: 40 Basic stats: COMPLETE Column stats: COMPLETE
                        Reduce Output Operator
                          key expressions: _col0 (type: timestamp)
                          null sort order: a
                          sort order: +
                          Map-reduce partition columns: _col0 (type: timestamp)
                          Statistics: Num rows: 1 Data size: 40 Basic stats: COMPLETE Column stats: COMPLETE
                          tag: -1
                          TopN: 5
                          TopN Hash Memory Usage: 0.04
                          auto parallelism: true
            Execution mode: llap
            LLAP IO: no inputs
            Path -&amp;gt; Alias:
              hdfs://os-r6-qugztu-hive-1-5.novalocal:8020/user/hcat/tests/data/all100k [all100k]
            Path -&amp;gt; Partition:
              hdfs://os-r6-qugztu-hive-1-5.novalocal:8020/user/hcat/tests/data/all100k 
                Partition
                  base file name: all100k
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                  properties:
                    COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"t":"true","si":"true","i":"true","b":"true","f":"true","d":"true","s":"true","dc":"true","bo":"true","v":"true","c":"true","ts":"true"}}
                    EXTERNAL TRUE
                    bucket_count -1
                    columns t,si,i,b,f,d,s,dc,bo,v,c,ts,dt
                    columns.comments 
                    columns.types tinyint:smallint:int:bigint:float:double:string:decimal(38,18):boolean:varchar(25):char(25):timestamp:date
                    field.delim |
                    file.inputformat org.apache.hadoop.mapred.TextInputFormat
                    file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                    location hdfs://os-r6-qugztu-hive-1-5.novalocal:8020/user/hcat/tests/data/all100k
                    name default.all100k
                    numFiles 1
                    numRows 100000
                    rawDataSize 15801336
                    serialization.ddl struct all100k { byte t, i16 si, i32 i, i64 b, float f, double d, string s, decimal(38,18) dc, bool bo, varchar(25) v, char(25) c, timestamp ts, date dt}
                    serialization.format |
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    totalSize 15901336
                    transient_lastDdlTime 1460612683
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                
                    input format: org.apache.hadoop.mapred.TextInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                    properties:
                      COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"t":"true","si":"true","i":"true","b":"true","f":"true","d":"true","s":"true","dc":"true","bo":"true","v":"true","c":"true","ts":"true"}}
                      EXTERNAL TRUE
                      bucket_count -1
                      columns t,si,i,b,f,d,s,dc,bo,v,c,ts,dt
                      columns.comments 
                      columns.types tinyint:smallint:int:bigint:float:double:string:decimal(38,18):boolean:varchar(25):char(25):timestamp:date
                      field.delim |
                      file.inputformat org.apache.hadoop.mapred.TextInputFormat
                      file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                      location hdfs://os-r6-qugztu-hive-1-5.novalocal:8020/user/hcat/tests/data/all100k
                      name default.all100k
                      numFiles 1
                      numRows 100000
                      rawDataSize 15801336
                      serialization.ddl struct all100k { byte t, i16 si, i32 i, i64 b, float f, double d, string s, decimal(38,18) dc, bool bo, varchar(25) v, char(25) c, timestamp ts, date dt}
                      serialization.format |
                      serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      totalSize 15901336
                      transient_lastDdlTime 1460612683
                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    name: default.all100k
                  name: default.all100k
            Truncated Path -&amp;gt; Alias:
              hdfs://os-r6-qugztu-hive-1-5.novalocal:8020/user/hcat/tests/data/all100k [all100k]
        Map 4 
            Map Operator Tree:
                TableScan
                  alias: over100k
                  Statistics: Num rows: 100000 Data size: 6631229 Basic stats: COMPLETE Column stats: COMPLETE
                  GatherStats: false
                  Select Operator
                    Statistics: Num rows: 100000 Data size: 4000000 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: 2016-04-14 18:31:19.0 (type: timestamp)
                      outputColumnNames: _col0
                      Statistics: Num rows: 200000 Data size: 8000000 Basic stats: COMPLETE Column stats: COMPLETE
                      Group By Operator
                        keys: _col0 (type: timestamp)
                        mode: hash
                        outputColumnNames: _col0
                        Statistics: Num rows: 1 Data size: 40 Basic stats: COMPLETE Column stats: COMPLETE
                        Reduce Output Operator
                          key expressions: _col0 (type: timestamp)
                          null sort order: a
                          sort order: +
                          Map-reduce partition columns: _col0 (type: timestamp)
                          Statistics: Num rows: 1 Data size: 40 Basic stats: COMPLETE Column stats: COMPLETE
                          tag: -1
                          TopN: 5
                          TopN Hash Memory Usage: 0.04
                          auto parallelism: true
            Execution mode: llap
            LLAP IO: no inputs
            Path -&amp;gt; Alias:
              hdfs://os-r6-qugztu-hive-1-5.novalocal:8020/user/hcat/tests/data/over100k [over100k]
            Path -&amp;gt; Partition:
              hdfs://os-r6-qugztu-hive-1-5.novalocal:8020/user/hcat/tests/data/over100k 
                Partition
                  base file name: over100k
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                  properties:
                    COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"t":"true","si":"true","i":"true","b":"true","f":"true","d":"true","bo":"true","s":"true","bin":"true"}}
                    EXTERNAL TRUE
                    bucket_count -1
                    columns t,si,i,b,f,d,bo,s,bin
                    columns.comments 
                    columns.types tinyint:smallint:int:bigint:float:double:boolean:string:binary
                    field.delim :
                    file.inputformat org.apache.hadoop.mapred.TextInputFormat
                    file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                    location hdfs://os-r6-qugztu-hive-1-5.novalocal:8020/user/hcat/tests/data/over100k
                    name default.over100k
                    numFiles 1
                    numRows 100000
                    rawDataSize 6631229
                    serialization.ddl struct over100k { byte t, i16 si, i32 i, i64 b, float f, double d, bool bo, string s, binary bin}
                    serialization.format :
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    totalSize 6731229
                    transient_lastDdlTime 1460612798
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                
                    input format: org.apache.hadoop.mapred.TextInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                    properties:
                      COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"t":"true","si":"true","i":"true","b":"true","f":"true","d":"true","bo":"true","s":"true","bin":"true"}}
                      EXTERNAL TRUE
                      bucket_count -1
                      columns t,si,i,b,f,d,bo,s,bin
                      columns.comments 
                      columns.types tinyint:smallint:int:bigint:float:double:boolean:string:binary
                      field.delim :
                      file.inputformat org.apache.hadoop.mapred.TextInputFormat
                      file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                      location hdfs://os-r6-qugztu-hive-1-5.novalocal:8020/user/hcat/tests/data/over100k
                      name default.over100k
                      numFiles 1
                      numRows 100000
                      rawDataSize 6631229
                      serialization.ddl struct over100k { byte t, i16 si, i32 i, i64 b, float f, double d, bool bo, string s, binary bin}
                      serialization.format :
                      serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      totalSize 6731229
                      transient_lastDdlTime 1460612798
                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    name: default.over100k
                  name: default.over100k
            Truncated Path -&amp;gt; Alias:
              hdfs://os-r6-qugztu-hive-1-5.novalocal:8020/user/hcat/tests/data/over100k [over100k]
        Reducer 3 
            Execution mode: vectorized, llap
            Needs Tagging: false
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: timestamp)
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 40 Basic stats: COMPLETE Column stats: COMPLETE
                Limit
                  Number of rows: 5
                  Statistics: Num rows: 1 Data size: 40 Basic stats: COMPLETE Column stats: COMPLETE
                  File Output Operator
                    compressed: false
                    GlobalTableId: 0
                    directory: hdfs://os-r6-qugztu-hive-1-5.novalocal:8020/tmp/hive/hrt_qa/ec0773d7-0ac2-45c7-b9cb-568bbed2c49c/hive_2016-04-14_18-31-19_532_3480081382837900888-1/-mr-10001/.hive-staging_hive_2016-04-14_18-31-19_532_3480081382837900888-1/-ext-10002
                    NumFilesPerFileSink: 1
                    Statistics: Num rows: 1 Data size: 40 Basic stats: COMPLETE Column stats: COMPLETE
                    Stats Publishing Key Prefix: hdfs://os-r6-qugztu-hive-1-5.novalocal:8020/tmp/hive/hrt_qa/ec0773d7-0ac2-45c7-b9cb-568bbed2c49c/hive_2016-04-14_18-31-19_532_3480081382837900888-1/-mr-10001/.hive-staging_hive_2016-04-14_18-31-19_532_3480081382837900888-1/-ext-10002/
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        properties:
                          columns _col0
                          columns.types timestamp
                          escape.delim \
                          hive.serialization.extend.additional.nesting.levels true
                          serialization.escape.crlf true
                          serialization.format 1
                          serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    TotalFiles: 1
                    GatherStats: false
                    MultiFileSpray: false
        Union 2 
            Vertex: Union 2

  Stage: Stage-0
    Fetch Operator
      limit: 5
      Processor Tree:
        ListSink

Time taken: 0.301 seconds, Fetched: 284 row(s)
{noformat}

Both the queries used return timestamp with YYYY-MM-DD HH:MM:SS.fff format in past releases.

</description>
			<version>2.0.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTBuilder.java</file>
		</fixedFiles>
	</bug>
	<bug id="13863" opendate="2016-05-26 13:52:55" fixdate="2016-05-31 14:09:39" resolution="Fixed">
		<buginformation>
			<summary>Improve AnnotateWithStatistics with support for cartesian product</summary>
			<description>Currently cartesian product stats based on cardinality of inputs are not inferred correctly.</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
		</fixedFiles>
	</bug>
	<bug id="13831" opendate="2016-05-24 10:58:38" fixdate="2016-05-31 14:19:54" resolution="Fixed">
		<buginformation>
			<summary>Error pushing predicates to HBase storage handler</summary>
			<description>Discovered while working on HIVE-13693.
There is an error on the predicates that we can push to HBaseStorageHandler. In particular, range predicates of the shape (bounded, open) and (open, bounded) over long or int columns get pushed and return wrong results.
The problem has to do with the storage order for keys in HBase. Keys are sorted lexicographically. Since the byte representation of negative values comes after the positive values, open range predicates need special handling that we do not have right now.
Thus, for instance, when we push the predicate key &amp;gt; 2, we return all records with column key greater than 2, plus the records with negative values for the column key. This problem does not get exposed if a filter is kept in the Hive operator tree, but we should not assume the latest.
This fix avoids pushing this kind of predicates to the storage handler, returning them in the residual part of the predicate that cannot be pushed. In the future, special handling might be added to support this kind of predicates.</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.accumulo.predicate.PushdownTuple.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.HiveStoragePredicateHandler.java</file>
			<file type="M">org.apache.hadoop.hive.ql.index.IndexPredicateAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.accumulo.predicate.AccumuloPredicateHandler.java</file>
			<file type="M">org.apache.hadoop.hive.hbase.HBaseStorageHandler.java</file>
			<file type="M">org.apache.hadoop.hive.hbase.AbstractHBaseKeyFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.index.IndexSearchCondition.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">13693</link>
		</links>
	</bug>
	<bug id="13693" opendate="2016-05-05 17:40:30" fixdate="2016-05-31 15:12:43" resolution="Fixed">
		<buginformation>
			<summary>Multi-insert query drops Filter before file output when there is a.val &lt;&gt; b.val</summary>
			<description>To reproduce:

CREATE TABLE T_A ( id STRING, val STRING ); 
CREATE TABLE T_B ( id STRING, val STRING ); 
CREATE TABLE join_result_1 ( ida STRING, vala STRING, idb STRING, valb STRING ); 
CREATE TABLE join_result_3 ( ida STRING, vala STRING, idb STRING, valb STRING ); 

INSERT INTO TABLE T_A 
VALUES (&amp;amp;apos;Id_1&amp;amp;apos;, &amp;amp;apos;val_101&amp;amp;apos;), (&amp;amp;apos;Id_2&amp;amp;apos;, &amp;amp;apos;val_102&amp;amp;apos;), (&amp;amp;apos;Id_3&amp;amp;apos;, &amp;amp;apos;val_103&amp;amp;apos;); 

INSERT INTO TABLE T_B 
VALUES (&amp;amp;apos;Id_1&amp;amp;apos;, &amp;amp;apos;val_103&amp;amp;apos;), (&amp;amp;apos;Id_2&amp;amp;apos;, &amp;amp;apos;val_104&amp;amp;apos;); 

explain
FROM T_A a LEFT JOIN T_B b ON a.id = b.id
INSERT OVERWRITE TABLE join_result_1
SELECT a.*, b.*
WHERE b.id = &amp;amp;apos;Id_1&amp;amp;apos; AND b.val = &amp;amp;apos;val_103&amp;amp;apos;
INSERT OVERWRITE TABLE join_result_3
SELECT a.*, b.*
WHERE b.val = &amp;amp;apos;val_104&amp;amp;apos; AND b.id = &amp;amp;apos;Id_2&amp;amp;apos; AND a.val &amp;lt;&amp;gt; b.val;


The (wrong) plan is the following:

STAGE DEPENDENCIES:
  Stage-2 is a root stage
  Stage-3 depends on stages: Stage-2
  Stage-0 depends on stages: Stage-3
  Stage-4 depends on stages: Stage-0
  Stage-1 depends on stages: Stage-3
  Stage-5 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-2
    Tez
      DagId: haha_20160504140944_174465c9-5d1a-42f9-9665-fae02eeb2767:2
      Edges:
        Reducer 2 &amp;lt;- Map 1 (SIMPLE_EDGE), Map 3 (SIMPLE_EDGE)
      DagName: 
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: a
                  Statistics: Num rows: 3 Data size: 36 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: id (type: string)
                    sort order: +
                    Map-reduce partition columns: id (type: string)
                    Statistics: Num rows: 3 Data size: 36 Basic stats: COMPLETE Column stats: NONE
                    value expressions: val (type: string)
        Map 3 
            Map Operator Tree:
                TableScan
                  alias: b
                  Statistics: Num rows: 2 Data size: 24 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: id (type: string)
                    sort order: +
                    Map-reduce partition columns: id (type: string)
                    Statistics: Num rows: 2 Data size: 24 Basic stats: COMPLETE Column stats: NONE
                    value expressions: val (type: string)
        Reducer 2 
            Reduce Operator Tree:
              Merge Join Operator
                condition map:
                     Left Outer Join0 to 1
                keys:
                  0 id (type: string)
                  1 id (type: string)
                outputColumnNames: _col0, _col1, _col6
                Statistics: Num rows: 3 Data size: 39 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: _col0 (type: string), _col1 (type: string), &amp;amp;apos;Id_1&amp;amp;apos; (type: string), &amp;amp;apos;val_103&amp;amp;apos; (type: string)
                  outputColumnNames: _col0, _col1, _col2, _col3
                  Statistics: Num rows: 3 Data size: 39 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 3 Data size: 39 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.TextInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                        name: bugtest2.join_result_1
                Filter Operator
                  predicate: (_col1 &amp;lt;&amp;gt; _col6) (type: boolean)
                  Statistics: Num rows: 3 Data size: 39 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: _col0 (type: string), _col1 (type: string), &amp;amp;apos;Id_2&amp;amp;apos; (type: string), &amp;amp;apos;val_104&amp;amp;apos; (type: string)
                    outputColumnNames: _col0, _col1, _col2, _col3
                    Statistics: Num rows: 3 Data size: 39 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      Statistics: Num rows: 3 Data size: 39 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.TextInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                          name: bugtest2.join_result_3

  Stage: Stage-3
    Dependency Collection

  Stage: Stage-0
    Move Operator
      tables:
          replace: true
          table:
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: bugtest2.join_result_1

  Stage: Stage-4
    Stats-Aggr Operator

  Stage: Stage-1
    Move Operator
      tables:
          replace: true
          table:
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: bugtest2.join_result_3

  Stage: Stage-5
    Stats-Aggr Operator

</description>
			<version>1.3.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="is blocked by">13831</link>
			<link type="Regression" description="is broken by">9695</link>
		</links>
	</bug>
	<bug id="13844" opendate="2016-05-25 09:58:36" fixdate="2016-05-31 15:36:37" resolution="Fixed">
		<buginformation>
			<summary>Invalid index handler in org.apache.hadoop.hive.ql.index.HiveIndex class</summary>
			<description>Class org.apache.hadoop.hive.ql.index.HiveIndex has invalid handler name &amp;amp;apos;org.apache.hadoop.hive.ql.AggregateIndexHandler&amp;amp;apos;. The actual FQ class name is &amp;amp;apos;org.apache.hadoop.hive.ql.index.AggregateIndexHandler&amp;amp;apos;


  public static enum IndexType {
    AGGREGATE_TABLE("aggregate", "org.apache.hadoop.hive.ql.AggregateIndexHandler"),
    COMPACT_SUMMARY_TABLE("compact", "org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler"),
    BITMAP_TABLE("bitmap","org.apache.hadoop.hive.ql.index.bitmap.BitmapIndexHandler");

    private IndexType(String indexType, String className) {
      indexTypeName = indexType;
      this.handlerClsName = className;
    }

    private final String indexTypeName;
    private final String handlerClsName;

    public String getName() {
      return indexTypeName;
    }

    public String getHandlerClsName() {
      return handlerClsName;
    }
  }
  


Because all of the above statement like &amp;amp;apos;SHOW INDEXES ON MY_TABLE&amp;amp;apos; doesn&amp;amp;apos;t work in case of configured &amp;amp;apos;org.apache.hadoop.hive.ql.index.AggregateIndexHandler&amp;amp;apos; as index handler. In hive server log is observed java.lang.NullPointerException.</description>
			<version>2.0.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.index.HiveIndex.java</file>
		</fixedFiles>
	</bug>
	<bug id="13859" opendate="2016-05-26 05:04:52" fixdate="2016-05-31 18:42:36" resolution="Fixed">
		<buginformation>
			<summary>mask() UDF not retaining day and month field values</summary>
			<description>For date type parameters, mask() UDF replaces year/month/day field values with the values given in arguments to the UDF. Argument value -1 is treated as special, to specify that mask() should retain the value in the parameter. This allows to selectively mask only year/month/day fields.
Specifying "-1" does not retain the values for day/month fields; however the year value is retained, as shown below.


0: jdbc:hive2://localhost:10000&amp;gt; select id, join_date from employee where id &amp;lt; 4;
+-----+-------------+--+
| id  |  join_date  |
+-----+-------------+--+
| 1   | 2012-01-01  |
| 2   | 2014-02-01  |
| 3   | 2013-03-01  |
+-----+-------------+--+
3 rows selected (0.435 seconds)
0: jdbc:hive2://localhost:10000&amp;gt; select id, mask(join_date, -1, -1, -1, -1,-1, -1,-1,-1) join_date from employee where id &amp;lt; 4;
+-----+-------------+--+
| id  |  join_date  |
+-----+-------------+--+
| 1   | 2012-01-01  |
| 2   | 2014-01-01  |
| 3   | 2013-01-01  |
+-----+-------------+--+
3 rows selected (0.344 seconds)

</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFMask.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">13568</link>
		</links>
	</bug>
	<bug id="13719" opendate="2016-05-09 17:28:52" fixdate="2016-05-31 18:46:34" resolution="Fixed">
		<buginformation>
			<summary>TestConverters fails on master</summary>
			<description>Can be reproduced locally also.</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.llap.tez.TestConverters.java</file>
		</fixedFiles>
	</bug>
	<bug id="13840" opendate="2016-05-25 01:23:53" fixdate="2016-05-31 18:50:58" resolution="Fixed">
		<buginformation>
			<summary>Orc split generation is reading file footers twice</summary>
			<description>Recent refactorings to move orc out introduced a regression in split generation. This leads to reading the orc file footers twice during split generation.</description>
			<version>2.1.0</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
			<file type="M">org.apache.orc.impl.ReaderImpl.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
		</fixedFiles>
	</bug>
	<bug id="13751" opendate="2016-05-12 19:35:30" fixdate="2016-05-31 20:56:04" resolution="Fixed">
		<buginformation>
			<summary>LlapOutputFormatService should have a configurable send buffer size</summary>
			<description>Netty channel buffer size is hard-coded 128KB now. It should be made configurable.</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.llap.LlapOutputFormatService.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
	</bug>
	<bug id="13867" opendate="2016-05-26 22:22:14" fixdate="2016-06-01 02:45:05" resolution="Fixed">
		<buginformation>
			<summary>restore HiveAuthorizer interface changes</summary>
			<description>TLDR: Some of the changes to hive authorizer interface made as part of HIVE-13360 are inappropriate and need to be restored.
Regarding the move of ip address from the query context object (HiveAuthzContext) to HiveAuthenticationProvider. That isn&amp;amp;apos;t the right place for it.
In HS2 HTTP mode, when proxies and knox servers are between end user and HS2 , every request for single session does not have to come via a single IP address.
Current assumption in hive code base is that the IP address is valid for the entire session. This might not hold true for ever.
A limitation in HS2 that it holds state for the session would currently force the user configure proxies and knox to remember which next Host it was using, because they need to have state to remember the HS2 instance to be used! But that is a limitation that ideally goes away some day, and when that happens, HiveAuthzContext would be the right place for keeping the IP address!</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.jdbc.authorization.TestHS2AuthzContext.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerShowFilters.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.HiveV1Authorizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizationValidator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.DummyAuthenticator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidatorForTest.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.HiveAuthenticationProvider.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizerImpl.java</file>
			<file type="D">org.apache.hadoop.hive.ql.security.authorization.plugin.QueryContext.java</file>
			<file type="M">org.apache.hive.service.cli.operation.MetadataOperation.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.HadoopDefaultAuthenticator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hive.jdbc.authorization.TestJdbcMetadataApiAuth.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.processors.CommandUtil.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.TableMask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.InjectableDummyAuthenticator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.DummyHiveAuthorizationValidator.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">13360</link>
		</links>
	</bug>
	<bug id="13876" opendate="2016-05-27 09:28:57" fixdate="2016-06-01 10:46:03" resolution="Fixed">
		<buginformation>
			<summary>Vectorization: Port HIVE-11544 to LazySimpleDeserializeRead</summary>
			<description>High CPU usage due to exception handling code.


 TezTaskRunner [RUNNABLE] [DAEMON]
java.lang.Throwable.fillInStackTrace(int) Throwable.java (native)
java.lang.Throwable.fillInStackTrace() Throwable.java:783
java.lang.Throwable.&amp;lt;init&amp;gt;(String) Throwable.java:265
java.lang.Exception.&amp;lt;init&amp;gt;(String) Exception.java:66
java.lang.RuntimeException.&amp;lt;init&amp;gt;(String) RuntimeException.java:62
java.lang.IllegalArgumentException.&amp;lt;init&amp;gt;(String) IllegalArgumentException.java:52
java.lang.NumberFormatException.&amp;lt;init&amp;gt;(String) NumberFormatException.java:55
sun.misc.FloatingDecimal.readJavaFormatString(String) FloatingDecimal.java:1842
sun.misc.FloatingDecimal.parseFloat(String) FloatingDecimal.java:122
java.lang.Float.parseFloat(String) Float.java:451
org.apache.hadoop.hive.serde2.lazy.fast.LazySimpleDeserializeRead.readCheckNull() LazySimpleDeserializeRead.java:309
org.apache.hadoop.hive.ql.exec.vector.VectorDeserializeRow.deserializeRowColumn(VectorizedRowBatch, int, int) VectorDeserializeRow.java:346
org.apache.hadoop.hive.ql.exec.vector.VectorDeserializeRow.deserialize(VectorizedRowBatch, int) VectorDeserializeRow.java:659
org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(Writable) VectorMapOperator.java:814
org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(Object) MapRecordSource.java:86
org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord() MapRecordSource.java:70
org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run() MapRecordProcessor.java:361
org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(Map, Map) TezProcessor.java:172
org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(Map, Map) TezProcessor.java:160
org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run() LogicalIOProcessorRuntimeTask.java:370
org.apache.tez.runtime.task.TaskRunner2Callable$1.run() TaskRunner2Callable.java:73
org.apache.tez.runtime.task.TaskRunner2Callable$1.run() TaskRunner2Callable.java:61
java.security.AccessController.doPrivileged(PrivilegedExceptionAction, AccessControlContext) AccessController.java (native)
javax.security.auth.Subject.doAs(Subject, PrivilegedExceptionAction) Subject.java:422
org.apache.hadoop.security.UserGroupInformation.doAs(PrivilegedExceptionAction) UserGroupInformation.java:1657
org.apache.tez.runtime.task.TaskRunner2Callable.callInternal() TaskRunner2Callable.java:61
org.apache.tez.runtime.task.TaskRunner2Callable.callInternal() TaskRunner2Callable.java:37
org.apache.tez.common.CallableWithNdc.call() CallableWithNdc.java:36
java.util.concurrent.FutureTask.run() FutureTask.java:266
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor$Worker) ThreadPoolExecutor.java:1142
java.util.concurrent.ThreadPoolExecutor$Worker.run() ThreadPoolExecutor.java:617
java.lang.Thread.run() Thread.java:745


</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.fast.LazySimpleDeserializeRead.java</file>
		</fixedFiles>
		<links>
			<link type="Incorporates" description="is part of">13878</link>
			<link type="Required" description="is required by">13887</link>
		</links>
	</bug>
	<bug id="13856" opendate="2016-05-25 21:50:49" fixdate="2016-06-01 16:32:35" resolution="Fixed">
		<buginformation>
			<summary>Fetching transaction batches during ACID streaming against Hive Metastore using Oracle DB fails</summary>
			<description>
2016-05-25 00:43:49,682 INFO  [pool-4-thread-5]: txn.TxnHandler (TxnHandler.java:checkRetryable(1585)) - Non-retryable error: ORA-00933: SQL command not properly ended
 (SQLState=42000, ErrorCode=933)
2016-05-25 00:43:49,685 ERROR [pool-4-thread-5]: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(159)) - MetaException(message:Unable to select from transaction database java.sql.SQLSyntaxErrorException: ORA-00933: SQL command not properly ended

	at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:440)
	at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:396)
	at oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:837)
	at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:445)
	at oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:191)
	at oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:523)
	at oracle.jdbc.driver.T4CStatement.doOall8(T4CStatement.java:193)
	at oracle.jdbc.driver.T4CStatement.executeForRows(T4CStatement.java:999)
	at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1315)
	at oracle.jdbc.driver.OracleStatement.executeInternal(OracleStatement.java:1890)
	at oracle.jdbc.driver.OracleStatement.execute(OracleStatement.java:1855)
	at oracle.jdbc.driver.OracleStatementWrapper.execute(OracleStatementWrapper.java:304)
	at com.jolbox.bonecp.StatementHandle.execute(StatementHandle.java:254)
	at org.apache.hadoop.hive.metastore.txn.TxnHandler.openTxns(TxnHandler.java:429)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.open_txns(HiveMetaStore.java:5647)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at com.sun.proxy.$Proxy15.open_txns(Unknown Source)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$open_txns.getResult(ThriftHiveMetastore.java:11604)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$open_txns.getResult(ThriftHiveMetastore.java:11589)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
)
	at org.apache.hadoop.hive.metastore.txn.TxnHandler.openTxns(TxnHandler.java:438)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.open_txns(HiveMetaStore.java:5647)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at com.sun.proxy.$Proxy15.open_txns(Unknown Source)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$open_txns.getResult(ThriftHiveMetastore.java:11604)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$open_txns.getResult(ThriftHiveMetastore.java:11589)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

2016-05-25 00:43:49,685 ERROR [pool-4-thread-5]: thrift.ProcessFunction (ProcessFunction.java:process(41)) - Internal error processing open_txns
MetaException(message:Unable to select from transaction database java.sql.SQLSyntaxErrorException: ORA-00933: SQL command not properly ended

	at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:440)
	at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:396)
	at oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:837)
	at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:445)
	at oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:191)
	at oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:523)
	at oracle.jdbc.driver.T4CStatement.doOall8(T4CStatement.java:193)
	at oracle.jdbc.driver.T4CStatement.executeForRows(T4CStatement.java:999)
	at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1315)
	at oracle.jdbc.driver.OracleStatement.executeInternal(OracleStatement.java:1890)
	at oracle.jdbc.driver.OracleStatement.execute(OracleStatement.java:1855)
	at oracle.jdbc.driver.OracleStatementWrapper.execute(OracleStatementWrapper.java:304)
	at com.jolbox.bonecp.StatementHandle.execute(StatementHandle.java:254)
	at org.apache.hadoop.hive.metastore.txn.TxnHandler.openTxns(TxnHandler.java:429)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.open_txns(HiveMetaStore.java:5647)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at com.sun.proxy.$Proxy15.open_txns(Unknown Source)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$open_txns.getResult(ThriftHiveMetastore.java:11604)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$open_txns.getResult(ThriftHiveMetastore.java:11589)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
)
	at org.apache.hadoop.hive.metastore.txn.TxnHandler.openTxns(TxnHandler.java:438)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.open_txns(HiveMetaStore.java:5647)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at com.sun.proxy.$Proxy15.open_txns(Unknown Source)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$open_txns.getResult(ThriftHiveMetastore.java:11604)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$open_txns.getResult(ThriftHiveMetastore.java:11589)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)


I think the reason here is that
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java

  public OpenTxnsResponse openTxns(OpenTxnRequest rqst) throws MetaException {
...
        String query;
        String insertClause = "insert into TXNS (txn_id, txn_state, txn_started, txn_last_heartbeat, txn_user, txn_host) values ";
        StringBuilder valuesClause = new StringBuilder();

        for (long i = first; i &amp;lt; first + numTxns; i++) {
          txnIds.add(i);

          if (i &amp;gt; first &amp;amp;&amp;amp;
              (i - first) % conf.getIntVar(HiveConf.ConfVars.METASTORE_DIRECT_SQL_MAX_ELEMENTS_VALUES_CLAUSE) == 0) {
            // wrap up the current query, and start a new one
            query = insertClause + valuesClause.toString();
            queries.add(query);

            valuesClause.setLength(0);
            valuesClause.append("(").append(i).append(", &amp;amp;apos;o&amp;amp;apos;, ").append(now).append(", ").append(now)
                .append(", &amp;amp;apos;").append(rqst.getUser()).append("&amp;amp;apos;, &amp;amp;apos;").append(rqst.getHostname())
                .append("&amp;amp;apos;)");

            continue;
          }

          if (i &amp;gt; first) {
            valuesClause.append(", ");
          }

          valuesClause.append("(").append(i).append(", &amp;amp;apos;o&amp;amp;apos;, ").append(now).append(", ").append(now)
              .append(", &amp;amp;apos;").append(rqst.getUser()).append("&amp;amp;apos;, &amp;amp;apos;").append(rqst.getHostname())
              .append("&amp;amp;apos;)");
        }

        query = insertClause + valuesClause.toString();
...
}


ends up building a query of the form


INSERT INTO TXNS (...) VALUES (...), (...)


Oracle doesn&amp;amp;apos;t like this way of inserting multiple rows of data. Couple of ways the following post describe is either inserting each row individually or use the INSERT ALL semantics.</description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TestTxnUtils.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="is blocked by">11956</link>
		</links>
	</bug>
	<bug id="13834" opendate="2016-05-24 18:09:17" fixdate="2016-06-01 17:37:29" resolution="Fixed">
		<buginformation>
			<summary>Use LinkedHashMap instead of HashMap for LockRequestBuilder to maintain predictable iteration order</summary>
			<description>In Java 7 it is assumed the iteration order is always the same as the insert order, but that&amp;amp;apos;s not guaranteed. In Java 8 some unit test breaks because of this ordering change. Solution is to use LinkedHashMap.</description>
			<version>1.3.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.LockRequestBuilder.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">13547</link>
		</links>
	</bug>
	<bug id="13858" opendate="2016-05-26 01:41:35" fixdate="2016-06-01 18:09:43" resolution="Fixed">
		<buginformation>
			<summary>LLAP: A preempted task can end up waiting on completeInitialization if some part of the executing code suppressed the interrupt</summary>
			<description>An interrupt along with a HiveProcessor.abort call is made when attempting to preempt a task.
In this specific case, the task was in the middle of HDFS IO - which &amp;amp;apos;handled&amp;amp;apos; the interrupt by retrying. As a result the interrupt status on the thread was reset - so instead of skipping the future.get in completeInitialization - the task ended up blocking there.
End result - a single executor slot permanently blocked in LLAP. Depending on what else is running - this can cause a cluster level deadlock.</description>
			<version>2.0.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Operator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.RecordProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.MergeFileRecordProcessor.java</file>
		</fixedFiles>
	</bug>
	<bug id="13412" opendate="2016-04-02 07:51:12" fixdate="2016-06-01 20:43:17" resolution="Duplicate">
		<buginformation>
			<summary>External table -  fields terminated by &amp;apos;\u0044&amp;apos; - 0044 is being interpreted as decimal and not hex</summary>
			<description>, (comma) as the decimal value of &amp;amp;apos;44&amp;amp;apos; and hex value of &amp;amp;apos;2c&amp;amp;apos;
In the following example I&amp;amp;apos;m using  &amp;amp;apos;\u0044&amp;amp;apos; as delimiter which is being interpreted as comma.
hive&amp;gt; create external table test_delimiter_dec_unicode (c1 int,c2 int,c3 int) row format delimited fields terminated by &amp;amp;apos;\u0044&amp;amp;apos;;
OK
Time taken: 0.035 seconds
hive&amp;gt; show create table test_delimiter_dec_unicode;
OK
CREATE EXTERNAL TABLE `test_delimiter_dec_unicode`(
  `c1` int,
  `c2` int,
  `c3` int)
ROW FORMAT DELIMITED
  FIELDS TERMINATED BY &amp;amp;apos;,&amp;amp;apos;
...</description>
			<version>0.14.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.TestSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">13434</link>
		</links>
	</bug>
	<bug id="4046" opendate="2013-02-20 22:34:02" fixdate="2016-06-01 22:00:56" resolution="Duplicate">
		<buginformation>
			<summary>Column masking</summary>
			<description>Sometimes data in a table needs to be kept around but made inaccessible. Right now it is possible to offline a table or a partition, but not a specific column of a partition. Also, accessing an offlined table results in an error. With this change, it will be possible to mask a column at the partition level, causing all further queries to that column to return null.</description>
			<version>0.11.0</version>
			<fixedVersion></fixedVersion>
			<type>New Feature</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.HiveV1Authorizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizationValidator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidatorForTest.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizerImpl.java</file>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.DummyHiveAuthorizationValidator.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">13125</link>
		</links>
	</bug>
	<bug id="13927" opendate="2016-06-02 17:12:38" fixdate="2016-06-02 17:24:57" resolution="Fixed">
		<buginformation>
			<summary>Adding missing header to Java files</summary>
			<description></description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.jdbc.miniHS2.StartMiniHS2Cluster.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.ThriftCliServiceMessageSizeTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="13924" opendate="2016-06-02 06:53:47" fixdate="2016-06-03 10:16:45" resolution="Fixed">
		<buginformation>
			<summary>(Vectorization) Error evaluating ((bool0 and (not bool1)) or (bool1 and (not bool0)))</summary>
			<description>Scratch column(s) in child expressions shouldn&amp;amp;apos;t be returned to the pool for PROJECTION.
Problem introduced with HIVE-13084.
Symptom:


Caused by: java.lang.IllegalStateException
at com.google.common.base.Preconditions.checkState(Preconditions.java:133)
        at org.apache.hadoop.hive.ql.exec.vector.expressions.ColOrCol.evaluate(ColOrCol.java:544)

</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0, 2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.ColAndCol.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.ColOrCol.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">14182</link>
		</links>
	</bug>
	<bug id="13929" opendate="2016-06-02 18:23:49" fixdate="2016-06-03 17:23:31" resolution="Fixed">
		<buginformation>
			<summary>org.apache.hadoop.hive.metastore.api.DataOperationType class not found error when a job is submitted by hive</summary>
			<description>
0: jdbc:hive2://os-r6-atlas-ha-re-re-2.openst&amp;gt; create table source1 (abc String);
No rows affected (0.268 seconds)
0: jdbc:hive2://os-r6-atlas-ha-re-re-2.openst&amp;gt; create table ctas_src as select * from source1;
INFO  : Tez session hasn&amp;amp;apos;t been created yet. Opening session
INFO  : Dag name: create table ctas_src as select * ...source1(Stage-1)
INFO  :

INFO  : Status: Running (Executing on YARN cluster with App id application_1464692782033_0005)

INFO  : Map 1: -/-
ERROR : Status: Failed
ERROR : Vertex failed, vertexName=Map 1, vertexId=vertex_1464692782033_0005_1_00, diagnostics=[Vertex vertex_1464692782033_0005_1_00 [Map 1] killed/failed due to:INIT_FAILURE, Fail to create InputInitializerManager, org.apache.tez.dag.api.TezReflectionException: Unable to instantiate class with 1 arguments: org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator
at org.apache.tez.common.ReflectionUtils.getNewInstance(ReflectionUtils.java:70)
at org.apache.tez.common.ReflectionUtils.createClazzInstance(ReflectionUtils.java:89)
at org.apache.tez.dag.app.dag.RootInputInitializerManager$1.run(RootInputInitializerManager.java:151)
at org.apache.tez.dag.app.dag.RootInputInitializerManager$1.run(RootInputInitializerManager.java:148)
at java..AccessController.doPrivileged(Native Method)
at javax..auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop..UserGroupInformation.doAs(UserGroupInformation.java:1724)
at org.apache.tez.dag.app.dag.RootInputInitializerManager.createInitializer(RootInputInitializerManager.java:148)
at org.apache.tez.dag.app.dag.RootInputInitializerManager.runInputInitializers(RootInputInitializerManager.java:121)
at org.apache.tez.dag.app.dag.impl.VertexImpl.setupInputInitializerManager(VertexImpl.java:4607)
at org.apache.tez.dag.app.dag.impl.VertexImpl.access$4400(VertexImpl.java:202)
at org.apache.tez.dag.app.dag.impl.VertexImpl$InitTransition.handleInitEvent(VertexImpl.java:3423)
at org.apache.tez.dag.app.dag.impl.VertexImpl$InitTransition.transition(VertexImpl.java:3372)
at org.apache.tez.dag.app.dag.impl.VertexImpl$InitTransition.transition(VertexImpl.java:3353)
at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)
at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)
at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)
at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
at org.apache.tez.state.StateMachineTez.doTransition(StateMachineTez.java:57)
at org.apache.tez.dag.app.dag.impl.VertexImpl.handle(VertexImpl.java:1925)
at org.apache.tez.dag.app.dag.impl.VertexImpl.handle(VertexImpl.java:201)
at org.apache.tez.dag.app.DAGAppMaster$VertexEventDispatcher.handle(DAGAppMaster.java:2053)
at org.apache.tez.dag.app.DAGAppMaster$VertexEventDispatcher.handle(DAGAppMaster.java:2039)
at org.apache.tez.common.AsyncDispatcher.dispatch(AsyncDispatcher.java:183)
at org.apache.tez.common.AsyncDispatcher$1.run(AsyncDispatcher.java:114)
at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.reflect.InvocationTargetException
at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
at org.apache.tez.common.ReflectionUtils.getNewInstance(ReflectionUtils.java:68)
... 25 more
Caused by: java.lang.NoClassDefFoundError: org/apache/hadoop/hive/metastore/api/DataOperationType
at org.apache.hadoop.hive.ql.io.AcidUtils$Operation.&amp;lt;clinit&amp;gt;(AcidUtils.java:196)
at org.apache.hadoop.hive.ql.plan.FileSinkDesc.&amp;lt;init&amp;gt;(FileSinkDesc.java:93)
at org.apache.hadoop.hive.ql.plan.FileSinkDescConstructorAccess.newInstance(Unknown Source)
at org.apache.hive.com.esotericsoftware.kryo.Kryo$1.newInstance(Kryo.java:1062)
at org.apache.hive.com.esotericsoftware.kryo.Kryo.newInstance(Kryo.java:1112)
at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.create(FieldSerializer.java:526)
at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:502)
at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:694)
at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106)
at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507)
at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:776)
at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:112)
at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:18)
at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:694)
at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106)
at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507)
at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:776)
at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:112)
at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:18)
at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:694)
at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106)
at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507)
at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:776)
at org.apache.hive.com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:139)
at org.apache.hive.com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:17)
at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:694)
at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106)
at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507)
at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:672)
at org.apache.hadoop.hive.ql.exec.Utilities.deserializeObjectByKryo(Utilities.java:1173)
at org.apache.hadoop.hive.ql.exec.Utilities.deserializePlan(Utilities.java:1062)
at org.apache.hadoop.hive.ql.exec.Utilities.deserializePlan(Utilities.java:1076)
at org.apache.hadoop.hive.ql.exec.Utilities.getBaseWork(Utilities.java:432)
at org.apache.hadoop.hive.ql.exec.Utilities.getMapWork(Utilities.java:311)
at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.&amp;lt;init&amp;gt;(HiveSplitGenerator.java:101)
... 30 more
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hive.metastore.api.DataOperationType
at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
... 65 more
]
ERROR : DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0
Error: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1464692782033_0005_1_00, diagnostics=[Vertex vertex_1464692782033_0005_1_00 [Map 1] killed/failed due to:INIT_FAILURE, Fail to create InputInitializerManager, org.apache.tez.dag.api.TezReflectionException: Unable to instantiate class with 1 arguments: org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator
at org.apache.tez.common.ReflectionUtils.getNewInstance(ReflectionUtils.java:70)
at org.apache.tez.common.ReflectionUtils.createClazzInstance(ReflectionUtils.java:89)
at org.apache.tez.dag.app.dag.RootInputInitializerManager$1.run(RootInputInitializerManager.java:151)
at org.apache.tez.dag.app.dag.RootInputInitializerManager$1.run(RootInputInitializerManager.java:148)
at java..AccessController.doPrivileged(Native Method)
at javax..auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop..UserGroupInformation.doAs(UserGroupInformation.java:1724)
at org.apache.tez.dag.app.dag.RootInputInitializerManager.createInitializer(RootInputInitializerManager.java:148)
at org.apache.tez.dag.app.dag.RootInputInitializerManager.runInputInitializers(RootInputInitializerManager.java:121)
at org.apache.tez.dag.app.dag.impl.VertexImpl.setupInputInitializerManager(VertexImpl.java:4607)
at org.apache.tez.dag.app.dag.impl.VertexImpl.access$4400(VertexImpl.java:202)
at org.apache.tez.dag.app.dag.impl.VertexImpl$InitTransition.handleInitEvent(VertexImpl.java:3423)
at org.apache.tez.dag.app.dag.impl.VertexImpl$InitTransition.transition(VertexImpl.java:3372)
at org.apache.tez.dag.app.dag.impl.VertexImpl$InitTransition.transition(VertexImpl.java:3353)
at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)
at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)
at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)
at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
at org.apache.tez.state.StateMachineTez.doTransition(StateMachineTez.java:57)
at org.apache.tez.dag.app.dag.impl.VertexImpl.handle(VertexImpl.java:1925)
at org.apache.tez.dag.app.dag.impl.VertexImpl.handle(VertexImpl.java:201)
at org.apache.tez.dag.app.DAGAppMaster$VertexEventDispatcher.handle(DAGAppMaster.java:2053)
at org.apache.tez.dag.app.DAGAppMaster$VertexEventDispatcher.handle(DAGAppMaster.java:2039)
at org.apache.tez.common.AsyncDispatcher.dispatch(AsyncDispatcher.java:183)
at org.apache.tez.common.AsyncDispatcher$1.run(AsyncDispatcher.java:114)
at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.reflect.InvocationTargetException
at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
at org.apache.tez.common.ReflectionUtils.getNewInstance(ReflectionUtils.java:68)
... 25 more
Caused by: java.lang.NoClassDefFoundError: org/apache/hadoop/hive/metastore/api/DataOperationType
at org.apache.hadoop.hive.ql.io.AcidUtils$Operation.&amp;lt;clinit&amp;gt;(AcidUtils.java:196)
at org.apache.hadoop.hive.ql.plan.FileSinkDesc.&amp;lt;init&amp;gt;(FileSinkDesc.java:93)
at org.apache.hadoop.hive.ql.plan.FileSinkDescConstructorAccess.newInstance(Unknown Source)
at org.apache.hive.com.esotericsoftware.kryo.Kryo$1.newInstance(Kryo.java:1062)
at org.apache.hive.com.esotericsoftware.kryo.Kryo.newInstance(Kryo.java:1112)
at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.create(FieldSerializer.java:526)
at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:502)
at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:694)
at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106)
at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507)
at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:776)
at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:112)
at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:18)
at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:694)
at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106)
at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507)
at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:776)
at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:112)
at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:18)
at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:694)
at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106)
at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507)
at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:776)
at org.apache.hive.com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:139)
at org.apache.hive.com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:17)
at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:694)
at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106)
at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507)
at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:672)
at org.apache.hadoop.hive.ql.exec.Utilities.deserializeObjectByKryo(Utilities.java:1173)
at org.apache.hadoop.hive.ql.exec.Utilities.deserializePlan(Utilities.java:1062)
at org.apache.hadoop.hive.ql.exec.Utilities.deserializePlan(Utilities.java:1076)
at org.apache.hadoop.hive.ql.exec.Utilities.getBaseWork(Utilities.java:432)
at org.apache.hadoop.hive.ql.exec.Utilities.getMapWork(Utilities.java:311)
at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.&amp;lt;init&amp;gt;(HiveSplitGenerator.java:101)
... 30 more
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hive.metastore.api.DataOperationType
at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
... 65 more
]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0 (state=08S01,code=2)
0: jdbc:hive2://os-r6-atlas-ha-re-re-2.openst&amp;gt;


This is the key

Caused by: java.lang.NoClassDefFoundError: org/apache/hadoop/hive/metastore/api/DataOperationType
at org.apache.hadoop.hive.ql.io.AcidUtils$Operation.&amp;lt;clinit&amp;gt;(AcidUtils.java:196)
at org.apache.hadoop.hive.ql.plan.FileSinkDesc.&amp;lt;init&amp;gt;(FileSinkDesc.java:93)
at org.apache.hadoop.hive.ql.plan.FileSinkDescConstructorAccess.newInstance(Unknown Source)
at org.apache.hive.com.esotericsoftware.kryo.Kryo$1.newInstance(Kryo.java:1062)
at org.apache.hive.com.esotericsoftware.kryo.Kryo.newInstance(Kryo.java:1112)

</description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
	</bug>
	<bug id="13882" opendate="2016-05-27 20:38:55" fixdate="2016-06-03 17:41:16" resolution="Fixed">
		<buginformation>
			<summary>When hive.server2.async.exec.async.compile is turned on, from JDBC we will get "The query did not generate a result set" </summary>
			<description> The following would fail with  "The query did not generate a result set"
    stmt.execute("SET hive.driver.parallel.compilation=true");
    stmt.execute("SET hive.server2.async.exec.async.compile=true");
    ResultSet res =  stmt.executeQuery("SELECT * FROM " + tableName);
    res.next();</description>
			<version>2.1.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.service.cli.OperationStatus.java</file>
			<file type="M">org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.ThriftCLIServiceClient.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
			<file type="M">org.apache.hive.jdbc.HiveStatement.java</file>
			<file type="M">org.apache.hive.service.cli.operation.Operation.java</file>
			<file type="M">org.apache.hive.service.rpc.thrift.TGetOperationStatusResp.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">4239</link>
		</links>
	</bug>
	<bug id="13912" opendate="2016-06-02 00:14:46" fixdate="2016-06-06 21:14:24" resolution="Fixed">
		<buginformation>
			<summary>DbTxnManager.commitTxn(): ORA-00918: column ambiguously defined</summary>
			<description>
Caused by: MetaException(message:Unable to update transaction database java.sql.SQLSyntaxErrorException: ORA-00918: column ambiguously defined

	at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:440)
	at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:396)
	at oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:837)
	at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:445)
	at oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:191)
	at oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:523)
	at oracle.jdbc.driver.T4CStatement.doOall8(T4CStatement.java:193)
	at oracle.jdbc.driver.T4CStatement.executeForDescribe(T4CStatement.java:852)
	at oracle.jdbc.driver.OracleStatement.executeMaybeDescribe(OracleStatement.java:1153)
	at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1275)
	at oracle.jdbc.driver.OracleStatement.executeQuery(OracleStatement.java:1477)
	at oracle.jdbc.driver.OracleStatementWrapper.executeQuery(OracleStatementWrapper.java:392)
	at com.jolbox.bonecp.StatementHandle.executeQuery(StatementHandle.java:464)
	at org.apache.hadoop.hive.metastore.txn.TxnHandler.commitTxn(TxnHandler.java:662)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.commit_txn(HiveMetaStore.java:5864)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:140)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:99)
	at com.sun.proxy.$Proxy49.commit_txn(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.commitTxn(HiveMetaStoreClient.java:2090)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:154)
	at com.sun.proxy.$Proxy50.commitTxn(Unknown Source)
	at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager$SynchronizedMetaStoreClient.commitTxn(DbTxnManager.java:655)
	at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.commitTxn(DbTxnManager.java:356)
	at org.apache.hadoop.hive.ql.Driver.releaseLocksAndCommitOrRollback(Driver.java:1024)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1321)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1083)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1071)


caused by 

            (sqlGenerator.addLimitClause(1, "committed.ws_txnid, committed.ws_commit_id, committed.ws_database," +
              "committed.ws_table, committed.ws_partition, cur.ws_commit_id " +
              "from WRITE_SET committed INNER JOIN WRITE_SET cur " +
              "ON committed.ws_database=cur.ws_database and committed.ws_table=cur.ws_table " +
              //For partitioned table we always track writes at partition level (never at table)
              //and for non partitioned - always at table level, thus the same table should never
              //have entries with partition key and w/o
              "and (committed.ws_partition=cur.ws_partition or (committed.ws_partition is null and cur.ws_partition is null)) " +
              "where cur.ws_txnid &amp;lt;= committed.ws_commit_id" + //txns overlap; could replace ws_txnid
              // with txnid, though any decent DB should infer this
              " and cur.ws_txnid=" + txnid + //make sure RHS of join only has rows we just inserted as
              // part of this commitTxn() op
              " and committed.ws_txnid &amp;lt;&amp;gt; " + txnid + //and LHS only has committed txns
              //U+U and U+D is a conflict but D+D is not and we don&amp;amp;apos;t currently track I in WRITE_SET at all
              " and (committed.ws_operation_type=" + quoteChar(OpertaionType.UPDATE.sqlConst) +
              " OR cur.ws_operation_type=" + quoteChar(OpertaionType.UPDATE.sqlConst) + ")"));



because addLimitClause on Oracle does
select * from (select  .....) where rownum &amp;lt;= N
so if original query has a join...</description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
		</fixedFiles>
	</bug>
	<bug id="13599" opendate="2016-04-24 05:06:05" fixdate="2016-06-07 03:38:12" resolution="Fixed">
		<buginformation>
			<summary>LLAP: Incorrect handling of the preemption queue on finishable state updates</summary>
			<description>When running some tests with pre-emption enabled, got the following exception
Looks like a race condition when removing items from pre-emption queue.


16/04/23 23:32:00 [Wait-Queue-Scheduler-0[]] ERROR impl.TaskExecutorService : Wait queue scheduler worker exited with failure!
java.util.NoSuchElementException
        at java.util.AbstractQueue.remove(AbstractQueue.java:117) ~[?:1.7.0_55]
        at org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.removeAndGetFromPreemptionQueue(TaskExecutorService.java:568) ~[hive-llap-server-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.handleScheduleAttemptedRejection(TaskExecutorService.java:493) ~[hive-llap-server-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.access$1100(TaskExecutorService.java:81) ~[hive-llap-server-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$WaitQueueWorker.run(TaskExecutorService.java:285) ~[hive-llap-server-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[?:1.7.0_55]
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) [?:1.7.0_55]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_55]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_55]
        at java.lang.Thread.run(Thread.java:745) [?:1.7.0_55]
16/04/23 23:32:00 [Wait-Queue-Scheduler-0[]] INFO impl.LlapDaemon : UncaughtExceptionHandler invoked
16/04/23 23:32:00 [Wait-Queue-Scheduler-0[]] ERROR impl.LlapDaemon : Thread Thread[Wait-Queue-Scheduler-0,5,main] threw an Exception. Shutting down now...
java.util.NoSuchElementException
        at java.util.AbstractQueue.remove(AbstractQueue.java:117) ~[?:1.7.0_55]
        at org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.removeAndGetFromPreemptionQueue(TaskExecutorService.java:568) ~[hive-llap-server-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.handleScheduleAttemptedRejection(TaskExecutorService.java:493) ~[hive-llap-server-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.access$1100(TaskExecutorService.java:81) ~[hive-llap-server-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$WaitQueueWorker.run(TaskExecutorService.java:285) ~[hive-llap-server-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[?:1.7.0_55]
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) [?:1.7.0_55]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_55]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_55]
        at java.lang.Thread.run(Thread.java:745) [?:1.7.0_55]

</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
			<file type="M">org.apache.hadoop.hive.llap.daemon.impl.TestTaskExecutorService.java</file>
		</fixedFiles>
	</bug>
	<bug id="13904" opendate="2016-06-01 15:50:15" fixdate="2016-06-07 09:46:08" resolution="Fixed">
		<buginformation>
			<summary>Ignore case when retrieving ColumnInfo from RowResolver</summary>
			<description>To reproduce:

-- upper case in subq
explain
select * from src b
where exists
  (select a.key from src a
  where b.VALUE = a.VALUE
  );

</description>
			<version>2.0.1</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.QBSubQuery.java</file>
		</fixedFiles>
	</bug>
	<bug id="13911" opendate="2016-06-01 23:57:09" fixdate="2016-06-07 15:32:08" resolution="Fixed">
		<buginformation>
			<summary>load inpath fails throwing org.apache.hadoop.security.AccessControlException</summary>
			<description>Similar to HIVE-13857</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
	</bug>
	<bug id="13932" opendate="2016-06-02 21:45:17" fixdate="2016-06-07 16:14:41" resolution="Fixed">
		<buginformation>
			<summary>Hive SMB Map Join with small set of LIMIT failed with NPE</summary>
			<description>1) prepare sample data:
a=1
while [[ $a -lt 100 ]]; do echo $a ; let a=$a+1; done &amp;gt; data
2) prepare source hive table:
CREATE TABLE `s`(`c` string);
load data local inpath &amp;amp;apos;data&amp;amp;apos; into table s;
3) prepare the bucketed table:
set hive.enforce.bucketing=true;
set hive.enforce.sorting=true;
CREATE TABLE `t`(`c` string) CLUSTERED BY (c) SORTED BY (c) INTO 5 BUCKETS;
insert into t select * from s;
4) reproduce this issue:
SET hive.auto.convert.sortmerge.join = true;
SET hive.auto.convert.sortmerge.join.bigtable.selection.policy = org.apache.hadoop.hive.ql.optimizer.LeftmostBigTableSelectorForAutoSMJ;
SET hive.auto.convert.sortmerge.join.noconditionaltask = true;
SET hive.optimize.bucketmapjoin = true;
SET hive.optimize.bucketmapjoin.sortedmerge = true;
select * from t join t t1 on t.c=t1.c limit 1;</description>
			<version>1.0.0</version>
			<fixedVersion>1.3.0, 2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
		</fixedFiles>
	</bug>
	<bug id="13838" opendate="2016-05-24 23:43:50" fixdate="2016-06-08 05:33:01" resolution="Fixed">
		<buginformation>
			<summary>Set basic stats as inaccurate for all ACID tables</summary>
			<description></description>
			<version>2.0.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Sub-task</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.StatsTask.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">13971</link>
		</links>
	</bug>
	<bug id="13540" opendate="2016-04-18 21:58:02" fixdate="2016-06-08 13:55:47" resolution="Fixed">
		<buginformation>
			<summary>Casts to numeric types don&amp;apos;t seem to work in hplsql</summary>
			<description>Maybe I&amp;amp;apos;m doing this wrong? But it seems to be broken.
Casts to string types seem to work fine, but not numbers.
This code:


temp_int     = CAST(&amp;amp;apos;1&amp;amp;apos; AS int);
print temp_int
temp_float   = CAST(&amp;amp;apos;1.2&amp;amp;apos; AS float);
print temp_float
temp_double  = CAST(&amp;amp;apos;1.2&amp;amp;apos; AS double);
print temp_double
temp_decimal = CAST(&amp;amp;apos;1.2&amp;amp;apos; AS decimal(10, 4));
print temp_decimal
temp_string = CAST(&amp;amp;apos;1.2&amp;amp;apos; AS string);
print temp_string


Produces this output:


[vagrant@hdp250 hplsql]$ hplsql -f temp2.hplsql
which: no hbase in (/usr/lib64/qt-3.3/bin:/usr/lib/jvm/java/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/puppetlabs/bin:/usr/local/share/jmeter/bin:/home/vagrant/bin)
WARNING: Use "yarn jar" to launch YARN applications.
null
null
null
null
1.2


The software I&amp;amp;apos;m using is not anything released but is pretty close to the trunk, 2 weeks old at most.</description>
			<version>2.2.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.hplsql.Stmt.java</file>
			<file type="M">org.apache.hive.hplsql.Ftp.java</file>
			<file type="M">org.apache.hive.hplsql.TestHplsqlOffline.java</file>
			<file type="M">org.apache.hive.hplsql.TestHplsqlLocal.java</file>
			<file type="M">org.apache.hive.hplsql.Var.java</file>
			<file type="M">org.apache.hive.hplsql.Utils.java</file>
			<file type="M">org.apache.hive.hplsql.Copy.java</file>
			<file type="M">org.apache.hive.hplsql.Select.java</file>
			<file type="M">org.apache.hive.hplsql.Package.java</file>
			<file type="M">org.apache.hive.hplsql.Exec.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">14804</link>
		</links>
	</bug>
	<bug id="13972" opendate="2016-06-08 05:27:19" fixdate="2016-06-08 20:53:44" resolution="Fixed">
		<buginformation>
			<summary>Resolve class dependency issue introduced by HIVE-13354</summary>
			<description>HIVE-13354 moved a helper class StringableMap from ql/txn/compactor/CompactorMR.java to metastore/txn/TxnUtils.java
This introduced a dependency from ql package to metastore package which is not allowed and fails in a real cluster.
Instead of moving it to metastore, it should be moved to common package.</description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.txn.compactor.TestWorker.java</file>
			<file type="M">org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">13354</link>
		</links>
	</bug>
	<bug id="13973" opendate="2016-06-08 14:37:12" fixdate="2016-06-09 09:30:02" resolution="Fixed">
		<buginformation>
			<summary>Extend support for other primitive types in windowing expressions</summary>
			<description>Following windowing query using boolean column in partitioning clause


create table all100k(t tinyint, si smallint, i int,
    b bigint, f float, d double, s string,
    dc decimal(38,18), bo boolean, v varchar(25),
    c char(25), ts timestamp, dt date);
select  rank() over (partition by i order by bo  nulls first, b nulls last range between unbounded preceding and current row),
    row_number()  over (partition by bo order by si desc, b nulls last range between unbounded preceding and unbounded following) as fv
from all100k order by fv;


fails with the following error:

FAILED: SemanticException Failed to breakup Windowing invocations into Groups. At least 1 group must only depend on input columns. Also check for circular dependencies.
Underlying error: Primitve type BOOLEAN not supported in Value Boundary expression

</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.PTFTranslator.java</file>
		</fixedFiles>
	</bug>
	<bug id="13563" opendate="2016-04-20 22:39:02" fixdate="2016-06-09 18:27:49" resolution="Fixed">
		<buginformation>
			<summary>Hive Streaming does not honor orc.compress.size and orc.stripe.size table properties</summary>
			<description>According to the doc:
https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC#LanguageManualORC-HiveQLSyntax
One should be able to specify tblproperties for many ORC options.
But the settings for orc.compress.size and orc.stripe.size don&amp;amp;apos;t take effect.</description>
			<version>2.1.0</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.TestOrcRecordUpdater.java</file>
			<file type="M">org.apache.orc.OrcConf.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">12498</link>
		</links>
	</bug>
	<bug id="13264" opendate="2016-03-11 02:46:31" fixdate="2016-06-10 18:28:35" resolution="Fixed">
		<buginformation>
			<summary>JDBC driver makes 2 Open Session Calls for every open session</summary>
			<description>When HTTP is used as the transport mode by the Hive JDBC driver, we noticed that there is an additional open/close session just to validate the connection. 
TCLIService.Iface client = new TCLIService.Client(new TBinaryProtocol(transport));
      TOpenSessionResp openResp = client.OpenSession(new TOpenSessionReq());
      if (openResp != null) 
{
        client.CloseSession(new TCloseSessionReq(openResp.getSessionHandle()));
      }

The open session call is a costly one and should not be used to test transport. </description>
			<version>1.2.1</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.minikdc.TestJdbcWithMiniKdc.java</file>
			<file type="M">org.apache.hive.jdbc.HiveConnection.java</file>
		</fixedFiles>
	</bug>
	<bug id="13971" opendate="2016-06-08 04:26:26" fixdate="2016-06-11 01:16:21" resolution="Fixed">
		<buginformation>
			<summary>Address testcase failures of acid_globallimit.q and etc</summary>
			<description></description>
			<version>2.0.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.StatsTask.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">13838</link>
		</links>
	</bug>
	<bug id="13900" opendate="2016-05-31 15:42:55" fixdate="2016-06-13 18:47:20" resolution="Fixed">
		<buginformation>
			<summary>HiveStatement.executeAsync() may not work properly when hive.server2.async.exec.async.compile is turned on</summary>
			<description>HIVE-13882 handles HiveStatement.executeQuery() when hive.server2.async.exec.async.compile is turned on. Notice we may also have similar issue when executeAsync() is called. Investigate what would be the good approach for it.</description>
			<version>2.2.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.jdbc.TestJdbcDriver2.java</file>
			<file type="M">org.apache.hive.jdbc.HiveStatement.java</file>
		</fixedFiles>
	</bug>
	<bug id="13833" opendate="2016-05-24 17:51:20" fixdate="2016-06-14 22:36:53" resolution="Fixed">
		<buginformation>
			<summary>Add an initial delay when starting the heartbeat</summary>
			<description>Since the scheduling of heartbeat happens immediately after lock acquisition, it&amp;amp;apos;s unnecessary to send heartbeat at the time when locks is acquired. Add an initial delay to skip this.</description>
			<version>2.0.0</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
		</fixedFiles>
	</bug>
	<bug id="13987" opendate="2016-06-09 22:38:31" fixdate="2016-06-15 15:31:27" resolution="Fixed">
		<buginformation>
			<summary>Clarify current error shown when HS2 is down</summary>
			<description>When HS2 is down and a query is run, the following error is shown in beeline:


0: jdbc:hive2://localhost:10000&amp;gt; show tables;
Error: org.apache.thrift.transport.TTransportException (state=08S01,code=0)


It may be more helpful to also indicate that the reason for this is that HS2 is down, such as:


0: jdbc:hive2://localhost:10000&amp;gt; show tables;
HS2 may be unavailable, check server status
Error: org.apache.thrift.transport.TTransportException (state=08S01,code=0)

</description>
			<version>2.0.1</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.beeline.BeeLine.java</file>
		</fixedFiles>
	</bug>
	<bug id="13961" opendate="2016-06-07 18:04:05" fixdate="2016-06-15 17:25:40" resolution="Fixed">
		<buginformation>
			<summary>ACID: Major compaction fails to include the original bucket files if there&amp;apos;s no delta directory</summary>
			<description>The issue can be reproduced by steps below:
1. Insert a row to Non-ACID table
2. Convert Non-ACID to ACID table (i.e. set transactional=true table property)
3. Perform Major compaction</description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java</file>
			<file type="M">org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">12724</link>
		</links>
	</bug>
	<bug id="14006" opendate="2016-06-13 20:32:42" fixdate="2016-06-16 15:16:23" resolution="Fixed">
		<buginformation>
			<summary>Hive query with UNION ALL fails with ArrayIndexOutOfBoundsException</summary>
			<description>set hive.cbo.enable=false;
DROP VIEW IF EXISTS a_view;
DROP TABLE IF EXISTS table_a1;
DROP TABLE IF EXISTS table_a2;
DROP TABLE IF EXISTS table_b1;
DROP TABLE IF EXISTS table_b2;
CREATE TABLE table_a1
(composite_key STRING);
CREATE TABLE table_a2
(composite_key STRING);
CREATE TABLE table_b1
(composite_key STRING, col1 STRING);
CREATE TABLE table_b2
(composite_key STRING);
CREATE VIEW a_view AS
SELECT
substring(a1.composite_key, 1, locate(&amp;amp;apos;|&amp;amp;apos;,a1.composite_key) - 1) AS autoname,
NULL AS col1
FROM table_a1 a1
FULL OUTER JOIN table_a2 a2
ON a1.composite_key = a2.composite_key
UNION ALL
SELECT
substring(b1.composite_key, 1, locate(&amp;amp;apos;|&amp;amp;apos;,b1.composite_key) - 1) AS autoname,
b1.col1 AS col1
FROM table_b1 b1
FULL OUTER JOIN table_b2 b2
ON b1.composite_key = b2.composite_key;
INSERT INTO TABLE table_b1
SELECT * FROM (
SELECT &amp;amp;apos;something|awful&amp;amp;apos;, &amp;amp;apos;col1&amp;amp;apos;
)s ;
SELECT autoname
FROM a_view
WHERE autoname=&amp;amp;apos;something&amp;amp;apos;;
fails with 
Diagnostic Messages for this Task:
Error: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
{"_col0":"something"}
	at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:179)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
{"_col0":"something"}
	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:507)
	at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:170)
	... 8 more
Caused by: java.lang.ArrayIndexOutOfBoundsException: 0
	at org.apache.hadoop.hive.ql.exec.UnionOperator.processOp(UnionOperator.java:134)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95)
	at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:157)
	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:497)
The same query succeeds when hive.ppd.remove.duplicatefilters=false with or without CBO on. It also succeeds with just CBO on.</description>
			<version>2.0.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.UnionOperator.java</file>
		</fixedFiles>
	</bug>
	<bug id="14022" opendate="2016-06-15 18:59:23" fixdate="2016-06-17 16:20:03" resolution="Fixed">
		<buginformation>
			<summary>left semi join should throw SemanticException if where clause contains columnname from right table</summary>
			<description>Left semi join throws following error if where clause contains column name with table alias

select * from src_emptybucket_partitioned_1 e1 left semi join src_emptybucket_partitioned_3 e3 on e1.age =  e3.age where e1.year = 2015 and e3.year1=2016;
16/06/10 22:37:37 [main]: INFO log.PerfLogger: &amp;lt;PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver&amp;gt;
16/06/10 22:37:37 [main]: INFO log.PerfLogger: &amp;lt;PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver&amp;gt;
16/06/10 22:37:37 [main]: INFO log.PerfLogger: &amp;lt;PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver&amp;gt;
16/06/10 22:37:37 [main]: INFO ql.Driver: We are setting the hadoop caller context from  to hrt_qa_20160610223737_c3821398-d8df-44d8-9dd5-e66c9b7ed7c7
16/06/10 22:37:37 [main]: DEBUG parse.VariableSubstitution: Substitution is on: select * from src_emptybucket_partitioned_1 e1 left semi join src_emptybucket_partitioned_3 e3 on e1.age =  e3.age where e1.year = 2015 and e3.year1=2016
16/06/10 22:37:37 [main]: INFO log.PerfLogger: &amp;lt;PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver&amp;gt;
16/06/10 22:37:37 [main]: INFO parse.ParseDriver: Parsing command: select * from src_emptybucket_partitioned_1 e1 left semi join src_emptybucket_partitioned_3 e3 on e1.age =  e3.age where e1.year = 2015 and e3.year1=2016
16/06/10 22:37:37 [main]: INFO parse.ParseDriver: Parse Completed
16/06/10 22:37:37 [main]: INFO log.PerfLogger: &amp;lt;/PERFLOG method=parse start=1465598257393 end=1465598257397 duration=4 from=org.apache.hadoop.hive.ql.Driver&amp;gt;
16/06/10 22:37:37 [main]: DEBUG ql.Driver: Encoding valid txns info 9223372036854775807:
16/06/10 22:37:37 [main]: INFO log.PerfLogger: &amp;lt;PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver&amp;gt;
16/06/10 22:37:37 [main]: INFO parse.CalcitePlanner: Starting Semantic Analysis
16/06/10 22:37:37 [main]: INFO parse.CalcitePlanner: Completed phase 1 of Semantic Analysis
16/06/10 22:37:37 [main]: INFO parse.CalcitePlanner: Get metadata for source tables
16/06/10 22:37:37 [main]: INFO parse.CalcitePlanner: Get metadata for subqueries
16/06/10 22:37:37 [main]: INFO parse.CalcitePlanner: Get metadata for destination tables
16/06/10 22:37:37 [IPC Parameter Sending Thread #0]: DEBUG ipc.Client: IPC Client (147022238) connection to jvaria-hive2-440-5.openstacklocal/172.22.126.47:8020 from hrt_qa sending #194
16/06/10 22:37:37 [IPC Client (147022238) connection to jvaria-hive2-440-5.openstacklocal/172.22.126.47:8020 from hrt_qa]: DEBUG ipc.Client: IPC Client (147022238) connection to jvaria-hive2-440-5.openstacklocal/172.22.126.47:8020 from hrt_qa got value #194
16/06/10 22:37:37 [main]: DEBUG ipc.ProtobufRpcEngine: Call: getEZForPath took 2ms
16/06/10 22:37:37 [IPC Parameter Sending Thread #0]: DEBUG ipc.Client: IPC Client (147022238) connection to jvaria-hive2-440-5.openstacklocal/172.22.126.47:8020 from hrt_qa sending #195
16/06/10 22:37:37 [IPC Client (147022238) connection to jvaria-hive2-440-5.openstacklocal/172.22.126.47:8020 from hrt_qa]: DEBUG ipc.Client: IPC Client (147022238) connection to jvaria-hive2-440-5.openstacklocal/172.22.126.47:8020 from hrt_qa got value #195
16/06/10 22:37:37 [main]: DEBUG ipc.ProtobufRpcEngine: Call: getEZForPath took 1ms
16/06/10 22:37:37 [main]: DEBUG hdfs.DFSClient: /tmp/hive/hrt_qa/d2568b75-6399-46df-82b9-34ec445e8f64/hive_2016-06-10_22-37-37_392_2780828105665881901-1: masked=rwx------
16/06/10 22:37:37 [IPC Parameter Sending Thread #0]: DEBUG ipc.Client: IPC Client (147022238) connection to jvaria-hive2-440-5.openstacklocal/172.22.126.47:8020 from hrt_qa sending #196
16/06/10 22:37:37 [IPC Client (147022238) connection to jvaria-hive2-440-5.openstacklocal/172.22.126.47:8020 from hrt_qa]: DEBUG ipc.Client: IPC Client (147022238) connection to jvaria-hive2-440-5.openstacklocal/172.22.126.47:8020 from hrt_qa got value #196
16/06/10 22:37:37 [main]: DEBUG ipc.ProtobufRpcEngine: Call: mkdirs took 2ms
16/06/10 22:37:37 [IPC Parameter Sending Thread #0]: DEBUG ipc.Client: IPC Client (147022238) connection to jvaria-hive2-440-5.openstacklocal/172.22.126.47:8020 from hrt_qa sending #197
16/06/10 22:37:37 [IPC Client (147022238) connection to jvaria-hive2-440-5.openstacklocal/172.22.126.47:8020 from hrt_qa]: DEBUG ipc.Client: IPC Client (147022238) connection to jvaria-hive2-440-5.openstacklocal/172.22.126.47:8020 from hrt_qa got value #197
16/06/10 22:37:37 [main]: DEBUG ipc.ProtobufRpcEngine: Call: getFileInfo took 1ms
16/06/10 22:37:37 [main]: INFO ql.Context: New scratch dir is hdfs://jvaria-hive2-440-5.openstacklocal:8020/tmp/hive/hrt_qa/d2568b75-6399-46df-82b9-34ec445e8f64/hive_2016-06-10_22-37-37_392_2780828105665881901-1
16/06/10 22:37:37 [main]: INFO parse.CalcitePlanner: Completed getting MetaData in Semantic Analysis
16/06/10 22:37:37 [main]: INFO parse.BaseSemanticAnalyzer: Not invoking CBO because the statement has too few joins
16/06/10 22:37:37 [main]: DEBUG hive.log: DDL: struct src_emptybucket_partitioned_1 { string name, i32 age, double gpa}
16/06/10 22:37:37 [main]: DEBUG parse.CalcitePlanner: Created Table Plan for e1 TS[0]
16/06/10 22:37:37 [main]: DEBUG hive.log: DDL: struct src_emptybucket_partitioned_3 { varchar(50) name1, i64 age1, decimal(38,18) gpa1}
16/06/10 22:37:37 [main]: DEBUG parse.CalcitePlanner: Created Table Plan for e3 TS[1]
16/06/10 22:37:37 [main]: DEBUG parse.TypeCheckCtx: Setting error: [Line 1:111 Invalid column reference &amp;amp;apos;age&amp;amp;apos;] from (. (TOK_TABLE_OR_COL e3) age)
java.lang.Exception
	at org.apache.hadoop.hive.ql.parse.TypeCheckCtx.setError(TypeCheckCtx.java:159)
	at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.processQualifiedColRef(TypeCheckProcFactory.java:1149)
	at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.process(TypeCheckProcFactory.java:1257)
	at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:95)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:79)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:133)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:110)
	at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.genExprNode(TypeCheckProcFactory.java:213)
	at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.genExprNode(TypeCheckProcFactory.java:157)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genAllExprNodeDesc(SemanticAnalyzer.java:10756)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genExprNodeDesc(SemanticAnalyzer.java:10712)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genExprNodeDesc(SemanticAnalyzer.java:10680)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genExprNodeDesc(SemanticAnalyzer.java:10674)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.insertSelectForSemijoin(SemanticAnalyzer.java:7704)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genJoinOperator(SemanticAnalyzer.java:7644)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genJoinPlan(SemanticAnalyzer.java:7821)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9821)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9710)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genOPTree(SemanticAnalyzer.java:10353)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:329)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10364)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:211)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:462)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:318)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1197)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1245)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1134)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1124)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:216)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:168)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:379)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:739)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:684)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:624)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
FAILED: SemanticException [Error 10002]: Line 1:111 Invalid column reference &amp;amp;apos;age&amp;amp;apos;
16/06/10 22:37:37 [main]: ERROR ql.Driver: FAILED: SemanticException [Error 10002]: Line 1:111 Invalid column reference &amp;amp;apos;age&amp;amp;apos;
org.apache.hadoop.hive.ql.parse.SemanticException: Line 1:111 Invalid column reference &amp;amp;apos;age&amp;amp;apos;
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genAllExprNodeDesc(SemanticAnalyzer.java:10764)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genExprNodeDesc(SemanticAnalyzer.java:10712)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genExprNodeDesc(SemanticAnalyzer.java:10680)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genExprNodeDesc(SemanticAnalyzer.java:10674)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.insertSelectForSemijoin(SemanticAnalyzer.java:7704)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genJoinOperator(SemanticAnalyzer.java:7644)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genJoinPlan(SemanticAnalyzer.java:7821)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9821)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9710)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genOPTree(SemanticAnalyzer.java:10353)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:329)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10364)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:211)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:462)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:318)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1197)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1245)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1134)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1124)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:216)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:168)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:379)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:739)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:684)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:624)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)


Similar inner join query works without any issues

select * from src_emptybucket_partitioned_1 e1 inner join src_emptybucket_partitioned_3 e3 on e1.age =  e3.age1 where e1.year = 2015 and e3.year1 = 2016;
Query ID = hrt_qa_20160610224945_9e5b40b7-8faf-4ef0-b1e1-741754fe2786
Total jobs = 1
Launching Job 1 out of 1


Status: Running (Executing on YARN cluster with App id application_1464125086069_0317)

--------------------------------------------------------------------------------
        VERTICES      STATUS  TOTAL  COMPLETED  RUNNING  PENDING  FAILED  KILLED
--------------------------------------------------------------------------------
Map 1 ..........   SUCCEEDED      1          1        0        0       0       0
Map 2 ..........   SUCCEEDED      1          1        0        0       0       0
--------------------------------------------------------------------------------
VERTICES: 02/02  [==========================&amp;gt;&amp;gt;] 100%  ELAPSED TIME: 6.92 s     
--------------------------------------------------------------------------------
OK
Time taken: 8.761 seconds


Source Tables Schema

desc src_emptybucket_partitioned_1;
OK
name                	string              	                    
age                 	int                 	                    
gpa                 	double              	                    
year                	int                 	                    
	 	 
# Partition Information	 	 
# col_name            	data_type           	comment             
	 	 
year                	int                 	                    
Time taken: 1.115 seconds, Fetched: 9 row(s)
hive&amp;gt; name1               	varchar(50)         	                    
age1                	bigint              	                    
gpa1                	decimal(38,18)      	                    
year1               	int                 	                    
	 	 
# Partition Information	 	 
# col_name            	data_type           	comment             
	 	 
year1               	int     


Left semi join query would work fine if I remove table alias from the query

hive&amp;gt; select * from src_emptybucket_partitioned_1  left semi join src_emptybucket_partitioned_3 on age =  age1 where year = 2015 and year1=2016;
Query ID = hrt_qa_20160610223932_4bf61489-b0eb-4533-9a24-77060aef417b
Total jobs = 1
Launching Job 1 out of 1


Status: Running (Executing on YARN cluster with App id application_1464125086069_0317)

--------------------------------------------------------------------------------
        VERTICES      STATUS  TOTAL  COMPLETED  RUNNING  PENDING  FAILED  KILLED
--------------------------------------------------------------------------------
Map 1 ..........   SUCCEEDED      1          1        0        0       0       0
Map 2 ..........   SUCCEEDED      1          1        0        0       0       0
--------------------------------------------------------------------------------
VERTICES: 02/02  [==========================&amp;gt;&amp;gt;] 100%  ELAPSED TIME: 8.33 s     
--------------------------------------------------------------------------------
OK
Time taken: 14.794 seconds


Left semi join query would also work if I keep column name in where clause from the table on left side of the join

hive&amp;gt; select * from src_emptybucket_partitioned_1 e1 left semi join src_emptybucket_partitioned_3 e3 on e1.age =  e3.age1 where e1.year = 2015;
Query ID = hrt_qa_20160610224621_25158eb4-ca9a-47ea-b33d-ee06f9979dec
Total jobs = 1
Launching Job 1 out of 1


Status: Running (Executing on YARN cluster with App id application_1464125086069_0317)

--------------------------------------------------------------------------------
        VERTICES      STATUS  TOTAL  COMPLETED  RUNNING  PENDING  FAILED  KILLED
--------------------------------------------------------------------------------
Map 1 ..........   SUCCEEDED      1          1        0        0       0       0
Map 2 ..........   SUCCEEDED      1          1        0        0       0       0
--------------------------------------------------------------------------------
VERTICES: 02/02  [==========================&amp;gt;&amp;gt;] 100%  ELAPSED TIME: 8.60 s     
--------------------------------------------------------------------------------
OK
Time taken: 10.404 seconds
hive&amp;gt; select * from src_emptybucket_partitioned_1 e1 left semi join src_emptybucket_partitioned_3 e3 on e1.age =  e3.age1 where e3.year1 = 2016;
FAILED: SemanticException [Error 10004]: Line 1:122 Invalid table alias or column reference &amp;amp;apos;e3&amp;amp;apos;: (possible column names are: name, age, gpa, year)

</description>
			<version>2.2.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
	</bug>
	<bug id="14003" opendate="2016-06-13 15:13:26" fixdate="2016-06-19 19:30:42" resolution="Fixed">
		<buginformation>
			<summary>queries running against llap hang at times - preemption issues</summary>
			<description>The preemption logic in the Hive processor needs some more work. There are definitely windows where the abort flag is completely dropped within the Hive processor.</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapredContext.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Operator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.RecordProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.llap.daemon.impl.LlapTaskReporter.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.MergeFileRecordProcessor.java</file>
		</fixedFiles>
	</bug>
	<bug id="14015" opendate="2016-06-14 23:47:06" fixdate="2016-06-20 02:00:17" resolution="Fixed">
		<buginformation>
			<summary>SMB MapJoin failed for Hive on Spark when kerberized</summary>
			<description>java.io.IOException: org.apache.hadoop.ipc.RemoteException(java.io.IOException): Delegation Token can be issued only with kerberos or web authentication
It could be reproduced:
1) prepare sample data:
a=1
while [[ $a -lt 100 ]]; do echo $a ; let a=$a+1; done &amp;gt; data
2) prepare source hive table:
CREATE TABLE `s`(`c` string);
load data local inpath &amp;amp;apos;data&amp;amp;apos; into table s;
3) prepare the bucketed table:
set hive.enforce.bucketing=true;
set hive.enforce.sorting=true;
CREATE TABLE `t`(`c` string) CLUSTERED BY (c) SORTED BY (c) INTO 5 BUCKETS;
insert into t select * from s;
4) reproduce this issue:
SET hive.execution.engine=spark;
SET hive.auto.convert.sortmerge.join = true;
SET hive.auto.convert.sortmerge.join.bigtable.selection.policy = org.apache.hadoop.hive.ql.optimizer.LeftmostBigTableSelectorForAutoSMJ;
SET hive.auto.convert.sortmerge.join.noconditionaltask = true;
SET hive.optimize.bucketmapjoin = true;
SET hive.optimize.bucketmapjoin.sortedmerge = true;
select * from t join t t1 on t.c=t1.c;
The stack is as following:

Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, ychencdh571-2.vpc.cloudera.com): java.lang.RuntimeException: Error processing row: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {"c":"13"}
	at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.processRow(SparkMapRecordHandler.java:154)
	at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:48)
	at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:27)
	at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList$ResultIterator.hasNext(HiveBaseFunctionResultList.java:95)
	at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:41)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$15.apply(AsyncRDDActions.scala:120)
	at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$15.apply(AsyncRDDActions.scala:120)
	at org.apache.spark.SparkContext$$anonfun$38.apply(SparkContext.scala:2003)
	at org.apache.spark.SparkContext$$anonfun$38.apply(SparkContext.scala:2003)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {"c":"13"}
	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:507)
	at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.processRow(SparkMapRecordHandler.java:141)
	... 16 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: org.apache.hadoop.ipc.RemoteException(java.io.IOException): Delegation Token can be issued only with kerberos or web authentication
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getDelegationToken(FSNamesystem.java:7454)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getDelegationToken(NameNodeRpcServer.java:542)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.getDelegationToken(AuthorizationProviderProxyClientProtocol.java:662)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getDelegationToken(ClientNamenodeProtocolServerSideTranslatorPB.java:966)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)

	at org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator$MergeQueue.nextHive(SMBMapJoinOperator.java:774)
	at org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator$MergeQueue.setupContext(SMBMapJoinOperator.java:711)
	at org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.setUpFetchContexts(SMBMapJoinOperator.java:538)
	at org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.processOp(SMBMapJoinOperator.java:248)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)
	at org.apache.hadoop.hive.ql.exec.FilterOperator.processOp(FilterOperator.java:120)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95)
	at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:157)
	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:497)
	... 17 more
Caused by: java.io.IOException: org.apache.hadoop.ipc.RemoteException(java.io.IOException): Delegation Token can be issued only with kerberos or web authentication
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getDelegationToken(FSNamesystem.java:7454)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getDelegationToken(NameNodeRpcServer.java:542)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.getDelegationToken(AuthorizationProviderProxyClientProtocol.java:662)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getDelegationToken(ClientNamenodeProtocolServerSideTranslatorPB.java:966)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)

	at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:507)
	at org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator$MergeQueue.next(SMBMapJoinOperator.java:795)
	at org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator$MergeQueue.nextHive(SMBMapJoinOperator.java:772)
	... 26 more
Caused by: org.apache.hadoop.ipc.RemoteException(java.io.IOException): Delegation Token can be issued only with kerberos or web authentication
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getDelegationToken(FSNamesystem.java:7454)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getDelegationToken(NameNodeRpcServer.java:542)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.getDelegationToken(AuthorizationProviderProxyClientProtocol.java:662)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getDelegationToken(ClientNamenodeProtocolServerSideTranslatorPB.java:966)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)

	at org.apache.hadoop.ipc.Client.call(Client.java:1471)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy18.getDelegationToken(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getDelegationToken(ClientNamenodeProtocolTranslatorPB.java:914)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy19.getDelegationToken(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getDelegationToken(DFSClient.java:1062)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getDelegationToken(DistributedFileSystem.java:1452)
	at org.apache.hadoop.fs.FileSystem.collectDelegationTokens(FileSystem.java:541)
	at org.apache.hadoop.fs.FileSystem.addDelegationTokens(FileSystem.java:519)
	at org.apache.hadoop.hdfs.DistributedFileSystem.addDelegationTokens(DistributedFileSystem.java:2181)
	at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:140)
	at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:100)
	at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:80)
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:206)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)
	at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextSplits(FetchOperator.java:362)
	at org.apache.hadoop.hive.ql.exec.FetchOperator.getRecordReader(FetchOperator.java:294)
	at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:445)
	... 28 more


</description>
			<version>1.1.0</version>
			<fixedVersion>2.1.0, 2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
		</fixedFiles>
	</bug>
	<bug id="14059" opendate="2016-06-20 15:02:23" fixdate="2016-06-20 15:37:53" resolution="Fixed">
		<buginformation>
			<summary>Missing license headers for two files</summary>
			<description>As noted by Sushanth Sowmyan, two files are missing the Apache headers:

/Users/sush/t/rel/apache-hive-2.1.0-src/common/src/java/org/apache/hive/common/util/DateParser.java
/Users/sush/t/rel/apache-hive-2.1.0-src/common/src/test/org/apache/hive/common/util/TestDateParser.java

</description>
			<version>2.1.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.common.util.DateParser.java</file>
			<file type="M">org.apache.hive.common.util.TestDateParser.java</file>
		</fixedFiles>
	</bug>
	<bug id="14054" opendate="2016-06-18 00:48:36" fixdate="2016-06-20 17:19:37" resolution="Fixed">
		<buginformation>
			<summary>TestHiveMetaStoreChecker fails on master </summary>
			<description></description>
			<version>2.2.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">14290</link>
		</links>
	</bug>
	<bug id="13809" opendate="2016-05-20 17:55:44" fixdate="2016-06-20 22:34:03" resolution="Fixed">
		<buginformation>
			<summary>Hybrid Grace Hash Join memory usage estimation didn&amp;apos;t take into account the bloom filter size</summary>
			<description>Memory estimation is important during hash table loading, because we need to make the decision of whether to load the next hash partition in memory or spill it. If the assumption is there&amp;amp;apos;s enough memory but it turns out not the case, we will run into OOM problem.
Currently hybrid grace hash join memory usage estimation didn&amp;amp;apos;t take into account the bloom filter size. In large test cases (TB scale) the bloom filter grows as big as hundreds of MB, big enough to cause estimation error.
The solution is to count in the bloom filter size into memory estimation.
Another issue this patch will fix is possible NPE due to object cache reuse during hybrid grace hash join.</description>
			<version>2.0.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.HashTableLoader.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="is blocked by">13934</link>
		</links>
	</bug>
	<bug id="13985" opendate="2016-06-09 21:00:00" fixdate="2016-06-21 01:10:02" resolution="Fixed">
		<buginformation>
			<summary>ORC improvements for reducing the file system calls in task side</summary>
			<description>HIVE-13840 fixed some issues with addition file system invocations during split generation. Similarly, this jira will fix issues with additional file system invocations on the task side. To avoid reading footers on the task side, users can set hive.orc.splits.include.file.footer to true which will serialize the orc footers on the splits. But this has issues with serializing unwanted information like column statistics and other metadata which are not really required for reading orc split on the task side. We can reduce the payload on the orc splits by serializing only the minimum required information (stripe information, types, compression details). This will decrease the payload on the orc splits and can potentially avoid OOMs in application master (AM) during split generation. This jira also address other issues concerning the AM cache. The local cache used by AM is soft reference cache. This can introduce unpredictability across multiple runs of the same query. We can cache the serialized footer in the local cache and also use strong reference cache which should avoid memory pressure and will have better predictability.
One other improvement that we can do is when hive.orc.splits.include.file.footer is set to false, on the task side we make one additional file system call to know the size of the file. If we can serialize the file length in the orc split this can be avoided.</description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcFile.java</file>
			<file type="M">org.apache.orc.OrcFile.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
			<file type="M">org.apache.orc.OrcUtils.java</file>
			<file type="M">org.apache.orc.OrcProto.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.ExternalCache.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.VectorizedOrcInputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcNewSplit.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcFileFormatProxy.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcSplit.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.LocalCache.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="D">org.apache.orc.FileMetaInfo.java</file>
			<file type="M">org.apache.orc.Reader.java</file>
			<file type="M">org.apache.orc.impl.ReaderImpl.java</file>
		</fixedFiles>
	</bug>
	<bug id="14060" opendate="2016-06-20 21:18:51" fixdate="2016-06-22 02:46:41" resolution="Fixed">
		<buginformation>
			<summary>Hive: Remove bogus "localhost" from Hive splits</summary>
			<description>On remote filesystems like Azure, GCP and S3, the splits contain a filler location of "localhost".
This is worse than having no location information at all - on large clusters yarn waits upto 200[1] seconds for heartbeat from "localhost" before allocating a container.
To speed up this process, the split affinity provider should scrub the bogus "localhost" from the locations and allow for the allocation of "*" containers instead on each heartbeat.
[1] - yarn.scheduler.capacity.node-locality-delay=40 x heartbeat of 5s</description>
			<version>2.1.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.Utils.java</file>
		</fixedFiles>
	</bug>
	<bug id="14062" opendate="2016-06-20 22:03:57" fixdate="2016-06-22 14:46:18" resolution="Fixed">
		<buginformation>
			<summary>Changes from HIVE-13502 overwritten by HIVE-13566</summary>
			<description>Appears that changes from HIVE-13566 overwrote the changes from HIVE-13502. I will confirm with the author that it was inadvertent before I re-add it. Thanks</description>
			<version>2.1.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.beeline.Commands.java</file>
			<file type="M">org.apache.hive.beeline.BeeLine.java</file>
			<file type="M">org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
			<file type="M">org.apache.hive.jdbc.Utils.java</file>
		</fixedFiles>
	</bug>
	<bug id="13744" opendate="2016-05-12 00:18:39" fixdate="2016-06-22 18:18:05" resolution="Fixed">
		<buginformation>
			<summary>LLAP IO - add complex types support</summary>
			<description>Recently, complex type column vectors were added to Hive. We should use them in IO elevator.
Vectorization itself doesn&amp;amp;apos;t support complex types (yet), but this would be useful when it does, also it will enable LLAP IO elevator to be used in non-vectorized context with complex types after HIVE-13617</description>
			<version>2.2.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.java</file>
		</fixedFiles>
	</bug>
	<bug id="13159" opendate="2016-02-25 22:20:09" fixdate="2016-06-22 18:37:47" resolution="Fixed">
		<buginformation>
			<summary>TxnHandler should support datanucleus.connectionPoolingType = None</summary>
			<description>Right now, one has to choose bonecp or dbcp.</description>
			<version>2.0.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">12579</link>
		</links>
	</bug>
	<bug id="13946" opendate="2016-06-04 00:18:06" fixdate="2016-06-22 22:54:53" resolution="Duplicate">
		<buginformation>
			<summary>Decimal value need to be single-quoted when selecting where clause with that decimal value in order to get row</summary>
			<description>Create a table withe a column of decimal type(38,18) and insert &amp;amp;apos;4327269606205.029297&amp;amp;apos;. Then select with that value does not return anything.

0: jdbc:hive2://ts-0531-1.openstacklocal:2181&amp;gt; drop table if exists test;
No rows affected (0.175 seconds)
0: jdbc:hive2://ts-0531-1.openstacklocal:2181&amp;gt;
0: jdbc:hive2://ts-0531-1.openstacklocal:2181&amp;gt; create table test (dc decimal(38,18));
No rows affected (0.098 seconds)
0: jdbc:hive2://ts-0531-1.openstacklocal:2181&amp;gt;
0: jdbc:hive2://ts-0531-1.openstacklocal:2181&amp;gt; insert into table test values (4327269606205.029297);
INFO  : Session is already open
INFO  : Dag name: insert into table tes...327269606205.029297)(Stage-1)
INFO  : Tez session was closed. Reopening...
INFO  : Session re-established.
INFO  :

INFO  : Status: Running (Executing on YARN cluster with App id application_1464727816747_0762)

INFO  : Map 1: -/-
INFO  : Map 1: 0/1
INFO  : Map 1: 0(+1)/1
INFO  : Map 1: 1/1
INFO  : Loading data to table default.test from hdfs://ts-0531-5.openstacklocal:8020/apps/hive/warehouse/test/.hive-staging_hive_2016-06-04_00-03-54_302_7708281807413586675-940/-ext-10000
INFO  : Table default.test stats: [numFiles=1, numRows=1, totalSize=21, rawDataSize=20]
No rows affected (13.821 seconds)
0: jdbc:hive2://ts-0531-1.openstacklocal:2181&amp;gt;
0: jdbc:hive2://ts-0531-1.openstacklocal:2181&amp;gt; select * from test;
+-----------------------+--+
|        test.dc        |
+-----------------------+--+
| 4327269606205.029297  |
+-----------------------+--+
1 row selected (0.078 seconds)
0: jdbc:hive2://ts-0531-1.openstacklocal:2181&amp;gt; select * from test where dc = 4327269606205.029297;
+----------+--+
| test.dc  |
+----------+--+
+----------+--+
No rows selected (0.224 seconds)


If you single quote that decimal value, a row is returned.

0: jdbc:hive2://ts-0531-1.openstacklocal:2181&amp;gt; select * from test where dc = &amp;amp;apos;4327269606205.029297&amp;amp;apos;;
+-----------------------+--+
|        test.dc        |
+-----------------------+--+
| 4327269606205.029297  |
+-----------------------+--+
1 row selected (0.085 seconds)


explain shows:

0: jdbc:hive2://ts-0531-1.openstacklocal:2181&amp;gt; explain select * from test where dc = 4327269606205.029297;
+----------------------------------------------------------------------+--+
|                               Explain                                |
+----------------------------------------------------------------------+--+
| STAGE DEPENDENCIES:                                                  |
|   Stage-0 is a root stage                                            |
|                                                                      |
| STAGE PLANS:                                                         |
|   Stage: Stage-0                                                     |
|     Fetch Operator                                                   |
|       limit: -1                                                      |
|       Processor Tree:                                                |
|         TableScan                                                    |
|           alias: test                                                |
|           filterExpr: (dc = 4.3272696062050293E12) (type: boolean)   |
|           Filter Operator                                            |
|             predicate: (dc = 4.3272696062050293E12) (type: boolean)  |
|             Select Operator                                          |
|               expressions: dc (type: decimal(38,18))                 |
|               outputColumnNames: _col0                               |
|               ListSink                                               |
|                                                                      |
+----------------------------------------------------------------------+--+
18 rows selected (0.512 seconds)

</description>
			<version>1.2.1</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPDivide.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFPrintf.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToLong.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTBuilder.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFToShort.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentileApprox.java</file>
			<file type="M">org.apache.orc.impl.ConvertTreeReaderFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.sarg.ConvertAstToSearchArg.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFToLong.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPDivide.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.common.type.HiveDecimal.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
			<file type="M">org.apache.orc.impl.RecordReaderImpl.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFLeadLag.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFToInteger.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.ParseUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">13945</link>
		</links>
	</bug>
	<bug id="13872" opendate="2016-05-27 03:57:24" fixdate="2016-06-23 10:42:40" resolution="Fixed">
		<buginformation>
			<summary>Vectorization: Fix cross-product reduce sink serialization</summary>
			<description>TPC-DS Q13 produces a cross-product without CBO simplifying the query


Caused by: java.lang.RuntimeException: null STRING entry: batchIndex 0 projection column num 1
        at org.apache.hadoop.hive.ql.exec.vector.VectorExtractRow.nullBytesReadError(VectorExtractRow.java:349)
        at org.apache.hadoop.hive.ql.exec.vector.VectorExtractRow.extractRowColumn(VectorExtractRow.java:267)
        at org.apache.hadoop.hive.ql.exec.vector.VectorExtractRow.extractRow(VectorExtractRow.java:343)
        at org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.process(VectorReduceSinkOperator.java:103)
        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:837)
        at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:130)
        at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:762)
        ... 18 more


Simplified query


set hive.cbo.enable=false;

-- explain

select count(1)  
 from store_sales
     ,customer_demographics
 where (
( 
  customer_demographics.cd_demo_sk = store_sales.ss_cdemo_sk
  and customer_demographics.cd_marital_status = &amp;amp;apos;M&amp;amp;apos;
     )or
     (
   customer_demographics.cd_demo_sk = ss_cdemo_sk
  and customer_demographics.cd_marital_status = &amp;amp;apos;U&amp;amp;apos;
     ))
;




        Map 3 
            Map Operator Tree:
                TableScan
                  alias: customer_demographics
                  Statistics: Num rows: 1920800 Data size: 717255532 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    sort order: 
                    Statistics: Num rows: 1920800 Data size: 717255532 Basic stats: COMPLETE Column stats: NONE
                    value expressions: cd_demo_sk (type: int), cd_marital_status (type: string)
            Execution mode: vectorized, llap
            LLAP IO: all inputs

</description>
			<version>2.1.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.NullRowsInputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorExtractRow.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorAssignRow.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.java</file>
			<file type="M">org.apache.orc.impl.TreeReaderFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.VectorizedOrcInputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorDeserializeRow.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.java</file>
		</fixedFiles>
	</bug>
	<bug id="14076" opendate="2016-06-22 18:25:49" fixdate="2016-06-23 16:50:36" resolution="Fixed">
		<buginformation>
			<summary>Vectorization is not supported for datatype:VOID error while inserting data into specific columns</summary>
			<description>Insert into specific columns fails due to following error:

Vertex failed, vertexName=Reducer 2, vertexId=vertex_1465261180142_0160_1_01, diagnostics=[Task failed, taskId=task_1465261180142_0160_1_01_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Error while running task ( failure ) : attempt_1465261180142_0160_1_01_000000_0:java.lang.RuntimeException: java.lang.RuntimeException: Reduce operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:198)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:160)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: Reduce operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.init(ReduceRecordSource.java:221)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.initializeSourceForTag(ReduceRecordProcessor.java:245)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.init(ReduceRecordProcessor.java:163)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)
	... 14 more
Caused by: java.lang.RuntimeException: Vectorizaton is not supported for datatype:VOID
	at org.apache.hadoop.hive.ql.exec.vector.VectorizedBatchUtil.createColumnVector(VectorizedBatchUtil.java:172)
	at org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.createVectorizedRowBatch(VectorizedRowBatchCtx.java:194)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.init(ReduceRecordSource.java:177)
	... 17 more
], TaskAttempt 1 failed, info=[Error: Error while running task ( failure ) : attempt_1465261180142_0160_1_01_000000_1:java.lang.RuntimeException: java.lang.RuntimeException: Reduce operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:198)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:160)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: Reduce operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.init(ReduceRecordSource.java:221)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.initializeSourceForTag(ReduceRecordProcessor.java:245)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.init(ReduceRecordProcessor.java:163)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)
	... 14 more
Caused by: java.lang.RuntimeException: Vectorizaton is not supported for datatype:VOID
	at org.apache.hadoop.hive.ql.exec.vector.VectorizedBatchUtil.createColumnVector(VectorizedBatchUtil.java:172)
	at org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.createVectorizedRowBatch(VectorizedRowBatchCtx.java:194)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.init(ReduceRecordSource.java:177)
	... 17 more


Steps to reproduce the issue:

set hive.vectorized.execution.enabled=true;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;

drop table if exists newtable;
create external table newtable(
            a string,
            b int,
            c double)
row format delimited
fields terminated by &amp;amp;apos;\t&amp;amp;apos;
stored as textfile;

drop table if exists newtable_acid;
create table newtable_acid (b int, a varchar(50),c decimal(3,2), d int)
clustered by (b) into 2 buckets
stored as orc
tblproperties (&amp;amp;apos;transactional&amp;amp;apos;=&amp;amp;apos;true&amp;amp;apos;);

insert into newtable_acid(a,b,c)
select * from newtable;

select a, b, c from newtable_acid;

</description>
			<version>2.2.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
	</bug>
	<bug id="13725" opendate="2016-05-10 00:45:58" fixdate="2016-06-27 06:44:18" resolution="Fixed">
		<buginformation>
			<summary>ACID: Streaming API should synchronize calls when multiple threads use the same endpoint</summary>
			<description>Currently, the streaming endpoint creates a metastore client which gets used for RPC. The client itself is not internally thread safe. Therefore, the API methods should provide the relevant synchronization so that the methods can be called from different threads. A sample use case is as follows:
1. Thread 1 creates a streaming endpoint and opens a txn batch.
2. Thread 2 heartbeats the txn batch.
With the current impl, this can result in an "out of sequence response", since the response of the calls in thread1 might end up going to thread2 and vice-versa.</description>
			<version>1.2.1</version>
			<fixedVersion>1.3.0, 2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnDbUtil.java</file>
			<file type="M">org.apache.hive.hcatalog.streaming.HiveEndPoint.java</file>
			<file type="M">org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
		</fixedFiles>
	</bug>
	<bug id="13991" opendate="2016-06-10 01:43:23" fixdate="2016-06-27 15:50:08" resolution="Fixed">
		<buginformation>
			<summary>Union All on view fail with no valid permission on underneath table</summary>
			<description>When sentry is enabled. 
create view V as select * from T;
When the user has read permission on view V, but does not have read permission on table T,
select * from V union all select * from V 
failed with:

0: jdbc:hive2://********&amp;gt; select * from s07view union all select * from s07view limit 1;
Error: Error while compiling statement: FAILED: SemanticException No valid privileges
 Required privileges for this query: Server=server1-&amp;gt;Db=default-&amp;gt;Table=sample_07-&amp;gt;action=select; (state=42000,code=40000)

 </description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">14248</link>
		</links>
	</bug>
	<bug id="13997" opendate="2016-06-12 02:39:16" fixdate="2016-06-27 15:54:01" resolution="Fixed">
		<buginformation>
			<summary>Insert overwrite directory doesn&amp;apos;t overwrite existing files</summary>
			<description>Can be easily reproduced by running INSERT OVERWRITE DIRECTORY to the same dir twice.</description>
			<version>2.1.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">13918</link>
		</links>
	</bug>
	<bug id="14092" opendate="2016-06-24 21:19:36" fixdate="2016-06-27 18:11:08" resolution="Fixed">
		<buginformation>
			<summary>Kryo exception when deserializing VectorFileSinkOperator</summary>
			<description>Following exception is thrown for queries using VectorFileSinkOperator


Caused by: java.lang.IllegalArgumentException: Unable to create serializer "org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer" for class: org.apache.hadoop.hive.ql.exec.vector.VectorFileSinkOperator
	at org.apache.hive.com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:67)
	at org.apache.hive.com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:45)
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.newDefaultSerializer(Kryo.java:380)
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.getDefaultSerializer(Kryo.java:364)
	at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.registerImplicit(DefaultClassResolver.java:74)
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.getRegistration(Kryo.java:490)
	at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:166)
	at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:133)
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClass(Kryo.java:670)
	at org.apache.hadoop.hive.ql.exec.SerializationUtilities$KryoWithHooks.readClass(SerializationUtilities.java:180)
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:781)
	at org.apache.hadoop.hive.ql.exec.SerializationUtilities$KryoWithHooks.readClassAndObject(SerializationUtilities.java:175)
	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:134)
	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:40)
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708)
	at org.apache.hadoop.hive.ql.exec.SerializationUtilities$KryoWithHooks.readObject(SerializationUtilities.java:213)
	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)
	... 46 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.GeneratedConstructorAccessor6.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hive.com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:54)
	... 62 more
Caused by: java.lang.StackOverflowError
	at java.util.HashMap.hash(HashMap.java:338)
	at java.util.HashMap.get(HashMap.java:556)
	at org.apache.hive.com.esotericsoftware.kryo.Generics.getConcreteClass(Generics.java:61)
	at org.apache.hive.com.esotericsoftware.kryo.Generics.getConcreteClass(Generics.java:62)
	at org.apache.hive.com.esotericsoftware.kryo.Generics.getConcreteClass(Generics.java:62)
	at org.apache.hive.com.esotericsoftware.kryo.Generics.getConcreteClass(Generics.java:62)

</description>
			<version>2.2.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.SerializationUtilities.java</file>
		</fixedFiles>
	</bug>
	<bug id="14073" opendate="2016-06-21 22:35:02" fixdate="2016-06-28 02:09:13" resolution="Fixed">
		<buginformation>
			<summary>update config whiltelist for sql std authorization </summary>
			<description>New configs that should go in security whitelist have been added. Whitelist needs updating.</description>
			<version>2.1.0</version>
			<fixedVersion>1.3.0, 2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.TestSQLStdHiveAccessControllerHS2.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">14900</link>
			<link type="Reference" description="is related to">10678</link>
			<link type="Reference" description="is related to">2355</link>
		</links>
	</bug>
	<bug id="14013" opendate="2016-06-14 19:06:04" fixdate="2016-06-28 17:25:20" resolution="Fixed">
		<buginformation>
			<summary>Describe table doesn&amp;apos;t show unicode properly</summary>
			<description>Describe table output will show comments incorrectly rather than the unicode itself.

hive&amp;gt; desc formatted t1;

# Detailed Table Information             
Table Type:             MANAGED_TABLE            
Table Parameters:                
        COLUMN_STATS_ACCURATE   {\"BASIC_STATS\":\"true\"}
        comment                 \u8868\u4E2D\u6587\u6D4B\u8BD5
        numFiles                0                   

</description>
			<version>2.2.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.common.util.HiveStringUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
		</fixedFiles>
		<links>
			<link type="Dependent" description="Dependent">14146</link>
		</links>
	</bug>
	<bug id="14083" opendate="2016-06-23 19:05:51" fixdate="2016-06-29 19:26:19" resolution="Fixed">
		<buginformation>
			<summary>ALTER INDEX in Tez causes NullPointerException</summary>
			<description>ALTER INDEX causes a NullPointerException when run under TEZ execution engine. Query runs without issue when submitted using MR execution mode.
To reproduce:
1. CREATE INDEX sample_08_index ON TABLE sample_08 (code) AS &amp;amp;apos;COMPACT&amp;amp;apos; WITH DEFERRED REBUILD; 
2. ALTER INDEX sample_08_index ON sample_08 REBUILD; 
Stacktrace from Hive 1.2.1


ERROR : Vertex failed, vertexName=Map 1, vertexId=vertex_1460577396252_0005_1_00, diagnostics=[Task failed, taskId=task_1460577396252_0005_1_00_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:344)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: java.lang.NullPointerException
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:196)
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.&amp;lt;init&amp;gt;(TezGroupedSplitsInputFormat.java:135)
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat.getRecordReader(TezGroupedSplitsInputFormat.java:101)
	at org.apache.tez.mapreduce.lib.MRReaderMapred.setupOldRecordReader(MRReaderMapred.java:149)
	at org.apache.tez.mapreduce.lib.MRReaderMapred.setSplit(MRReaderMapred.java:80)
	at org.apache.tez.mapreduce.input.MRInput.initFromEventInternal(MRInput.java:650)
	at org.apache.tez.mapreduce.input.MRInput.initFromEvent(MRInput.java:621)
	at org.apache.tez.mapreduce.input.MRInputLegacy.checkAndAwaitRecordReaderInitialization(MRInputLegacy.java:145)
	at org.apache.tez.mapreduce.input.MRInputLegacy.init(MRInputLegacy.java:109)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getMRInput(MapRecordProcessor.java:390)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:128)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:147)
	... 14 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.init(HiveInputFormat.java:269)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:233)
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:193)
	... 25 more

</description>
			<version>2.2.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
		</fixedFiles>
	</bug>
	<bug id="14126" opendate="2016-06-29 17:59:16" fixdate="2016-07-01 17:44:53" resolution="Fixed">
		<buginformation>
			<summary>With ranger enabled, partitioned columns is returned first when you execute select star</summary>
			<description></description>
			<version>2.1.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Table.java</file>
		</fixedFiles>
	</bug>
	<bug id="14122" opendate="2016-06-29 07:50:40" fixdate="2016-07-02 06:31:36" resolution="Fixed">
		<buginformation>
			<summary>VectorMapOperator: Missing update to AbstractMapOperator::numRows</summary>
			<description>The INPUT_RECORDS counter is out of sync with the actual # of rows-read in vectorized and non-vectorized modes.
This means Tez record summaries are off by a large margin or is 0 for those vertices.</description>
			<version>2.1.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">13871</link>
		</links>
	</bug>
	<bug id="14109" opendate="2016-06-27 21:26:47" fixdate="2016-07-04 15:26:33" resolution="Fixed">
		<buginformation>
			<summary>query execuction throws NPE when hive.exec.submitviachild is set to true</summary>
			<description>If we set hive.exec.submitviachild to true and execute select count from src, the following exception is thrown.
Seems queryState is not initialized when ExecDriver is called from main() in ExecDriver.

java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.progress(HadoopJobExecHelper.java:262)
        at org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.progress(HadoopJobExecHelper.java:555)
        at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:436)
        at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.main(ExecDriver.java:756)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)

</description>
			<version>2.2.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.merge.MergeFileTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask.java</file>
		</fixedFiles>
		<links>
			<link type="Required" description="is required by">13424</link>
		</links>
	</bug>
	<bug id="14142" opendate="2016-07-01 02:27:34" fixdate="2016-07-05 13:29:28" resolution="Fixed">
		<buginformation>
			<summary>java.lang.ClassNotFoundException for the jar in hive.reloadable.aux.jars.path for Hive on Spark</summary>
			<description>Similar to HIVE-14037, seems HOS also has the same issue. The jars in hive.reloadable.aux.jars.path are not available during runtime.

java.lang.RuntimeException: Reduce operator initialization failed
	at org.apache.hadoop.hive.ql.exec.spark.SparkReduceRecordHandler.init(SparkReduceRecordHandler.java:232)
	at org.apache.hadoop.hive.ql.exec.spark.HiveReduceFunction.call(HiveReduceFunction.java:46)
	at org.apache.hadoop.hive.ql.exec.spark.HiveReduceFunction.call(HiveReduceFunction.java:28)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:192)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:192)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: xudf.XAdd
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.getUdfClass(GenericUDFBridge.java:134)
	at org.apache.hadoop.hive.ql.exec.FunctionRegistry.isStateful(FunctionRegistry.java:1365)
	at org.apache.hadoop.hive.ql.exec.FunctionRegistry.isDeterministic(FunctionRegistry.java:1328)
	at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator.isDeterministic(ExprNodeGenericFuncEvaluator.java:153)
	at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory.iterate(ExprNodeEvaluatorFactory.java:100)
	at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory.toCachedEvals(ExprNodeEvaluatorFactory.java:74)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:59)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:469)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:425)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.initializeOp(GroupByOperator.java:406)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)
	at org.apache.hadoop.hive.ql.exec.spark.SparkReduceRecordHandler.init(SparkReduceRecordHandler.java:217)
	... 15 more
Caused by: java.lang.ClassNotFoundException: xudf.XAdd
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:270)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.getUdfClass(GenericUDFBridge.java:132)
	... 27 more

</description>
			<version>2.2.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.java</file>
		</fixedFiles>
	</bug>
	<bug id="14090" opendate="2016-06-24 16:15:34" fixdate="2016-07-06 21:36:17" resolution="Fixed">
		<buginformation>
			<summary>JDOExceptions thrown by the Metastore have their full stack trace returned to clients</summary>
			<description>When user try to create any database or table with a name longer than 128 characters:


create database test_longname_looooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooongNametableFAIL;


It dumps the full exception stack-trace in a non-user-friendly message. The lends to relatively negative user-experience for Beeline users who hit this exception, they are generally not interested in the full stack-trace.
The formatted stack-trace is below:


Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:javax.jdo.JDOFatalUserException: Attempt to store value "test_longname_looooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooongnametablefail2" in column "`NAME`" that has maximum length of 128. Please correct your data!
at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:528)
at org.datanucleus.api.jdo.JDOPersistenceManager.jdoMakePersistent(JDOPersistenceManager.java:732)
at org.datanucleus.api.jdo.JDOPersistenceManager.makePersistent(JDOPersistenceManager.java:752)
at org.apache.hadoop.hive.metastore.ObjectStore.createDatabase(ObjectStore.java:569)
at sun.reflect.GeneratedMethodAccessor31.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:114)
at com.sun.proxy.$Proxy10.createDatabase(Unknown Source)
at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database_core(HiveMetaStore.java:923)
at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database(HiveMetaStore.java:962)
at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:138)
at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:99)
at com.sun.proxy.$Proxy12.create_database(Unknown Source)
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$create_database.getResult(ThriftHiveMetastore.java:8863)
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$create_database.getResult(ThriftHiveMetastore.java:8847)
at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:707)
at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:702)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:415)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)
at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:702)
at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745) NestedThrowablesStackTrace: Attempt to store value "test_longname_looooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooongnametablefail2" in column "`NAME`" that has maximum length of 128. Please correct your data! org.datanucleus.exceptions.NucleusUserException: Attempt to store value "test_longname_looooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooongnametablefail2" in column "`NAME`" that has maximum length of 128. Please correct your data!
at org.datanucleus.store.rdbms.mapping.datastore.CharRDBMSMapping.setString(CharRDBMSMapping.java:263)
at org.datanucleus.store.rdbms.mapping.java.SingleFieldMapping.setString(SingleFieldMapping.java:201)
at org.datanucleus.store.rdbms.fieldmanager.ParameterSetter.storeStringField(ParameterSetter.java:159)
at org.datanucleus.state.JDOStateManager.providedStringField(JDOStateManager.java:1256)
at org.apache.hadoop.hive.metastore.model.MDatabase.jdoProvideField(MDatabase.java)
at org.apache.hadoop.hive.metastore.model.MDatabase.jdoProvideFields(MDatabase.java)
at org.datanucleus.state.JDOStateManager.provideFields(JDOStateManager.java:1346)
at org.datanucleus.store.rdbms.request.InsertRequest.execute(InsertRequest.java:289)
at org.datanucleus.store.rdbms.RDBMSPersistenceHandler.insertTable(RDBMSPersistenceHandler.java:167)
at org.datanucleus.store.rdbms.RDBMSPersistenceHandler.insertObject(RDBMSPersistenceHandler.java:143)
at org.datanucleus.state.JDOStateManager.internalMakePersistent(JDOStateManager.java:3784)
at org.datanucleus.state.JDOStateManager.makePersistent(JDOStateManager.java:3760)
at org.datanucleus.ExecutionContextImpl.persistObjectInternal(ExecutionContextImpl.java:2219)
at org.datanucleus.ExecutionContextImpl.persistObjectWork(ExecutionContextImpl.java:2065)
at org.datanucleus.ExecutionContextImpl.persistObject(ExecutionContextImpl.java:1913)
at org.datanucleus.ExecutionContextThreadedImpl.persistObject(ExecutionContextThreadedImpl.java:217)
at org.datanucleus.api.jdo.JDOPersistenceManager.jdoMakePersistent(JDOPersistenceManager.java:727)
at org.datanucleus.api.jdo.JDOPersistenceManager.makePersistent(JDOPersistenceManager.java:752)
at org.apache.hadoop.hive.metastore.ObjectStore.createDatabase(ObjectStore.java:569)
at sun.reflect.GeneratedMethodAccessor31.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:114)
at com.sun.proxy.$Proxy10.createDatabase(Unknown Source)
at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database_core(HiveMetaStore.java:923)
at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database(HiveMetaStore.java:962)
at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:138)
at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:99)
at com.sun.proxy.$Proxy12.create_database(Unknown Source)
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$create_database.getResult(ThriftHiveMetastore.java:8863)
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$create_database.getResult(ThriftHiveMetastore.java:8847)
at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:707)
at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:702)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:415)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)
at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:702)
at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745) )


The ideal situation would be to just return the following message to Beeline users:


Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:javax.jdo.JDOFatalUserException: Attempt to store value "test_longname_looooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooongnametablefail2" in column "`NAME`" that has maximum length of 128. Please correct your data!)


And have the full stack trace should up in the HiveServer2 logs.</description>
			<version>1.1.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.RetryingHMSHandler.java</file>
		</fixedFiles>
	</bug>
	<bug id="14174" opendate="2016-07-06 22:04:25" fixdate="2016-07-06 22:09:57" resolution="Duplicate">
		<buginformation>
			<summary>Fix creating buckets without scheme information</summary>
			<description>If a table is created on a non-default filesystem (i.e. non-hdfs), the empty files will be created with incorrect scheme information. This patch extracts the scheme and authority information for the new paths.</description>
			<version>1.2.1</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.TestUtilities.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">14175</link>
		</links>
	</bug>
	<bug id="14019" opendate="2016-06-15 18:08:02" fixdate="2016-07-07 00:20:21" resolution="Duplicate">
		<buginformation>
			<summary>HiveServer2: Enable Kerberos with SSL for TCP transport</summary>
			<description>Currently, there is a limitation where an HS2 user needs to use the auth-conf SASL qop value to achieve encryption when Kerberos is used as the authentication mechanism and transport is TCP. </description>
			<version>1.2.1</version>
			<fixedVersion>2.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.jdbc.HiveConnection.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">10048</link>
		</links>
	</bug>
	<bug id="13945" opendate="2016-06-03 23:59:56" fixdate="2016-07-07 19:50:31" resolution="Fixed">
		<buginformation>
			<summary>Decimal value is displayed as rounded when selecting where clause with that decimal value.</summary>
			<description>Create a table withe a column of decimal type(38,18) and insert &amp;amp;apos;4327269606205.029297&amp;amp;apos;. Then select with that value displays its rounded value, which is 4327269606205.029300000000000000

0: jdbc:hive2://os-r7-mvjkcu-hiveserver2-11-4&amp;gt; drop table if exists test;
No rows affected (0.229 seconds)
0: jdbc:hive2://os-r7-mvjkcu-hiveserver2-11-4&amp;gt;
0: jdbc:hive2://os-r7-mvjkcu-hiveserver2-11-4&amp;gt; create table test (dc decimal(38,18));
No rows affected (0.125 seconds)
0: jdbc:hive2://os-r7-mvjkcu-hiveserver2-11-4&amp;gt;
0: jdbc:hive2://os-r7-mvjkcu-hiveserver2-11-4&amp;gt; insert into table test values (4327269606205.029297);
No rows affected (2.372 seconds)
0: jdbc:hive2://os-r7-mvjkcu-hiveserver2-11-4&amp;gt;
0: jdbc:hive2://os-r7-mvjkcu-hiveserver2-11-4&amp;gt; select * from test;
+-----------------------------------+--+
|              test.dc              |
+-----------------------------------+--+
| 4327269606205.029297000000000000  |
+-----------------------------------+--+
1 row selected (0.123 seconds)
0: jdbc:hive2://os-r7-mvjkcu-hiveserver2-11-4&amp;gt;
0: jdbc:hive2://os-r7-mvjkcu-hiveserver2-11-4&amp;gt; select * from test where dc = 4327269606205.029297000000000000;
+-----------------------------------+--+
|              test.dc              |
+-----------------------------------+--+
| 4327269606205.029300000000000000  |
+-----------------------------------+--+
1 row selected (0.109 seconds)

</description>
			<version>2.1.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPDivide.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFPrintf.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToLong.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTBuilder.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFToShort.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentileApprox.java</file>
			<file type="M">org.apache.orc.impl.ConvertTreeReaderFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.sarg.ConvertAstToSearchArg.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFToLong.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPDivide.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.common.type.HiveDecimal.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
			<file type="M">org.apache.orc.impl.RecordReaderImpl.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFLeadLag.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFToInteger.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.ParseUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">13946</link>
			<link type="Reference" description="relates to">14077</link>
			<link type="Reference" description="is related to">14134</link>
		</links>
	</bug>
	<bug id="14132" opendate="2016-06-29 22:48:48" fixdate="2016-07-07 22:35:40" resolution="Fixed">
		<buginformation>
			<summary>Don&amp;apos;t fail config validation for removed configs</summary>
			<description>Users may have set config in their scripts. If we remove said config in later version then config validation code will throw exception for scripts containing said config. This unnecessary incompatibility can be avoided.</description>
			<version>2.0.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.processors.SetProcessor.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">14133</link>
		</links>
	</bug>
	<bug id="14133" opendate="2016-06-29 22:50:00" fixdate="2016-07-07 22:36:00" resolution="Duplicate">
		<buginformation>
			<summary>Don&amp;apos;t fail config validation for removed configs</summary>
			<description>Users may have set config in their scripts. If we remove said config in later version then config validation code will throw exception for scripts containing said config. This unnecessary incompatibility can be avoided.</description>
			<version>2.0.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.processors.SetProcessor.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">14132</link>
		</links>
	</bug>
	<bug id="13749" opendate="2016-05-12 16:31:29" fixdate="2016-07-08 13:56:19" resolution="Fixed">
		<buginformation>
			<summary>Memory leak in Hive Metastore</summary>
			<description>Looking a heap dump of 10GB, a large number of Configuration objects(&amp;gt; 66k instances) are being retained. These objects along with its retained set is occupying about 95% of the heap space. This leads to HMS crashes every few days.
I will attach an exported snapshot from the eclipse MAT.</description>
			<version>1.1.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
		</fixedFiles>
	</bug>
	<bug id="14038" opendate="2016-06-16 18:45:06" fixdate="2016-07-08 17:44:01" resolution="Fixed">
		<buginformation>
			<summary>miscellaneous acid improvements</summary>
			<description>1. fix thread name inHouseKeeperServiceBase (currently they are all "org.apache.hadoop.hive.ql.txn.compactor.HouseKeeperServiceBase$1-0")
2. dump metastore configs from HiveConf on start up to help record values of properties
3. add some tests</description>
			<version>2.0.0</version>
			<fixedVersion>1.3.0, 2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.txn.compactor.HouseKeeperServiceBase.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConfUtil.java</file>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.java</file>
			<file type="M">org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
			<file type="M">org.apache.hive.hcatalog.templeton.AppConfig.java</file>
		</fixedFiles>
	</bug>
	<bug id="14147" opendate="2016-07-01 15:41:50" fixdate="2016-07-08 19:40:54" resolution="Fixed">
		<buginformation>
			<summary>Hive PPD might remove predicates when they are defined as a simple expr e.g. WHERE &amp;apos;a&amp;apos;</summary>
			<description></description>
			<version>2.2.0</version>
			<fixedVersion>1.3.0, 2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.ppd.ExprWalkerInfo.java</file>
			<file type="M">org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.ppd.ExprWalkerProcFactory.java</file>
		</fixedFiles>
	</bug>
	<bug id="14197" opendate="2016-07-08 17:45:53" fixdate="2016-07-08 21:05:46" resolution="Fixed">
		<buginformation>
			<summary>LLAP service driver precondition failure should include the values</summary>
			<description>LLAP service driver&amp;amp;apos;s precondition failure message are like below


Working memory + cache has to be smaller than the container sizing


It will be better to include the actual values for the sizes in the precondition failure message.
NO PRECOMMIT TESTS</description>
			<version>2.1.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
		</fixedFiles>
	</bug>
	<bug id="13392" opendate="2016-03-30 22:32:50" fixdate="2016-07-08 21:26:46" resolution="Fixed">
		<buginformation>
			<summary>disable speculative execution for ACID Compactor</summary>
			<description>https://developer.yahoo.com/hadoop/tutorial/module4.html
Speculative execution is enabled by default. You can disable speculative execution for the mappers and reducers by setting the mapred.map.tasks.speculative.execution and mapred.reduce.tasks.speculative.execution JobConf options to false, respectively.
CompactorMR is currently not set up to handle speculative execution and may lead to something like


2016-02-08 22:56:38,256 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException): Failed to CREATE_FILE /apps/hive/warehouse/service_logs_v2/ds=2016-01-20/_tmp_6cf08b9f-c2e2-4182-bc81-e032801b147f/base_13858600/bucket_00004 for DFSClient_attempt_1454628390210_27756_m_000001_1_131224698_1 on 172.18.129.12 because this file lease is currently owned by DFSClient_attempt_1454628390210_27756_m_000001_0_-2027182532_1 on 172.18.129.18
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLeaseInternal(FSNamesystem.java:2937)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2562)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2451)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2335)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:688)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:397)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2151)


Short term: disable speculative execution for this job
Longer term perhaps make each task write to dir with UUID...
</description>
			<version>1.0.0</version>
			<fixedVersion>1.3.0, 2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnUtils.java</file>
			<file type="D">org.apache.hadoop.hive.metastore.txn.ValidCompactorTxnList.java</file>
			<file type="M">org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TestValidCompactorTxnList.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.CompactionInfo.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.TestAcidUtils.java</file>
		</fixedFiles>
	</bug>
	<bug id="14178" opendate="2016-07-07 00:13:51" fixdate="2016-07-08 23:24:47" resolution="Fixed">
		<buginformation>
			<summary>Hive::needsToCopy should reuse FileUtils::equalsFileSystem</summary>
			<description>Clear bug triggered from missing FS checks in Hive.java


//Check if different FileSystems
if (!srcFs.getClass().equals(destFs.getClass()))
{ 
return true;
 }

</description>
			<version>1.2.1</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
		<links>
			<link type="Regression" description="is broken by">9159</link>
		</links>
	</bug>
	<bug id="14114" opendate="2016-06-28 03:46:51" fixdate="2016-07-09 03:21:33" resolution="Fixed">
		<buginformation>
			<summary>Ensure RecordWriter in streaming API is using the same UserGroupInformation as StreamingConnection</summary>
			<description>currently both DelimitedInputWriter and StrictJsonWriter perform some Metastore access operations but without using UGI created by the caller for Metastore operations made by matching StreamingConnection &amp;amp; TransactionBatch</description>
			<version>1.0.0</version>
			<fixedVersion>1.3.0, 2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.hcatalog.streaming.AbstractRecordWriter.java</file>
			<file type="M">org.apache.hive.hcatalog.streaming.HiveEndPoint.java</file>
			<file type="M">org.apache.hive.hcatalog.streaming.StreamingConnection.java</file>
			<file type="M">org.apache.hive.hcatalog.streaming.StrictJsonWriter.java</file>
			<file type="M">org.apache.hive.hcatalog.streaming.DelimitedInputWriter.java</file>
			<file type="M">org.apache.hive.hcatalog.streaming.TestStreaming.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">11089</link>
		</links>
	</bug>
	<bug id="14192" opendate="2016-07-08 00:30:56" fixdate="2016-07-09 03:22:00" resolution="Fixed">
		<buginformation>
			<summary>False positive error due to thrift</summary>
			<description>Given Thrift definition like this

struct LockComponent {
    1: required LockType type,
    2: required LockLevel level,
    3: required string dbname,
    4: optional string tablename,
    5: optional string partitionname,
    6: optional DataOperationType operationType = DataOperationType.UNSET,
    7: optional bool isAcid = false
}


The generated LockComponent has 

  public LockComponent() {
    this.operationType = org.apache.hadoop.hive.metastore.api.DataOperationType.UNSET;

    this.isAcid = false;

  }
  public boolean isSetOperationType() {
    return this.operationType != null;
  }
  public boolean isSetIsAcid() {
    return EncodingUtils.testBit(__isset_bitfield, __ISACID_ISSET_ID);
  }


So bottom line is even if LockComponent is created by old version of the client which doesn&amp;amp;apos;t have operationType filed, isSetOperationType() will still return true on the server.
This causes a false positive exception in TxnHandler.enqueueLockWithRetry() during Rolling Upgrade scenarios.</description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
		</fixedFiles>
	</bug>
	<bug id="14176" opendate="2016-07-06 22:45:47" fixdate="2016-07-09 10:31:52" resolution="Fixed">
		<buginformation>
			<summary>CBO nesting windowing function within each other when merging Project operators</summary>
			<description>The translation into a physical plan does not support this way of expressing windowing functions. Instead, we will not merge the Project operators when we find this pattern.</description>
			<version>2.1.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveProjectMergeRule.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
		</fixedFiles>
	</bug>
	<bug id="14115" opendate="2016-06-28 05:52:42" fixdate="2016-07-10 17:22:56" resolution="Fixed">
		<buginformation>
			<summary>Custom FetchFormatter is not supported</summary>
			<description>The following code is supported only FetchFormatter of ThriftFormatter and DefaultFetchFormatter. It can not be used Custom FetchFormatter.


        if (SessionState.get().isHiveServerQuery()) {
          conf.set(SerDeUtils.LIST_SINK_OUTPUT_FORMATTER,ThriftFormatter.class.getName());
        } else {
          conf.set(SerDeUtils.LIST_SINK_OUTPUT_FORMATTER, DefaultFetchFormatter.class.getName());
        }

</description>
			<version>2.1.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
		</fixedFiles>
	</bug>
	<bug id="14113" opendate="2016-06-28 03:10:09" fixdate="2016-07-10 17:28:16" resolution="Fixed">
		<buginformation>
			<summary>Create function failed but function in show function list</summary>
			<description>1. create function with invalid hdfs path, /udf/udf-test.jar does not exists

create function my_lower as &amp;amp;apos;com.tang.UDFLower&amp;amp;apos; using jar &amp;amp;apos;hdfs:///udf/udf-test.jar&amp;amp;apos;;
Failed with following exception:

0: jdbc:hive2://189.39.151.44:10000/&amp;gt; create function my_lower as &amp;amp;apos;com.tang.UDFLower&amp;amp;apos; using jar &amp;amp;apos;hdfs:///udf/udf-test.jar&amp;amp;apos;;
INFO  : converting to local hdfs:///udf/udf-test.jar
ERROR : Failed to read external resource hdfs:///udf/udf-test.jar
java.lang.RuntimeException: Failed to read external resource hdfs:///udf/udf-test.jar
	at org.apache.hadoop.hive.ql.session.SessionState.downloadResource(SessionState.java:1384)
	at org.apache.hadoop.hive.ql.session.SessionState.resolveAndDownload(SessionState.java:1340)
	at org.apache.hadoop.hive.ql.session.SessionState.add_resources(SessionState.java:1264)
	at org.apache.hadoop.hive.ql.session.SessionState.add_resources(SessionState.java:1250)
	at org.apache.hadoop.hive.ql.exec.FunctionTask.addFunctionResources(FunctionTask.java:306)
	at org.apache.hadoop.hive.ql.exec.Registry.registerToSessionRegistry(Registry.java:466)
	at org.apache.hadoop.hive.ql.exec.Registry.registerPermanentFunction(Registry.java:206)
	at org.apache.hadoop.hive.ql.exec.FunctionRegistry.registerPermanentFunction(FunctionRegistry.java:1551)
	at org.apache.hadoop.hive.ql.exec.FunctionTask.createPermanentFunction(FunctionTask.java:136)
	at org.apache.hadoop.hive.ql.exec.FunctionTask.execute(FunctionTask.java:75)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:158)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:101)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1965)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1723)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1475)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1283)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1278)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:167)
	at org.apache.hive.service.cli.operation.SQLOperation.access$200(SQLOperation.java:75)
	at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:245)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1711)
	at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:258)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.FileNotFoundException: File does not exist: hdfs:/udf/udf-test.jar
	at org.apache.hadoop.hdfs.DistributedFileSystem$25.doCall(DistributedFileSystem.java:1391)
	at org.apache.hadoop.hdfs.DistributedFileSystem$25.doCall(DistributedFileSystem.java:1383)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1383)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:340)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:292)
	at org.apache.hadoop.fs.FileSystem.copyToLocalFile(FileSystem.java:2034)
	at org.apache.hadoop.fs.FileSystem.copyToLocalFile(FileSystem.java:2003)
	at org.apache.hadoop.fs.FileSystem.copyToLocalFile(FileSystem.java:1979)
	at org.apache.hadoop.hive.ql.session.SessionState.downloadResource(SessionState.java:1370)
	... 28 more
ERROR : Failed to register default.my_lower using class com.tang.UDFLower
Error: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.FunctionTask (state=08S01,code=1)
2. Execute show functions, the failed function my_lower is in the function list

0: jdbc:hive2://189.39.151.44:21066/&amp;gt; show functions;
-------------------------+


        tab_name         


-------------------------+


 day                     


 dayofmonth              


 decode                  


 default.my_lower       


 degrees                 


 dense_rank              


0: jdbc:hive2://189.39.151.44:10000/&amp;gt; select my_lower(name) from stu;
Error: Error while compiling statement: FAILED: SemanticException [Error 10011]: Invalid function my_lower (state=42000,code=10011)

</description>
			<version>1.2.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.Registry.java</file>
		</fixedFiles>
	</bug>
	<bug id="13887" opendate="2016-05-29 05:34:19" fixdate="2016-07-10 18:12:57" resolution="Fixed">
		<buginformation>
			<summary>LazySimpleSerDe should parse "NULL" dates faster</summary>
			<description>Date string which contain "NULL" or "(null)" are being parsed through a very slow codepath involving exception handling as a normal codepath.
These are currently ~4x slower than parsing an actual date field.</description>
			<version>2.1.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyTimestamp.java</file>
			<file type="M">org.apache.hive.benchmark.serde.LazySimpleSerDeBench.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.fast.LazySimpleDeserializeRead.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyUtils.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyDate.java</file>
		</fixedFiles>
		<links>
			<link type="Incorporates" description="is part of">13878</link>
			<link type="Required" description="requires">13876</link>
		</links>
	</bug>
	<bug id="14027" opendate="2016-06-15 22:52:26" fixdate="2016-07-11 17:19:38" resolution="Fixed">
		<buginformation>
			<summary>NULL values produced by left outer join do not behave as NULL</summary>
			<description>Consider the following setup:


create table tbl (n bigint, t string); 

insert into tbl values (1, &amp;amp;apos;one&amp;amp;apos;); 
insert into tbl values(2, &amp;amp;apos;two&amp;amp;apos;);

select a.n, a.t, isnull(b.n), isnull(b.t) from (select * from tbl where n = 1) a  left outer join  (select * from tbl where 1 = 2) b on a.n = b.n;

1    one    false    true


The query should return true for isnull(b.n).
I&amp;amp;apos;ve tested by inserting a row with null value for the bigint column into tbl, and isnull returns true in that case. </description>
			<version>1.2.1</version>
			<fixedVersion>1.3.0, 2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.UnwrapRowContainer.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">13977</link>
			<link type="Reference" description="relates to">14208</link>
		</links>
	</bug>
	<bug id="13977" opendate="2016-06-09 08:04:00" fixdate="2016-07-11 17:20:35" resolution="Duplicate">
		<buginformation>
			<summary>nvl funtion not working after left outer join </summary>
			<description>Recreating problem.
1).Create table with sample data.
create table tabletest (n bigint, t string); 
insert into tabletest values (1, &amp;amp;apos;one&amp;amp;apos;); 
insert into tabletest values(2, &amp;amp;apos;two&amp;amp;apos;); 
2) Run leftouter join query on single table.
select a.n as leftHandN 
, b.n as rightHandN 
, b.t as rightHandT 
, nvl(b.t,"empty") as rightHandTnvl  Expected empty --&amp;gt; received empty
, nvl(b.n,-1) as rightHandNnvl  Expected -1 --&amp;gt; received 1 
from 
(
select *
from tabletest 
where n=1
) a
left outer join
(
select *
from tabletest 
where 1=2
) b
on a.n = b.n;
nvl(b.n,-1) should return -1 but returns 1.
I have found b.n always returning a.n value.if a.n is 1 ,b.n is returning 1 and if it is 2,same 2 will be returned.
More information:
length(b.n) --gives--&amp;gt;1
cast(b.n as string) -gives--&amp;gt;1
ascii(b.n) -gives---&amp;gt;49 i.e 1</description>
			<version>1.2.1</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.UnwrapRowContainer.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">14027</link>
		</links>
	</bug>
	<bug id="14116" opendate="2016-06-28 13:38:34" fixdate="2016-07-11 17:23:13" resolution="Duplicate">
		<buginformation>
			<summary>TBLPROPERTIES does not allow empty string values when Metastore is backed by Oracle database.</summary>
			<description>DDL commands like:
ALTER TABLE test SET TBLPROPERTIES(&amp;amp;apos;serialization.null.format&amp;amp;apos;=&amp;amp;apos;&amp;amp;apos;);
are silently ignored if the database backing Metastore is Oracle. This appears to be because Oracle treats an empty string as null.
Unlike when using MySql, no entry is created in the TBL_PARAMS table.
Steps to reproduce:
Create a table with a string field.
eg table mytable, field mystringfield.
ALTER TABLE mytable SET TBLPROPERTIES(&amp;amp;apos;serialization.null.format&amp;amp;apos;=&amp;amp;apos;&amp;amp;apos;);
DESCRIBE FORMATTED mytable;
with mysql backed Metastore, the entry will be displayed:
serialization.null.format 
and an entry is created in the TBL_PARAMS for the parameter.
With Oracle backed metastore, it is not, and no entry is created in TBL_PARAMS.</description>
			<version>1.1.0</version>
			<fixedVersion>1.0.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">8485</link>
		</links>
	</bug>
	<bug id="14141" opendate="2016-06-30 21:45:44" fixdate="2016-07-11 18:57:00" resolution="Fixed">
		<buginformation>
			<summary>Fix for HIVE-14062 breaks indirect urls in beeline</summary>
			<description>Looks like the patch for HIVE-14062 breaks indirect urls which uses environment variables to get the url in beeline
In order to reproduce this issue:

$ export BEELINE_URL_DEFAULT="jdbc:hive2://localhost:10000"
$ beeline -u default

</description>
			<version>2.1.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.beeline.Commands.java</file>
		</fixedFiles>
	</bug>
	<bug id="14175" opendate="2016-07-06 22:04:26" fixdate="2016-07-11 20:20:35" resolution="Fixed">
		<buginformation>
			<summary>Fix creating buckets without scheme information</summary>
			<description>If a table is created on a non-default filesystem (i.e. non-hdfs), the empty files will be created with incorrect scheme information. This patch extracts the scheme and authority information for the new paths.</description>
			<version>1.2.1</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.TestUtilities.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">14174</link>
			<link type="Reference" description="is related to">9928</link>
		</links>
	</bug>
	<bug id="13704" opendate="2016-05-06 11:25:19" fixdate="2016-07-12 18:36:59" resolution="Fixed">
		<buginformation>
			<summary>Don&amp;apos;t call DistCp.execute() instead of DistCp.run()</summary>
			<description>HIVE-11607 switched DistCp from using run to execute. The run method runs added logic that drives the state of SimpleCopyListing which runs in the driver, and of CopyCommitter which runs in the job runtime.
When Hive ends up running DistCp for copy work (Between non matching FS or between encrypted/non-encrypted zones, for sizes above a configured value) this state not being set causes wrong paths to appear on the target (subdirs named after the file, instead of just the file).
Hive should call DistCp&amp;amp;apos;s Tool run method and not the execute method directly, to not skip the target exists flag that the setTargetPathExists call would set:
https://github.com/apache/hadoop/blob/release-2.7.1/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCp.java#L108-L126</description>
			<version>1.3.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
		</fixedFiles>
		<links>
			<link type="Regression" description="is broken by">11607</link>
		</links>
	</bug>
	<bug id="14210" opendate="2016-07-11 23:58:12" fixdate="2016-07-13 04:37:59" resolution="Fixed">
		<buginformation>
			<summary>ExecDriver should call jobclient.close() to trigger cleanup</summary>
			<description>We found an issue in a customer environment where the HS2 crashed after a few days and the Java core dump contained several thousands of truststore reloader threads:
"Truststore reloader thread" #126 daemon prio=5 os_prio=0 tid=0x00007f680d2e3000 nid=0x98fd waiting on 
condition [0x00007f67e482c000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.security.ssl.ReloadingX509TrustManager.run
(ReloadingX509TrustManager.java:225)
        at java.lang.Thread.run(Thread.java:745)
We found the issue to be caused by a bug in Hadoop where the TimelineClientImpl is not destroying the SSLFactory if SSL is enabled in Hadoop and the timeline server is running. I opened YARN-5309 which has more details on the problem, and a patch was submitted a few days back.
In addition to the changes in Hadoop, there are a couple of Hive changes required:

ExecDriver needs to call jobclient.close() to trigger the clean-up of the resources after the submitted job is done/failed
Hive needs to pick up a newer release of Hadoop to pick up MAPREDUCE-6618 and MAPREDUCE-6621 that fixed issues with calling jobclient.close(). Both fixes are included in Hadoop 2.6.4.
However, since we also need to pick up YARN-5309, we need to wait for a new release of Hadoop.

</description>
			<version>1.2.1</version>
			<fixedVersion>1.3.0, 1.2.2, 2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="is blocked by">5309</link>
			<link type="Reference" description="is related to">12766</link>
		</links>
	</bug>
	<bug id="14195" opendate="2016-07-08 15:50:11" fixdate="2016-07-13 04:38:17" resolution="Fixed">
		<buginformation>
			<summary>HiveMetaStoreClient getFunction() does not throw NoSuchObjectException</summary>
			<description>HiveMetaStoreClient getFunction(dbName, funcName) does not throw NoSuchObjectException when no function with funcName exists in the db. Instead, I need to search the MetaException message for &amp;amp;apos;NoSuchObjectException&amp;amp;apos;.</description>
			<version>2.2.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
		</fixedFiles>
	</bug>
	<bug id="13258" opendate="2016-03-10 02:50:44" fixdate="2016-07-13 16:53:03" resolution="Fixed">
		<buginformation>
			<summary>LLAP: Add hdfs bytes read and spilled bytes to tez print summary</summary>
			<description>When printing counters to console it will be useful to print hdfs bytes read and spilled bytes which will help with debugging issues faster. </description>
			<version>2.1.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.llap.counters.QueryFragmentCounters.java</file>
			<file type="M">org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.llap.LlapUtil.java</file>
			<file type="M">org.apache.hadoop.hive.llap.tezplugins.LlapTezUtils.java</file>
			<file type="M">org.apache.hadoop.hive.llap.daemon.impl.LlapTaskReporter.java</file>
			<file type="M">org.apache.hadoop.hive.llap.io.api.impl.LlapIoImpl.java</file>
			<file type="M">org.apache.hadoop.hive.ql.hooks.PostExecTezSummaryPrinter.java</file>
			<file type="M">org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
			<file type="M">org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
			<file type="M">org.apache.hadoop.hive.llap.counters.LlapIOCounters.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="is blocked by">3250</link>
			<link type="Reference" description="is related to">3290</link>
		</links>
	</bug>
	<bug id="14164" opendate="2016-07-05 21:33:32" fixdate="2016-07-13 21:00:42" resolution="Duplicate">
		<buginformation>
			<summary>JDBC: Add retry in JDBC driver when reading config values from ZK</summary>
			<description>Sometimes ZK may intermittently experience network partitioning. During this time, clients trying to open a JDBC connection get an exception. To improve user experience, we should implement a retry logic and fail after retrying.</description>
			<version>1.2.1</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.jdbc.HiveConnection.java</file>
			<file type="M">org.apache.hive.jdbc.Utils.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">13400</link>
		</links>
	</bug>
	<bug id="14215" opendate="2016-07-12 11:47:08" fixdate="2016-07-14 16:28:34" resolution="Fixed">
		<buginformation>
			<summary>Displaying inconsistent CPU usage data with MR execution engine</summary>
			<description>If the MR task is finished after printing the cumulative CPU time then there is the possibility to print inconsistent CPU usage information.
Correct one:

2016-07-12 11:31:42,961 Stage-3 map = 0%,  reduce = 0%
2016-07-12 11:31:48,237 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 2.5 sec
MapReduce Total cumulative CPU time: 2 seconds 500 msec
Ended Job = job_1468321038188_0003
MapReduce Jobs Launched: 
Stage-Stage-3: Map: 1   Cumulative CPU: 2.5 sec   HDFS Read: 5864 HDFS Write: 103 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 500 msec


One type of inconsistent data (easily reproducible one):

2016-07-12 11:39:00,540 Stage-3 map = 0%,  reduce = 0%
Ended Job = job_1468321038188_0004
MapReduce Jobs Launched: 
Stage-Stage-3: Map: 1   Cumulative CPU: 2.51 sec   HDFS Read: 5864 HDFS Write: 103 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 510 msec

</description>
			<version>2.2.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.java</file>
		</fixedFiles>
	</bug>
	<bug id="14074" opendate="2016-06-22 05:47:37" fixdate="2016-07-14 19:54:18" resolution="Fixed">
		<buginformation>
			<summary>RELOAD FUNCTION should update dropped functions</summary>
			<description>Due to HIVE-2573, functions are stored in a per-session registry and only loaded in from the metastore when hs2 or hive cli is started. Running RELOAD FUNCTION in the current session is a way to force a reload of the functions, so that changes that occurred in other running sessions will be reflected in the current session, without having to restart the current session. However, while functions that are created in other sessions will now appear in the current session, functions that have been dropped are not removed from the current session&amp;amp;apos;s registry. It seems inconsistent that created functions are updated while dropped functions are not.</description>
			<version>2.0.1</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
	</bug>
	<bug id="14222" opendate="2016-07-13 00:23:20" fixdate="2016-07-14 22:23:03" resolution="Fixed">
		<buginformation>
			<summary>PTF: Operator initialization does not clean state</summary>
			<description>PTFOperator::initializeOp() does not reset currentKeys to null.


      if (currentKeys != null &amp;amp;&amp;amp; !keysAreEqual) {
        ptfInvocation.finishPartition();
      }
....
      if (currentKeys == null) {
          currentKeys = newKeys.copyKey();
        } else {
          currentKeys.copyKey(newKeys);
        }

</description>
			<version>1.2.1</version>
			<fixedVersion>1.3.0, 2.2.0, 2.1.1, 2.0.2</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.PTFOperator.java</file>
		</fixedFiles>
	</bug>
	<bug id="13871" opendate="2016-05-27 02:01:42" fixdate="2016-07-14 23:11:04" resolution="Duplicate">
		<buginformation>
			<summary>Tez exec summary does not get the HIVE counters right</summary>
			<description>

2016-05-26T21:59:51,421 INFO  [main]: exec.Task (:()) - HIVE:
2016-05-26T21:59:51,421 INFO  [main]: exec.Task (:()) -    CREATED_FILES: 1
2016-05-26T21:59:51,421 INFO  [main]: exec.Task (:()) -    DESERIALIZE_ERRORS: 0
2016-05-26T21:59:51,421 INFO  [main]: exec.Task (:()) -    RECORDS_IN_Map_1: 0
2016-05-26T21:59:51,421 INFO  [main]: exec.Task (:()) -    RECORDS_IN_Map_4: 0
2016-05-26T21:59:51,421 INFO  [main]: exec.Task (:()) -    RECORDS_IN_Map_5: 0
2016-05-26T21:59:51,421 INFO  [main]: exec.Task (:()) -    RECORDS_IN_Map_6: 0
2016-05-26T21:59:51,421 INFO  [main]: exec.Task (:()) -    RECORDS_IN_Map_7: 0
2016-05-26T21:59:51,421 INFO  [main]: exec.Task (:()) -    RECORDS_OUT_0: 10
2016-05-26T21:59:51,421 INFO  [main]: exec.Task (:()) -    RECORDS_OUT_INTERMEDIATE_Map_1: 418284
2016-05-26T21:59:51,421 INFO  [main]: exec.Task (:()) -    RECORDS_OUT_INTERMEDIATE_Map_4: 27440
2016-05-26T21:59:51,421 INFO  [main]: exec.Task (:()) -    RECORDS_OUT_INTERMEDIATE_Map_5: 365
2016-05-26T21:59:51,421 INFO  [main]: exec.Task (:()) -    RECORDS_OUT_INTERMEDIATE_Map_6: 101
2016-05-26T21:59:51,421 INFO  [main]: exec.Task (:()) -    RECORDS_OUT_INTERMEDIATE_Map_7: 48000
2016-05-26T21:59:51,421 INFO  [main]: exec.Task (:()) -    RECORDS_OUT_INTERMEDIATE_Reducer_2: 10
2016-05-26T21:59:51,421 INFO  [main]: exec.Task (:()) - Shuffle Errors:


However, the actual operator counters do indicate the total # of vectors.


2016-05-26T21:59:51,421 INFO  [main]: exec.Task (:()) - TaskCounter_Map_1_INPUT_Map_4:
2016-05-26T21:59:51,421 INFO  [main]: exec.Task (:()) -    FIRST_EVENT_RECEIVED: 1
2016-05-26T21:59:51,421 INFO  [main]: exec.Task (:()) -    INPUT_RECORDS_PROCESSED: 27440

</description>
			<version>2.1.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">14122</link>
		</links>
	</bug>
	<bug id="14238" opendate="2016-07-14 09:19:36" fixdate="2016-07-15 01:32:07" resolution="Duplicate">
		<buginformation>
			<summary>Ownership shouldn&amp;apos;t be checked if external table location doesn&amp;apos;t exist</summary>
			<description>When creating external table with SQL authorization, we require RWX permission + ownership of the table location. If the location doesn&amp;amp;apos;t exist, we check on parent dir (recursively), which means we require the user owns everything under parent dir. I think this is not necessary - we don&amp;amp;apos;t have to check ownership of parent dir, or we just check non-recursively.</description>
			<version>0.14.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils.java</file>
			<file type="M">org.apache.hadoop.hive.common.FileUtils.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">10022</link>
		</links>
	</bug>
	<bug id="14144" opendate="2016-07-01 07:20:08" fixdate="2016-07-15 08:43:01" resolution="Fixed">
		<buginformation>
			<summary>Permanent functions are showing up in show functions, but describe says it doesn&amp;apos;t exist</summary>
			<description></description>
			<version>2.1.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.util.ResourceDownloader.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Registry.java</file>
		</fixedFiles>
		<links>
			<link type="Regression" description="is broken by">13903</link>
		</links>
	</bug>
	<bug id="14234" opendate="2016-07-14 00:59:08" fixdate="2016-07-15 15:22:34" resolution="Fixed">
		<buginformation>
			<summary>TestHiveMetaStorePartitionSpecs does not drop database created in this test causes other test failure</summary>
			<description>TestHiveMetaStorePartitionSpecs creates a database named testpartitionspecs_db, but never drop it, sometimes causes TestObjectStore#testDatabaseOps failed:


testDatabaseOps(org.apache.hadoop.hive.metastore.TestObjectStore)  Time elapsed: 0.188 sec  &amp;lt;&amp;lt;&amp;lt; FAILURE!
java.lang.AssertionError: expected:&amp;lt;2&amp;gt; but was:&amp;lt;3&amp;gt;
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:743)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:555)
	at org.junit.Assert.assertEquals(Assert.java:542)
	at org.apache.hadoop.hive.metastore.TestObjectStore.testDatabaseOps(TestObjectStore.java:120)

</description>
			<version>1.3.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.TestObjectStore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.TestHiveMetaStorePartitionSpecs.java</file>
		</fixedFiles>
	</bug>
	<bug id="14004" opendate="2016-06-13 17:50:20" fixdate="2016-07-15 16:45:47" resolution="Fixed">
		<buginformation>
			<summary>Minor compaction produces ArrayIndexOutOfBoundsException: 7 in SchemaEvolution.getFileType</summary>
			<description>Easiest way to repro is to add TestTxnCommands2

  @Test
  public void testCompactWithDelete() throws Exception {
    int[][] tableData = {{1,2},{3,4}};
    runStatementOnDriver("insert into " + Table.ACIDTBL + "(a,b) " + makeValuesClause(tableData));
    runStatementOnDriver("alter table "+ Table.ACIDTBL + " compact &amp;amp;apos;MAJOR&amp;amp;apos;");
    Worker t = new Worker();
    t.setThreadId((int) t.getId());
    t.setHiveConf(hiveConf);
    AtomicBoolean stop = new AtomicBoolean();
    AtomicBoolean looped = new AtomicBoolean();
    stop.set(true);
    t.init(stop, looped);
    t.run();
    runStatementOnDriver("delete from " + Table.ACIDTBL + " where b = 4");
    runStatementOnDriver("update " + Table.ACIDTBL + " set b = -2 where b = 2");
    runStatementOnDriver("alter table "+ Table.ACIDTBL + " compact &amp;amp;apos;MINOR&amp;amp;apos;");
    t.run();
  }


to TestTxnCommands2 and run it.
Test won&amp;amp;apos;t fail but if you look 
in target/tmp/log/hive.log for the following exception (from Minor compaction).

2016-06-09T18:36:39,071 WARN  [Thread-190[]]: mapred.LocalJobRunner (LocalJobRunner.java:run(560)) - job_local1233973168_0005
java.lang.Exception: java.lang.ArrayIndexOutOfBoundsException: 7
        at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462) ~[hadoop-mapreduce-client-common-2.6.1.jar:?]
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522) [hadoop-mapreduce-client-common-2.6.1.jar:?]
Caused by: java.lang.ArrayIndexOutOfBoundsException: 7
        at org.apache.orc.impl.SchemaEvolution.getFileType(SchemaEvolution.java:67) ~[hive-orc-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.orc.impl.TreeReaderFactory.createTreeReader(TreeReaderFactory.java:2031) ~[hive-orc-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.orc.impl.TreeReaderFactory$StructTreeReader.&amp;lt;init&amp;gt;(TreeReaderFactory.java:1716) ~[hive-orc-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.orc.impl.TreeReaderFactory.createTreeReader(TreeReaderFactory.java:2077) ~[hive-orc-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.orc.impl.TreeReaderFactory$StructTreeReader.&amp;lt;init&amp;gt;(TreeReaderFactory.java:1716) ~[hive-orc-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.orc.impl.TreeReaderFactory.createTreeReader(TreeReaderFactory.java:2077) ~[hive-orc-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.orc.impl.RecordReaderImpl.&amp;lt;init&amp;gt;(RecordReaderImpl.java:208) ~[hive-orc-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.&amp;lt;init&amp;gt;(RecordReaderImpl.java:63) ~[classes/:?]
        at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.rowsOptions(ReaderImpl.java:365) ~[classes/:?]
        at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger$ReaderPair.&amp;lt;init&amp;gt;(OrcRawRecordMerger.java:207) ~[classes/:?]
        at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.&amp;lt;init&amp;gt;(OrcRawRecordMerger.java:508) ~[classes/:?]
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRawReader(OrcInputFormat.java:1977) ~[classes/:?]
        at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:630) ~[classes/:?]
        at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:609) ~[classes/:?]
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54) ~[hadoop-mapreduce-client-core-2.6.1.jar:?]
        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450) ~[hadoop-mapreduce-client-core-2.6.1.jar:?]
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343) ~[hadoop-mapreduce-client-core-2.6.1.jar:?]
        at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243) ~[hadoop-mapreduce-client-common-2.6.1.jar:?]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[?:1.7.0_71]
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[?:1.7.0_71]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[?:1.7.0_71]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[?:1.7.0_71]
        at java.lang.Thread.run(Thread.java:745) ~[?:1.7.0_71]


I observed the same on a real cluster.
Based on my observations, running Major compaction instead of minor, works fine.
Replacing the DELETE operation with update, makes both Major/Minor run fine.
The issue itself should be addressed by HIVE-13974 but need to make sure to add the test.</description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
			<file type="M">org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">13432</link>
		</links>
	</bug>
	<bug id="14241" opendate="2016-07-14 20:04:21" fixdate="2016-07-15 19:20:07" resolution="Fixed">
		<buginformation>
			<summary>Acid clashes with ConfVars.HIVEFETCHTASKCONVERSION &lt;&gt; "none"</summary>
			<description>Some queries are optimized so as not to create an MR job. This somehow causes the Configuration object in FetchOperator to be passed to the operator before Driver.recordValidTxns() is called. So then to this op it looks like there are no valid txns and it returns nothing.</description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FetchTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
			<file type="M">org.apache.hive.hcatalog.streaming.TestStreaming.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">14250</link>
		</links>
	</bug>
	<bug id="14244" opendate="2016-07-14 23:54:46" fixdate="2016-07-16 18:14:07" resolution="Fixed">
		<buginformation>
			<summary>bucketmap right outer join query throws ArrayIndexOutOfBoundsException</summary>
			<description>bucketmap right outer join on partitioned bucketed table throws this error:

Vertex failed, vertexName=Map 1, vertexId=vertex_1466710232033_0539_6_00, diagnostics=[Task failed, taskId=task_1466710232033_0539_6_00_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Error while running task ( failure ) : attempt_1466710232033_0539_6_00_000000_0:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:211)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:95)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:70)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:393)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:185)
	... 14 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:850)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:86)
	... 17 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ArrayIndexOutOfBoundsException: -1
	at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.process(ReduceSinkOperator.java:416)
	at org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.process(VectorReduceSinkOperator.java:104)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:879)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:130)
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:762)
	... 18 more
Caused by: java.lang.ArrayIndexOutOfBoundsException: -1
	at org.apache.tez.runtime.library.common.writers.UnorderedPartitionedKVWriter.write(UnorderedPartitionedKVWriter.java:314)
	at org.apache.tez.runtime.library.common.writers.UnorderedPartitionedKVWriter.write(UnorderedPartitionedKVWriter.java:257)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor$TezKVOutputCollector.collect(TezProcessor.java:253)
	at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.collect(ReduceSinkOperator.java:552)
	at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.process(ReduceSinkOperator.java:398)
	... 22 more
], TaskAttempt 1 failed, info=[Error: Error while running task ( failure ) : attempt_1466710232033_0539_6_00_000000_1:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:211)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:95)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:70)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:393)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:185)
	... 14 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:850)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:86)
	... 17 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ArrayIndexOutOfBoundsException: -1
	at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.process(ReduceSinkOperator.java:416)
	at org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.process(VectorReduceSinkOperator.java:104)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:879)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:130)
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:762)
	... 18 more
Caused by: java.lang.ArrayIndexOutOfBoundsException: -1
	at org.apache.tez.runtime.library.common.writers.UnorderedPartitionedKVWriter.write(UnorderedPartitionedKVWriter.java:314)
	at org.apache.tez.runtime.library.common.writers.UnorderedPartitionedKVWriter.write(UnorderedPartitionedKVWriter.java:257)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor$TezKVOutputCollector.collect(TezProcessor.java:253)
	at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.collect(ReduceSinkOperator.java:552)
	at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.process(ReduceSinkOperator.java:398)
	... 22 more
], TaskAttempt 2 failed, info=[Error: Error while running task ( failure ) : attempt_1466710232033_0539_6_00_000000_2:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:211)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:95)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:70)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:393)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:185)
	... 14 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:850)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:86)
	... 17 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ArrayIndexOutOfBoundsException: -1
	at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.process(ReduceSinkOperator.java:416)
	at org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.process(VectorReduceSinkOperator.java:104)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:879)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:130)
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:762)
	... 18 more
Caused by: java.lang.ArrayIndexOutOfBoundsException: -1
	at org.apache.tez.runtime.library.common.writers.UnorderedPartitionedKVWriter.write(UnorderedPartitionedKVWriter.java:314)
	at org.apache.tez.runtime.library.common.writers.UnorderedPartitionedKVWriter.write(UnorderedPartitionedKVWriter.java:257)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor$TezKVOutputCollector.collect(TezProcessor.java:253)
	at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.collect(ReduceSinkOperator.java:552)
	at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.process(ReduceSinkOperator.java:398)
	... 22 more
], TaskAttempt 3 failed, info=[Error: Error while running task ( failure ) : attempt_1466710232033_0539_6_00_000000_3:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:211)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:95)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:70)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:393)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:185)
	... 14 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:850)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:86)
	... 17 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ArrayIndexOutOfBoundsException: -1
	at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.process(ReduceSinkOperator.java:416)
	at org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.process(VectorReduceSinkOperator.java:104)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:879)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:130)
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:762)
	... 18 more
Caused by: java.lang.ArrayIndexOutOfBoundsException: -1
	at org.apache.tez.runtime.library.common.writers.UnorderedPartitionedKVWriter.write(UnorderedPartitionedKVWriter.java:314)
	at org.apache.tez.runtime.library.common.writers.UnorderedPartitionedKVWriter.write(UnorderedPartitionedKVWriter.java:257)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor$TezKVOutputCollector.collect(TezProcessor.java:253)
	at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.collect(ReduceSinkOperator.java:552)
	at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.process(ReduceSinkOperator.java:398)
	... 22 more
]], Vertex did not succeed due to OWN_TASK_FAILURE, failedTasks:1 killedTasks:1, Vertex vertex_1466710232033_0539_6_00 [Map 1] killed/failed due to:OWN_TASK_FAILURE]
DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0
FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1466710232033_0539_6_00, diagnostics=[Task failed, taskId=task_1466710232033_0539_6_00_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Error while running task ( failure ) : attempt_1466710232033_0539_6_00_000000_0:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:211)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:95)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:70)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:393)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:185)
	... 14 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:850)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:86)
	... 17 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ArrayIndexOutOfBoundsException: -1
	at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.process(ReduceSinkOperator.java:416)
	at org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.process(VectorReduceSinkOperator.java:104)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:879)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:130)
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:762)
	... 18 more
Caused by: java.lang.ArrayIndexOutOfBoundsException: -1
	at org.apache.tez.runtime.library.common.writers.UnorderedPartitionedKVWriter.write(UnorderedPartitionedKVWriter.java:314)
	at org.apache.tez.runtime.library.common.writers.UnorderedPartitionedKVWriter.write(UnorderedPartitionedKVWriter.java:257)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor$TezKVOutputCollector.collect(TezProcessor.java:253)
	at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.collect(ReduceSinkOperator.java:552)
	at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.process(ReduceSinkOperator.java:398)
	... 22 more
], TaskAttempt 1 failed, info=[Error: Error while running task ( failure ) : attempt_1466710232033_0539_6_00_000000_1:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:211)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:95)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:70)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:393)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:185)
	... 14 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:850)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:86)
	... 17 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ArrayIndexOutOfBoundsException: -1
	at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.process(ReduceSinkOperator.java:416)
	at org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.process(VectorReduceSinkOperator.java:104)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:879)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:130)
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:762)
	... 18 more
Caused by: java.lang.ArrayIndexOutOfBoundsException: -1
	at org.apache.tez.runtime.library.common.writers.UnorderedPartitionedKVWriter.write(UnorderedPartitionedKVWriter.java:314)
	at org.apache.tez.runtime.library.common.writers.UnorderedPartitionedKVWriter.write(UnorderedPartitionedKVWriter.java:257)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor$TezKVOutputCollector.collect(TezProcessor.java:253)
	at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.collect(ReduceSinkOperator.java:552)
	at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.process(ReduceSinkOperator.java:398)
	... 22 more
], TaskAttempt 2 failed, info=[Error: Error while running task ( failure ) : attempt_1466710232033_0539_6_00_000000_2:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:211)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:95)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:70)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:393)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:185)
	... 14 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:850)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:86)
	... 17 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ArrayIndexOutOfBoundsException: -1
	at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.process(ReduceSinkOperator.java:416)
	at org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.process(VectorReduceSinkOperator.java:104)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:879)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:130)
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:762)
	... 18 more
Caused by: java.lang.ArrayIndexOutOfBoundsException: -1
	at org.apache.tez.runtime.library.common.writers.UnorderedPartitionedKVWriter.write(UnorderedPartitionedKVWriter.java:314)
	at org.apache.tez.runtime.library.common.writers.UnorderedPartitionedKVWriter.write(UnorderedPartitionedKVWriter.java:257)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor$TezKVOutputCollector.collect(TezProcessor.java:253)
	at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.collect(ReduceSinkOperator.java:552)
	at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.process(ReduceSinkOperator.java:398)
	... 22 more
], TaskAttempt 3 failed, info=[Error: Error while running task ( failure ) : attempt_1466710232033_0539_6_00_000000_3:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:211)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:95)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:70)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:393)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:185)
	... 14 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:850)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:86)
	... 17 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ArrayIndexOutOfBoundsException: -1
	at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.process(ReduceSinkOperator.java:416)
	at org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.process(VectorReduceSinkOperator.java:104)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:879)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:130)
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:762)
	... 18 more
Caused by: java.lang.ArrayIndexOutOfBoundsException: -1
	at org.apache.tez.runtime.library.common.writers.UnorderedPartitionedKVWriter.write(UnorderedPartitionedKVWriter.java:314)
	at org.apache.tez.runtime.library.common.writers.UnorderedPartitionedKVWriter.write(UnorderedPartitionedKVWriter.java:257)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor$TezKVOutputCollector.collect(TezProcessor.java:253)
	at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.collect(ReduceSinkOperator.java:552)
	at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.process(ReduceSinkOperator.java:398)
	... 22 more
]], Vertex did not succeed due to OWN_TASK_FAILURE, failedTasks:1 killedTasks:1, Vertex vertex_1466710232033_0539_6_00 [Map 1] killed/failed due to:OWN_TASK_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0


Steps to reproduce the issue:

set hive.execution.engine=tez;
drop table if exists src_nonemptybucket_partitioned_1; 
create table src_nonemptybucket_partitioned_1 (name string, age int, gpa double) partitioned by (year int) clustered by (age) into 120 buckets stored as orc;
insert into table src_nonemptybucket_partitioned_1 partition(year=2015) select * from studenttab10k;
drop table if exists src_nonemptybucket_partitioned_2; 
create table src_nonemptybucket_partitioned_2 (name varchar(50), age bigint, gpa decimal(38,18)) partitioned by (year int) clustered by (age) into 240 buckets stored as orc;
insert into table src_nonemptybucket_partitioned_2 partition(year=2015) select * from studenttab10k ;

set hive.optimize.bucketmapjoin=true;
set hive.convert.join.bucket.mapjoin.tez=true;

select /*+ MAPJOIN(e2) */ e1.name as e1_name, e1.age as e1_age, e1.gpa as e1_gpa, e2.name as e2_name, e2.age as e2_age, e2.gpa as e2_gpa from src_nonemptybucket_partitioned_1 e1 right outer join src_nonemptybucket_partitioned_2 e2 on e1.age = e2.age where e1.year = 2015 and e2.year = 2016;

</description>
			<version>1.3.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex.java</file>
		</fixedFiles>
	</bug>
	<bug id="11863" opendate="2015-09-17 07:01:18" fixdate="2016-07-16 19:12:39" resolution="Fixed">
		<buginformation>
			<summary>FS based stats collection generates wrong results for tez (for union queries)</summary>
			<description>FS based stats collection is the default way to collect stats. However, there are some cases (involving unions) where it generates wrong results. Refer test case in HIVE-11860 and compare test cli driver results against tez results. Also it will be good to extend statsfs.q test case with union queries.</description>
			<version>1.3.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.FileSinkDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.stats.StatsCollectionContext.java</file>
			<file type="M">org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">11860</link>
			<link type="Duplicate" description="duplicates">14236</link>
			<link type="Reference" description="relates to">12065</link>
		</links>
	</bug>
	<bug id="13191" opendate="2016-03-01 19:29:23" fixdate="2016-07-17 17:16:14" resolution="Fixed">
		<buginformation>
			<summary>DummyTable map joins mix up columns between tables</summary>
			<description>

SELECT
  a.key,
  a.a_one,
  b.b_one,
  a.a_zero,
  b.b_zero
FROM
(
    SELECT
      11 key,
      0 confuse_you,
      1 a_one,
      0 a_zero
) a
LEFT JOIN
(
    SELECT
      11 key,
      0 confuse_you,
      1 b_one,
      0 b_zero
) b
ON a.key = b.key
;

11      1       0       0       1


This should be 11, 1, 1, 0, 0 instead. 
Disabling map-joins &amp;amp; using shuffle-joins returns the right result.</description>
			<version>2.0.0</version>
			<fixedVersion>1.3.0, 2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.java</file>
		</fixedFiles>
	</bug>
	<bug id="14236" opendate="2016-07-14 04:42:02" fixdate="2016-07-17 19:31:27" resolution="Fixed">
		<buginformation>
			<summary>CTAS with UNION ALL puts the wrong stats in Tez</summary>
			<description>to repo. in Tez, create table t as select * from src union all select * from src;</description>
			<version>2.0.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.FileSinkDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.stats.StatsCollectionContext.java</file>
			<file type="M">org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">11863</link>
		</links>
	</bug>
	<bug id="14248" opendate="2016-07-15 08:46:20" fixdate="2016-07-18 11:07:26" resolution="Duplicate">
		<buginformation>
			<summary>query with view in union adds underlying table as direct input</summary>
			<description>In the following case,


create view V as select * from T;
select * from V union all select * from V


The semantic analyzer inputs contain input table T as a direct input instead of adding it as an indirect input.</description>
			<version>1.3.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">13991</link>
		</links>
	</bug>
	<bug id="14135" opendate="2016-06-29 23:59:39" fixdate="2016-07-18 16:13:56" resolution="Fixed">
		<buginformation>
			<summary>beeline output not formatted correctly for large column widths</summary>
			<description>If the column width is too large then beeline uses the maximum column width when normalizing all the column widths. In order to reproduce the issue, run set -v; 
Once the configuration variables is classpath which can be extremely large width (41k characters in my environment).</description>
			<version>2.2.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.beeline.BufferedRows.java</file>
			<file type="M">org.apache.hive.beeline.BeeLineOpts.java</file>
		</fixedFiles>
	</bug>
	<bug id="14258" opendate="2016-07-16 05:06:09" fixdate="2016-07-18 20:51:35" resolution="Fixed">
		<buginformation>
			<summary>Reduce task timed out because CommonJoinOperator.genUniqueJoinObject took too long to finish without reporting progress</summary>
			<description>Reduce task timed out because CommonJoinOperator.genUniqueJoinObject took too long to finish without reporting progress.
This timeout happened when reducer.close() is called in ReduceTask.java.
CommonJoinOperator.genUniqueJoinObject() called by reducer.close() will loop over every row in the AbstractRowContainer. This can take a long time if there are a large number or rows, and during this time, it does not report progress. If this runs for long enough more than "mapreduce.task.timeout", ApplicationMaster will kill the task for failing to report progress.
we configured "mapreduce.task.timeout" as 10 minutes. I captured the stack trace in the 10 minutes before AM killed the reduce task at 2016-07-15 07:19:11.
The following three stack traces can prove it:
at 2016-07-15 07:09:42:


"main" prio=10 tid=0x00007f90ec017000 nid=0xd193 runnable [0x00007f90f62e5000]
   java.lang.Thread.State: RUNNABLE
        at java.io.FileInputStream.readBytes(Native Method)
        at java.io.FileInputStream.read(FileInputStream.java:272)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:154)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
        at java.io.BufferedInputStream.read1(BufferedInputStream.java:275)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
        - locked &amp;lt;0x00000007deecefb0&amp;gt; (a org.apache.hadoop.fs.BufferedFSInputStream)
        at java.io.DataInputStream.read(DataInputStream.java:149)
        at org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:436)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:252)
        at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:276)
        at org.apache.hadoop.fs.FSInputChecker.fill(FSInputChecker.java:214)
        at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:232)
        at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:196)
        - locked &amp;lt;0x00000007deecb978&amp;gt; (a org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker)
        at java.io.DataInputStream.readFully(DataInputStream.java:195)
        at org.apache.hadoop.io.DataOutputBuffer$Buffer.write(DataOutputBuffer.java:70)
        at org.apache.hadoop.io.DataOutputBuffer.write(DataOutputBuffer.java:120)
        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2359)
        - locked &amp;lt;0x00000007deec8f70&amp;gt; (a org.apache.hadoop.io.SequenceFile$Reader)
        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2491)
        - locked &amp;lt;0x00000007deec8f70&amp;gt; (a org.apache.hadoop.io.SequenceFile$Reader)
        at org.apache.hadoop.mapred.SequenceFileRecordReader.next(SequenceFileRecordReader.java:82)
        - locked &amp;lt;0x00000007deec82f0&amp;gt; (a org.apache.hadoop.mapred.SequenceFileRecordReader)
        at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.nextBlock(RowContainer.java:360)
        at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.next(RowContainer.java:267)
        at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.next(RowContainer.java:74)
        at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genUniqueJoinObject(CommonJoinOperator.java:644)
        at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject(CommonJoinOperator.java:750)
        at org.apache.hadoop.hive.ql.exec.JoinOperator.endGroup(JoinOperator.java:256)
        at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.close(ExecReducer.java:284)
        at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:453)
        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)


at 2016-07-15 07:15:35


"main" prio=10 tid=0x00007f90ec017000 nid=0xd193 runnable [0x00007f90f62e5000]
   java.lang.Thread.State: RUNNABLE
        at java.util.zip.CRC32.updateBytes(Native Method)
        at java.util.zip.CRC32.update(CRC32.java:65)
        at org.apache.hadoop.fs.FSInputChecker.verifySums(FSInputChecker.java:316)
        at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:279)
        at org.apache.hadoop.fs.FSInputChecker.fill(FSInputChecker.java:214)
        at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:232)
        at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:196)
        - locked &amp;lt;0x00000007d68db510&amp;gt; (a org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker)
        at java.io.DataInputStream.readFully(DataInputStream.java:195)
        at org.apache.hadoop.io.DataOutputBuffer$Buffer.write(DataOutputBuffer.java:70)
        at org.apache.hadoop.io.DataOutputBuffer.write(DataOutputBuffer.java:120)
        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2359)
        - locked &amp;lt;0x00000007d68d8b68&amp;gt; (a org.apache.hadoop.io.SequenceFile$Reader)
        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2491)
        - locked &amp;lt;0x00000007d68d8b68&amp;gt; (a org.apache.hadoop.io.SequenceFile$Reader)
        at org.apache.hadoop.mapred.SequenceFileRecordReader.next(SequenceFileRecordReader.java:82)
        - locked &amp;lt;0x00000007d68d7f08&amp;gt; (a org.apache.hadoop.mapred.SequenceFileRecordReader)
        at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.nextBlock(RowContainer.java:360)
        at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.next(RowContainer.java:267)
        at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.next(RowContainer.java:74)
        at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genUniqueJoinObject(CommonJoinOperator.java:644)
        at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject(CommonJoinOperator.java:750)
        at org.apache.hadoop.hive.ql.exec.JoinOperator.endGroup(JoinOperator.java:256)
        at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.close(ExecReducer.java:284)
        at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:453)
        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)


at 2016-07-15 07:19:10


"main" prio=10 tid=0x00007f90ec017000 nid=0xd193 runnable [0x00007f90f62e5000]
   java.lang.Thread.State: RUNNABLE
        at java.io.FileInputStream.readBytes(Native Method)
        at java.io.FileInputStream.read(FileInputStream.java:272)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:154)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
        at java.io.BufferedInputStream.read1(BufferedInputStream.java:275)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
        - locked &amp;lt;0x00000007df731218&amp;gt; (a org.apache.hadoop.fs.BufferedFSInputStream)
        at java.io.DataInputStream.read(DataInputStream.java:149)
        at org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:436)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:252)
        at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:276)
        at org.apache.hadoop.fs.FSInputChecker.fill(FSInputChecker.java:214)
        at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:232)
        at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:196)
        - locked &amp;lt;0x00000007df72dc20&amp;gt; (a org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker)
        at java.io.DataInputStream.readFully(DataInputStream.java:195)
        at org.apache.hadoop.io.DataOutputBuffer$Buffer.write(DataOutputBuffer.java:70)
        at org.apache.hadoop.io.DataOutputBuffer.write(DataOutputBuffer.java:120)
        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2359)
        - locked &amp;lt;0x00000007df72b278&amp;gt; (a org.apache.hadoop.io.SequenceFile$Reader)
        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2491)
        - locked &amp;lt;0x00000007df72b278&amp;gt; (a org.apache.hadoop.io.SequenceFile$Reader)
        at org.apache.hadoop.mapred.SequenceFileRecordReader.next(SequenceFileRecordReader.java:82)
        - locked &amp;lt;0x00000007df72a618&amp;gt; (a org.apache.hadoop.mapred.SequenceFileRecordReader)
        at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.nextBlock(RowContainer.java:360)
        at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.nextBlock(RowContainer.java:373)
        at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.next(RowContainer.java:267)
        at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.next(RowContainer.java:74)
        at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genUniqueJoinObject(CommonJoinOperator.java:644)
        at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject(CommonJoinOperator.java:750)
        at org.apache.hadoop.hive.ql.exec.JoinOperator.endGroup(JoinOperator.java:256)
        at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.close(ExecReducer.java:284)
        at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:453)
        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)


You can see all three stack traces show CommonJoinOperator.genUniqueJoinObject was called by ExecReducer.close.</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
		</fixedFiles>
	</bug>
	<bug id="13369" opendate="2016-03-28 19:56:49" fixdate="2016-07-18 21:55:16" resolution="Fixed">
		<buginformation>
			<summary>AcidUtils.getAcidState() is not paying attention toValidTxnList when choosing the "best" base file</summary>
			<description>The JavaDoc on getAcidState() reads, in part:
"Note that because major compactions don&amp;amp;apos;t
   preserve the history, we can&amp;amp;apos;t use a base directory that includes a
   transaction id that we must exclude."
which is correct but there is nothing in the code that does this.
And if we detect a situation where txn X must be excluded but and there are deltas that contain X, we&amp;amp;apos;ll have to abort the txn.  This can&amp;amp;apos;t (reasonably) happen with auto commit mode, but with multi statement txns it&amp;amp;apos;s possible.
Suppose some long running txn starts and lock in snapshot at 17 (HWM).  An hour later it decides to access some partition for which all txns &amp;lt; 20 (for example) have already been compacted (i.e. GC&amp;amp;apos;d).  
==========================================================
Here is a more concrete example.  Let&amp;amp;apos;s say the file for table A are as follows and created in the order listed.
delta_4_4
delta_5_5
delta_4_5
base_5
delta_16_16
delta_17_17
base_17  (for example user ran major compaction)
let&amp;amp;apos;s say getAcidState() is called with ValidTxnList(20:16), i.e. with HWM=20 and ExceptionList=&amp;lt;16&amp;gt;
Assume that all txns &amp;lt;= 20 commit.
Reader can&amp;amp;apos;t use base_17 because it has result of txn16.  So it should chose base_5 "TxnBase bestBase" in getChildState().
Then the reset of the logic in getAcidState() should choose delta_16_16 and delta_17_17 in Directory object.  This would represent acceptable snapshot for such reader.
The issue is if at the same time the Cleaner process is running.  It will see everything with txnid&amp;lt;17 as obsolete.  Then it will check lock manger state and decide to delete (as there may not be any locks in LM for table A).  The order in which the files are deleted is undefined right now.  It may delete delta_16_16 and delta_17_17 first and right at this moment the read request with ValidTxnList(20:16) arrives (such snapshot may have bee locked in by some multi-stmt txn that started some time ago.  It acquires locks after the Cleaner checks LM state and calls getAcidState(). This request will choose base_5 but it won&amp;amp;apos;t see delta_16_16 and delta_17_17 and thus return the snapshot w/o modifications made by those txns.
[This is not possible currently since we only support autoCommit=true.  The reason is the a query (0) opens txn (if appropriate), (1) acquires locks, (2) locks in the snapshot.  The cleaner won't delete anything for a given compaction (partition) if there are locks on it.  Thus for duration of the transaction, nothing will be deleted so it's safe to use base_5]
This is a subtle race condition but possible.
1. So the safest thing to do to ensure correctness is to use the latest base_x as the "best" and check against exceptions in ValidTxnList and throw an exception if there is an exception &amp;lt;=x.
2. A better option is to keep 2 exception lists: aborted and open and only throw if there is an open txn &amp;lt;=x.  Compaction throws away data from aborted txns and thus there is no harm using base with aborted txns in its range.
3. You could make each txn record the lowest open txn id at its start and prevent the cleaner from cleaning anything delta with id range that includes this open txn id for any txn that is still running.  This has a drawback of potentially delaying GC of old files for arbitrarily long periods.  So this should be a user config choice.   The implementation is not trivial.
I would go with 1 now and do 2/3 together with multi-statement txn work.
Side note:  if 2 deltas have overlapping ID range, then 1 must be a subset of the other
A more concrete example (autoCommit=true)  (given 2.1 codebase)
Suppose base_2 exists.
1. lock table T
2. Lock in the snapshot, txnid 3 open, hwm 5.
3. a long GC pause or more practically the query is submitted but there are no resources to start App Master which is where getAcidState() is called from.
4. getAcidState() is called
It&amp;amp;apos;s not unreasonable that during #3 txnid 3 commits and base_5 is produced and is seen in #4.
</description>
			<version>1.0.0</version>
			<fixedVersion>1.3.0, 2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.txn.AcidOpenTxnsCounterService.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.TestAcidUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.ErrorMsg.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
		</fixedFiles>
		<links>
			<link type="Cloners" description="is cloned by">14211</link>
			<link type="Required" description="is required by">14350</link>
		</links>
	</bug>
	<bug id="11918" opendate="2015-09-22 20:18:14" fixdate="2016-07-19 00:23:50" resolution="Fixed">
		<buginformation>
			<summary>Implement/Enable constant related optimization rules in Calcite</summary>
			<description>Right now, Hive optimizer (Calcite) is short of the constant related optimization rules. For example, constant folding, constant propagation and constant transitive rules. Although Hive later provides those rules in the logical optimizer, we would like to implement those inside Calcite. This will benefit the current optimization as well as the optimization based on return path that we are planning to use in the future. This JIRA is the umbrella JIRA to implement/enable those rules.</description>
			<version>2.0.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.QTestUtil.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">13608</link>
			<link type="Reference" description="is related to">902</link>
			<link type="Reference" description="is related to">909</link>
			<link type="Reference" description="is related to">11110</link>
			<link type="Reference" description="is related to">9132</link>
		</links>
	</bug>
	<bug id="13883" opendate="2016-05-28 02:18:32" fixdate="2016-07-19 06:06:47" resolution="Fixed">
		<buginformation>
			<summary>WebHCat leaves token crc file never gets deleted</summary>
			<description>In one of our long run environment, there are thousands of /tmp/.templeton*.tmp.crc files, 

omm@szxciitslx17645:/&amp;gt; ll /tmp/.templeton*.tmp.crc 
...
rw-rr- 1 omm  wheel 12 May 26 18:15 /tmp/.templeton6676048390600607654.tmp.crc
rw-rr- 1 omm  wheel 12 May 26 18:14 /tmp/.templeton2733383617337556503.tmp.crc
rw-rr- 1 omm  wheel 12 May 26 18:12 /tmp/.templeton2183121761801669064.tmp.crc
rw-rr- 1 omm  wheel 12 May 26 18:11 /tmp/.templeton2689764046140543879.tmp.crc
...

omm@szxciitslx17645:/&amp;gt; ll /tmp/.templeton*.tmp.crc  | wc -l
17986
It&amp;amp;apos;s created by webhcat, https://github.com/apache/hive/blob/master/hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/SecureProxySupport.java#L193  and never gets deleted https://github.com/apache/hive/blob/master/hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/SecureProxySupport.java#L110</description>
			<version>1.1.1</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.hcatalog.templeton.SecureProxySupport.java</file>
		</fixedFiles>
	</bug>
	<bug id="14254" opendate="2016-07-15 21:05:35" fixdate="2016-07-19 16:39:23" resolution="Fixed">
		<buginformation>
			<summary>Correct the hive version by changing "svn" to "git"</summary>
			<description>When running "hive --version", "subversion" is displayed below, which should be "git".
$ hive --version
Hive 2.1.0-SNAPSHOT
Subversion git://</description>
			<version>2.1.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.common.util.HiveVersionInfo.java</file>
			<file type="M">org.apache.hive.common.HiveVersionAnnotation.java</file>
		</fixedFiles>
	</bug>
	<bug id="14282" opendate="2016-07-19 17:32:34" fixdate="2016-07-20 22:20:09" resolution="Fixed">
		<buginformation>
			<summary>HCatLoader ToDate() exception with hive partition table ,partitioned by column of DATE datatype</summary>
			<description>ToDate() function doesnt work with a partitioned table, partitioned by the column of DATE Datatype.
Below are the steps I followed to recreate the problem.
--&amp;gt;Sample input file to hive table :
hdfs@testhost ~$ cat test.log 
2012-06-13,16:11:17,574,140.134.127.109,SearchPage,Google.com,Win8,5,HTC
2012-06-13,16:11:17,466,43.176.108.158,Electronics,Google.com,Win8,3,iPhone
2012-06-13,16:11:17,501,97.73.102.79,Appliances,Google.com,Android,4,iPhone
2012-06-13,16:11:17,469,166.98.157.122,Recommendations,Google.com,Win8,5,HTC
2012-06-13,16:11:17,557,36.159.147.50,Sporting,Google.com,Win8,3,Samsung
2012-06-13,16:11:17,449,128.215.122.234,ShoppingCart,Google.com,Win8,5,HTC
2012-06-13,16:11:17,502,46.81.131.92,Electronics,Google.com,Android,5,Samsung
2012-06-13,16:11:17,554,120.187.105.127,Automotive,Google.com,Win8,5,HTC
2012-06-13,16:11:17,447,127.94.64.59,DetailPage,Google.com,Win8,3,Samsung
2012-06-13,16:11:17,490,132.54.25.75,ShoppingCart,Google.com,Win8,3,iPhone
2012-06-13,16:11:17,578,79.201.53.179,Automotive,Google.com,Win8,5,Samsung
2012-06-13,16:11:17,435,158.106.164.38,HomePage,Google.com,Web,5,Chrome
2012-06-13,16:11:17,523,17.131.82.171,Recommendations,Google.com,Web,3,IE9
2012-06-13,16:11:17,575,178.95.126.105,Appliances,Google.com,iOS,3,iPhone
2012-06-13,16:11:17,468,225.143.39.176,SearchPage,Google.com,iOS,5,HTC
2012-06-13,16:11:17,511,43.103.102.147,ShoppingCart,Google.com,iOS,5,Samsung
--&amp;gt; Copied to hdfs directory:
hdfs@testhost ~$ hdfs dfs -put -f test.log /user/hdfs/
--&amp;gt;Create partitoned table (partitioned with date data type column) in hive:
0: jdbc:hive2://hdp2.raghav.com:10000/default&amp;gt; create table mytable(Dt DATE,Time STRING,Number INT,IPAddr STRING,Type STRING,Site STRING,OSType STRING,Visit INT,PhModel STRING) row format delimited fields terminated by &amp;amp;apos;,&amp;amp;apos; stored as textfile;
0: jdbc:hive2://testhost.com:10000/default&amp;gt; load data inpath &amp;amp;apos;/user/hdfs/test.log&amp;amp;apos; overwrite into table mytable;
0: jdbc:hive2://testhost..com:10000/default&amp;gt; SET hive.exec.dynamic.partition = true;
0: jdbc:hive2://testhost.com:10000/default&amp;gt; SET hive.exec.dynamic.partition.mode = nonstrict;
0: jdbc:hive2://testhost.com:10000/default&amp;gt; create table partmytable(Number INT,IPAddr STRING,Type STRING,Site STRING,OSType STRING,Visit INT,PhModel STRING) partitioned by (Dt DATE,Time STRING) row format delimited fields terminated by &amp;amp;apos;,&amp;amp;apos; stored as textfile;
0: jdbc:hive2://testhost.com:10000/default&amp;gt; insert overwrite table partmytable partition(Dt,Time) select Number,IPAddr,Type,Site,OSType,Visit,PhModel,Dt,Time from mytable;
0: jdbc:hive2://hdp2.raghav.com:10000/default&amp;gt; describe partmytable;
--&amp;gt; Try to filter with ToDate function which fails with error:
hdfs@testhost ~$ pig -useHCatalog
grunt&amp;gt;
grunt&amp;gt; temp = LOAD &amp;amp;apos;partmytable&amp;amp;apos; using org.apache.hive.hcatalog.pig.HCatLoader();
grunt&amp;gt; temp1 = FILTER temp by dt == ToDate(&amp;amp;apos;2012-06-13&amp;amp;apos;,&amp;amp;apos;yyyy-MM-dd&amp;amp;apos;);
grunt&amp;gt; dump temp1;
--&amp;gt;Try to filter the normal table with same statement works;
grunt&amp;gt;
grunt&amp;gt; temp = LOAD &amp;amp;apos;mytable&amp;amp;apos; using org.apache.hive.hcatalog.pig.HCatLoader();
grunt&amp;gt; temp1 = FILTER temp by dt == ToDate(&amp;amp;apos;2012-06-13&amp;amp;apos;,&amp;amp;apos;yyyy-MM-dd&amp;amp;apos;);
grunt&amp;gt; dump temp1;
Workaround :
Use below statement instead of direct ToDate();
grunt&amp;gt;temp1 = FILTER temp5 by DaysBetween(dt,(datetime)ToDate(&amp;amp;apos;2012-06-13&amp;amp;apos;, &amp;amp;apos;yyyy-MM-dd&amp;amp;apos;)) &amp;gt;=(long)0 AND DaysBetween(dt,(datetime)ToDate(&amp;amp;apos;2012-06-13&amp;amp;apos;, &amp;amp;apos;yyyy-MM-dd&amp;amp;apos;)) &amp;lt;=(long)0;</description>
			<version>1.2.1</version>
			<fixedVersion>1.3.0, 2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.hcatalog.pig.HCatLoader.java</file>
			<file type="M">org.apache.hive.hcatalog.pig.TestHCatLoader.java</file>
		</fixedFiles>
	</bug>
	<bug id="14229" opendate="2016-07-13 20:03:59" fixdate="2016-07-21 15:24:21" resolution="Fixed">
		<buginformation>
			<summary>the jars in hive.aux.jar.paths are not added to session classpath </summary>
			<description>The jars in hive.reloadable.aux.jar.paths are being added to HiveServer2 classpath while hive.aux.jar.paths is not. 
Then the local task like &amp;amp;apos;select udf from src&amp;amp;apos; will fail to find needed udf class.</description>
			<version>2.0.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.session.SessionState.java</file>
			<file type="M">org.apache.hadoop.hive.ql.session.TestSessionState.java</file>
			<file type="M">org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			<file type="M">org.apache.hadoop.hive.ql.processors.ReloadProcessor.java</file>
		</fixedFiles>
	</bug>
	<bug id="14292" opendate="2016-07-19 23:54:43" fixdate="2016-07-22 17:15:46" resolution="Fixed">
		<buginformation>
			<summary>ACID table creation fails on mysql with MySQLIntegrityConstraintViolationException</summary>
			<description>While creating a ACID table ran into the following error:

&amp;gt;&amp;gt;&amp;gt;  create table acidcount1 (id int) 
clustered by (id) into 2 buckets 
stored as orc 
tblproperties(&amp;amp;apos;transactional&amp;amp;apos;=&amp;amp;apos;true&amp;amp;apos;);
INFO  : Compiling command(queryId=hive_20160719105944_bfe65377-59fa-4e17-941e-1f86b8daca15): create table acidcount1 (id int) 
clustered by (id) into 2 buckets 
stored as orc 
tblproperties(&amp;amp;apos;transactional&amp;amp;apos;=&amp;amp;apos;true&amp;amp;apos;)
INFO  : Semantic Analysis Completed
INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
INFO  : Completed compiling command(queryId=hive_20160719105944_bfe65377-59fa-4e17-941e-1f86b8daca15); Time taken: 0.111 seconds
Error: Error running query: java.lang.RuntimeException: Unable to lock &amp;amp;apos;CheckLock&amp;amp;apos; due to: Duplicate entry &amp;amp;apos;CheckLock-0&amp;amp;apos; for key &amp;amp;apos;PRIMARY&amp;amp;apos; (SQLState=23000, ErrorCode=1062) (state=,code=0)
Aborting command set because "force" is false and command failed: "create table acidcount1 (id int) 
clustered by (id) into 2 buckets 
stored as orc 
tblproperties(&amp;amp;apos;transactional&amp;amp;apos;=&amp;amp;apos;true&amp;amp;apos;);"


Saw the following detailed stack in the server log:

2016-07-19T10:59:46,213 ERROR [HiveServer2-Background-Pool: Thread-463]: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invokeInternal(196)) - java.lang.RuntimeException: Unable to lock &amp;amp;apos;CheckLock&amp;amp;apos; due to: Duplicate entry &amp;amp;apos;CheckLock-0&amp;amp;apos; for key &amp;amp;apos;PRIMARY&amp;amp;apos; (SQLState=23000, ErrorCode=1062)
        at org.apache.hadoop.hive.metastore.txn.TxnHandler.acquireLock(TxnHandler.java:3235)
        at org.apache.hadoop.hive.metastore.txn.TxnHandler.checkLock(TxnHandler.java:2309)
        at org.apache.hadoop.hive.metastore.txn.TxnHandler.checkLockWithRetry(TxnHandler.java:1012)
        at org.apache.hadoop.hive.metastore.txn.TxnHandler.lock(TxnHandler.java:784)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.lock(HiveMetaStore.java:5941)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:140)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:99)
        at com.sun.proxy.$Proxy26.lock(Unknown Source)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.lock(HiveMetaStoreClient.java:2109)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:154)
        at com.sun.proxy.$Proxy28.lock(Unknown Source)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2259)
        at com.sun.proxy.$Proxy28.lock(Unknown Source)
        at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager$SynchronizedMetaStoreClient.lock(DbTxnManager.java:740)
        at org.apache.hadoop.hive.ql.lockmgr.DbLockManager.lock(DbLockManager.java:103)
        at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.acquireLocks(DbTxnManager.java:341)
        at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.acquireLocksWithHeartbeatDelay(DbTxnManager.java:357)
        at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.acquireLocks(DbTxnManager.java:167)
        at org.apache.hadoop.hive.ql.Driver.acquireLocksAndOpenTxn(Driver.java:980)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1316)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1090)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1083)
        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:242)
        at org.apache.hive.service.cli.operation.SQLOperation.access$800(SQLOperation.java:91)
        at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:334)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
        at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:347)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Duplicate entry &amp;amp;apos;CheckLock-0&amp;amp;apos; for key &amp;amp;apos;PRIMARY&amp;amp;apos;
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
        at com.mysql.jdbc.Util.handleNewInstance(Util.java:377)
        at com.mysql.jdbc.Util.getInstance(Util.java:360)
        at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:971)
        at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3887)
        at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3823)
        at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2435)
        at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2582)
        at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2526)
        at com.mysql.jdbc.StatementImpl.executeUpdate(StatementImpl.java:1618)
        at com.mysql.jdbc.StatementImpl.executeUpdate(StatementImpl.java:1549)
        at com.jolbox.bonecp.StatementHandle.executeUpdate(StatementHandle.java:497)
        at org.apache.hadoop.hive.metastore.txn.TxnHandler.acquireLock(TxnHandler.java:3231)
        ... 47 more

</description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
		</fixedFiles>
	</bug>
	<bug id="14268" opendate="2016-07-18 15:43:09" fixdate="2016-07-22 18:29:21" resolution="Fixed">
		<buginformation>
			<summary>INSERT-OVERWRITE is not generating an INSERT event during hive replication</summary>
			<description>During Hive replication invoked from falcon, the source cluster did not generate appropriate INSERT events associated with the INSERT OVERWRITE, generating only an ALTER PARTITION event. However, an ALTER PARTITION is a metadata-only event, and thus, only metadata changes were replicated across, modifying the metadata of the destination, while not updating the data. </description>
			<version>2.2.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
			<file type="M">org.apache.hive.hcatalog.listener.TestDbNotificationListener.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">12907</link>
		</links>
	</bug>
	<bug id="13934" opendate="2016-06-02 23:45:00" fixdate="2016-07-22 20:33:44" resolution="Fixed">
		<buginformation>
			<summary>Configure Tez to make nocondiional task size memory available for the Processor</summary>
			<description>Currently, noconditionaltasksize is not validated against the container size, the reservations made in the container by Tez for Inputs / Outputs etc.
Check this at compile time to see if enough memory is available, or set up the vertex to reserve additional memory for the Processor.</description>
			<version>2.1.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.plan.BaseWork.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TestTezTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">13809</link>
			<link type="Blocker" description="is blocked by">3286</link>
			<link type="Reference" description="relates to">3353</link>
		</links>
	</bug>
	<bug id="14295" opendate="2016-07-20 11:43:00" fixdate="2016-07-22 21:50:30" resolution="Fixed">
		<buginformation>
			<summary>Some metastore event listeners always initialize deleteData as false</summary>
			<description>DropTableEvent:


  public DropTableEvent(Table table, boolean status, boolean deleteData, HMSHandler handler) {
    super(status, handler);
    this.table = table;
    // In HiveMetaStore, the deleteData flag indicates whether DFS data should be
    // removed on a drop.
    this.deleteData = false;
  }


Same as PreDropPartitionEvent and PreDropTableEvent</description>
			<version>1.3.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.events.PreDropPartitionEvent.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.events.PreDropTableEvent.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.events.DropTableEvent.java</file>
		</fixedFiles>
	</bug>
	<bug id="14327" opendate="2016-07-25 13:33:48" fixdate="2016-07-25 15:47:03" resolution="Duplicate">
		<buginformation>
			<summary>Make RPC server inside Hive on Spark port range configuration </summary>
			<description>Currently the RPC server inside the hive on spark binds to a random port, which causes the issue in production environment that firewall is enabled and certain ports are enabled. Make the range configurable so the admin can control it. 

 .option(ChannelOption.SO_BACKLOG, 1)
      .option(ChannelOption.SO_REUSEADDR, true)
      .childOption(ChannelOption.SO_KEEPALIVE, true)
      .bind(0)
      .sync()
      .channel();

</description>
			<version>2.0.1</version>
			<fixedVersion></fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.spark.client.rpc.TestRpc.java</file>
			<file type="M">org.apache.hive.spark.client.rpc.RpcServer.java</file>
			<file type="M">org.apache.hive.spark.client.rpc.RpcConfiguration.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">12222</link>
		</links>
	</bug>
	<bug id="14311" opendate="2016-07-22 17:56:38" fixdate="2016-07-25 16:33:10" resolution="Fixed">
		<buginformation>
			<summary>No need to schedule Heartbeat task if the query doesn&amp;apos;t require locks</summary>
			<description>Otherwise the Heartbeat task will just stay there and not be cleaned up, which may cause OOM eventually.</description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
		</fixedFiles>
	</bug>
	<bug id="14308" opendate="2016-07-21 15:47:07" fixdate="2016-07-25 16:39:24" resolution="Fixed">
		<buginformation>
			<summary>While using column stats estimated data size may become 0</summary>
			<description>Found during a run of HIVE-12181</description>
			<version>2.1.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
		</fixedFiles>
	</bug>
	<bug id="14297" opendate="2016-07-20 19:23:52" fixdate="2016-07-25 17:42:57" resolution="Fixed">
		<buginformation>
			<summary>OrcRecordUpdater floods logs trying to create _orc_acid_version file</summary>
			<description>
    try {
      FSDataOutputStream strm = fs.create(new Path(path, ACID_FORMAT), false);
      strm.writeInt(ORC_ACID_VERSION);
      strm.close();
    } catch (IOException ioe) {
      if (LOG.isDebugEnabled()) {
        LOG.debug("Failed to create " + path + "/" + ACID_FORMAT + " with " +
            ioe);
      }
    }


this file is created in the table/partition dir.  So in streaming ingest cases this happens repeatedly and HDFS prints long stack trace with a WARN

2016-07-18 09:22:13.051 o.a.h.i.r.RetryInvocationHandler [WARN] Exception while invoking ClientNamenodeProtocolTranslatorPB.create over null. Not retrying because try once and fail.
org.apache.hadoop.ipc.RemoteException: /apps/hive/warehouse/stormdb.db/store_sales/dt=2016%2F07%2F18/_orc_acid_version for client 172.22.111.42 already exists
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2639)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2526)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2410)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:729)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:405)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2313)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2309)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2307)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1552) ~[stormjar.jar:?]
	at org.apache.hadoop.ipc.Client.call(Client.java:1496) ~[stormjar.jar:?]
	at org.apache.hadoop.ipc.Client.call(Client.java:1396) ~[stormjar.jar:?]
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233) ~[stormjar.jar:?]
	at com.sun.proxy.$Proxy44.create(Unknown Source) ~[?:?]
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:311) ~[stormjar.jar:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_77]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_77]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_77]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_77]
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:278) [stormjar.jar:?]
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:194) [stormjar.jar:?]
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:176) [stormjar.jar:?]
	at com.sun.proxy.$Proxy45.create(Unknown Source) [?:?]
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1719) [stormjar.jar:?]
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1699) [stormjar.jar:?]
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1634) [stormjar.jar:?]
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:478) [stormjar.jar:?]
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:474) [stormjar.jar:?]
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81) [stormjar.jar:?]
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:474) [stormjar.jar:?]
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:415) [stormjar.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:926) [stormjar.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:907) [stormjar.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:803) [stormjar.jar:?]
	at org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.&amp;lt;init&amp;gt;(OrcRecordUpdater.java:238) [stormjar.jar:?]
	at org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat.getRecordUpdater(OrcOutputFormat.java:289) [stormjar.jar:?]
	at org.apache.hive.hcatalog.streaming.AbstractRecordWriter.createRecordUpdater(AbstractRecordWriter.java:253) [stormjar.jar:?]
	at org.apache.hive.hcatalog.streaming.AbstractRecordWriter.createRecordUpdaters(AbstractRecordWriter.java:245) [stormjar.jar:?]
	at org.apache.hive.hcatalog.streaming.AbstractRecordWriter.newBatch(AbstractRecordWriter.java:189) [stormjar.jar:?]
	at org.apache.hive.hcatalog.streaming.DelimitedInputWriter.newBatch(DelimitedInputWriter.java:50) [stormjar.jar:?]
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.&amp;lt;init&amp;gt;(HiveEndPoint.java:607) [stormjar.jar:?]
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.&amp;lt;init&amp;gt;(HiveEndPoint.java:555) [stormjar.jar:?]
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$ConnectionImpl.fetchTransactionBatchImpl(HiveEndPoint.java:441) [stormjar.jar:?]
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$ConnectionImpl.access$600(HiveEndPoint.java:278) [stormjar.jar:?]
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$ConnectionImpl$2.run(HiveEndPoint.java:428) [stormjar.jar:?]
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$ConnectionImpl$2.run(HiveEndPoint.java:425) [stormjar.jar:?]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_77]
	at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_77]
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724) [stormjar.jar:?]
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$ConnectionImpl.fetchTransactionBatch(HiveEndPoint.java:424) [stormjar.jar:?]
	at org.apache.storm.hive.common.HiveWriter$6.call(HiveWriter.java:259) [stormjar.jar:?]
	at org.apache.storm.hive.common.HiveWriter$6.call(HiveWriter.java:256) [stormjar.jar:?]
	at org.apache.storm.hive.common.HiveWriter$9.call(HiveWriter.java:369) [stormjar.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_77]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_77]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_77]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_77]


Should check for existence of the file first.
also move "strm.close()" to finally block</description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.java</file>
		</fixedFiles>
	</bug>
	<bug id="13422" opendate="2016-04-05 11:45:20" fixdate="2016-07-25 21:32:40" resolution="Fixed">
		<buginformation>
			<summary>Analyse command not working for column having datatype as decimal(38,0)</summary>
			<description>For the repro


drop table sample_test;
CREATE TABLE IF NOT EXISTS sample_test( key decimal(38,0),b int ) ROW FORMAT DELIMITED FIELDS TERMINATED BY &amp;amp;apos;,&amp;amp;apos; STORED AS TEXTFILE;
load data local inpath &amp;amp;apos;/home/hive/analyse.txt&amp;amp;apos; into table sample_test;
ANALYZE TABLE sample_test COMPUTE STATISTICS FOR COLUMNS;


Sample data


20234567894567498250824983000004 0
50320807548878498250695083000004 0
40120807548878498250687183000004 0
20120807548878498250667783000004 0
40120807548878496250656783000004 0

</description>
			<version>1.1.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.java</file>
		</fixedFiles>
	</bug>
	<bug id="14326" opendate="2016-07-25 13:04:25" fixdate="2016-07-25 23:18:41" resolution="Fixed">
		<buginformation>
			<summary>Merging outer joins without conditions can lead to wrong results</summary>
			<description>HIVE-13069 enabled cartesian product merging. However, merge should only be performed between INNER joins.</description>
			<version>2.1.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
	</bug>
	<bug id="14324" opendate="2016-07-25 08:23:34" fixdate="2016-07-26 19:29:09" resolution="Fixed">
		<buginformation>
			<summary>ORC PPD for floats is broken</summary>
			<description>ORC stores min/max stats, bloom filters by passing floats as doubles using java&amp;amp;apos;s widening conversion. So if we write a float value of 0.22 to ORC file, the min/max stats and bloom filter will use 0.2199999988079071 double value.
But when we do PPD, SARG creates literals by converting float to string and then to double which compares 0.22 to 0.2199999988079071 and fails PPD evaluation. 


hive&amp;gt; create table orc_float (f float) stored as orc;
hive&amp;gt; insert into table orc_float values(0.22);
hive&amp;gt; set hive.optimize.index.filter=true;
hive&amp;gt; select * from orc_float where f=0.22;
OK
hive&amp;gt; set hive.optimize.index.filter=false;
hive&amp;gt; select * from orc_float where f=0.22;
OK
0.22


This is not a problem for doubles and decimals.
This issue was introduced in HIVE-8460 but back then there was no strict type check when SARGs are created and also PPD evaluation does not convert to column type. But now predicate leaf creation in SARG enforces strict type check for boxed literals and predicate type and PPD evaluation converts stats and constants to column type (predicate).</description>
			<version>1.3.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.sarg.TestConvertAstToSearchArg.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.sarg.ConvertAstToSearchArg.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">14310</link>
		</links>
	</bug>
	<bug id="10022" opendate="2015-03-19 20:47:47" fixdate="2016-07-26 21:05:31" resolution="Fixed">
		<buginformation>
			<summary>Authorization checks for non existent file/directory should not be recursive</summary>
			<description>I am testing a query like : 
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;
set user.name=user1;
create table auth_noupd(i int) clustered by  into 2 buckets stored as orc location &amp;amp;apos;$
{OUTPUT}
&amp;amp;apos; TBLPROPERTIES (&amp;amp;apos;transactional&amp;amp;apos;=&amp;amp;apos;true&amp;amp;apos;);
Now, in the above query,  since authorization is true, 
we would end up calling doAuthorizationV2() which ultimately ends up calling SQLAuthorizationUtils.getPrivilegesFromFS() which calls a recursive method : FileUtils.isActionPermittedForFileHierarchy() with the object or the ancestor of the object we are trying to authorize if the object does not exist. 
The logic in FileUtils.isActionPermittedForFileHierarchy() is DFS.
Now assume, we have a path as a/b/c/d that we are trying to authorize.
In case, a/b/c/d does not exist, we would call FileUtils.isActionPermittedForFileHierarchy() with say a/b/ assuming a/b/c also does not exist.
If under the subtree at a/b, we have millions of files, then FileUtils.isActionPermittedForFileHierarchy()  is going to check file permission on each of those objects. 
I do not completely understand why do we have to check for file permissions in all the objects in  branch of the tree that we are not  trying to read from /write to.  
We could have checked file permission on the ancestor that exists and if it matches what we expect, the return true.
Please confirm if this is a bug so that I can submit a patch else let me know what I am missing ?</description>
			<version>0.14.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils.java</file>
			<file type="M">org.apache.hadoop.hive.common.FileUtils.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">14238</link>
		</links>
	</bug>
	<bug id="14330" opendate="2016-07-25 22:25:01" fixdate="2016-07-26 22:36:24" resolution="Fixed">
		<buginformation>
			<summary>fix LockHandle TxnHandler.acquireLock(String key) retry logic</summary>
			<description>stupid bug: return statement is missing.  See patch.</description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
		</fixedFiles>
	</bug>
	<bug id="14313" opendate="2016-07-22 19:02:31" fixdate="2016-07-27 17:14:34" resolution="Fixed">
		<buginformation>
			<summary>Test failure TestMetaStoreMetrics.testConnections</summary>
			<description>Looks like this test has been failing for the past 84 builds prior to
https://builds.apache.org/job/PreCommit-HIVE-MASTER-Build/597/testReport/org.apache.hadoop.hive.metastore/TestMetaStoreMetrics/testConnections/
org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.testConnections
Failing for the past 84 builds (Since #514 )
Took 28 ms.
Error Message
expected:&amp;lt;[1]&amp;gt; but was:&amp;lt;[2]&amp;gt;
Stacktrace
org.junit.ComparisonFailure: expected:&amp;lt;[1]&amp;gt; but was:&amp;lt;[2]&amp;gt;
	at org.junit.Assert.assertEquals(Assert.java:115)
	at org.junit.Assert.assertEquals(Assert.java:144)
	at org.apache.hadoop.hive.common.metrics.MetricsTestUtils.verifyMetricsJson(MetricsTestUtils.java:50)
	at org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.testConnections(TestMetaStoreMetrics.java:146)</description>
			<version>2.2.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.common.metrics.MetricsTestUtils.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.java</file>
		</fixedFiles>
	</bug>
	<bug id="14296" opendate="2016-07-20 19:22:16" fixdate="2016-07-27 20:10:01" resolution="Fixed">
		<buginformation>
			<summary>Session count is not decremented when HS2 clients do not shutdown cleanly.</summary>
			<description>When a JDBC client like beeline abruptly disconnects from HS2, the session gets closed on the serverside but the session count reported in the logs is incorrect. It never gets decremented.
For example, I created 6 connections from the same instance of beeline to HS2.


2016-07-20T15:05:17,987  INFO [HiveServer2-Handler-Pool: Thread-40] thrift.ThriftCLIService: Opened a session SessionHandle [28b225ee-204f-4b3e-b4fd-0039ef8e276e], current sessions: 1
.....
2016-07-20T15:05:24,239  INFO [HiveServer2-Handler-Pool: Thread-45] thrift.ThriftCLIService: Opened a session SessionHandle [1d267de8-ff9a-4e76-ac5c-e82c871588e7], current sessions: 2
.....
2016-07-20T15:05:25,710  INFO [HiveServer2-Handler-Pool: Thread-50] thrift.ThriftCLIService: Opened a session SessionHandle [04d53deb-8965-464b-aa3f-7042304cfb54], current sessions: 3
.....
2016-07-20T15:05:26,795  INFO [HiveServer2-Handler-Pool: Thread-55] thrift.ThriftCLIService: Opened a session SessionHandle [b4bb8b86-74e1-4e3c-babb-674d34ad1caf], current sessions: 4
2016-07-20T15:05:28,160  INFO [HiveServer2-Handler-Pool: Thread-60] thrift.ThriftCLIService: Opened a session SessionHandle [6d3c3ed9-fadb-4673-8c15-3315b7e2995d], current sessions: 5
.....
2016-07-20T15:05:29,136  INFO [HiveServer2-Handler-Pool: Thread-65] thrift.ThriftCLIService: Opened a session SessionHandle [88b630c0-f272-427d-8263-febfe2222f8d], current sessions: 6


When I CNTRL-C the beeline process, in the HS2 logs I see


2016-07-20T15:11:37,858  INFO [HiveServer2-Handler-Pool: Thread-55] thrift.ThriftCLIService: Session disconnected without closing properly. 
2016-07-20T15:11:37,858  INFO [HiveServer2-Handler-Pool: Thread-40] thrift.ThriftCLIService: Session disconnected without closing properly. 
2016-07-20T15:11:37,858  INFO [HiveServer2-Handler-Pool: Thread-65] thrift.ThriftCLIService: Session disconnected without closing properly. 
2016-07-20T15:11:37,858  INFO [HiveServer2-Handler-Pool: Thread-60] thrift.ThriftCLIService: Session disconnected without closing properly. 
2016-07-20T15:11:37,859  INFO [HiveServer2-Handler-Pool: Thread-50] thrift.ThriftCLIService: Session disconnected without closing properly. 
2016-07-20T15:11:37,859  INFO [HiveServer2-Handler-Pool: Thread-45] thrift.ThriftCLIService: Session disconnected without closing properly. 
2016-07-20T15:11:37,859  INFO [HiveServer2-Handler-Pool: Thread-55] thrift.ThriftCLIService: Closing the session: SessionHandle [b4bb8b86-74e1-4e3c-babb-674d34ad1caf]
2016-07-20T15:11:37,859  INFO [HiveServer2-Handler-Pool: Thread-40] thrift.ThriftCLIService: Closing the session: SessionHandle [28b225ee-204f-4b3e-b4fd-0039ef8e276e]
2016-07-20T15:11:37,859  INFO [HiveServer2-Handler-Pool: Thread-65] thrift.ThriftCLIService: Closing the session: SessionHandle [88b630c0-f272-427d-8263-febfe2222f8d]
2016-07-20T15:11:37,859  INFO [HiveServer2-Handler-Pool: Thread-60] thrift.ThriftCLIService: Closing the session: SessionHandle [6d3c3ed9-fadb-4673-8c15-3315b7e2995d]
2016-07-20T15:11:37,859  INFO [HiveServer2-Handler-Pool: Thread-45] thrift.ThriftCLIService: Closing the session: SessionHandle [1d267de8-ff9a-4e76-ac5c-e82c871588e7]
2016-07-20T15:11:37,859  INFO [HiveServer2-Handler-Pool: Thread-50] thrift.ThriftCLIService: Closing the session: SessionHandle [04d53deb-8965-464b-aa3f-7042304cfb54]


The next time I connect to HS2 via beeline, I see


2016-07-20T15:14:33,679  INFO [HiveServer2-Handler-Pool: Thread-50] thrift.ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V8
2016-07-20T15:14:33,710  INFO [HiveServer2-Handler-Pool: Thread-50] session.SessionState: Created HDFS directory: /tmp/hive/hive/d47759e8-df3a-4504-9f28-99ff5247352c
2016-07-20T15:14:33,725  INFO [HiveServer2-Handler-Pool: Thread-50] session.SessionState: Created local directory: /var/folders/_3/0w477k4j5bjd6h967rw4vflw0000gp/T/ngangam/d47759e8-df3a-4504-9f28-99ff5247352c
2016-07-20T15:14:33,735  INFO [HiveServer2-Handler-Pool: Thread-50] session.SessionState: Created HDFS directory: /tmp/hive/hive/d47759e8-df3a-4504-9f28-99ff5247352c/_tmp_space.db
2016-07-20T15:14:33,737  INFO [HiveServer2-Handler-Pool: Thread-50] session.HiveSessionImpl: Operation log session directory is created: /var/folders/_3/0w477k4j5bjd6h967rw4vflw0000gp/T/ngangam/operation_logs/d47759e8-df3a-4504-9f28-99ff5247352c
2016-07-20T15:14:33,737  INFO [HiveServer2-Handler-Pool: Thread-50] thrift.ThriftCLIService: Opened a session SessionHandle [d47759e8-df3a-4504-9f28-99ff5247352c], current sessions: 7


So while the sessions itself are closed and cleaned up, the session count reported is not accurate.</description>
			<version>2.0.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.service.cli.session.SessionManager.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
		</fixedFiles>
	</bug>
	<bug id="14293" opendate="2016-07-19 23:56:36" fixdate="2016-07-27 22:08:45" resolution="Fixed">
		<buginformation>
			<summary>PerfLogger.openScopes should be transient</summary>
			<description>See the following exception when running Hive e2e tests:


0: jdbc:hive2://nat-r6-ojss-hsihs2-1.openstac&amp;gt; SELECT s.name, s2.age, s.gpa, v.registration, v2.contributions FROM student s INNER JOIN voter v ON (s.name = v.name) INNER JOIN student s2 ON (s2.age = v.age and s.name = s2.name) INNER JOIN voter v2 ON (v2.name = s2.name and v2.age = s2.age) WHERE v2.age = s.age ORDER BY s.name, s2.age, s.gpa, v.registration, v2.contributions;
INFO  : Compiling command(queryId=hive_20160717224915_3a52719f-539f-4f82-a9cd-0c0af4e09ef8): SELECT s.name, s2.age, s.gpa, v.registration, v2.contributions FROM student s INNER JOIN voter v ON (s.name = v.name) INNER JOIN student s2 ON (s2.age = v.age and s.name = s2.name) INNER JOIN voter v2 ON (v2.name = s2.name and v2.age = s2.age) WHERE v2.age = s.age ORDER BY s.name, s2.age, s.gpa, v.registration, v2.contributions
INFO  : Semantic Analysis Completed
INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:s.name, type:string, comment:null), FieldSchema(name:s2.age, type:int, comment:null), FieldSchema(name:s.gpa, type:double, comment:null), FieldSchema(name:v.registration, type:string, comment:null), FieldSchema(name:v2.contributions, type:float, comment:null)], properties:null)
INFO  : Completed compiling command(queryId=hive_20160717224915_3a52719f-539f-4f82-a9cd-0c0af4e09ef8); Time taken: 1.165 seconds
INFO  : Executing command(queryId=hive_20160717224915_3a52719f-539f-4f82-a9cd-0c0af4e09ef8): SELECT s.name, s2.age, s.gpa, v.registration, v2.contributions FROM student s INNER JOIN voter v ON (s.name = v.name) INNER JOIN student s2 ON (s2.age = v.age and s.name = s2.name) INNER JOIN voter v2 ON (v2.name = s2.name and v2.age = s2.age) WHERE v2.age = s.age ORDER BY s.name, s2.age, s.gpa, v.registration, v2.contributions
INFO  : Query ID = hive_20160717224915_3a52719f-539f-4f82-a9cd-0c0af4e09ef8
INFO  : Total jobs = 1
INFO  : Launching Job 1 out of 1
INFO  : Starting task [Stage-1:MAPRED] in serial mode
INFO  : Session is already open
INFO  : Dag name: SELECT s.name, s2.age, s....v2.contributions(Stage-1)
ERROR : Failed to execute tez graph.
java.lang.RuntimeException: Error caching map.xml: org.apache.hive.com.esotericsoftware.kryo.KryoException: java.util.ConcurrentModificationException
Serialization trace:
classes (sun.misc.Launcher$AppClassLoader)
classloader (java.security.ProtectionDomain)
context (java.security.AccessControlContext)
acc (org.apache.hadoop.hive.ql.exec.UDFClassLoader)
classLoader (org.apache.hadoop.hive.conf.HiveConf)
conf (org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics)
metrics (org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics$CodahaleMetricsScope)
openScopes (org.apache.hadoop.hive.ql.log.PerfLogger)
perfLogger (org.apache.hadoop.hive.ql.exec.MapJoinOperator)
childOperators (org.apache.hadoop.hive.ql.exec.MapJoinOperator)
childOperators (org.apache.hadoop.hive.ql.exec.MapJoinOperator)
childOperators (org.apache.hadoop.hive.ql.exec.SelectOperator)
childOperators (org.apache.hadoop.hive.ql.exec.FilterOperator)
childOperators (org.apache.hadoop.hive.ql.exec.TableScanOperator)
aliasToWork (org.apache.hadoop.hive.ql.plan.MapWork)
	at org.apache.hadoop.hive.ql.exec.Utilities.setBaseWork(Utilities.java:582) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hadoop.hive.ql.exec.Utilities.setMapWork(Utilities.java:516) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hadoop.hive.ql.exec.tez.DagUtils.createVertex(DagUtils.java:601) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hadoop.hive.ql.exec.tez.DagUtils.createVertex(DagUtils.java:1147) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hadoop.hive.ql.exec.tez.TezTask.build(TezTask.java:390) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:164) [hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:197) [hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100) [hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1865) [hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1569) [hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1321) [hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1090) [hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1083) [hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:242) [hive-service-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.service.cli.operation.SQLOperation.access$800(SQLOperation.java:91) [hive-service-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:334) [hive-service-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_77]
	at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_77]
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724) [hadoop-common-2.7.1.2.5.0.0-1009.jar:?]
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:347) [hive-service-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_77]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_77]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_77]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_77]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_77]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_77]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_77]
Caused by: org.apache.hive.com.esotericsoftware.kryo.KryoException: java.util.ConcurrentModificationException
Serialization trace:
classes (sun.misc.Launcher$AppClassLoader)
classloader (java.security.ProtectionDomain)
context (java.security.AccessControlContext)
acc (org.apache.hadoop.hive.ql.exec.UDFClassLoader)
classLoader (org.apache.hadoop.hive.conf.HiveConf)
conf (org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics)
metrics (org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics$CodahaleMetricsScope)
openScopes (org.apache.hadoop.hive.ql.log.PerfLogger)
perfLogger (org.apache.hadoop.hive.ql.exec.MapJoinOperator)
childOperators (org.apache.hadoop.hive.ql.exec.MapJoinOperator)
childOperators (org.apache.hadoop.hive.ql.exec.MapJoinOperator)
childOperators (org.apache.hadoop.hive.ql.exec.SelectOperator)
childOperators (org.apache.hadoop.hive.ql.exec.FilterOperator)
childOperators (org.apache.hadoop.hive.ql.exec.TableScanOperator)
aliasToWork (org.apache.hadoop.hive.ql.plan.MapWork)
	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:101) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:366) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:307) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObjectOrNull(Kryo.java:606) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:87) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:113) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:39) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:100) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:40) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:100) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:40) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:100) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:40) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:100) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:40) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:100) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:40) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:113) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:39) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:534) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hadoop.hive.ql.exec.SerializationUtilities.serializeObjectByKryo(SerializationUtilities.java:578) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hadoop.hive.ql.exec.SerializationUtilities.serializePlan(SerializationUtilities.java:454) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hadoop.hive.ql.exec.SerializationUtilities.serializePlan(SerializationUtilities.java:435) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hadoop.hive.ql.exec.Utilities.setBaseWork(Utilities.java:537) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	... 26 more
Caused by: java.util.ConcurrentModificationException
	at java.util.Vector$Itr.checkForComodification(Vector.java:1184) ~[?:1.8.0_77]
	at java.util.Vector$Itr.next(Vector.java:1137) ~[?:1.8.0_77]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:92) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:40) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:366) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:307) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObjectOrNull(Kryo.java:606) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:87) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:113) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:39) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:100) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:40) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:100) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:40) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:100) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:40) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:100) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:40) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:100) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:40) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:113) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:39) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:534) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hadoop.hive.ql.exec.SerializationUtilities.serializeObjectByKryo(SerializationUtilities.java:578) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hadoop.hive.ql.exec.SerializationUtilities.serializePlan(SerializationUtilities.java:454) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hadoop.hive.ql.exec.SerializationUtilities.serializePlan(SerializationUtilities.java:435) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	at org.apache.hadoop.hive.ql.exec.Utilities.setBaseWork(Utilities.java:537) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]
	... 26 more
ERROR : FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask
INFO  : Completed executing command(queryId=hive_20160717224915_3a52719f-539f-4f82-a9cd-0c0af4e09ef8); Time taken: 0.171 seconds
Error: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask (state=08S01,code=1)


The reason is openScopes shall not be serialized to the backend. Need to mark it transient.</description>
			<version>2.1.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.log.PerfLogger.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">14232</link>
		</links>
	</bug>
	<bug id="14338" opendate="2016-07-26 17:19:57" fixdate="2016-07-28 00:00:41" resolution="Fixed">
		<buginformation>
			<summary>Delete/Alter table calls failing with HiveAccessControlException</summary>
			<description>Many Hcatalog/Webhcat tests are failing with below error, when tests try to alter/delete/describe tables. Error is thrown when the same user or a different user (same group) who created the table is trying to run the delete/alter table call.</description>
			<version>2.1.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.hcatalog.cli.HCatCli.java</file>
			<file type="M">org.apache.hive.hcatalog.cli.TestPermsGrp.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">14221</link>
		</links>
	</bug>
	<bug id="14333" opendate="2016-07-26 01:29:09" fixdate="2016-07-28 06:51:17" resolution="Fixed">
		<buginformation>
			<summary>ORC schema evolution from float to double changes precision and breaks filters</summary>
			<description>ORC vs text schema evolution from float to double changes precision
Text Schema Evolution

hive&amp;gt; create table float_text(f float);
hive&amp;gt; insert into float_text values(74.72);
hive&amp;gt; select f from float_text;
OK
74.72
hive&amp;gt; alter table float_text change column f f double;
hive&amp;gt; select f from float_text;
OK
74.72


Orc Schema Evolution

hive&amp;gt; create table float_orc(f float) stored as orc;
hive&amp;gt; insert into float_orc values(74.72);
hive&amp;gt; select f from float_orc;
OK
74.72
hive&amp;gt; alter table float_orc change column f f double;
hive&amp;gt; select f from float_orc;
OK
74.72000122070312


This will break all filters on the evolved column "f"
Filter returning no results

hive&amp;gt; set hive.optimize.index.filter=false;
hive&amp;gt; select f from float_orc where f=74.72;
OK

</description>
			<version>2.1.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.orc.impl.TestSchemaEvolution.java</file>
			<file type="M">org.apache.orc.impl.ConvertTreeReaderFactory.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">14310</link>
			<link type="Reference" description="relates to">14214</link>
		</links>
	</bug>
	<bug id="14310" opendate="2016-07-21 22:52:17" fixdate="2016-07-28 19:07:55" resolution="Fixed">
		<buginformation>
			<summary>ORC schema evolution should not completely disable PPD</summary>
			<description>Follow up for HIVE-14214 which completely shuts off PPD when there is any schema evolution. Some evolutions are safer for PPD like
byte -&amp;gt; short -&amp;gt; int -&amp;gt; long
float -&amp;gt; double (This is unsafe, see comments in SchemaEvolution.java in the patch)
varchar &amp;lt;-&amp;gt; string (string to char, varchar to char and vice versa is also unsafe conversion as Orc stores internal index with padded spaces for char)
For all other conversions we can disable PPD for that specific column that has evolved by returning TruthValue.YES_NO</description>
			<version>2.1.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
			<file type="M">org.apache.orc.impl.SchemaEvolution.java</file>
			<file type="M">org.apache.orc.impl.TestSchemaEvolution.java</file>
			<file type="M">org.apache.orc.impl.RecordReaderImpl.java</file>
			<file type="M">org.apache.orc.impl.ConvertTreeReaderFactory.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">14355</link>
			<link type="Blocker" description="is blocked by">14324</link>
			<link type="Blocker" description="is blocked by">14333</link>
			<link type="Reference" description="is related to">14214</link>
		</links>
	</bug>
	<bug id="14349" opendate="2016-07-26 23:53:35" fixdate="2016-07-28 21:09:12" resolution="Fixed">
		<buginformation>
			<summary>Vectorization: LIKE should anchor the regexes</summary>
			<description>RLIKE works like contains() and LIKE works like matches().
The UDFLike LIKE -&amp;gt; Regex conversion returns unanchored regexes making the vectorized LIKE behave like RLIKE.


create temporary table x (a string) stored as orc;
insert into x values(&amp;amp;apos;XYZa&amp;amp;apos;), (&amp;amp;apos;badXYZa&amp;amp;apos;);

select * from x where a LIKE &amp;amp;apos;XYZ%a%&amp;amp;apos; order by 1;
OK
XYZa
badXYZa
Time taken: 4.029 seconds, Fetched: 2 row(s)

</description>
			<version>1.2.1</version>
			<fixedVersion>1.3.0, 2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.FilterStringColLikeStringScalar.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">14318</link>
			<link type="Required" description="requires">13196</link>
		</links>
	</bug>
	<bug id="14294" opendate="2016-07-20 04:28:20" fixdate="2016-07-29 15:26:57" resolution="Fixed">
		<buginformation>
			<summary>HiveSchemaConverter for Parquet doesn&amp;apos;t translate TINYINT and SMALLINT into proper Parquet types</summary>
			<description>To reproduce this issue, run the following DDL:


CREATE TABLE foo STORED AS PARQUET AS SELECT CAST(1 AS TINYINT);


And then check the schema of the written Parquet file:

$ parquet-schema $WAREHOUSE_PATH/foo/000000_0
message hive_schema {
  optional int32 _c0;
}


When translating Hive types into Parquet types, TINYINT and SMALLINT should be translated into the int32 (INT_8) and int32 (INT_16) respectively. However, HiveSchemaConverter converts all of TINYINT, SMALLINT, and INT into Parquet int32. This causes problem when accessing Parquet files generated by Hive in other systems since type information gets wrong.</description>
			<version>1.2.1</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.parquet.convert.HiveSchemaConverter.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.parquet.TestHiveSchemaConverter.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">16632</link>
		</links>
	</bug>
	<bug id="14355" opendate="2016-07-27 09:03:24" fixdate="2016-07-31 03:55:42" resolution="Fixed">
		<buginformation>
			<summary>Schema evolution for ORC in llap is broken for int to string conversion</summary>
			<description>When schema is evolved from any integer type to string then following exceptions are thrown in LLAP (Works fine in Tez). I guess this should happen even for other conversions.


hive&amp;gt; create table orc_integer(b bigint) stored as orc;
hive&amp;gt; insert into orc_integer values(100);
hive&amp;gt; select count(*) from orc_integer where b=100;
OK
1
hive&amp;gt; alter table orc_integer change column b b string;
hive&amp;gt; select count(*) from orc_integer where b=100;
// FAIL with following exception


When vectorization is enabled

2016-07-27T01:48:05,611  INFO [TezTaskRunner ()] vector.VectorReduceSinkOperator: RECORDS_OUT_INTERMEDIATE_Map_1:0,
2016-07-27T01:48:05,611 ERROR [TezTaskRunner ()] tez.TezProcessor: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row
        at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:95)
        at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:70)
        at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:393)
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:185)
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168)
        at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370)
        at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)
        at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
        at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)
        at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)
        at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
        at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:110)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row
        at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:866)
        at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:86)
        ... 18 more
Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.LongColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector
        at org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterStringGroupColEqualStringGroupScalarBase.evaluate(FilterStringGroupColEqualStringGroupScalarBase.java:42)
        at org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.process(VectorFilterOperator.java:110)
        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:879)
        at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:130)
        at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:774)
        ... 19 more


When vectorization is disabled

2016-07-27T01:52:43,328  INFO [TezTaskRunner (1469608604787_0002_26_00_000000_0)] exec.ReduceSinkOperator: Using tag = -1
2016-07-27T01:52:43,328  INFO [TezTaskRunner (1469608604787_0002_26_00_000000_0)] exec.OperatorUtils: Setting output collector: RS[4] --&amp;gt; Reducer 2
2016-07-27T01:52:43,329 ERROR [TezTaskRunner (1469608604787_0002_26_00_000000_0)] io.BatchToRowReader: Error at row 0/1, column 0/1 org.apache.hadoop.hive.ql.exec.vector.LongColumnVector@7630e56a
java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.LongColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector
        at org.apache.hadoop.hive.ql.io.BatchToRowReader.nextString(BatchToRowReader.java:334) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.io.BatchToRowReader.nextValue(BatchToRowReader.java:602) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.io.BatchToRowReader.next(BatchToRowReader.java:149) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.io.BatchToRowReader.next(BatchToRowReader.java:78) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:350) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:79) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:33) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.next(HiveContextAwareRecordReader.java:116) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.next(TezGroupedSplitsInputFormat.java:151) ~[tez-mapreduce-0.8.4.jar:0.8.4]
        at org.apache.tez.mapreduce.lib.MRReaderMapred.next(MRReaderMapred.java:116) ~[tez-mapreduce-0.8.4.jar:0.8.4]
        at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:62) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:393) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:185) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370) ~[tez-runtime-internals-0.8.4.jar:0.8.4]
        at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73) ~[tez-runtime-internals-0.8.4.jar:0.8.4]
        at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61) ~[tez-runtime-internals-0.8.4.jar:0.8.4]
        at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_91]
        at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_91]
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628) ~[hadoop-common-2.6.0.jar:?]
        at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61) ~[tez-runtime-internals-0.8.4.jar:0.8.4]
        at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37) ~[tez-runtime-internals-0.8.4.jar:0.8.4]
        at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36) ~[tez-common-0.8.4.jar:0.8.4]
        at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:110) ~[hive-llap-server-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_91]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_91]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_91]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_91]


</description>
			<version>2.1.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.java</file>
			<file type="M">org.apache.hadoop.hive.llap.io.decode.ReadPipeline.java</file>
			<file type="M">org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
			<file type="M">org.apache.hadoop.hive.llap.io.decode.OrcColumnVectorProducer.java</file>
			<file type="M">org.apache.hadoop.hive.llap.io.decode.ColumnVectorProducer.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="is blocked by">14310</link>
		</links>
	</bug>
	<bug id="14363" opendate="2016-07-27 21:44:56" fixdate="2016-07-31 05:27:44" resolution="Fixed">
		<buginformation>
			<summary>bucketmap inner join query fails due to NullPointerException in some cases</summary>
			<description>Bucketmap inner join query between bucketed tables throws following exception when one table contains all the empty buckets while other has all the non-empty buckets.

Vertex failed, vertexName=Map 2, vertexId=vertex_1466710232033_0432_4_01, diagnostics=[Task failed, taskId=task_1466710232033_0432_4_01_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Error while running task ( failure ) : attempt_1466710232033_0432_4_01_000000_0:java.lang.RuntimeException: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:211)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:330)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:184)
	... 14 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getKeyValueReader(MapRecordProcessor.java:372)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.initializeMapRecordSources(MapRecordProcessor.java:344)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:292)
	... 15 more
], TaskAttempt 1 failed, info=[Error: Error while running task ( failure ) : attempt_1466710232033_0432_4_01_000000_1:java.lang.RuntimeException: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:211)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:330)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:184)
	... 14 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getKeyValueReader(MapRecordProcessor.java:372)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.initializeMapRecordSources(MapRecordProcessor.java:344)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:292)
	... 15 more
], TaskAttempt 2 failed, info=[Error: Error while running task ( failure ) : attempt_1466710232033_0432_4_01_000000_2:java.lang.RuntimeException: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:211)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:330)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:184)
	... 14 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getKeyValueReader(MapRecordProcessor.java:372)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.initializeMapRecordSources(MapRecordProcessor.java:344)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:292)
	... 15 more
], TaskAttempt 3 failed, info=[Error: Error while running task ( failure ) : attempt_1466710232033_0432_4_01_000000_3:java.lang.RuntimeException: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:211)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:330)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:184)
	... 14 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getKeyValueReader(MapRecordProcessor.java:372)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.initializeMapRecordSources(MapRecordProcessor.java:344)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:292)
	... 15 more
]], Vertex did not succeed due to OWN_TASK_FAILURE, failedTasks:1 killedTasks:59, Vertex vertex_1466710232033_0432_4_01 [Map 2] killed/failed due to:OWN_TASK_FAILURE]
DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0
FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 2, vertexId=vertex_1466710232033_0432_4_01, diagnostics=[Task failed, taskId=task_1466710232033_0432_4_01_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Error while running task ( failure ) : attempt_1466710232033_0432_4_01_000000_0:java.lang.RuntimeException: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:211)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:330)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:184)
	... 14 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getKeyValueReader(MapRecordProcessor.java:372)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.initializeMapRecordSources(MapRecordProcessor.java:344)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:292)
	... 15 more
], TaskAttempt 1 failed, info=[Error: Error while running task ( failure ) : attempt_1466710232033_0432_4_01_000000_1:java.lang.RuntimeException: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:211)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:330)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:184)
	... 14 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getKeyValueReader(MapRecordProcessor.java:372)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.initializeMapRecordSources(MapRecordProcessor.java:344)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:292)
	... 15 more
], TaskAttempt 2 failed, info=[Error: Error while running task ( failure ) : attempt_1466710232033_0432_4_01_000000_2:java.lang.RuntimeException: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:211)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:330)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:184)
	... 14 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getKeyValueReader(MapRecordProcessor.java:372)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.initializeMapRecordSources(MapRecordProcessor.java:344)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:292)
	... 15 more
], TaskAttempt 3 failed, info=[Error: Error while running task ( failure ) : attempt_1466710232033_0432_4_01_000000_3:java.lang.RuntimeException: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:211)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:330)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:184)
	... 14 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getKeyValueReader(MapRecordProcessor.java:372)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.initializeMapRecordSources(MapRecordProcessor.java:344)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:292)
	... 15 more


Steps to reproduce the issue:

set hive.execution.engine=tez;
set hive.exeucution.mode=container;
drop table if exists src_emptybucket;
create table src_emptybucket (name string, age int, gpa double)
clustered by (age)
into 30 buckets
stored as orc;
insert into table src_emptybucket
select * from studenttab10k limit 0;

drop table if exists src_nonemptybucket;
create table src_nonemptybucket (name varchar(50), age int, gpa decimal(38,18))
clustered by (age)
into 60 buckets
stored as orc;
insert into table src_nonemptybucket
select * from studenttab10k 
where age in (
select distinct(age) from studenttab10k 
limit 60);

set hive.optimize.bucketmapjoin=true;
set hive.convert.join.bucket.mapjoin.tez=true;

select e1.name as e1_name, e1.age as e1_age, e1.gpa as e1_gpa, e2.name as e2_name, e2.age as e2_age, e2.gpa as e2_gpa from src_emptybucket e1 inner join src_nonemptybucket e2 on e1.age = e2.age;

</description>
			<version>2.2.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.java</file>
		</fixedFiles>
	</bug>
	<bug id="14381" opendate="2016-07-29 06:22:59" fixdate="2016-07-31 22:08:55" resolution="Fixed">
		<buginformation>
			<summary>Handle null value in WindowingTableFunction.WindowingIterator.next()</summary>
			<description></description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.java</file>
		</fixedFiles>
	</bug>
	<bug id="14367" opendate="2016-07-28 02:20:54" fixdate="2016-08-01 07:46:56" resolution="Fixed">
		<buginformation>
			<summary>Estimated size for constant nulls is 0</summary>
			<description>since type is incorrectly assumed as void.</description>
			<version>2.0.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.java</file>
		</fixedFiles>
	</bug>
	<bug id="14357" opendate="2016-07-27 12:35:48" fixdate="2016-08-01 18:39:51" resolution="Fixed">
		<buginformation>
			<summary>TestDbTxnManager2#testLocksInSubquery failing in branch-2.1</summary>
			<description>
checkCmdOnDriver(driver.compileAndRespond("insert into R select * from S where a in (select a from T where b = 1)"));
    txnMgr.openTxn("three");
    txnMgr.acquireLocks(driver.getPlan(), ctx, "three");
    locks = getLocks();
    Assert.assertEquals("Unexpected lock count", 3, locks.size());
    checkLock(LockType.SHARED_READ, LockState.ACQUIRED, "default", "T", null, locks.get(0));
    checkLock(LockType.SHARED_READ, LockState.ACQUIRED, "default", "S", null, locks.get(1));
    checkLock(LockType.SHARED_READ, LockState.ACQUIRED, "default", "R", null, locks.get(2));


This test case is failing. The expected order of locks is supposed to be T, S, R. But upon closer inspection, it seems to be R,S,T. 
I&amp;amp;apos;m not much familiar with what these locks are and why the order is important. Raising this jira so while I try to understand it all. Meanwhile, if somebody can explain here, would be helpful. 
</description>
			<version>2.1.1</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.java</file>
			<file type="M">org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
		</fixedFiles>
	</bug>
	<bug id="14322" opendate="2016-07-24 06:42:50" fixdate="2016-08-01 18:44:45" resolution="Fixed">
		<buginformation>
			<summary>Postgres db issues after Datanucleus 4.x upgrade</summary>
			<description>With the upgrade to  datanucleus 4.x versions in HIVE-6113, hive does not work properly with postgres.
The nullable fields in the database have string "NULL::character varying" instead of real NULL values. This causes various issues.
One example is -


hive&amp;gt; create table t(i int);
OK
Time taken: 1.9 seconds
hive&amp;gt; create view v as select * from t;
OK
Time taken: 0.542 seconds
hive&amp;gt; select * from v;
FAILED: SemanticException Unable to fetch table v. java.net.URISyntaxException: Relative path in absolute URI: NULL::character%20varying


</description>
			<version>2.0.0</version>
			<fixedVersion>2.2.0, 2.1.1, 2.0.2</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">15204</link>
			<link type="Reference" description="is related to">14371</link>
			<link type="Reference" description="is related to">6113</link>
			<link type="Reference" description="is related to">1841</link>
		</links>
	</bug>
	<bug id="14366" opendate="2016-07-28 02:16:52" fixdate="2016-08-01 19:25:35" resolution="Fixed">
		<buginformation>
			<summary>Conversion of a Non-ACID table to an ACID table produces non-unique primary keys</summary>
			<description>When a Non-ACID table is converted to an ACID table, the primary key consisting of (original transaction id, bucket_id, row_id) is not generated uniquely. Currently, the row_id is always set to 0 for most rows. This leads to correctness issue for such tables.
Quickest way to reproduce is to add the following unit test to ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands2.java
ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands2.java

  @Test
  public void testOriginalReader() throws Exception {
    FileSystem fs = FileSystem.get(hiveConf);
    FileStatus[] status;

    // 1. Insert five rows to Non-ACID table.
    runStatementOnDriver("insert into " + Table.NONACIDORCTBL + "(a,b) values(1,2),(3,4),(5,6),(7,8),(9,10)");

    // 2. Convert NONACIDORCTBL to ACID table.
    runStatementOnDriver("alter table " + Table.NONACIDORCTBL + " SET TBLPROPERTIES (&amp;amp;apos;transactional&amp;amp;apos;=&amp;amp;apos;true&amp;amp;apos;)");

    // 3. Perform a major compaction.
    runStatementOnDriver("alter table "+ Table.NONACIDORCTBL + " compact &amp;amp;apos;MAJOR&amp;amp;apos;");
    runWorker(hiveConf);

    // 4. Perform a delete.
    runStatementOnDriver("delete from " + Table.NONACIDORCTBL + " where a = 1");

    // 5. Now do a projection should have (3,4) (5,6),(7,8),(9,10) only since (1,2) has been deleted.
    List&amp;lt;String&amp;gt; rs = runStatementOnDriver("select a,b from " + Table.NONACIDORCTBL + " order by a,b");
    int[][] resultData = new int[][] {{3,4}, {5,6}, {7,8}, {9,10}};
    Assert.assertEquals(stringifyValues(resultData), rs);
  }

</description>
			<version>1.0.0</version>
			<fixedVersion>1.3.0, 2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.java</file>
			<file type="M">org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">12724</link>
		</links>
	</bug>
	<bug id="14350" opendate="2016-07-27 02:47:09" fixdate="2016-08-01 19:26:18" resolution="Fixed">
		<buginformation>
			<summary>Aborted txns cause false positive "Not enough history available..." msgs</summary>
			<description>this is a followup to HIVE-13369.  Only open txns should prevent use of a base file.  But ValidTxnList does not make a distinction between open and aborted txns.  The presence of aborted txns causes false positives which can happen too often since the flow is 
1. Worker generates a new base file, 
2. then asynchronously Cleaner removes now-compacted aborted txns.  (strictly speaking it&amp;amp;apos;s Initiator that does the actual clean up)
So we may have base_5 and base_10 and txnid 7 aborted.  Then current impl will disallow use of base_10 though there is no need for that.  Worse, if txnid_4 is aborted and hasn&amp;amp;apos;t been purged yet, base_5 will be rejected as well and then an error will be raised since there is no suitable base file left.
ErrorMsg.ACID_NOT_ENOUGH_HISTORY is msg produced</description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TestValidCompactorTxnList.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.TestOrcRawRecordMerger.java</file>
			<file type="M">org.apache.hadoop.hive.common.TestValidReadTxnList.java</file>
			<file type="M">org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.TestAcidUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
			<file type="M">org.apache.hadoop.hive.common.ValidTxnList.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
			<file type="M">org.apache.hadoop.hive.common.ValidReadTxnList.java</file>
			<file type="M">org.apache.hadoop.hive.common.ValidCompactorTxnList.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">14414</link>
			<link type="Reference" description="is related to">14865</link>
			<link type="Required" description="requires">13369</link>
		</links>
	</bug>
	<bug id="14278" opendate="2016-07-19 08:48:02" fixdate="2016-08-02 00:49:43" resolution="Fixed">
		<buginformation>
			<summary>Migrate TestHadoop23SAuthBridge.java from Unit3 to Unit4</summary>
			<description>Migrate TestHadoop23SAuthBridge.java from unit3 to unit4</description>
			<version>2.2.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.thrift.TestHadoopAuthBridge23.java</file>
		</fixedFiles>
	</bug>
	<bug id="14346" opendate="2016-07-26 21:18:44" fixdate="2016-08-02 16:27:42" resolution="Fixed">
		<buginformation>
			<summary>Change the default value for hive.mapred.mode to null</summary>
			<description>HIVE-12727 introduces three new configurations to replace the existing hive.mapred.mode, which is deprecated. However, the default value for the latter is &amp;amp;apos;nonstrict&amp;amp;apos;, which prevent the new configurations from being used (see comments in that JIRA for more details).
This proposes to change the default value for hive.mapred.mode to null. Users can then set the three new configurations to get more fine-grained control over the strict checking. If user want to use the old configuration, they can set hive.mapred.mode to strict/nonstrict.
</description>
			<version>2.2.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">12727</link>
		</links>
	</bug>
	<bug id="14400" opendate="2016-08-02 00:13:38" fixdate="2016-08-03 03:00:13" resolution="Fixed">
		<buginformation>
			<summary>Handle concurrent insert with dynamic partition</summary>
			<description>With multiple users concurrently issuing insert statements on the same partition has a side effect that some queries may not see a partition at the time when they&amp;amp;apos;re issued, but will realize the partition is actually there when it is trying to add such partition to the metastore and thus get AlreadyExistsException, because some earlier query just created it (race condition).
For example, imagine such a table is created:


create table T (name char(50)) partitioned by (ds string);


and the following two queries are launched at the same time, from different sessions:


insert into table T partition (ds) values (&amp;amp;apos;Bob&amp;amp;apos;, &amp;amp;apos;today&amp;amp;apos;); -- creates the partition &amp;amp;apos;today&amp;amp;apos;
insert into table T partition (ds) values (&amp;amp;apos;Joe&amp;amp;apos;, &amp;amp;apos;today&amp;amp;apos;); -- will fail with AlreadyExistsException

</description>
			<version>2.2.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">8797</link>
		</links>
	</bug>
	<bug id="14397" opendate="2016-08-01 07:57:42" fixdate="2016-08-03 20:27:12" resolution="Fixed">
		<buginformation>
			<summary>Queries ran after reopening of tez session launches additional sessions</summary>
			<description>Say we have configured hive.server2.tez.default.queues with 2 queues q1 and q2 with default expiry interval of 5 mins.
After 5 mins of non-usage the sessions corresponding to queues q1 and q2 will be expired. When new set of queries are issue after this expiry, the default sessions backed by q1 and q2 and reopened again. Now when we run more queries the reopened sessions are not used instead new session is opened. 
At this point there will be 4 sessions running (2 abandoned sessions and 2 current sessions). </description>
			<version>2.1.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TestTezSessionPool.java</file>
		</fixedFiles>
	</bug>
	<bug id="14414" opendate="2016-08-03 18:26:36" fixdate="2016-08-03 23:37:24" resolution="Fixed">
		<buginformation>
			<summary>Fix TestHiveMetaStoreTxns UTs</summary>
			<description>Missed updating these tests in HIVE-14350 commit</description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.TestHiveMetaStoreTxns.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">14350</link>
		</links>
	</bug>
	<bug id="14411" opendate="2016-08-03 11:27:45" fixdate="2016-08-05 02:01:23" resolution="Fixed">
		<buginformation>
			<summary>selecting Hive on Hbase table may cause FileNotFoundException</summary>
			<description>1. create a Hbase table hbase_table
2. create a external Hive table test_table mapping to the hbase table 
example: 
create &amp;amp;apos;hbase_t&amp;amp;apos; ,
{NAME=&amp;gt;&amp;amp;apos;cf&amp;amp;apos;,COMPRESSION=&amp;gt;&amp;amp;apos;snappy&amp;amp;apos;}
,
{NUMREGIONS=&amp;gt;15,SPLITALGO=&amp;gt;&amp;amp;apos;HexStringSplit&amp;amp;apos;}

create external table hbase_t_hive(key1 string,cf_train string,cf_flight string,cf_wbsw string,cf_wbxw string,cf_bgrz string,cf_bgtf string) 
stored by &amp;amp;apos;org.apache.hadoop.hive.hbase.HBaseStorageHandler&amp;amp;apos; 
with serdeproperties("hbase.columns.mapping"=":key,cf:train,cf:flight,cf:wbsw,cf:wbxw,cf:bgrz,cf:bgtf") tblproperties("hbase.table.name"="hbase_t");
create table test3 as select * from hbase_t_hive where 1=2;
====
if hive.optimize.null.scan=true, it will return an FileNotFoundException
</description>
			<version>1.3.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.NullScanTaskDispatcher.java</file>
		</fixedFiles>
	</bug>
	<bug id="14402" opendate="2016-08-02 18:17:36" fixdate="2016-08-05 10:33:16" resolution="Fixed">
		<buginformation>
			<summary>Vectorization: Fix Mapjoin overflow deserialization </summary>
			<description>This is in a codepath currently disabled in master, however enabling it triggers OOB.


Caused by: java.lang.ArrayIndexOutOfBoundsException: 1024
        at org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector.setRef(BytesColumnVector.java:92)
        at org.apache.hadoop.hive.ql.exec.vector.VectorDeserializeRow.deserializeRowColumn(VectorDeserializeRow.java:415)
        at org.apache.hadoop.hive.ql.exec.vector.VectorDeserializeRow.deserialize(VectorDeserializeRow.java:674)
        at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator.generateHashMapResultLargeMultiValue(VectorMapJoinGenerateResultOperator.java:307)
        at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator.generateHashMapResultMultiValue(VectorMapJoinGenerateResultOperator.java:226)
        at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator.generateHashMapResultRepeatedAll(VectorMapJoinGenerateResultOperator.java:391)

</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator.java</file>
		</fixedFiles>
	</bug>
	<bug id="14424" opendate="2016-08-04 08:25:33" fixdate="2016-08-05 18:40:48" resolution="Fixed">
		<buginformation>
			<summary>Address CLIRestoreTest failure</summary>
			<description>

java.lang.RuntimeException: Error applying authorization policy on hive configuration: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassNotFoundException: org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest

	at org.apache.hive.service.cli.CLIService.init(CLIService.java:113)
	at org.apache.hive.service.cli.CLIServiceRestoreTest.getService(CLIServiceRestoreTest.java:48)
	at org.apache.hive.service.cli.CLIServiceRestoreTest.&amp;lt;init&amp;gt;(CLIServiceRestoreTest.java:28)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:195)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:244)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:241)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:234)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:74)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassNotFoundException: org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest
	at org.apache.hadoop.hive.ql.session.SessionState.setupAuth(SessionState.java:836)
	at org.apache.hadoop.hive.ql.session.SessionState.applyAuthorizationPolicy(SessionState.java:1602)
	at org.apache.hive.service.cli.CLIService.applyAuthorizationConfigPolicy(CLIService.java:126)
	at org.apache.hive.service.cli.CLIService.init(CLIService.java:110)
	... 22 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassNotFoundException: org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest
	at org.apache.hadoop.hive.ql.metadata.HiveUtils.getAuthorizeProviderManager(HiveUtils.java:385)
	at org.apache.hadoop.hive.ql.session.SessionState.setupAuth(SessionState.java:812)
	... 25 more
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.hadoop.hive.ql.metadata.HiveUtils.getAuthorizeProviderManager(HiveUtils.java:375)
	... 26 more



But is caused by HIVE-14221. Code changes are here: https://github.com/apache/hive/commit/de5ae86ee70d9396d5cefc499507b5f31fecc916
So the issue is that, in this patch, everywhere the class org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory has been mentioned, except at one place. That one place is using org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest class, which happens to not be in the classpath while running hive-service tests. 
Seems like the wrong class was mentioned by mistake in the patch. 
Pengcheng Xiong Since you are the original author, can you confirm whether it indeed was a mistake. 
</description>
			<version>2.1.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.service.cli.CLIServiceRestoreTest.java</file>
		</fixedFiles>
	</bug>
	<bug id="14390" opendate="2016-07-30 08:18:39" fixdate="2016-08-06 23:04:29" resolution="Fixed">
		<buginformation>
			<summary>Wrong Table alias when CBO is on</summary>
			<description>There are 5 web_sales references in query95 of tpcds ,with alias ws1-ws5.
But the query plan only has ws1 when CBO is on.
query95 :

SELECT count(distinct ws1.ws_order_number) as order_count,
               sum(ws1.ws_ext_ship_cost) as total_shipping_cost,
               sum(ws1.ws_net_profit) as total_net_profit
FROM web_sales ws1
JOIN customer_address ca ON (ws1.ws_ship_addr_sk = ca.ca_address_sk)
JOIN web_site s ON (ws1.ws_web_site_sk = s.web_site_sk)
JOIN date_dim d ON (ws1.ws_ship_date_sk = d.d_date_sk)
LEFT SEMI JOIN (SELECT ws2.ws_order_number as ws_order_number
                               FROM web_sales ws2 JOIN web_sales ws3
                               ON (ws2.ws_order_number = ws3.ws_order_number)
                               WHERE ws2.ws_warehouse_sk &amp;lt;&amp;gt; ws3.ws_warehouse_sk
                        ) ws_wh1
ON (ws1.ws_order_number = ws_wh1.ws_order_number)
LEFT SEMI JOIN (SELECT wr_order_number
                               FROM web_returns wr
                               JOIN (SELECT ws4.ws_order_number as ws_order_number
                                          FROM web_sales ws4 JOIN web_sales ws5
                                          ON (ws4.ws_order_number = ws5.ws_order_number)
                                         WHERE ws4.ws_warehouse_sk &amp;lt;&amp;gt; ws5.ws_warehouse_sk
                                ) ws_wh2
                               ON (wr.wr_order_number = ws_wh2.ws_order_number)) tmp1
ON (ws1.ws_order_number = tmp1.wr_order_number)
WHERE d.d_date between &amp;amp;apos;2002-05-01&amp;amp;apos; and &amp;amp;apos;2002-06-30&amp;amp;apos; and
               ca.ca_state = &amp;amp;apos;GA&amp;amp;apos; and
               s.web_company_name = &amp;amp;apos;pri&amp;amp;apos;;

</description>
			<version>1.2.1</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveTableScan.java</file>
		</fixedFiles>
	</bug>
	<bug id="14447" opendate="2016-08-05 21:54:27" fixdate="2016-08-07 06:25:01" resolution="Fixed">
		<buginformation>
			<summary>Set HIVE_TRANSACTIONAL_TABLE_SCAN to the correct job conf for FetchOperator</summary>
			<description></description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
		</fixedFiles>
	</bug>
	<bug id="13813" opendate="2016-05-20 21:54:24" fixdate="2016-08-08 20:29:25" resolution="Fixed">
		<buginformation>
			<summary>Add Metrics for the number of Hive operations waiting for compile</summary>
			<description>Currently, without hive.driver.parallel.compilation introduced in HIVE-4239, only one SQL operation can enter the compilation block per HS2 instance, and all the rest will be blocked. We should add metrics info for the number of operations that are blocked.</description>
			<version>1.3.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.common.metrics.common.MetricsConstant.java</file>
		</fixedFiles>
	</bug>
	<bug id="14436" opendate="2016-08-05 11:03:50" fixdate="2016-08-10 05:58:30" resolution="Fixed">
		<buginformation>
			<summary>Hive 1.2.1/Hitting "ql.Driver: FAILED: IllegalArgumentException Error: , expected at the end of &amp;apos;decimal(9&amp;apos;" after enabling hive.optimize.skewjoin and with MR engine</summary>
			<description>PROBLEM:
The following Query run with MapReduce engine with "hive.optimize.skewjoin = true" fails with error:
"FAILED: IllegalArgumentException Error: , expected at the end of &amp;amp;apos;decimal(9&amp;amp;apos;" 
&amp;gt; SELECT a.col1 FROM db.tableA a  INNER JOIN  db.tableB b  ON b.key=a.key limit 5;
FAILED: IllegalArgumentException Error: , expected at the end of &amp;amp;apos;decimal(9&amp;amp;apos;
16/08/04 12:47:50 [main]: ERROR ql.Driver: FAILED: IllegalArgumentException Error: , expected at the end of &amp;amp;apos;decimal(9&amp;amp;apos;
java.lang.IllegalArgumentException: Error: , expected at the end of &amp;amp;apos;decimal(9&amp;amp;apos;
	at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser.expect(TypeInfoUtils.java:336)
	at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser.parseParams(TypeInfoUtils.java:378)
	at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser.parsePrimitiveParts(TypeInfoUtils.java:518)
	at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.parsePrimitiveParts(TypeInfoUtils.java:533)
	at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.createPrimitiveTypeInfo(TypeInfoFactory.java:136)
	at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.getPrimitiveTypeInfo(TypeInfoFactory.java:109)
	at org.apache.hadoop.hive.ql.optimizer.physical.GenMRSkewJoinProcessor.processSkewJoin(GenMRSkewJoinProcessor.java:214)
	at org.apache.hadoop.hive.ql.optimizer.physical.SkewJoinProcFactory$SkewJoinJoinProcessor.process(SkewJoinProcFactory.java:60)
	at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:95)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:79)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:133)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:110)
	at org.apache.hadoop.hive.ql.optimizer.physical.SkewJoinResolver$SkewJoinTaskDispatcher.dispatch(SkewJoinResolver.java:100)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:95)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:79)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:133)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:110)
	at org.apache.hadoop.hive.ql.optimizer.physical.SkewJoinResolver.resolve(SkewJoinResolver.java:55)
	at org.apache.hadoop.hive.ql.optimizer.physical.PhysicalOptimizer.optimize(PhysicalOptimizer.java:107)
	at org.apache.hadoop.hive.ql.parse.MapReduceCompiler.optimizeTaskPlan(MapReduceCompiler.java:270)
	at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:227)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10219)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:211)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:459)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:316)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1189)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1237)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1126)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1116)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:216)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:168)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:379)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:739)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:684)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:624)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
--------------
2) However same query works fine when we set "hive.optimize.skewjoin = false" .  And we dont find this issue using Tez execution engine.</description>
			<version>1.2.1</version>
			<fixedVersion>1.3.0, 2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">14968</link>
		</links>
	</bug>
	<bug id="14387" opendate="2016-07-29 21:07:34" fixdate="2016-08-10 06:19:21" resolution="Duplicate">
		<buginformation>
			<summary>Add an option to skip the table names for the column headers</summary>
			<description>It would be good to have an option where the beeline output could skip reporting the &amp;lt;table_name&amp;gt;.&amp;lt;column_name&amp;gt; in the headers.
Eg:

0: jdbc:hive2://:&amp;gt; select * from sample_07 limit 1; 
--------------------------------------------------------------------------------------------------
sample_07.code	sample_07.description	sample_07.total_emp	sample_07.salary
--------------------------------------------------------------------------------------------------
00-0000	Operations	123	12345
--------------------------------------------------------------------------------------------------


b) After the option is set:

0: jdbc:hive2://:&amp;gt; select * from sample_07 limit 1; 
---------------------------------------------------
code	 description	total_emp	 salary
---------------------------------------------------
00-0000	Operations	123	12345

</description>
			<version>0.12.0</version>
			<fixedVersion></fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hive.jdbc.TestJdbcDriver2.java</file>
			<file type="M">org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">6687</link>
		</links>
	</bug>
	<bug id="13756" opendate="2016-05-13 09:20:22" fixdate="2016-08-10 16:56:35" resolution="Fixed">
		<buginformation>
			<summary>Map failure attempts to delete reducer _temporary directory on multi-query pig query</summary>
			<description>A pig script, executed with multi-query enabled, that reads the source data and writes it as-is into TABLE_A as well as performing a group-by operation on the data which is written into TABLE_B can produce erroneous results if any map fails. This results in a single MR job that writes the map output to a scratch directory relative to TABLE_A and the reducer output to a scratch directory relative to TABLE_B.
If one or more maps fail it will delete the attempt data relative to TABLE_A, but it also deletes the _temporary directory relative to TABLE_B. This has the unintended side-effect of preventing subsequent maps from committing their data. This means that any maps which successfully completed before the first map failure will have its data committed as expected, other maps not, resulting in an incomplete result set.</description>
			<version>1.2.1</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.java</file>
		</fixedFiles>
	</bug>
	<bug id="13754" opendate="2016-05-13 01:17:59" fixdate="2016-08-10 17:13:08" resolution="Fixed">
		<buginformation>
			<summary>Fix resource leak in HiveClientCache</summary>
			<description>Found that the users reference count can go into negative values, which prevents tearDownIfUnused from closing the client connection when called.
This leads to a build up of clients which have been evicted from the cache, are no longer in use, but have not been shutdown.
GC will eventually call finalize, which forcibly closes the connection and cleans up the client, but I have seen as many as several hundred open client connections as a result.
The main resource for this is caused by RetryingMetaStoreClient, which will call reconnect on acquire, which calls close. This will decrement users to -1 on the reconnect, then acquire will increase this to 0 while using it, and back to -1 when it releases it.</description>
			<version>1.2.1</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.hcatalog.common.HCatUtil.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.java</file>
			<file type="M">org.apache.hive.hcatalog.common.TestHiveClientCache.java</file>
			<file type="M">org.apache.hive.hcatalog.common.HiveClientCache.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
			<file type="M">org.apache.hive.hcatalog.common.HCatConstants.java</file>
		</fixedFiles>
	</bug>
	<bug id="12954" opendate="2016-01-28 10:57:16" fixdate="2016-08-11 17:29:50" resolution="Fixed">
		<buginformation>
			<summary>NPE with str_to_map on null strings</summary>
			<description>Running str_to_map on a null string will return a NullPointerException.
Workaround is to use coalesce.</description>
			<version>0.14.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFStringToMap.java</file>
		</fixedFiles>
	</bug>
	<bug id="14513" opendate="2016-08-10 19:44:58" fixdate="2016-08-12 15:32:25" resolution="Fixed">
		<buginformation>
			<summary>Enhance custom query feature in LDAP atn to support resultset of ldap groups</summary>
			<description>LDAP Authenticator can be configured to use a result set from a LDAP query to authenticate. However, is it expected that this LDAP query would only result a set of users (aka full DNs for the users in LDAP).
However, its not always straightforward to be able to author queries that return users. For example, say you would like to allow "all users from group1 and group2" to be authenticated. The LDAP query has to return a union of all members of the group1 and group2.
For example, one common configuration is that groups contain a list of its users
      "dn: uid=group1,ou=Groups,dc=example,dc=com",
      "distinguishedName: uid=group1,ou=Groups,dc=example,dc=com",
      "objectClass: top",
      "objectClass: groupOfNames",
      "objectClass: ExtensibleObject",
      "cn: group1",
      "ou: Groups",
      "sn: group1",
      "member: uid=user1,ou=People,dc=example,dc=com",
The query 
(&amp;amp;(objectClass=groupOfNames)(|(cn=group1)(cn=group2)))
will return the entries
uid=group1,ou=Groups,dc=example,dc=com
uid=group2,ou=Groups,dc=example,dc=com
but there is no means to form a query that would return just the values of "member" attributes. (ldap client tools are able to do by filtering out the attributes on these entries.
So it will be useful to have such support to be able to specify queries that return groups.</description>
			<version>1.0.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.service.auth.LdapAuthenticationProviderImpl.java</file>
			<file type="M">org.apache.hive.service.auth.TestLdapAtnProviderWithMiniDS.java</file>
		</fixedFiles>
	</bug>
	<bug id="14433" opendate="2016-08-04 21:02:14" fixdate="2016-08-12 20:58:12" resolution="Fixed">
		<buginformation>
			<summary>refactor LLAP plan cache avoidance and fix issue in merge processor</summary>
			<description>Map and reduce processors do this:

    if (LlapProxy.isDaemon()) {
      cache = new org.apache.hadoop.hive.ql.exec.mr.ObjectCache(); // do not cache plan
...


but merge processor just gets the plan. If it runs in LLAP, it can get a cached plan. Need to move this logic into ObjectCache itself, via a isPlan arg or something. That will also fix this issue for merge processor</description>
			<version>2.0.1</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.MergeFileRecordProcessor.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ObjectCacheFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.java</file>
		</fixedFiles>
	</bug>
	<bug id="14479" opendate="2016-08-08 22:56:11" fixdate="2016-08-12 21:28:07" resolution="Fixed">
		<buginformation>
			<summary>Add some join tests for acid table</summary>
			<description></description>
			<version>2.2.0</version>
			<fixedVersion>1.3.0, 2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.TestAcidOnTez.java</file>
			<file type="M">org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
		</fixedFiles>
		<links>
			<link type="Required" description="is required by">14534</link>
		</links>
	</bug>
	<bug id="14521" opendate="2016-08-11 20:07:40" fixdate="2016-08-12 23:27:53" resolution="Duplicate">
		<buginformation>
			<summary>codahale metrics exceptions</summary>
			<description>One some random setup, I see bazillions of errors like this in HS2 log:

2016-08-08 04:52:18,619 WARN  [HiveServer2-Handler-Pool: Thread-101]: log.PerfLogger (PerfLogger.java:beginMetrics(226)) - Error recording metrics
java.io.IOException: Scope named api_Driver.run is not closed, cannot be opened.
        at org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics$CodahaleMetricsScope.open(CodahaleMetrics.java:133)
        at org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics.startStoredScope(CodahaleMetrics.java:220)
        at org.apache.hadoop.hive.ql.log.PerfLogger.beginMetrics(PerfLogger.java:223)
        at org.apache.hadoop.hive.ql.log.PerfLogger.PerfLogBegin(PerfLogger.java:143)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:378)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:320)
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1214)
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1208)
        at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:146)
        at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:226)
        at org.apache.hive.service.cli.operation.Operation.run(Operation.java:276)
        at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:468)
        at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:456)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)


I suspect that either, just like the metastore deadline, this needs better error handling when whatever the metrics surround fails; or, it is just not thread safe.
But I actually haven&amp;amp;apos;t looked at the code yet.</description>
			<version>2.0.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.log.PerfLogger.java</file>
			<file type="M">org.apache.hive.jdbc.miniHS2.TestHs2Metrics.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">13410</link>
			<link type="Reference" description="is related to">13410</link>
		</links>
	</bug>
	<bug id="14448" opendate="2016-08-05 23:50:04" fixdate="2016-08-13 06:24:05" resolution="Fixed">
		<buginformation>
			<summary>Queries with predicate fail when ETL split strategy is chosen for ACID tables</summary>
			<description>When ETL split strategy is applied to ACID tables with predicate pushdown (SARG enabled), split generation fails for ACID. This bug will be usually exposed when working with data at scale, because in most otherwise cases only BI split strategy is chosen. My guess is that this is happening because the correct readerSchema is not being picked up when we try to extract SARG column names.
Quickest way to reproduce is to add the following unit test to ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands2.java
ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands2.java

 @Test
public void testETLSplitStrategyForACID() throws Exception {
hiveConf.setVar(HiveConf.ConfVars.HIVE_ORC_SPLIT_STRATEGY, "ETL");
hiveConf.setBoolVar(HiveConf.ConfVars.HIVEOPTINDEXFILTER, true);
runStatementOnDriver("insert into " + Table.ACIDTBL + " values(1,2)");
runStatementOnDriver("alter table " + Table.ACIDTBL + " compact &amp;amp;apos;MAJOR&amp;amp;apos;");
runWorker(hiveConf);
List&amp;lt;String&amp;gt; rs = runStatementOnDriver("select * from " +Table.ACIDTBL+ " where a = 1");
int[][] resultData = new int[][] {{1,2}};
Assert.assertEquals(stringifyValues(resultData), rs);
}


Back-trace for this failed test is as follows:


exec.Task: Job Submission failed with exception &amp;amp;apos;java.lang.RuntimeException(ORC split generation failed with exception: java.lang.NegativeArraySizeException)&amp;amp;apos;
java.lang.RuntimeException: ORC split generation failed with exception: java.lang.NegativeArraySizeException
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1570)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getSplits(OrcInputFormat.java:1656)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:370)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:488)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeOldSplits(JobSubmitter.java:329)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:321)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:197)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1297)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1294)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1656)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1294)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1656)
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)
	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)
	at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:417)
	at org.apache.hadoop.hive.ql.exec.mr.MapRedTask.execute(MapRedTask.java:141)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:197)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1962)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1653)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1389)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1131)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1119)
	at org.apache.hadoop.hive.ql.TestTxnCommands2.runStatementOnDriver(TestTxnCommands2.java:1292)
	at org.apache.hadoop.hive.ql.TestTxnCommands2.testETLSplitStrategyForACID(TestTxnCommands2.java:280)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:254)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:149)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)
Caused by: java.util.concurrent.ExecutionException: java.lang.NegativeArraySizeException
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1560)
	... 57 more
Caused by: java.lang.NegativeArraySizeException
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getSargColumnNames(OrcInputFormat.java:378)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.extractNeededColNames(OrcInputFormat.java:444)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.extractNeededColNames(OrcInputFormat.java:439)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.access$2800(OrcInputFormat.java:146)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.callInternal(OrcInputFormat.java:1274)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.access$2600(OrcInputFormat.java:1068)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator$1.run(OrcInputFormat.java:1248)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator$1.run(OrcInputFormat.java:1245)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1656)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:1245)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$ETLSplitStrategy.runGetSplitsSync(OrcInputFormat.java:883)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$ETLSplitStrategy.access$1300(OrcInputFormat.java:700)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$ETLSplitStrategy$1.run(OrcInputFormat.java:857)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$ETLSplitStrategy$1.run(OrcInputFormat.java:854)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1656)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$ETLSplitStrategy.call(OrcInputFormat.java:854)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$ETLSplitStrategy.call(OrcInputFormat.java:700)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

</description>
			<version>2.2.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">14035</link>
			<link type="Required" description="is required by">14607</link>
		</links>
	</bug>
	<bug id="14345" opendate="2016-07-26 20:28:59" fixdate="2016-08-15 16:35:03" resolution="Fixed">
		<buginformation>
			<summary>Beeline result table has erroneous characters </summary>
			<description>Beeline returns query results with erroneous characters. For example:


0: jdbc:hive2://xxxx:10000/def&amp;gt; select 10;
+------+--+
| _c0  |
+------+--+
| 10   |
+------+--+
1 row selected (3.207 seconds)

</description>
			<version>1.1.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.beeline.TableOutputFormat.java</file>
		</fixedFiles>
	</bug>
	<bug id="14483" opendate="2016-08-09 14:22:31" fixdate="2016-08-15 20:59:37" resolution="Fixed">
		<buginformation>
			<summary> java.lang.ArrayIndexOutOfBoundsException org.apache.orc.impl.TreeReaderFactory$BytesColumnVectorUtil.commonReadByteArrays</summary>
			<description>Error message:
Caused by: java.lang.ArrayIndexOutOfBoundsException: 1024
at org.apache.orc.impl.RunLengthIntegerReaderV2.nextVector(RunLengthIntegerReaderV2.java:369)
at org.apache.orc.impl.TreeReaderFactory$BytesColumnVectorUtil.commonReadByteArrays(TreeReaderFactory.java:1231)
at org.apache.orc.impl.TreeReaderFactory$BytesColumnVectorUtil.readOrcByteArrays(TreeReaderFactory.java:1268)
at org.apache.orc.impl.TreeReaderFactory$StringDirectTreeReader.nextVector(TreeReaderFactory.java:1368)
at org.apache.orc.impl.TreeReaderFactory$StringTreeReader.nextVector(TreeReaderFactory.java:1212)
at org.apache.orc.impl.TreeReaderFactory$ListTreeReader.nextVector(TreeReaderFactory.java:1902)
at org.apache.orc.impl.TreeReaderFactory$StructTreeReader.nextBatch(TreeReaderFactory.java:1737)
at org.apache.orc.impl.RecordReaderImpl.nextBatch(RecordReaderImpl.java:1045)
at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.ensureBatch(RecordReaderImpl.java:77)
at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.hasNext(RecordReaderImpl.java:89)
at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$OrcRecordReader.next(OrcInputFormat.java:230)
at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$OrcRecordReader.next(OrcInputFormat.java:205)
at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:350)
... 22 more
How to reproduce?
Configure StringTreeReader  which contains StringDirectTreeReader as TreeReader (DIRECT or DIRECT_V2 column encoding)
batchSize = 1026;
invoke method nextVector(ColumnVector previousVector,boolean[] isNull, final int batchSize)
scratchlcv is LongColumnVector with long[] vector  (length 1024)
 which execute BytesColumnVectorUtil.readOrcByteArrays(stream, lengths, scratchlcv,result, batchSize);
as result in method commonReadByteArrays(stream, lengths, scratchlcv,
            result, (int) batchSize) we received ArrayIndexOutOfBoundsException.
If we use StringDictionaryTreeReader, then there is no exception, as we have a verification  scratchlcv.ensureSize((int) batchSize, false) before reader.nextVector(scratchlcv, scratchlcv.vector, batchSize);
These changes were made for Hive 2.1.0 by corresponding commit https://github.com/apache/hive/commit/0ac424f0a17b341efe299da167791112e4a953e9#diff-a1cec556fb2db4b69a1a4127a6908177R1467 for task  https://issues.apache.org/jira/browse/HIVE-12159 by Owen O&amp;amp;apos;Malley
How to fix?
add  only one line :
scratchlcv.ensureSize((int) batchSize, false) ;
in method org.apache.orc.impl.TreeReaderFactory#BytesColumnVectorUtil#commonReadByteArrays(InStream stream, IntegerReader lengths,
        LongColumnVector scratchlcv,
        BytesColumnVector result, final int batchSize) before invocation lengths.nextVector(scratchlcv, scratchlcv.vector, batchSize);
</description>
			<version>2.1.0</version>
			<fixedVersion>1.3.0, 2.2.0, 2.1.1, 2.0.2</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.orc.impl.TreeReaderFactory.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">14968</link>
			<link type="Regression" description="is broken by">12159</link>
		</links>
	</bug>
	<bug id="14463" opendate="2016-08-08 08:21:29" fixdate="2016-08-16 18:08:03" resolution="Fixed">
		<buginformation>
			<summary>hcatalog server extensions test cases getting stuck</summary>
			<description>The module is getting stuck in tests and not coming out for as long as 2 days. 
Specifically, TestMsgBusConnection is the test case which has this problem. I ran the tests on local environment and took a thread dump after it got stuck. 

Full thread dump Java HotSpot(TM) 64-Bit Server VM (24.80-b11 mixed mode):
"InactivityMonitor Async Task: java.util.concurrent.ThreadPoolExecutor$Worker@2c040428[State = -1, empty queue]" daemon prio=5 tid=0x00007fe90d89e000 nid=0x8827 waiting on condition [0x0000000117b74000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  &amp;lt;0x000000078166f0b8&amp;gt; (a java.util.concurrent.SynchronousQueue$TransferStack)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
	at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:359)
	at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:942)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
"InactivityMonitor Async Task: java.util.concurrent.ThreadPoolExecutor$Worker@182a483f[State = -1, empty queue]" daemon prio=5 tid=0x00007fe90d801000 nid=0x585f waiting on condition [0x000000011786b000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  &amp;lt;0x000000078166f0b8&amp;gt; (a java.util.concurrent.SynchronousQueue$TransferStack)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
	at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:359)
	at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:942)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
"ActiveMQ Transport: tcp:///127.0.0.1:56883" daemon prio=5 tid=0x00007fe90c83e800 nid=0x8403 runnable [0x00000001196ab000]
   java.lang.Thread.State: RUNNABLE
	at java.net.SocketInputStream.socketRead0(Native Method)
	at java.net.SocketInputStream.read(SocketInputStream.java:152)
	at java.net.SocketInputStream.read(SocketInputStream.java:122)
	at org.apache.activemq.transport.tcp.TcpBufferedInputStream.fill(TcpBufferedInputStream.java:50)
	at org.apache.activemq.transport.tcp.TcpTransport$2.fill(TcpTransport.java:576)
	at org.apache.activemq.transport.tcp.TcpBufferedInputStream.read(TcpBufferedInputStream.java:58)
	at org.apache.activemq.transport.tcp.TcpTransport$2.read(TcpTransport.java:561)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.activemq.openwire.OpenWireFormat.unmarshal(OpenWireFormat.java:269)
	at org.apache.activemq.transport.tcp.TcpTransport.readCommand(TcpTransport.java:227)
	at org.apache.activemq.transport.tcp.TcpTransport.doRun(TcpTransport.java:219)
	at org.apache.activemq.transport.tcp.TcpTransport.run(TcpTransport.java:202)
	at java.lang.Thread.run(Thread.java:745)
"ActiveMQ Transport: tcp://localhost/127.0.0.1:61616" prio=5 tid=0x00007fe90b81e800 nid=0x8003 runnable [0x00000001194a5000]
   java.lang.Thread.State: RUNNABLE
	at java.net.SocketInputStream.socketRead0(Native Method)
	at java.net.SocketInputStream.read(SocketInputStream.java:152)
	at java.net.SocketInputStream.read(SocketInputStream.java:122)
	at org.apache.activemq.transport.tcp.TcpBufferedInputStream.fill(TcpBufferedInputStream.java:50)
	at org.apache.activemq.transport.tcp.TcpTransport$2.fill(TcpTransport.java:576)
	at org.apache.activemq.transport.tcp.TcpBufferedInputStream.read(TcpBufferedInputStream.java:58)
	at org.apache.activemq.transport.tcp.TcpTransport$2.read(TcpTransport.java:561)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.activemq.openwire.OpenWireFormat.unmarshal(OpenWireFormat.java:269)
	at org.apache.activemq.transport.tcp.TcpTransport.readCommand(TcpTransport.java:227)
	at org.apache.activemq.transport.tcp.TcpTransport.doRun(TcpTransport.java:219)
	at org.apache.activemq.transport.tcp.TcpTransport.run(TcpTransport.java:202)
	at java.lang.Thread.run(Thread.java:745)
"ActiveMQConnection[ID:IM1258-X1-56879-1470642083403-2:2] Scheduler" daemon prio=5 tid=0x00007fe90c861800 nid=0x7e03 in Object.wait() [0x000000011926e000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on &amp;lt;0x00000007f594cc20&amp;gt; (a java.util.TaskQueue)
	at java.lang.Object.wait(Object.java:503)
	at java.util.TimerThread.mainLoop(Timer.java:526)
	- locked &amp;lt;0x00000007f594cc20&amp;gt; (a java.util.TaskQueue)
	at java.util.TimerThread.run(Timer.java:505)
"BoneCP-pool-watch-thread" daemon prio=5 tid=0x00007fe90b23b000 nid=0x6d07 waiting on condition [0x0000000118a4f000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  &amp;lt;0x00000007f5b29bc0&amp;gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
	at java.util.concurrent.ArrayBlockingQueue.take(ArrayBlockingQueue.java:374)
	at com.jolbox.bonecp.PoolWatchThread.run(PoolWatchThread.java:75)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
"BoneCP-keep-alive-scheduler" daemon prio=5 tid=0x00007fe90c937000 nid=0x7107 waiting on condition [0x0000000118746000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  &amp;lt;0x00000007f5b40a30&amp;gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1090)
	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:807)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
"com.google.common.base.internal.Finalizer" daemon prio=5 tid=0x00007fe90c8fd000 nid=0x7707 in Object.wait() [0x0000000118540000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on &amp;lt;0x00000007f5b40608&amp;gt; (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:135)
	- locked &amp;lt;0x00000007f5b40608&amp;gt; (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:151)
	at com.google.common.base.internal.Finalizer.run(Finalizer.java:132)
	at java.lang.Thread.run(Thread.java:745)
"BoneCP-pool-watch-thread" daemon prio=5 tid=0x00007fe90b464800 nid=0x7c03 waiting on condition [0x00000001193a2000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  &amp;lt;0x00000007f5b41470&amp;gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
	at java.util.concurrent.ArrayBlockingQueue.take(ArrayBlockingQueue.java:374)
	at com.jolbox.bonecp.PoolWatchThread.run(PoolWatchThread.java:75)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
"BoneCP-keep-alive-scheduler" daemon prio=5 tid=0x00007fe90c07f800 nid=0x7a03 waiting on condition [0x0000000118e28000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  &amp;lt;0x00000007f5b41748&amp;gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1090)
	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:807)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
"com.google.common.base.internal.Finalizer" daemon prio=5 tid=0x00007fe90c157800 nid=0x300b in Object.wait() [0x0000000118c55000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on &amp;lt;0x00000007f5b41b18&amp;gt; (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:135)
	- locked &amp;lt;0x00000007f5b41b18&amp;gt; (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:151)
	at com.google.common.base.internal.Finalizer.run(Finalizer.java:132)
	at java.lang.Thread.run(Thread.java:745)
"derby.rawStoreDaemon" daemon prio=5 tid=0x00007fe90c866800 nid=0x3207 in Object.wait() [0x0000000118b52000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on &amp;lt;0x00000007f58bb788&amp;gt; (a org.apache.derby.impl.services.daemon.BasicDaemon)
	at org.apache.derby.impl.services.daemon.BasicDaemon.rest(Unknown Source)
	- locked &amp;lt;0x00000007f58bb788&amp;gt; (a org.apache.derby.impl.services.daemon.BasicDaemon)
	at org.apache.derby.impl.services.daemon.BasicDaemon.run(Unknown Source)
	at java.lang.Thread.run(Thread.java:745)
"Timer-0" daemon prio=5 tid=0x00007fe90c842000 nid=0x2f07 in Object.wait() [0x00000001144ed000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on &amp;lt;0x00000007f59c9230&amp;gt; (a java.util.TaskQueue)
	at java.lang.Object.wait(Object.java:503)
	at java.util.TimerThread.mainLoop(Timer.java:526)
	- locked &amp;lt;0x00000007f59c9230&amp;gt; (a java.util.TaskQueue)
	at java.util.TimerThread.run(Timer.java:505)
"InactivityMonitor WriteCheck" daemon prio=5 tid=0x00007fe90b21d000 nid=0x7503 in Object.wait() [0x000000011894c000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on &amp;lt;0x0000000781663e20&amp;gt; (a java.util.TaskQueue)
	at java.util.TimerThread.mainLoop(Timer.java:552)
	- locked &amp;lt;0x0000000781663e20&amp;gt; (a java.util.TaskQueue)
	at java.util.TimerThread.run(Timer.java:505)
"InactivityMonitor ReadCheck" daemon prio=5 tid=0x00007fe90b21c000 nid=0x7303 in Object.wait() [0x0000000118849000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on &amp;lt;0x000000078166ec48&amp;gt; (a java.util.TaskQueue)
	at java.util.TimerThread.mainLoop(Timer.java:552)
	- locked &amp;lt;0x000000078166ec48&amp;gt; (a java.util.TaskQueue)
	at java.util.TimerThread.run(Timer.java:505)
"ActiveMQ Transport: tcp:///127.0.0.1:56880" daemon prio=5 tid=0x00007fe90b227800 nid=0x6f03 runnable [0x0000000118643000]
   java.lang.Thread.State: RUNNABLE
	at java.net.SocketInputStream.socketRead0(Native Method)
	at java.net.SocketInputStream.read(SocketInputStream.java:152)
	at java.net.SocketInputStream.read(SocketInputStream.java:122)
	at org.apache.activemq.transport.tcp.TcpBufferedInputStream.fill(TcpBufferedInputStream.java:50)
	at org.apache.activemq.transport.tcp.TcpTransport$2.fill(TcpTransport.java:576)
	at org.apache.activemq.transport.tcp.TcpBufferedInputStream.read(TcpBufferedInputStream.java:58)
	at org.apache.activemq.transport.tcp.TcpTransport$2.read(TcpTransport.java:561)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.activemq.openwire.OpenWireFormat.unmarshal(OpenWireFormat.java:269)
	at org.apache.activemq.transport.tcp.TcpTransport.readCommand(TcpTransport.java:227)
	at org.apache.activemq.transport.tcp.TcpTransport.doRun(TcpTransport.java:219)
	at org.apache.activemq.transport.tcp.TcpTransport.run(TcpTransport.java:202)
	at java.lang.Thread.run(Thread.java:745)
"ActiveMQ Transport: tcp://localhost/127.0.0.1:61616" prio=5 tid=0x00007fe9099a7000 nid=0x6b03 runnable [0x000000011843d000]
   java.lang.Thread.State: RUNNABLE
	at java.net.SocketInputStream.socketRead0(Native Method)
	at java.net.SocketInputStream.read(SocketInputStream.java:152)
	at java.net.SocketInputStream.read(SocketInputStream.java:122)
	at org.apache.activemq.transport.tcp.TcpBufferedInputStream.fill(TcpBufferedInputStream.java:50)
	at org.apache.activemq.transport.tcp.TcpTransport$2.fill(TcpTransport.java:576)
	at org.apache.activemq.transport.tcp.TcpBufferedInputStream.read(TcpBufferedInputStream.java:58)
	at org.apache.activemq.transport.tcp.TcpTransport$2.read(TcpTransport.java:561)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.activemq.openwire.OpenWireFormat.unmarshal(OpenWireFormat.java:269)
	at org.apache.activemq.transport.tcp.TcpTransport.readCommand(TcpTransport.java:227)
	at org.apache.activemq.transport.tcp.TcpTransport.doRun(TcpTransport.java:219)
	at org.apache.activemq.transport.tcp.TcpTransport.run(TcpTransport.java:202)
	at java.lang.Thread.run(Thread.java:745)
"ActiveMQConnection[ID:IM1258-X1-56879-1470642083403-2:1] Scheduler" daemon prio=5 tid=0x00007fe90aa08800 nid=0x6903 in Object.wait() [0x000000011833a000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on &amp;lt;0x00000007816689e8&amp;gt; (a java.util.TaskQueue)
	at java.lang.Object.wait(Object.java:503)
	at java.util.TimerThread.mainLoop(Timer.java:526)
	- locked &amp;lt;0x00000007816689e8&amp;gt; (a java.util.TaskQueue)
	at java.util.TimerThread.run(Timer.java:505)
"ActiveMQ Transport Server: tcp://localhost:61616?broker.persistent=false" daemon prio=5 tid=0x00007fe90b211000 nid=0x6703 runnable [0x0000000118237000]
   java.lang.Thread.State: RUNNABLE
	at java.net.PlainSocketImpl.socketAccept(Native Method)
	at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:398)
	at java.net.ServerSocket.implAccept(ServerSocket.java:530)
	at java.net.ServerSocket.accept(ServerSocket.java:498)
	at org.apache.activemq.transport.tcp.TcpTransportServer.run(TcpTransportServer.java:280)
	at java.lang.Thread.run(Thread.java:745)
"ActiveMQ Transport Server Thread Handler: tcp://localhost:61616?broker.persistent=false" daemon prio=5 tid=0x00007fe90b1ff000 nid=0x6503 waiting on condition [0x0000000118134000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  &amp;lt;0x000000078009d210&amp;gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
	at org.apache.activemq.transport.tcp.TcpTransportServer$1.run(TcpTransportServer.java:352)
	at java.lang.Thread.run(Thread.java:745)
"ActiveMQ Broker[localhost] Scheduler" daemon prio=5 tid=0x00007fe90aa08000 nid=0x6303 in Object.wait() [0x0000000117e7d000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on &amp;lt;0x000000078159b640&amp;gt; (a java.util.TaskQueue)
	at java.util.TimerThread.mainLoop(Timer.java:552)
	- locked &amp;lt;0x000000078159b640&amp;gt; (a java.util.TaskQueue)
	at java.util.TimerThread.run(Timer.java:505)
"ActiveMQ Data File Writer" daemon prio=5 tid=0x00007fe90b201800 nid=0x6103 in Object.wait() [0x0000000117d7a000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on &amp;lt;0x00000007814e62f8&amp;gt; (a org.apache.kahadb.journal.DataFileAppender$1)
	at java.lang.Object.wait(Object.java:503)
	at org.apache.kahadb.journal.DataFileAppender.processQueue(DataFileAppender.java:312)
	- locked &amp;lt;0x00000007814e62f8&amp;gt; (a org.apache.kahadb.journal.DataFileAppender$1)
	at org.apache.kahadb.journal.DataFileAppender$2.run(DataFileAppender.java:203)
"ActiveMQ Journal Checkpoint Worker" daemon prio=5 tid=0x00007fe90b1fc000 nid=0x5f03 waiting on condition [0x0000000117c77000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.apache.activemq.store.kahadb.MessageDatabase$3.run(MessageDatabase.java:296)
"KahaDB Scheduler" daemon prio=5 tid=0x00007fe90aad6000 nid=0x4f07 in Object.wait() [0x0000000117455000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on &amp;lt;0x00000007814f6ea0&amp;gt; (a java.util.TaskQueue)
	at java.util.TimerThread.mainLoop(Timer.java:552)
	- locked &amp;lt;0x00000007814f6ea0&amp;gt; (a java.util.TaskQueue)
	at java.util.TimerThread.run(Timer.java:505)
"RMI RenewClean-[10.14.123.167:56875]" daemon prio=5 tid=0x00007fe90b1fe000 nid=0x5b03 in Object.wait() [0x0000000117a71000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on &amp;lt;0x0000000781607678&amp;gt; (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:135)
	- locked &amp;lt;0x0000000781607678&amp;gt; (a java.lang.ref.ReferenceQueue$Lock)
	at sun.rmi.transport.DGCClient$EndpointEntry$RenewCleanThread.run(DGCClient.java:535)
	at java.lang.Thread.run(Thread.java:745)
"RMI Scheduler(0)" daemon prio=5 tid=0x00007fe90b1fd800 nid=0x5903 waiting on condition [0x000000011796e000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  &amp;lt;0x00000007801bcf88&amp;gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1090)
	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:807)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
"GC Daemon" daemon prio=5 tid=0x00007fe9099c8800 nid=0x5503 in Object.wait() [0x000000011775e000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on &amp;lt;0x00000007800106b0&amp;gt; (a sun.misc.GC$LatencyLock)
	at sun.misc.GC$Daemon.run(GC.java:117)
	- locked &amp;lt;0x00000007800106b0&amp;gt; (a sun.misc.GC$LatencyLock)
"RMI Reaper" prio=5 tid=0x00007fe90b1c0800 nid=0x5303 in Object.wait() [0x000000011765b000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on &amp;lt;0x0000000780000b50&amp;gt; (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:135)
	- locked &amp;lt;0x0000000780000b50&amp;gt; (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:151)
	at sun.rmi.transport.ObjectTable$Reaper.run(ObjectTable.java:351)
	at java.lang.Thread.run(Thread.java:745)
"RMI TCP Accept-0" daemon prio=5 tid=0x00007fe90b1ad800 nid=0x5103 runnable [0x0000000117558000]
   java.lang.Thread.State: RUNNABLE
	at java.net.PlainSocketImpl.socketAccept(Native Method)
	at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:398)
	at java.net.ServerSocket.implAccept(ServerSocket.java:530)
	at java.net.ServerSocket.accept(ServerSocket.java:498)
	at sun.rmi.transport.tcp.TCPTransport$AcceptLoop.executeAcceptLoop(TCPTransport.java:399)
	at sun.rmi.transport.tcp.TCPTransport$AcceptLoop.run(TCPTransport.java:371)
	at java.lang.Thread.run(Thread.java:745)
"RMI TCP Accept-1099" daemon prio=5 tid=0x00007fe90b0b0800 nid=0x4e07 runnable [0x0000000117352000]
   java.lang.Thread.State: RUNNABLE
	at java.net.PlainSocketImpl.socketAccept(Native Method)
	at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:398)
	at java.net.ServerSocket.implAccept(ServerSocket.java:530)
	at java.net.ServerSocket.accept(ServerSocket.java:498)
	at sun.rmi.transport.tcp.TCPTransport$AcceptLoop.executeAcceptLoop(TCPTransport.java:399)
	at sun.rmi.transport.tcp.TCPTransport$AcceptLoop.run(TCPTransport.java:371)
	at java.lang.Thread.run(Thread.java:745)
"Service Thread" daemon prio=5 tid=0x00007fe909852000 nid=0x4503 runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE
"C2 CompilerThread1" daemon prio=5 tid=0x00007fe90b023000 nid=0x4303 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE
"C2 CompilerThread0" daemon prio=5 tid=0x00007fe90b021800 nid=0x4103 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE
"Signal Dispatcher" daemon prio=5 tid=0x00007fe90b01c800 nid=0x340f waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE
"Finalizer" daemon prio=5 tid=0x00007fe909848800 nid=0x2d03 in Object.wait() [0x00000001143a9000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on &amp;lt;0x0000000780019310&amp;gt; (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:135)
	- locked &amp;lt;0x0000000780019310&amp;gt; (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:151)
	at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:209)
"Reference Handler" daemon prio=5 tid=0x00007fe90b00c800 nid=0x2b03 in Object.wait() [0x00000001142a6000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on &amp;lt;0x0000000780018fd8&amp;gt; (a java.lang.ref.Reference$Lock)
	at java.lang.Object.wait(Object.java:503)
	at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:133)
	- locked &amp;lt;0x0000000780018fd8&amp;gt; (a java.lang.ref.Reference$Lock)
"main" prio=5 tid=0x00007fe90980b000 nid=0x1303 in Object.wait() [0x0000000109b16000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on &amp;lt;0x0000000781669298&amp;gt; (a java.lang.Object)
	at java.lang.Object.wait(Object.java:503)
	at org.apache.activemq.SimplePriorityMessageDispatchChannel.dequeue(SimplePriorityMessageDispatchChannel.java:87)
	- locked &amp;lt;0x0000000781669298&amp;gt; (a java.lang.Object)
	at org.apache.activemq.ActiveMQMessageConsumer.dequeue(ActiveMQMessageConsumer.java:452)
	at org.apache.activemq.ActiveMQMessageConsumer.receive(ActiveMQMessageConsumer.java:504)
	at org.apache.hive.hcatalog.listener.TestMsgBusConnection.testConnection(TestMsgBusConnection.java:91)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at junit.framework.TestCase.runTest(TestCase.java:176)
	at junit.framework.TestCase.runBare(TestCase.java:141)
	at junit.framework.TestResult$1.protect(TestResult.java:122)
	at junit.framework.TestResult.runProtected(TestResult.java:142)
	at junit.framework.TestResult.run(TestResult.java:125)
	at junit.framework.TestCase.run(TestCase.java:129)
	at junit.framework.TestSuite.runTest(TestSuite.java:255)
	at junit.framework.TestSuite.run(TestSuite.java:250)
	at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:84)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:264)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)
"VM Thread" prio=5 tid=0x00007fe90a804000 nid=0x2903 runnable
"GC task thread#0 (ParallelGC)" prio=5 tid=0x00007fe90b004800 nid=0x2103 runnable
"GC task thread#1 (ParallelGC)" prio=5 tid=0x00007fe90b005800 nid=0x2303 runnable
"GC task thread#2 (ParallelGC)" prio=5 tid=0x00007fe90b006000 nid=0x2503 runnable
"GC task thread#3 (ParallelGC)" prio=5 tid=0x00007fe90a006000 nid=0x2703 runnable
"VM Periodic Task Thread" prio=5 tid=0x00007fe90b013000 nid=0x4703 waiting on condition
JNI global references: 156
Heap
 PSYoungGen      total 526336K, used 184063K [0x00000007d5500000, 0x00000007f6d00000, 0x0000000800000000)
  eden space 503808K, 33% used [0x00000007d5500000,0x00000007dfa320c0,0x00000007f4100000)
  from space 22528K, 66% used [0x00000007f5700000,0x00000007f658de08,0x00000007f6d00000)
  to   space 22528K, 0% used [0x00000007f4100000,0x00000007f4100000,0x00000007f5700000)
 ParOldGen       total 174592K, used 27797K [0x0000000780000000, 0x000000078aa80000, 0x00000007d5500000)
  object space 174592K, 15% used [0x0000000780000000,0x0000000781b257c8,0x000000078aa80000)
 PSPermGen       total 53760K, used 53538K [0x0000000760000000, 0x0000000763480000, 0x0000000780000000)
  object space 53760K, 99% used [0x0000000760000000,0x0000000763448810,0x0000000763480000)



 Ashutosh Chauhan Can you check since you&amp;amp;apos;re the author of this test. 
</description>
			<version>2.1.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.hcatalog.listener.TestMsgBusConnection.java</file>
		</fixedFiles>
	</bug>
	<bug id="12656" opendate="2015-12-11 19:18:26" fixdate="2016-08-16 22:22:07" resolution="Fixed">
		<buginformation>
			<summary>Turn hive.compute.query.using.stats on by default</summary>
			<description>We now have hive.compute.query.using.stats=false by default. We plan to turn it on by default so that we can have better performance. We can also set it to false in some test cases to maintain the original purpose of those tests..</description>
			<version>2.1.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">11160</link>
		</links>
	</bug>
	<bug id="14556" opendate="2016-08-17 09:19:53" fixdate="2016-08-18 01:03:13" resolution="Fixed">
		<buginformation>
			<summary>Load data into text table fail caused by IndexOutOfBoundsException</summary>
			<description>

echo "1" &amp;gt; foo.txt

0: jdbc:hive2://189.39.151.74:21066/&amp;gt; create table foo(id int) stored as textfile;
No rows affected (1.846 seconds)
0: jdbc:hive2://189.39.151.74:21066/&amp;gt; load data local inpath &amp;amp;apos;/foo.txt&amp;amp;apos; into table foo;
Error: Error while compiling statement: FAILED: SemanticException Unable to load data to destination table. Error: java.lang.IndexOutOfBoundsException (state=42000,code=40000)


Exception:


2016-08-17 17:15:36,301 | ERROR | HiveServer2-Handler-Pool: Thread-55 | FAILED: SemanticException Unable to load data to destination table. Error: java.lang.IndexOutOfBoundsException
org.apache.hadoop.hive.ql.parse.SemanticException: Unable to load data to destination table. Error: java.lang.IndexOutOfBoundsException
        at org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.ensureFileFormatsMatch(LoadSemanticAnalyzer.java:356)
        at org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.analyzeInternal(LoadSemanticAnalyzer.java:236)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:238)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:473)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:325)
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1358)
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1340

</description>
			<version>1.3.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">13185</link>
		</links>
	</bug>
	<bug id="14566" opendate="2016-08-18 01:35:12" fixdate="2016-08-19 07:13:28" resolution="Fixed">
		<buginformation>
			<summary>LLAP IO reads timestamp wrongly</summary>
			<description>HIVE-10127 is causing incorrect results when orc_merge12.q is run in llap.
It reads timestamp wrongly.
LLAP IO Enabled

hive&amp;gt; select atimestamp1 from alltypesorc3xcols limit 10;
OK
1969-12-31 15:59:46.674
NULL
1969-12-31 15:59:55.787
1969-12-31 15:59:44.187
1969-12-31 15:59:50.434
1969-12-31 16:00:15.007
1969-12-31 16:00:07.021
1969-12-31 16:00:04.963
1969-12-31 15:59:52.176
1969-12-31 15:59:44.569


LLAP IO Disabled

hive&amp;gt; select atimestamp1 from alltypesorc3xcols limit 10;
OK
1969-12-31 15:59:46.674
NULL
1969-12-31 15:59:55.787
1969-12-31 15:59:44.187
1969-12-31 15:59:50.434
1969-12-31 16:00:14.007
1969-12-31 16:00:06.021
1969-12-31 16:00:03.963
1969-12-31 15:59:52.176
1969-12-31 15:59:44.569

</description>
			<version>2.0.1</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.orc.impl.TreeReaderFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.encoded.EncodedTreeReaderFactory.java</file>
			<file type="M">org.apache.hadoop.hive.llap.io.metadata.OrcStripeMetadata.java</file>
			<file type="M">org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.java</file>
		</fixedFiles>
	</bug>
	<bug id="14563" opendate="2016-08-17 23:49:07" fixdate="2016-08-19 18:29:52" resolution="Fixed">
		<buginformation>
			<summary>StatsOptimizer treats NULL in a wrong way</summary>
			<description>

OSTHOOK: query: explain select count(key) from (select null as key from src)src
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: 1
      Processor Tree:
        ListSink

PREHOOK: query: select count(key) from (select null as key from src)src
PREHOOK: type: QUERY
PREHOOK: Input: default@src
#### A masked pattern was here ####
POSTHOOK: query: select count(key) from (select null as key from src)src
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
#### A masked pattern was here ####
500

</description>
			<version>2.1.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.service.cli.session.TestSessionManagerMetrics.java</file>
			<file type="M">org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
			<file type="M">org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.StatsOptimizer.java</file>
		</fixedFiles>
	</bug>
	<bug id="14435" opendate="2016-08-05 09:47:42" fixdate="2016-08-19 23:39:00" resolution="Fixed">
		<buginformation>
			<summary>Vectorization: missed vectorization for const varchar()</summary>
			<description>

2016-08-05T09:45:16,488  INFO [main] physical.Vectorizer: Failed to vectorize
2016-08-05T09:45:16,488  INFO [main] physical.Vectorizer: Cannot vectorize select expression: Const varchar(1) f


The constant throws an illegal argument because the varchar precision is lost in the pipeline.</description>
			<version>2.2.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.java</file>
		</fixedFiles>
	</bug>
	<bug id="14446" opendate="2016-08-05 21:51:18" fixdate="2016-08-24 18:25:22" resolution="Fixed">
		<buginformation>
			<summary>Add switch to control BloomFilter in Hybrid grace hash join and make the FPP adjustable</summary>
			<description>When row count exceeds certain limit, it doesn&amp;amp;apos;t make sense to generate a bloom filter, since its size will be a few hundred MB or even a few GB.</description>
			<version>1.3.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.common.util.TestBloomFilter.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
	</bug>
	<bug id="13403" opendate="2016-03-31 21:11:19" fixdate="2016-08-24 23:37:56" resolution="Fixed">
		<buginformation>
			<summary>Make Streaming API not create empty buckets</summary>
			<description>as of HIVE-11983, when a TransactionBatch is opened in StreamingAPI, a full compliment of bucket files (AbstractRecordWriter.createRecordUpdaters()) is created on disk even though some may end up receiving no data.
It would be better to create them on demand and not clog the FS.
Tez can handle missing (empty) buckets and on MR bucket join algorithms will check if all buckets are there and bail out if not.  </description>
			<version>1.3.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.hcatalog.streaming.AbstractRecordWriter.java</file>
			<file type="M">org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
			<file type="M">org.apache.hive.hcatalog.streaming.StrictJsonWriter.java</file>
			<file type="M">org.apache.hive.hcatalog.streaming.TestStreaming.java</file>
			<file type="M">org.apache.hive.hcatalog.streaming.DelimitedInputWriter.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">11983</link>
			<link type="Reference" description="is related to">11719</link>
		</links>
	</bug>
	<bug id="14617" opendate="2016-08-24 16:30:55" fixdate="2016-08-25 18:17:28" resolution="Fixed">
		<buginformation>
			<summary>NPE in UDF MapValues() if input is null</summary>
			<description>For query


select exploded_traits from hdrone.vehiclestore_udr_vehicle 
lateral view explode(map_values(vehicle_traits.vehicle_traits)) traits as exploded_traits 
where datestr &amp;gt; &amp;amp;apos;2016-08-22&amp;amp;apos; LIMIT 100


Job fails with error msg as follows:


Error: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {"ts":null,"_max_added_id":null,"identity_info":null,"vehicle_specs":null,"tracking_info":null,"color_info":null,"vehicle_traits":null,"detail_info":null,"_row_key":null,"_shard":null,"image_info":null,"vehicle_tags":null,"activation_info":null,"flavor_info":null,"sounds":null,"legacy_info":null,"images":null,"datestr":"2016-08-24"} at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:179) at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54) at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158) Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {"ts":null,"_max_added_id":null,"identity_info":null,"vehicle_specs":null,"tracking_info":null,"color_info":null,"vehicle_traits":null,"detail_info":null,"_row_key":null,"_shard":null,"image_info":null,"vehicle_tags":null,"activation_info":null,"flavor_info":null,"sounds":null,"legacy_info":null,"images":null,"datestr":"2016-08-24"} at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:507) at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:170) ... 8 more Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Error evaluating map_values(vehicle_traits.vehicle_traits) at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:82) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815) at org.apache.hadoop.hive.ql.exec.LateralViewForwardOperator.processOp(LateralViewForwardOperator.java:37) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815) at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95) at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:157) at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:497) ... 9 more Caused by: java.lang.NullPointerException at org.apache.hadoop.hive.ql.udf.generic.GenericUDFMapValues.evaluate(GenericUDFMapValues.java:64) at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:185) at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:77) at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:65) at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:77) ... 15 more 


It appears that null is not properly handled in GenericUDFMapValues.evaluate() method.</description>
			<version>2.1.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFMapValues.java</file>
		</fixedFiles>
	</bug>
	<bug id="14619" opendate="2016-08-24 19:05:06" fixdate="2016-08-26 00:31:00" resolution="Fixed">
		<buginformation>
			<summary>CASE folding can produce wrong expression</summary>
			<description>This is a case that was not covered in the testsuite. For the following query:


select (CASE WHEN key = value THEN &amp;amp;apos;1&amp;amp;apos; WHEN true THEN &amp;amp;apos;0&amp;amp;apos; ELSE NULL END)
from src


Currently, we end up folding the select expression to &amp;amp;apos;0&amp;amp;apos;, as we fail bail out in the second statement and fail to account that there are two different possible values for the CASE expression (&amp;amp;apos;1&amp;amp;apos; and &amp;amp;apos;0&amp;amp;apos;).</description>
			<version>2.1.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.HiveRexUtil.java</file>
		</fixedFiles>
	</bug>
	<bug id="14360" opendate="2016-07-27 16:16:45" fixdate="2016-08-26 15:11:17" resolution="Fixed">
		<buginformation>
			<summary>Starting BeeLine after using !save, there is an error logged: "Error setting configuration: conf"</summary>
			<description>When saving the configuration in BeeLine the conf attribute is persisted, which should not. When loading the configuration this causes an error message to be printed:

Error setting configuration: conf: java.lang.IllegalArgumentException: No method matching "setconf" was found in org.apache.hive.beeline.BeeLineOpts.

</description>
			<version>2.2.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.beeline.BeeLineOpts.java</file>
		</fixedFiles>
	</bug>
	<bug id="14155" opendate="2016-07-02 18:25:27" fixdate="2016-08-27 08:26:13" resolution="Fixed">
		<buginformation>
			<summary>Vectorization: Custom UDF Vectorization annotations are ignored</summary>
			<description>

@VectorizedExpressions(value = { VectorStringRot13.class })


in a custom UDF Is ignored because the check for annotations happens after custom UDF detection.
The custom UDF codepath is on the fail-over track of annotation lookups, so the detection during validation of SEL is sufficient, instead of during expression creation.</description>
			<version>2.2.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
		</fixedFiles>
	</bug>
	<bug id="14648" opendate="2016-08-25 22:47:26" fixdate="2016-08-27 08:39:07" resolution="Fixed">
		<buginformation>
			<summary>LLAP: Avoid private pages in the SSD cache</summary>
			<description>There&amp;amp;apos;s no reason for the SSD cache to have private mappings to the cache file, there&amp;amp;apos;s only one reader and the memory overheads aren&amp;amp;apos;t worth it.</description>
			<version>2.2.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.llap.cache.BuddyAllocator.java</file>
		</fixedFiles>
	</bug>
	<bug id="14659" opendate="2016-08-27 16:14:46" fixdate="2016-08-29 18:37:49" resolution="Fixed">
		<buginformation>
			<summary>OutputStream won&amp;apos;t close if caught exception in funtion unparseExprForValuesClause in SemanticAnalyzer.java</summary>
			<description>I hava met the problem that Hive process cannot create new threads because of lots of OutputStream not closed.
Here is the part of jstack info:
"Thread-35783" daemon prio=10 tid=0x00007f8f58f02800 nid=0x18cc in Object.wait() [0x00007f8e632c0000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:577)

locked &amp;lt;0x000000061af52d50&amp;gt; (a java.util.LinkedList)

and the related error log:
org.apache.hadoop.hive.ql.parse.SemanticException: Unable to create temp file for insert values Expression of type TOK_TABLE_OR_COL not supported in insert/values
    at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genValuesTempTable(SemanticAnalyzer.java:812)
    at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.doPhase1(SemanticAnalyzer.java:1207)
    at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.doPhase1(SemanticAnalyzer.java:1410)
    at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genResolvedParseTree(SemanticAnalyzer.java:10136)
Caused by: org.apache.hadoop.hive.ql.parse.SemanticException: Expression of type TOK_TABLE_OR_COL not supported in insert/values
    at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.unparseExprForValuesClause(SemanticAnalyzer.java:858)
    at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genValuesTempTable(SemanticAnalyzer.java:785)
    ... 15 more
It shows the output stream won&amp;amp;apos;t close if caught exception in funtion unparseExprForValuesClause in SemanticAnalyzer.java</description>
			<version>1.1.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
	</bug>
	<bug id="13610" opendate="2016-04-26 08:53:17" fixdate="2016-08-30 19:30:13" resolution="Fixed">
		<buginformation>
			<summary>Hive exec module won&amp;apos;t compile with IBM JDK</summary>
			<description>org.apache.hadoop.hive.ql.debug.Utils explicitly import com.sun.management.HotSpotDiagnosticMXBean which is not supported by IBM JDK.
So we can make HotSpotDiagnosticMXBean as runtime but not compile.</description>
			<version>1.1.0</version>
			<fixedVersion>1.3.0, 2.2.0, 2.1.1, 2.0.2</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.debug.Utils.java</file>
		</fixedFiles>
	</bug>
	<bug id="14658" opendate="2016-08-27 07:14:33" fixdate="2016-08-31 00:30:37" resolution="Fixed">
		<buginformation>
			<summary>UDF abs throws NPE when input arg type is string</summary>
			<description>I know this is not the right use case, but NPE is not exptected.


0: jdbc:hive2://10.64.35.144:21066/&amp;gt; select abs("foo");
Error: Error while compiling statement: FAILED: NullPointerException null (state=42000,code=40000)

</description>
			<version>1.3.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFAbs.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFAbs.java</file>
		</fixedFiles>
	</bug>
	<bug id="14674" opendate="2016-08-31 00:29:45" fixdate="2016-08-31 22:38:12" resolution="Fixed">
		<buginformation>
			<summary> Incorrect syntax near the keyword &amp;apos;with&amp;apos; using MS SQL Server</summary>
			<description>addForUpdateClause() in TxnHandler incorrectly handles queries with WHERE clause
</description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TestTxnUtils.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
		</fixedFiles>
	</bug>
	<bug id="14652" opendate="2016-08-26 01:33:05" fixdate="2016-08-31 22:43:40" resolution="Fixed">
		<buginformation>
			<summary>incorrect results for not in on partition columns</summary>
			<description>
create table foo (i int) partitioned by (s string);

insert overwrite table foo partition(s=&amp;amp;apos;foo&amp;amp;apos;) select cint from alltypesorc limit 10;
insert overwrite table foo partition(s=&amp;amp;apos;bar&amp;amp;apos;) select cint from alltypesorc limit 10;

select * from foo where s not in (&amp;amp;apos;bar&amp;amp;apos;);


No results. IN ... works correctly</description>
			<version>2.1.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcFactory.java</file>
		</fixedFiles>
		<links>
			<link type="Regression" description="is broken by">11424</link>
		</links>
	</bug>
	<bug id="14677" opendate="2016-08-31 08:41:24" fixdate="2016-09-01 02:29:25" resolution="Duplicate">
		<buginformation>
			<summary>Beeline should support executing an initial SQL script</summary>
			<description></description>
			<version>0.10.0</version>
			<fixedVersion></fixedVersion>
			<type>Sub-task</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.beeline.BeeLine.java</file>
			<file type="M">org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
			<file type="M">org.apache.hive.beeline.Commands.java</file>
			<file type="M">org.apache.hive.beeline.BeeLineOpts.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">6561</link>
		</links>
	</bug>
	<bug id="14538" opendate="2016-08-15 20:29:45" fixdate="2016-09-01 22:22:39" resolution="Fixed">
		<buginformation>
			<summary>beeline throws exceptions with parsing hive config when using !sh statement</summary>
			<description>When beeline has a connection to a server, in some env it has following problem:

0: jdbc:hive2://localhost&amp;gt; !verbose
verbose: on
0: jdbc:hive2://localhost&amp;gt; !sh id
java.lang.ArrayIndexOutOfBoundsException: 1
at org.apache.hive.beeline.Commands.addConf(Commands.java:758)
at org.apache.hive.beeline.Commands.getHiveConf(Commands.java:704)
at org.apache.hive.beeline.Commands.sh(Commands.java:1002)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.hive.beeline.ReflectiveCommandHandler.execute(ReflectiveCommandHandler.java:52)
at org.apache.hive.beeline.BeeLine.dispatch(BeeLine.java:1081)
at org.apache.hive.beeline.BeeLine.execute(BeeLine.java:917)
at org.apache.hive.beeline.BeeLine.begin(BeeLine.java:845)
at org.apache.hive.beeline.BeeLine.mainWithInputRedirection(BeeLine.java:482)
at org.apache.hive.beeline.BeeLine.main(BeeLine.java:465)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
0: jdbc:hive2://localhost&amp;gt; !sh echo hello
java.lang.ArrayIndexOutOfBoundsException: 1
at org.apache.hive.beeline.Commands.addConf(Commands.java:758)
at org.apache.hive.beeline.Commands.getHiveConf(Commands.java:704)
at org.apache.hive.beeline.Commands.sh(Commands.java:1002)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.hive.beeline.ReflectiveCommandHandler.execute(ReflectiveCommandHandler.java:52)
at org.apache.hive.beeline.BeeLine.dispatch(BeeLine.java:1081)
at org.apache.hive.beeline.BeeLine.execute(BeeLine.java:917)
at org.apache.hive.beeline.BeeLine.begin(BeeLine.java:845)
at org.apache.hive.beeline.BeeLine.mainWithInputRedirection(BeeLine.java:482)
at org.apache.hive.beeline.BeeLine.main(BeeLine.java:465)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
0: jdbc:hive2://localhost&amp;gt;


Also it breaks if there is no connection established:

beeline&amp;gt; !sh id
java.lang.NullPointerException
at org.apache.hive.beeline.BeeLine.createStatement(BeeLine.java:1897)
at org.apache.hive.beeline.Commands.getConfInternal(Commands.java:724)
at org.apache.hive.beeline.Commands.getHiveConf(Commands.java:702)
at org.apache.hive.beeline.Commands.sh(Commands.java:1002)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.hive.beeline.ReflectiveCommandHandler.execute(ReflectiveCommandHandler.java:52)
at org.apache.hive.beeline.BeeLine.dispatch(BeeLine.java:1081)
at org.apache.hive.beeline.BeeLine.execute(BeeLine.java:917)
at org.apache.hive.beeline.BeeLine.begin(BeeLine.java:845)
at org.apache.hive.beeline.BeeLine.mainWithInputRedirection(BeeLine.java:482)
at org.apache.hive.beeline.BeeLine.main(BeeLine.java:465)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
at org.apache.hadoop.util.RunJar.main(RunJar.java:136)


</description>
			<version>1.1.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
			<file type="M">org.apache.hive.beeline.Commands.java</file>
		</fixedFiles>
	</bug>
	<bug id="14530" opendate="2016-08-12 08:17:14" fixdate="2016-09-02 07:11:22" resolution="Fixed">
		<buginformation>
			<summary>Union All query returns incorrect results</summary>
			<description>
create table dw_tmp.l_test1 (id bigint,val string,trans_date string) row format delimited fields terminated by &amp;amp;apos; &amp;amp;apos; ;
create table dw_tmp.l_test2 (id bigint,val string,trans_date string) row format delimited fields terminated by &amp;amp;apos; &amp;amp;apos; ;  
select * from dw_tmp.l_test1;
1       table_1      2016-08-11
select * from dw_tmp.l_test2;
2       table_2      2016-08-11
 right like this
select 
    id,
    &amp;amp;apos;table_1&amp;amp;apos; ,
    trans_date
from dw_tmp.l_test1
union all
select 
    id,
    val,
    trans_date
from dw_tmp.l_test2 ;
1       table_1     2016-08-11
2       table_2     2016-08-11
 incorrect
select 
    id,
    999,
    &amp;amp;apos;table_1&amp;amp;apos; ,
    trans_date
from dw_tmp.l_test1
union all
select 
    id,
    999,
    val,
    trans_date
from dw_tmp.l_test2 ;
1       999     table_1     2016-08-11
2       999     table_1     2016-08-11     &amp;lt;-- here is wrong
 incorrect
select 
    id,
    999,
    666,
    &amp;amp;apos;table_1&amp;amp;apos; ,
    trans_date
from dw_tmp.l_test1
union all
select 
    id,
    999,
    666,
    val,
    trans_date
from dw_tmp.l_test2 ;
1       999     666     table_1 2016-08-11
2       999     666     table_1 2016-08-11     &amp;lt;-- here is wrong
 right
select 
    id,
    999,
    &amp;amp;apos;table_1&amp;amp;apos; ,
    trans_date,
    &amp;amp;apos;2016-11-11&amp;amp;apos;
from dw_tmp.l_test1
union all
select 
    id,
    999,
    val,
    trans_date,
    trans_date
from dw_tmp.l_test2 ;
1       999     table_1 2016-08-11      2016-11-11
2       999     table_2 2016-08-11      2016-08-11</description>
			<version>2.1.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdPredicates.java</file>
		</fixedFiles>
		<links>
			<link type="Regression" description="is broken by">13639</link>
		</links>
	</bug>
	<bug id="14607" opendate="2016-08-22 23:04:36" fixdate="2016-09-03 02:56:28" resolution="Fixed">
		<buginformation>
			<summary> ORC split generation failed with exception: java.lang.ArrayIndexOutOfBoundsException: 1</summary>
			<description>Steps to repro: 
in TestTxnCommands2WithSplitUpdate remove the overridden method testOrcPPD().
Then run:
mvn test -Dtest=TestTxnCommands2WithSplitUpdate#testOrcPPD
it will fail with ArrayIndexOutOfBounds.  HIVE-14448 was supposed to have fixed it....

2016-08-22T15:54:17,654  INFO [main] mapreduce.JobSubmitter: Cleaning up the staging area file:/Users/ekoifman/dev/hiverwgit/ql/target/tmp/hadoop-tmp/mapred/staging/ekoifman99742506\
0/.staging/job_local997425060_0002
2016-08-22T15:54:17,663 ERROR [main] exec.Task: Job Submission failed with exception &amp;amp;apos;java.lang.RuntimeException(ORC split generation failed with exception: java.lang.ArrayIndexOutO\
fBoundsException: 1)&amp;amp;apos;
java.lang.RuntimeException: ORC split generation failed with exception: java.lang.ArrayIndexOutOfBoundsException: 1
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1670)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getSplits(OrcInputFormat.java:1756)
        at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:370)
        at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:488)
        at org.apache.hadoop.mapreduce.JobSubmitter.writeOldSplits(JobSubmitter.java:329)
        at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:321)
        at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:197)
        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1297)
        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1294)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1656)
        at org.apache.hadoop.mapreduce.Job.submit(Job.java:1294)
        at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)
        at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1656)
        at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)
        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)
        at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:417)
        at org.apache.hadoop.hive.ql.exec.mr.MapRedTask.execute(MapRedTask.java:141)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:197)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1983)
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1674)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1410)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1134)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1122)
        at org.apache.hadoop.hive.ql.TestTxnCommands2.runStatementOnDriver(TestTxnCommands2.java:1392)
        at org.apache.hadoop.hive.ql.TestTxnCommands2.testOrcPPD(TestTxnCommands2.java:195)
        at org.apache.hadoop.hive.ql.TestTxnCommands2.testOrcPPD(TestTxnCommands2.java:157)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:483)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
        at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:168)
        at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
        at org.junit.rules.RunRules.evaluate(RunRules.java:20)
        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
        at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:367)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:274)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
        at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:161)
        at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)
Caused by: java.util.concurrent.ExecutionException: java.lang.ArrayIndexOutOfBoundsException: 1
        at java.util.concurrent.FutureTask.report(FutureTask.java:122)
        at java.util.concurrent.FutureTask.get(FutureTask.java:192)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1664)
        ... 60 more
Caused by: java.lang.ArrayIndexOutOfBoundsException: 1
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getSargColumnNames(OrcInputFormat.java:394)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.extractNeededColNames(OrcInputFormat.java:447)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.extractNeededColNames(OrcInputFormat.java:442)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.access$2900(OrcInputFormat.java:149)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.callInternal(OrcInputFormat.java:1360)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.access$2700(OrcInputFormat.java:1154)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator$1.run(OrcInputFormat.java:1334)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator$1.run(OrcInputFormat.java:1331)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1656)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:1331)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:1154)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

</description>
			<version>2.2.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.TestTxnCommands2WithSplitUpdate.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">14516</link>
			<link type="Required" description="requires">14448</link>
			<link type="Required" description="requires">14035</link>
		</links>
	</bug>
	<bug id="13383" opendate="2016-03-30 07:11:39" fixdate="2016-09-03 04:13:15" resolution="Fixed">
		<buginformation>
			<summary>RetryingMetaStoreClient retries non retriable embedded metastore client </summary>
			<description>Embedded metastore clients can&amp;amp;apos;t be retried, they throw an exception - "For direct MetaStore DB connections, we don&amp;amp;apos;t support retries at the client level."
This tends to mask the real error that caused the attempts to retry. RetryingMetaStoreClient shouldn&amp;amp;apos;t even attempt to reconnect when direct/embedded metastore client is used.
</description>
			<version>2.0.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.java</file>
		</fixedFiles>
	</bug>
	<bug id="14697" opendate="2016-09-02 16:54:04" fixdate="2016-09-05 15:03:24" resolution="Fixed">
		<buginformation>
			<summary>Can not access kerberized HS2 Web UI</summary>
			<description>Failed to access kerberized HS2 WebUI with following error msg:


curl -v -u : --negotiate http://util185.phx2.cbsig.net:10002/ 
&amp;gt; GET / HTTP/1.1 
&amp;gt; Host: util185.phx2.cbsig.net:10002 
&amp;gt; Authorization: Negotiate YIIU7...[redacted]... 
&amp;gt; User-Agent: curl/7.42.1 
&amp;gt; Accept: */* 
&amp;gt; 
&amp;lt; HTTP/1.1 413 FULL head 
&amp;lt; Content-Length: 0 
&amp;lt; Connection: close 
&amp;lt; Server: Jetty(7.6.0.v20120127) 


It is because the Jetty default request header (4K) is too small in some kerberos case.
So this patch is to increase the request header to 64K.</description>
			<version>2.1.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.http.HttpServer.java</file>
		</fixedFiles>
	</bug>
	<bug id="14581" opendate="2016-08-19 01:13:42" fixdate="2016-09-06 14:51:19" resolution="Duplicate">
		<buginformation>
			<summary>Add chr udf</summary>
			<description>http://docs.aws.amazon.com/redshift/latest/dg/r_CHR.html</description>
			<version>1.2.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Sub-task</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">13063</link>
			<link type="Duplicate" description="is duplicated by">13063</link>
		</links>
	</bug>
	<bug id="14570" opendate="2016-08-18 09:23:00" fixdate="2016-09-06 17:26:36" resolution="Fixed">
		<buginformation>
			<summary>Create table with column names ROW__ID, INPUT__FILE__NAME, BLOCK__OFFSET__INSIDE__FILE sucess but query fails</summary>
			<description>

0: jdbc:hive2://189.39.151.74:21066/&amp;gt; create table foo1(ROW__ID string);
No rows affected (0.281 seconds)
0: jdbc:hive2://189.39.151.74:21066/&amp;gt; create table foo2(BLOCK__OFFSET__INSIDE__FILE string);
No rows affected (0.323 seconds)
0: jdbc:hive2://189.39.151.74:21066/&amp;gt; create table foo3(INPUT__FILE__NAME string);
No rows affected (0.307 seconds)
0: jdbc:hive2://189.39.151.74:21066/&amp;gt; select * from foo1;
Error: Error while compiling statement: FAILED: SemanticException Line 0:-1 Invalid column reference &amp;amp;apos;TOK_ALLCOLREF&amp;amp;apos; (state=42000,code=40000)
0: jdbc:hive2://189.39.151.74:21066/&amp;gt; select * from foo2;
Error: Error while compiling statement: FAILED: SemanticException Line 0:-1 Invalid column reference &amp;amp;apos;TOK_ALLCOLREF&amp;amp;apos; (state=42000,code=40000)
0: jdbc:hive2://189.39.151.74:21066/&amp;gt; select * from foo3;
Error: Error while compiling statement: FAILED: SemanticException Line 0:-1 Invalid column reference &amp;amp;apos;TOK_ALLCOLREF&amp;amp;apos; (state=42000,code=40000)


We should prevent user from creating table with column names the same as Virtual Column names</description>
			<version>1.3.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.ErrorMsg.java</file>
		</fixedFiles>
	</bug>
	<bug id="14694" opendate="2016-09-02 08:34:30" fixdate="2016-09-06 17:34:21" resolution="Fixed">
		<buginformation>
			<summary>UDF rand throws NPE when input data is NULL</summary>
			<description>When use rand function with null, HiveServer throws NPE:


0: jdbc:hive2://10.64.35.144:21066/&amp;gt; desc foo1;
+-----------+------------+----------+--+
| col_name  | data_type  | comment  |
+-----------+------------+----------+--+
| c1        | bigint     |          |
+-----------+------------+----------+--+
1 row selected (0.075 seconds)
0: jdbc:hive2://10.64.35.144:21066/&amp;gt; select * from foo1;
+----------+--+
| foo1.c1  |
+----------+--+
| NULL     |
| 1        |
| 2        |
+----------+--+
3 rows selected (0.124 seconds)
0: jdbc:hive2://10.64.35.144:21066/&amp;gt; select rand(c1) from foo1;
Error: java.io.IOException: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to execute method public org.apache.hadoop.hive.serde2.io.DoubleWritable org.apache.hadoop.hive.ql.udf.UDFRand.evaluate(org.apache.hadoop.io.LongWritable)  on object org.apache.hadoop.hive.ql.udf.UDFRand@37a2b47b of class org.apache.hadoop.hive.ql.udf.UDFRand with arguments {null} of size 1 (state=,code=0)


Stack trace:


Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.GeneratedMethodAccessor79.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.ql.exec.FunctionRegistry.invoke(FunctionRegistry.java:1010)
        ... 36 more
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.udf.UDFRand.evaluate(UDFRand.java:57)
        ... 40 more

</description>
			<version>2.2.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFRand.java</file>
		</fixedFiles>
	</bug>
	<bug id="14591" opendate="2016-08-19 23:40:16" fixdate="2016-09-09 09:44:39" resolution="Fixed">
		<buginformation>
			<summary>HS2 is shut down unexpectedly during the startup time</summary>
			<description>If there is issue with Zookeeper (e.g. connection issues), then it takes HS2 some time to connect. During this time, Ambari could issue health checks against HS2 and the CloseSession call will trigger the shutdown of HS2, which is not expected. That triggering should happen only when the HS2 has been deregistered with Zookeeper, not during the startup time when HS2 is not registered with ZK yet.</description>
			<version>1.2.1</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.jdbc.miniHS2.AbstractHiveService.java</file>
			<file type="M">org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
			<file type="M">org.apache.hive.service.server.HiveServer2.java</file>
			<file type="M">org.apache.hive.service.cli.session.SessionManager.java</file>
		</fixedFiles>
	</bug>
	<bug id="14686" opendate="2016-09-01 09:33:56" fixdate="2016-09-09 11:05:43" resolution="Fixed">
		<buginformation>
			<summary>Get unexpected command type when execute query "CREATE TABLE IF NOT EXISTS ... AS"</summary>
			<description>See the query: 


create table if not exists DST as select * from SRC;


if the table DST doesn&amp;amp;apos;t exist, SessionState.get().getHiveOperation() will return HiveOperation.CREATETABLE_AS_SELECT;
But if the table DST already exists, it will return HiveOperation.CREATETABLE;
It really makes some trouble for those who judge operation type by SessionState.get().getHiveOperation().
The reason I find out is that the function analyzeCreateTable in SemanticAnalyzer.java will return null and won&amp;amp;apos;t set the correct command type if the table already exists.
Here is the related code:


// check for existence of table
    if (ifNotExists) {
      try {
        Table table = getTable(qualifiedTabName, false);
        if (table != null) { // table exists
          return null;
        }
      } catch (HiveException e) {
        // should not occur since second parameter to getTableWithQN is false
        throw new IllegalStateException("Unxpected Exception thrown: " + e.getMessage(), e);
      }
    }

</description>
			<version>1.2.1</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
	</bug>
	<bug id="14727" opendate="2016-09-09 09:04:52" fixdate="2016-09-12 19:53:59" resolution="Fixed">
		<buginformation>
			<summary>llap-server may case file descriptor leak in BuddyAllocator class</summary>
			<description>llap-server,the method preallocate(int) of  BuddyAllocator may case file descriptor leak when FileChannel map allocate memory error.
the code:
        //here if failed
         ByteBuffer rwbuf = rwf.getChannel().map(MapMode.READ_WRITE, 0, arenaSize);
        // A mapping, once established, is not dependent upon the file channel that was used to
        // create it. delete file and hold onto the map
       //can not close() and delete file
        rwf.close();
        rf.delete();</description>
			<version>2.2.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.llap.cache.BuddyAllocator.java</file>
		</fixedFiles>
	</bug>
	<bug id="14726" opendate="2016-09-08 23:15:07" fixdate="2016-09-13 14:48:27" resolution="Fixed">
		<buginformation>
			<summary>delete statement fails when spdo is on</summary>
			<description></description>
			<version>2.1.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">14783</link>
		</links>
	</bug>
	<bug id="14739" opendate="2016-09-12 21:33:27" fixdate="2016-09-13 21:50:21" resolution="Fixed">
		<buginformation>
			<summary>Replace runnables directly added to runtime shutdown hooks to avoid deadlock</summary>
			<description>Deepesh Khandelwal reported that a deadlock can occur when running queries through hive cli. Chris Nauroth analyzed it and reported that hive adds shutdown hooks directly to java Runtime which may execute in non-deterministic order causing deadlocks with hadoop&amp;amp;apos;s shutdown hooks. In one case, hadoop shutdown locked FileSystem#Cache and FileSystem.close whereas hive shutdown hook locked FileSystem.close and FileSystem#Cache order causing a deadlock. 
Hive and Hadoop has ShutdownHookManager that runs the shutdown hooks in deterministic order based on priority. We should use that to avoid deadlock throughout the code.</description>
			<version>2.2.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionManagerImpl.java</file>
			<file type="M">org.apache.hive.hcatalog.common.HiveClientCache.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.zookeeper.CuratorFrameworkSingleton.java</file>
			<file type="M">org.apache.hadoop.hive.ql.hooks.ATSHook.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
			<file type="M">org.apache.hive.service.server.HiveServer2.java</file>
			<file type="M">org.apache.hive.beeline.BeeLine.java</file>
			<file type="M">org.apache.hadoop.hive.cli.CliDriver.java</file>
			<file type="M">org.apache.hive.common.util.ShutdownHookManager.java</file>
			<file type="M">org.apache.hive.common.util.TestShutdownHookManager.java</file>
			<file type="M">org.apache.hive.ptest.api.server.ExecutionController.java</file>
		</fixedFiles>
	</bug>
	<bug id="13878" opendate="2016-05-27 14:22:15" fixdate="2016-09-14 06:18:44" resolution="Fixed">
		<buginformation>
			<summary>Vectorization: Column pruning for Text vectorization</summary>
			<description>Column pruning in TextFile vectorization does not work with Vector SerDe settings due to LazySimple deser codepath issues.</description>
			<version>2.1.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.TestVectorSerDeRow.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.fast.DeserializeRead.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastStringCommon.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.fast.LazySimpleDeserializeRead.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazybinary.fast.LazyBinaryDeserializeRead.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VerifyFastRow.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastLongHashTable.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazybinary.TestLazyBinaryFast.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorDeserializeRow.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.TestLazySimpleFast.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.CheckFastRowHashMap.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.VerifyFast.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.optimized.VectorMapJoinOptimizedStringCommon.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.binarysortable.TestBinarySortableFast.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.binarysortable.fast.BinarySortableDeserializeRead.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.optimized.VectorMapJoinOptimizedLongCommon.java</file>
		</fixedFiles>
		<links>
			<link type="Incorporates" description="incorporates">13887</link>
			<link type="Incorporates" description="incorporates">13876</link>
		</links>
	</bug>
	<bug id="14251" opendate="2016-07-15 18:25:23" fixdate="2016-09-14 18:52:54" resolution="Fixed">
		<buginformation>
			<summary>Union All of different types resolves to incorrect data</summary>
			<description>create table src(c1 date, c2 int, c3 double);
insert into src values (&amp;amp;apos;2016-01-01&amp;amp;apos;,5,1.25);
select * from 
(select c1 from src union all
select c2 from src union all
select c3 from src) t;
It will return NULL for the c1 values. Seems the common data type is resolved to the last c3 which is double.</description>
			<version>2.0.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.java</file>
		</fixedFiles>
	</bug>
	<bug id="14743" opendate="2016-09-13 16:47:26" fixdate="2016-09-15 16:16:21" resolution="Fixed">
		<buginformation>
			<summary>ArrayIndexOutOfBoundsException - HBASE-backed views&amp;apos; query with JOINs</summary>
			<description>The stack:

2016-09-13T09:38:49,972 ERROR [186b4545-65b5-4bfc-bc8e-3e14e251bb12 main] exec.Task: Job Submission failed with exception &amp;amp;apos;java.lang.ArrayIndexOutOfBoundsException(1)&amp;amp;apos;
java.lang.ArrayIndexOutOfBoundsException: 1
        at org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.createFilterScan(HiveHBaseTableInputFormat.java:224)
        at org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.getSplitsInternal(HiveHBaseTableInputFormat.java:492)
        at org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.getSplits(HiveHBaseTableInputFormat.java:449)
        at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:370)
        at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:466)
        at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getCombineSplits(CombineHiveInputFormat.java:356)
        at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getSplits(CombineHiveInputFormat.java:546)
        at org.apache.hadoop.mapreduce.JobSubmitter.writeOldSplits(JobSubmitter.java:329)
        at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:320)
        at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:196)
        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1290)
        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1287)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
        at org.apache.hadoop.mapreduce.Job.submit(Job.java:1287)
        at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:575)
        at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:570)



Repro:

CREATE TABLE HBASE_TABLE_TEST_1(
  cvalue string ,
  pk string,
 ccount int   )
ROW FORMAT SERDE
  &amp;amp;apos;org.apache.hadoop.hive.hbase.HBaseSerDe&amp;amp;apos;
STORED BY
  &amp;amp;apos;org.apache.hadoop.hive.hbase.HBaseStorageHandler&amp;amp;apos;
WITH SERDEPROPERTIES (
  &amp;amp;apos;hbase.columns.mapping&amp;amp;apos;=&amp;amp;apos;cf:val,:key,cf2:count&amp;amp;apos;,
  &amp;amp;apos;hbase.scan.cache&amp;amp;apos;=&amp;amp;apos;500&amp;amp;apos;,
  &amp;amp;apos;hbase.scan.cacheblocks&amp;amp;apos;=&amp;amp;apos;false&amp;amp;apos;,
  &amp;amp;apos;serialization.format&amp;amp;apos;=&amp;amp;apos;1&amp;amp;apos;)
TBLPROPERTIES (
  &amp;amp;apos;hbase.table.name&amp;amp;apos;=&amp;amp;apos;hbase_table_test_1&amp;amp;apos;,
  &amp;amp;apos;serialization.null.format&amp;amp;apos;=&amp;amp;apos;&amp;amp;apos;  );


  CREATE VIEW VIEW_HBASE_TABLE_TEST_1 AS SELECT hbase_table_test_1.cvalue,hbase_table_test_1.pk,hbase_table_test_1.ccount FROM hbase_table_test_1 WHERE hbase_table_test_1.ccount IS NOT NULL;

CREATE TABLE HBASE_TABLE_TEST_2(
  cvalue string ,
    pk string ,
   ccount int  )
ROW FORMAT SERDE
  &amp;amp;apos;org.apache.hadoop.hive.hbase.HBaseSerDe&amp;amp;apos;
STORED BY
  &amp;amp;apos;org.apache.hadoop.hive.hbase.HBaseStorageHandler&amp;amp;apos;
WITH SERDEPROPERTIES (
  &amp;amp;apos;hbase.columns.mapping&amp;amp;apos;=&amp;amp;apos;cf:val,:key,cf2:count&amp;amp;apos;,
  &amp;amp;apos;hbase.scan.cache&amp;amp;apos;=&amp;amp;apos;500&amp;amp;apos;,
  &amp;amp;apos;hbase.scan.cacheblocks&amp;amp;apos;=&amp;amp;apos;false&amp;amp;apos;,
  &amp;amp;apos;serialization.format&amp;amp;apos;=&amp;amp;apos;1&amp;amp;apos;)
TBLPROPERTIES (
  &amp;amp;apos;hbase.table.name&amp;amp;apos;=&amp;amp;apos;hbase_table_test_2&amp;amp;apos;,
  &amp;amp;apos;serialization.null.format&amp;amp;apos;=&amp;amp;apos;&amp;amp;apos;);


CREATE VIEW VIEW_HBASE_TABLE_TEST_2 AS SELECT hbase_table_test_2.cvalue,hbase_table_test_2.pk,hbase_table_test_2.ccount FROM hbase_table_test_2 WHERE  hbase_table_test_2.pk &amp;gt;=&amp;amp;apos;3-0000h-0&amp;amp;apos; AND hbase_table_test_2.pk &amp;lt;= &amp;amp;apos;3-0000h-g&amp;amp;apos; AND hbase_table_test_2.ccount IS NOT NULL;

set hive.auto.convert.join=false;

  SELECT  p.cvalue cvalue
FROM `VIEW_HBASE_TABLE_TEST_1` `p`
LEFT OUTER JOIN `VIEW_HBASE_TABLE_TEST_2` `A1`
ON `p`.cvalue = `A1`.cvalue
LEFT OUTER JOIN `VIEW_HBASE_TABLE_TEST_1` `A2`
ON `p`.cvalue = `A2`.cvalue;



</description>
			<version>1.0.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.java</file>
		</fixedFiles>
	</bug>
	<bug id="14779" opendate="2016-09-16 18:18:29" fixdate="2016-09-20 00:37:10" resolution="Fixed">
		<buginformation>
			<summary>make DbTxnManager.HeartbeaterThread a daemon</summary>
			<description>setDaemon(true);
make heartbeaterThreadPoolSize static </description>
			<version>1.3.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
		</fixedFiles>
	</bug>
	<bug id="14624" opendate="2016-08-24 21:10:23" fixdate="2016-09-20 18:32:06" resolution="Fixed">
		<buginformation>
			<summary>LLAP: Use FQDN when submitting work to LLAP </summary>
			<description>

llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapFixedRegistryImpl.java:                + socketAddress.getHostName());
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapFixedRegistryImpl.java:            host = socketAddress.getHostName();
llap-common/src/java/org/apache/hadoop/hive/llap/metrics/MetricsUtils.java:  public static String getHostName() {
llap-common/src/java/org/apache/hadoop/hive/llap/metrics/MetricsUtils.java:      return InetAddress.getLocalHost().getHostName();
llap-ext-client/src/java/org/apache/hadoop/hive/llap/LlapBaseInputFormat.java:    String name = address.getHostName();
llap-ext-client/src/java/org/apache/hadoop/hive/llap/LlapBaseInputFormat.java:    builder.setAmHost(address.getHostName());
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java:    nodeId = LlapNodeId.getInstance(localAddress.get().getHostName(), localAddress.get().getPort());
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/ContainerRunnerImpl.java:        localAddress.get().getHostName(), vertex.getDagName(), qIdProto.getDagIndex(),
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/ContainerRunnerImpl.java:          new ExecutionContextImpl(localAddress.get().getHostName()), env,
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapDaemon.java:    String hostName = MetricsUtils.getHostName();
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapProtocolServerImpl.java:        .setBindAddress(addr.getHostName())
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskRunnerCallable.java:          request.getContainerIdString(), executionContext.getHostName(), vertex.getDagName(),
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java:    String displayName = "LlapDaemonCacheMetrics-" + MetricsUtils.getHostName();
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java:    displayName = "LlapDaemonIOMetrics-" + MetricsUtils.getHostName();
llap-server/src/test/org/apache/hadoop/hive/llap/daemon/impl/TestLlapDaemonProtocolServerImpl.java:          new LlapProtocolClientImpl(new Configuration(), serverAddr.getHostName(),
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java:    builder.setAmHost(getAddress().getHostName());
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java:      String displayName = "LlapTaskSchedulerMetrics-" + MetricsUtils.getHostName();


In systems where the hostnames do not match FQDN, calling the getCanonicalHostName() will allow for resolution of the hostname when accessing from a different base domain.</description>
			<version>2.2.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.java</file>
			<file type="M">org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
			<file type="M">org.apache.hadoop.hive.llap.LlapBaseInputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.llap.LlapUtil.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
		<links>
			<link type="Cloners" description="is cloned by">14791</link>
		</links>
	</bug>
	<bug id="14714" opendate="2016-09-07 15:46:07" fixdate="2016-09-21 08:19:55" resolution="Fixed">
		<buginformation>
			<summary>Avoid misleading "java.io.IOException: Stream closed" when shutting down HoS</summary>
			<description>After execute hive command with Spark, finishing the beeline session or
even switch the engine causes IOException. The following executed Ctrl-D to
finish the session but "!quit" or even "set hive.execution.engine=mr;" causes
the issue.
From HS2 log:


2016-09-06 16:15:12,291 WARN  org.apache.hive.spark.client.SparkClientImpl: [HiveServer2-Handler-Pool: Thread-106]: Timed out shutting down remote driver, interrupting...
2016-09-06 16:15:12,291 WARN  org.apache.hive.spark.client.SparkClientImpl: [Driver]: Waiting thread interrupted, killing child process.
2016-09-06 16:15:12,296 WARN  org.apache.hive.spark.client.SparkClientImpl: [stderr-redir-1]: Error in redirector thread.
java.io.IOException: Stream closed
        at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:162)
        at java.io.BufferedInputStream.read1(BufferedInputStream.java:272)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
        at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:283)
        at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:325)
        at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:177)
        at java.io.InputStreamReader.read(InputStreamReader.java:184)
        at java.io.BufferedReader.fill(BufferedReader.java:154)
        at java.io.BufferedReader.readLine(BufferedReader.java:317)
        at java.io.BufferedReader.readLine(BufferedReader.java:382)
        at org.apache.hive.spark.client.SparkClientImpl$Redirector.run(SparkClientImpl.java:617)
        at java.lang.Thread.run(Thread.java:745)

</description>
			<version>1.1.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.spark.client.SparkClientImpl.java</file>
		</fixedFiles>
	</bug>
	<bug id="14098" opendate="2016-06-27 09:01:13" fixdate="2016-09-21 20:23:47" resolution="Fixed">
		<buginformation>
			<summary>Logging task properties, and environment variables might contain passwords</summary>
			<description>Hive MapredLocalTask Can Print Environment Passwords, like -Djavax.net.ssl.trustStorePassword.
The same could happen, when logging spark properties</description>
			<version>2.1.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.TestUtilities.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
		</fixedFiles>
	</bug>
	<bug id="14783" opendate="2016-09-17 01:15:01" fixdate="2016-09-21 22:20:27" resolution="Fixed">
		<buginformation>
			<summary>bucketing column should be part of sorting for delete/update operation when spdo is on</summary>
			<description></description>
			<version>2.2.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">14726</link>
		</links>
	</bug>
	<bug id="14766" opendate="2016-09-15 20:20:54" fixdate="2016-09-22 07:08:36" resolution="Fixed">
		<buginformation>
			<summary>ObjectStore.initialize() needs retry mechanisms in case of connection failures</summary>
			<description>RetryingHMSHandler handles retries to most HMSHandler calls. However, one area where we do not have retries is in the very instantiation of ObjectStore. The lack of retries here sometimes means that a flaky db connect around the time the metastore is started yields an unresponsive metastore.</description>
			<version>2.1.1</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
		</fixedFiles>
	</bug>
	<bug id="14814" opendate="2016-09-22 00:57:11" fixdate="2016-09-22 17:52:27" resolution="Fixed">
		<buginformation>
			<summary>metastoreClient is used directly in Hive cause NPE</summary>
			<description>Changes introduced by HIVE-13622 uses metastoreClient directly in Hive.java which may be null causing NPE. Instead it should use getMSC() which will initialize metastoreClient variable when null.</description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
		</fixedFiles>
	</bug>
	<bug id="14805" opendate="2016-09-21 16:39:58" fixdate="2016-09-23 13:09:22" resolution="Fixed">
		<buginformation>
			<summary>Subquery inside a view will have the object in the subquery as the direct input </summary>
			<description>Here is the repro steps.

create table t1(col string);
create view v1 as select * from t1;
create view dataview as select * from  (select * from v1) v2;
select * from dataview;


If hive is configured with authorization hook like Sentry, it will require the access not only for dataview but also for v1, which should not be required.
The subquery seems to not carry insideview property from the parent query.
</description>
			<version>2.0.1</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.TestViewEntity.java</file>
		</fixedFiles>
	</bug>
	<bug id="14820" opendate="2016-09-22 19:45:31" fixdate="2016-09-23 13:16:47" resolution="Fixed">
		<buginformation>
			<summary>RPC server for spark inside HS2 is not getting server address properly</summary>
			<description>When hive.spark.client.rpc.server.address is configured, this property is not retrieved properly because we are getting the value by String hiveHost = config.get(HiveConf.ConfVars.SPARK_RPC_SERVER_ADDRESS);  which always returns null in getServerAddress() call of RpcConfiguration.java. Rather it should be String hiveHost = config.get(HiveConf.ConfVars.SPARK_RPC_SERVER_ADDRESS.varname);.
</description>
			<version>2.0.1</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.spark.client.rpc.RpcConfiguration.java</file>
			<file type="M">org.apache.hive.spark.client.rpc.TestRpc.java</file>
		</fixedFiles>
	</bug>
	<bug id="3173" opendate="2012-06-22 02:23:14" fixdate="2016-09-26 17:50:14" resolution="Fixed">
		<buginformation>
			<summary>implement getTypeInfo database metadata method </summary>
			<description>The JDBC driver does not implement the database metadata method getTypeInfo. Hence, an application cannot dynamically determine the available type information and associated properties. </description>
			<version>0.8.1</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.jdbc.TestJdbcDriver2.java</file>
		</fixedFiles>
	</bug>
	<bug id="14751" opendate="2016-09-14 07:27:16" fixdate="2016-09-27 05:36:23" resolution="Fixed">
		<buginformation>
			<summary>Add support for date truncation</summary>
			<description>Add support for floor (&amp;lt;time&amp;gt; to &amp;lt;timeunit&amp;gt;), which is equivalent to date_trunc(&amp;lt;timeunit&amp;gt;, &amp;lt;time&amp;gt;).
https://www.postgresql.org/docs/9.1/static/functions-datetime.html#FUNCTIONS-DATETIME-TRUNC</description>
			<version>2.2.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.udf.TestUDFDateFormatGranularity.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFDateFloor.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">14579</link>
			<link type="Reference" description="is related to">14832</link>
			<link type="Reference" description="is related to">14833</link>
			<link type="Regression" description="breaks">14843</link>
		</links>
	</bug>
	<bug id="14194" opendate="2016-07-08 14:43:23" fixdate="2016-09-27 14:45:59" resolution="Duplicate">
		<buginformation>
			<summary>Investigate optimizing the query compilation of long or-list in where statement </summary>
			<description>The following query will take long time to compile if the where statement has a long list of &amp;amp;apos;or&amp;amp;apos;. Investigate if we can optimize it.
select * from src 
where key = 1
or key =2
or ....</description>
			<version>1.0.0</version>
			<fixedVersion></fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">11424</link>
		</links>
	</bug>
	<bug id="12222" opendate="2015-10-21 17:29:47" fixdate="2016-09-28 16:09:02" resolution="Fixed">
		<buginformation>
			<summary>Define port range in property for RPCServer</summary>
			<description>Creating this JIRA after discussin with Xuefu on the dev mailing list. Would need some help to review and update the fields in this JIRA ticket, thanks.
I notice that in 
./spark-client/src/main/java/org/apache/hive/spark/client/rpc/RpcServer.java
The port number is assigned with 0 which means it will be a random port every time when the RPC Server is created to talk to Spark in the same session.
Because of this, this is causing problems to configure firewall between the 
HiveCLI RPC Server and Spark due to unpredictable port numbers here. In other word, users need to open all hive ports range 
from Data Node =&amp;gt; HiveCLI (edge node).


 this.channel = new ServerBootstrap()
      .group(group)
      .channel(NioServerSocketChannel.class)
      .childHandler(new ChannelInitializer&amp;lt;SocketChannel&amp;gt;() {
          @Override
          public void initChannel(SocketChannel ch) throws Exception {
            SaslServerHandler saslHandler = new SaslServerHandler(config);
            final Rpc newRpc = Rpc.createServer(saslHandler, config, ch, group);
            saslHandler.rpc = newRpc;

            Runnable cancelTask = new Runnable() {
                @Override
                public void run() {
                  LOG.warn("Timed out waiting for hello from client.");
                  newRpc.close();
                }
            };
            saslHandler.cancelTask = group.schedule(cancelTask,
                RpcServer.this.config.getServerConnectTimeoutMs(),
                TimeUnit.MILLISECONDS);

          }
      })


2 Main reasons.

Most users (what I see and encounter) use HiveCLI as a command line tool, and in order to use that, they need to login to the edge node (via SSH). Now, here comes the interesting part.
Could be true or not, but this is what I observe and encounter from time to time. Most users will abuse the resource on that edge node (increasing HADOOP_HEAPSIZE, dumping output to local disk, running huge python workflow, etc), this may cause the HS2 process to run into OOME, choke and die, etc. various resource issues including others like login, etc.


Analyst connects to Hive via HS2 + ODBC. So HS2 needs to be highly available. This makes sense to run it on the gateway node or a service node and separated from the HiveCLI.
The logs are located in different location, monitoring and auditing is easier to run HS2 with a daemon user account, etc. so we don&amp;amp;apos;t want users to run HiveCLI where HS2 is running.
It&amp;amp;apos;s better to isolate the resource this way to avoid any memory, file handlers, disk space, issues.

From a security standpoint, 

Since users can login to edge node (via SSH), the security on the edge node needs to be fortified and enhanced. Therefore, all the FW comes in and auditing.


Regulation/compliance for auditing is another requirement to monitor all traffic, specifying ports and locking down the ports makes it easier since we can focus
on a range to monitor and audit.

</description>
			<version>1.2.1</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Improvement</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.spark.client.rpc.TestRpc.java</file>
			<file type="M">org.apache.hive.spark.client.rpc.RpcServer.java</file>
			<file type="M">org.apache.hive.spark.client.rpc.RpcConfiguration.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">14327</link>
		</links>
	</bug>
	<bug id="14778" opendate="2016-09-16 18:14:28" fixdate="2016-09-29 17:44:23" resolution="Fixed">
		<buginformation>
			<summary>document threading model of Streaming API</summary>
			<description>The model is not obvious and needs to be documented properly.
A StreamingConnection internally maintains 2 MetaStoreClient objects (each has 1 Thrift client for actual RPC). Let&amp;amp;apos;s call them "primary" and "heartbeat". Each TransactionBatch created from a given StreamingConnection, gets a reference to both of these MetaStoreClients. 
So the model is that there is at most 1 outstanding (not closed) TransactionBatch for any given StreamingConnection and for any given TransactionBatch there can be at most 2 threads accessing it concurrently. 1 thread calling TransactionBatch.heartbeat() (and nothing else) and the other calling all other methods.</description>
			<version>0.14.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.hcatalog.streaming.StreamingConnection.java</file>
			<file type="M">org.apache.hive.hcatalog.streaming.TransactionBatch.java</file>
		</fixedFiles>
	</bug>
	<bug id="14819" opendate="2016-09-22 19:16:01" fixdate="2016-09-29 20:59:36" resolution="Fixed">
		<buginformation>
			<summary>FunctionInfo for permanent functions shows TEMPORARY FunctionType</summary>
			<description>The FunctionInfo has a FunctionType field which describes if the function is a builtin/persistent/temporary function. But for permanent functions, the FunctionInfo being returned by the FunctionRegistry is showing the type to be TEMPORARY.
This affects things which may be depending on function type, for example LlapDecider, which will allow builtin/persistent UDFs to be used in LLAP but not temporary functions.</description>
			<version>2.1.0</version>
			<fixedVersion>2.1.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FunctionInfo.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.WindowFunctionInfo.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Registry.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.translator.SqlFunctionConverter.java</file>
		</fixedFiles>
	</bug>
	<bug id="14784" opendate="2016-09-17 02:00:28" fixdate="2016-09-30 14:43:35" resolution="Fixed">
		<buginformation>
			<summary>Operation logs are disabled automatically if the parent directory does not exist.</summary>
			<description>Operation logging is disabled automatically for the query if for some reason the parent directory (named after the hive session id) that gets created when the session is established gets deleted (for any reason). For ex: if the operation logdir is /tmp which automatically can get purged at a configured interval by the OS.
Running a query from that session leads to


2016-09-15 15:09:16,723 WARN org.apache.hive.service.cli.operation.Operation: Unable to create operation log file: /tmp/hive/operation_logs/b8809985-6b38-47ec-a49b-6158a67cd9fc/d35414f7-2418-426c-8489-c6f643ca4599
java.io.IOException: No such file or directory
	at java.io.UnixFileSystem.createFileExclusively(Native Method)
	at java.io.File.createNewFile(File.java:1012)
	at org.apache.hive.service.cli.operation.Operation.createOperationLog(Operation.java:195)
	at org.apache.hive.service.cli.operation.Operation.beforeRun(Operation.java:237)
	at org.apache.hive.service.cli.operation.Operation.run(Operation.java:255)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:398)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:385)
	at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:271)
	at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:490)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:692)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)



This later leads to errors like (more prominent when using HUE as HUE does not close hive sessions and attempts to retrieve the operations logs days after they were created).


WARN org.apache.hive.service.cli.thrift.ThriftCLIService: Error fetching results: 
org.apache.hive.service.cli.HiveSQLException: Couldn&amp;amp;apos;t find log associated with operation handle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=d35414f7-2418-426c-8489-c6f643ca4599]
	at org.apache.hive.service.cli.operation.OperationManager.getOperationLogRowSet(OperationManager.java:259)
	at org.apache.hive.service.cli.session.HiveSessionImpl.fetchResults(HiveSessionImpl.java:701)
	at org.apache.hive.service.cli.CLIService.fetchResults(CLIService.java:451)
	at org.apache.hive.service.cli.thrift.ThriftCLIService.FetchResults(ThriftCLIService.java:676)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$FetchResults.getResult(TCLIService.java:1553)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$FetchResults.getResult(TCLIService.java:1538)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:692)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745) 

</description>
			<version>1.1.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.service.cli.operation.Operation.java</file>
		</fixedFiles>
	</bug>
	<bug id="14865" opendate="2016-09-30 01:54:05" fixdate="2016-09-30 22:10:59" resolution="Fixed">
		<buginformation>
			<summary>Fix comments after HIVE-14350</summary>
			<description>there are still some comments in the code that should&amp;amp;apos;ve been updated in HIVE-14350</description>
			<version>2.1.1</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">14350</link>
		</links>
	</bug>
	<bug id="14873" opendate="2016-10-01 06:44:00" fixdate="2016-10-05 16:55:43" resolution="Fixed">
		<buginformation>
			<summary>Add UDF for extraction of &amp;apos;day of week&amp;apos;</summary>
			<description></description>
			<version>2.1.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">14579</link>
		</links>
	</bug>
	<bug id="14773" opendate="2016-09-16 12:51:07" fixdate="2016-10-05 17:44:18" resolution="Fixed">
		<buginformation>
			<summary>NPE aggregating column statistics for date column in partitioned table</summary>
			<description>Hive runs into a NPE when the query has a filter on a date column and the partitioned column 
eg: 


create table date_dim (d_date date) partitioned by (d_date_sk bigint) stored as orc;
set hive.exec.dynamic.partition.mode=nonstrict;
insert into date_dim partition(d_date_sk=2416945) values(&amp;amp;apos;1905-04-09&amp;amp;apos;);
insert into date_dim partition(d_date_sk=2416946) values(&amp;amp;apos;1905-04-10&amp;amp;apos;);
insert into date_dim partition(d_date_sk=2416947) values(&amp;amp;apos;1905-04-11&amp;amp;apos;);
analyze table date_dim partition(d_date_sk) compute statistics for columns;

explain select count(*) from date_dim where d_date &amp;gt; date "1900-01-02" and d_date_sk= 2416945;


Here d_date_sk is a partition column and d_date is of type date.


2016-09-16T08:27:06,510 DEBUG [90d4780f-77e4-4704-9907-4860ce11a206 main] metastore.AggregateStatsCache: No aggregate stats cached for database:default, table:date_dim, column:d_date
2016-09-16T08:27:06,512 DEBUG [90d4780f-77e4-4704-9907-4860ce11a206 main] metastore.MetaStoreDirectSql: Direct SQL query in 1.302231ms + 0.00653ms, the query is [select "COLUMN_NAME", "COLUMN_TYPE", min("LONG_LOW_VALUE"), max("LONG_HIGH_VALUE"), min("DOUBLE_LOW_VALUE"), max("DOUBLE_HIGH_VALUE"), min(cast("BIG_DECIMAL_LOW_VALUE" as decimal)), max(cast("BIG_DECIMAL_HIGH_VALUE" as decimal)), sum("NUM_NULLS"), max("NUM_DISTINCTS"), max("AVG_COL_LEN"), max("MAX_COL_LEN"), sum("NUM_TRUES"), sum("NUM_FALSES"), avg(("LONG_HIGH_VALUE"-"LONG_LOW_VALUE")/cast("NUM_DISTINCTS" as decimal)),avg(("DOUBLE_HIGH_VALUE"-"DOUBLE_LOW_VALUE")/"NUM_DISTINCTS"),avg((cast("BIG_DECIMAL_HIGH_VALUE" as decimal)-cast("BIG_DECIMAL_LOW_VALUE" as decimal))/"NUM_DISTINCTS"),sum("NUM_DISTINCTS") from "PART_COL_STATS" where "DB_NAME" = ? and "TABLE_NAME" = ?  and "COLUMN_NAME" in (?) and "PARTITION_NAME" in (?) group by "COLUMN_NAME", "COLUMN_TYPE"]
2016-09-16T08:27:06,526  INFO [90d4780f-77e4-4704-9907-4860ce11a206 main] metastore.MetaStoreDirectSql: useDensityFunctionForNDVEstimation = false
partsFound = 1
ColumnStatisticsObj = [ColumnStatisticsObj(colName:d_date, colType:date, statsData:&amp;lt;ColumnStatisticsData &amp;gt;)]
2016-09-16T08:27:06,526 DEBUG [90d4780f-77e4-4704-9907-4860ce11a206 main] metastore.ObjectStore: Commit transaction: count = 0, isactive true at:
        org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.commit(ObjectStore.java:2827)
2016-09-16T08:27:06,531 DEBUG [90d4780f-77e4-4704-9907-4860ce11a206 main] metastore.ObjectStore: null retrieved using SQL in 43.425925ms
2016-09-16T08:27:06,545 ERROR [90d4780f-77e4-4704-9907-4860ce11a206 main] ql.Driver: FAILED: NullPointerException null
java.lang.NullPointerException
        at org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.getFieldDesc(ColumnStatisticsData.java:451)
        at org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.getDateStats(ColumnStatisticsData.java:574)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.getColStatistics(StatsUtils.java:759)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.convertColStats(StatsUtils.java:806)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:304)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:152)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:140)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$TableScanStatsRule.process(StatsRulesProcFactory.java:126)
        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89)
        at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.walk(LevelOrderWalker.java:143)
        at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.startWalking(LevelOrderWalker.java:122)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateWithStatistics.transform(AnnotateWithStatistics.java:78)
        at org.apache.hadoop.hive.ql.parse.TezCompiler.runStatsAnnotation(TezCompiler.java:260)
        at org.apache.hadoop.hive.ql.parse.TezCompiler.optimizeOperatorPlan(TezCompiler.java:129)
        at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:140)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10928)
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:255)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:251)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:467)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:342)
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1235)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1355)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1143)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1131)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:184)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:400)
        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:777)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:715)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:642)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:233)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:148)

</description>
			<version>1.2.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.StatObjectConverter.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.IExtrapolatePartStatus.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">10226</link>
		</links>
	</bug>
	<bug id="14146" opendate="2016-07-01 13:22:30" fixdate="2016-10-10 14:18:07" resolution="Fixed">
		<buginformation>
			<summary>Column comments with "\n" character "corrupts" table metadata</summary>
			<description>Create a table with the following(noting the \n in the COMMENT):

CREATE TABLE commtest(first_nm string COMMENT &amp;amp;apos;Indicates First name\nof an individual);


Describe shows that now the metadata is messed up:

beeline&amp;gt; describe commtest;
+-------------------+------------+-----------------------+--+
|     col_name      | data_type  |        comment        |
+-------------------+------------+-----------------------+--+
| first_nm             | string       | Indicates First name  |
| of an individual  | NULL       | NULL                  |
+-------------------+------------+-----------------------+--+

</description>
			<version>2.2.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.common.util.HiveStringUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.formatting.TextMetaDataFormatter.java</file>
			<file type="M">org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
		</fixedFiles>
		<links>
			<link type="Dependent" description="Dependent">14013</link>
		</links>
	</bug>
	<bug id="14690" opendate="2016-09-01 22:39:35" fixdate="2016-10-11 22:55:05" resolution="Fixed">
		<buginformation>
			<summary>Query fail when hive.exec.parallel=true, with conflicting session dir</summary>
			<description>This happens when hive.scratchdir.lock=true. Error message:


/hive/scratch/343hdirdp/cab907fc-5e1d-4d69-aa72-d7b442495c7a/inuse.info (inode 19537): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_1572639975_1, pendingcreates: 2]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3430)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3235)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3073)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3033)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:725)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2137)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2133)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1668)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2131)

	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:535)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.run(TaskRunner.java:74)

</description>
			<version>1.3.0</version>
			<fixedVersion>1.3.0, 2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.session.SessionState.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">13429</link>
		</links>
	</bug>
	<bug id="14839" opendate="2016-09-25 12:07:41" fixdate="2016-10-14 16:04:30" resolution="Fixed">
		<buginformation>
			<summary>Improve the stability of TestSessionManagerMetrics</summary>
			<description>The TestSessionManagerMetrics fails occasionally with the following error: 

org.junit.ComparisonFailure: expected:&amp;lt;[0]&amp;gt; but was:&amp;lt;[1]&amp;gt;
	at org.apache.hive.service.cli.session.TestSessionManagerMetrics.testThreadPoolMetrics(TestSessionManagerMetrics.java:98)

Failed tests: 
  TestSessionManagerMetrics.testThreadPoolMetrics:98 expected:&amp;lt;[0]&amp;gt; but was:&amp;lt;[1]&amp;gt;


This test starts four background threads with a "wait" call in their run method. The threads are using the common "barrier" object as lock. 
The expected behaviour is that two threads will be in the async pool (because the hive.server2.async.exec.threads is set to 2) and the other two thread will be waiting in the queue. This condition is checked like this:

MetricsTestUtils.verifyMetricsJson(json, MetricsTestUtils.GAUGE, MetricsConstant.EXEC_ASYNC_POOL_SIZE, 2);
MetricsTestUtils.verifyMetricsJson(json, MetricsTestUtils.GAUGE, MetricsConstant.EXEC_ASYNC_QUEUE_SIZE, 2);


Then a notifyAll is called on the lock object, so the two threads in the pool should "wake up" and complete and the other two threads should go from the queue to the pool. This is checked like this in the test:

MetricsTestUtils.verifyMetricsJson(json, MetricsTestUtils.GAUGE, MetricsConstant.EXEC_ASYNC_POOL_SIZE, 2);
MetricsTestUtils.verifyMetricsJson(json, MetricsTestUtils.GAUGE, MetricsConstant.EXEC_ASYNC_QUEUE_SIZE, 0);


There are two use cases which can cause error in this test:

The notifyAll call happens before both threads in the pool are up and running and in the "wait" phase.
In this case the thread which is not up in time will stuck in the pool, so the other two threads can not move from the queue to the pool. 
After the notifyAll call, the threads in the pool "wake up" with some delay. So they don&amp;amp;apos;t complete and removed from the pool and the other two threads are not moved from the queue to the pool until the metrics are checked. Therefore the check fails, since the queue is not empty.

</description>
			<version>2.1.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.service.cli.session.TestSessionManagerMetrics.java</file>
		</fixedFiles>
	</bug>
	<bug id="14966" opendate="2016-10-14 20:25:34" fixdate="2016-10-15 07:46:53" resolution="Fixed">
		<buginformation>
			<summary>JDBC: Make cookie-auth work in HTTP mode</summary>
			<description>HiveServer2 cookie-auth is non-functional and forces authentication to be repeated for the status check loop, row fetch loop and the get logs loop.
The repeated auth in the fetch-loop is a performance issue, but is also causing occasional DoS responses from the remote auth-backend if this is not using local /etc/passwd.
The HTTP-Cookie auth once made functional will behave similarly to the binary protocol, authenticating exactly once per JDBC session and not causing further load on the authentication backend irrespective how many rows are returned from the JDBC request.
This due to the fact that the cookies are not sent out with matching flags for SSL usage.</description>
			<version>1.2.1</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.service.cli.thrift.ThriftHttpServlet.java</file>
			<file type="M">org.apache.hive.service.cli.thrift.ThriftCliServiceTestWithCookie.java</file>
			<file type="M">org.apache.hive.minikdc.TestJdbcWithMiniKdcCookie.java</file>
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
		</fixedFiles>
	</bug>
	<bug id="14959" opendate="2016-10-14 07:13:03" fixdate="2016-10-17 19:41:30" resolution="Fixed">
		<buginformation>
			<summary>Fix DISTINCT with windowing when CBO is enabled/disabled</summary>
			<description>For instance, the following query with CBO off:


select distinct last_value(i) over ( partition by si order by i ),
  first_value(t)  over ( partition by si order by i )
from over10k limit 50;


will fail, with the following message:

SELECT DISTINCT not allowed in the presence of windowing functions when CBO is off

</description>
			<version>2.1.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">13242</link>
		</links>
	</bug>
	<bug id="14991" opendate="2016-10-17 21:03:46" fixdate="2016-10-17 21:09:53" resolution="Fixed">
		<buginformation>
			<summary>JDBC result set iterator has useless DEBUG log</summary>
			<description>Result set iterator prints the following debug lines for every row. The row string is always empty as per code.


2016-10-17T11:49:52,792 DEBUG [main] jdbc.HiveQueryResultSet: Fetched row string: 
2016-10-17T11:49:52,792 DEBUG [main] jdbc.HiveQueryResultSet: Fetched row string: 
2016-10-17T11:49:52,792 DEBUG [main] jdbc.HiveQueryResultSet: Fetched row string: 
2016-10-17T11:49:52,793 DEBUG [main] jdbc.HiveQueryResultSet: Fetched row string: 
2016-10-17T11:49:52,793 DEBUG [main] jdbc.HiveQueryResultSet: Fetched row string: 
2016-10-17T11:49:52,793 DEBUG [main] jdbc.HiveQueryResultSet: Fetched row string: 
2016-10-17T11:49:52,793 DEBUG [main] jdbc.HiveQueryResultSet: Fetched row string: 


NO PRECOMMIT TESTS</description>
			<version>2.2.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.jdbc.HiveQueryResultSet.java</file>
		</fixedFiles>
	</bug>
	<bug id="14989" opendate="2016-10-17 20:45:53" fixdate="2016-10-18 04:47:13" resolution="Duplicate">
		<buginformation>
			<summary>FIELDS TERMINATED BY parsing broken when delimiter is more than 1 byte</summary>
			<description>FIELDS TERMINATED BY parsing broken when delimiter is more than 1 byte. Delimiter starting from 2nd character becomes part of returned data. No parsed properly.
Test case:

CREATE external TABLE test_muldelim
(  string1 STRING,
   string2 STRING,
   string3 STRING
)
 ROW FORMAT 
       DELIMITED FIELDS TERMINATED BY &amp;amp;apos;&amp;lt;&amp;gt;&amp;amp;apos;
      LINES TERMINATED BY &amp;amp;apos;\n&amp;amp;apos;
 STORED AS TEXTFILE
  location &amp;amp;apos;/user/hive/test_muldelim&amp;amp;apos;


Create a text file under /user/hive/test_muldelim with following 2 lines:

data1&amp;lt;&amp;gt;data2&amp;lt;&amp;gt;data3
aa&amp;lt;&amp;gt;bb&amp;lt;&amp;gt;cc


Now notice that two-character delimiter wasn&amp;amp;apos;t parsed properly:

jdbc:hive2://host.domain.com:1&amp;gt; select * from ruslan_test.test_muldelim ;
+------------------------+------------------------+------------------------+--+
| test_muldelim.string1  | test_muldelim.string2  | test_muldelim.string3  |
+------------------------+------------------------+------------------------+--+
| data1                  | &amp;gt;data2                 | &amp;gt;data3                 |
| aa                     | &amp;gt;bb                    | &amp;gt;cc                    |
+------------------------+------------------------+------------------------+--+
2 rows selected (0.453 seconds)


The second delimiter&amp;amp;apos;s character (&amp;amp;apos;&amp;gt;&amp;amp;apos;) became part of the columns to the right (`string2` and `string3`).
Table DDL:

0: jdbc:hive2://host.domain.com:1&amp;gt; show create table dafault.test_muldelim ;
+-----------------------------------------------------------------+--+
|                         createtab_stmt                          |
+-----------------------------------------------------------------+--+
| CREATE EXTERNAL TABLE `default.test_muldelim`(              |
|   `string1` string,                                             |
|   `string2` string,                                             |
|   `string3` string)                                             |
| ROW FORMAT DELIMITED                                            |
|   FIELDS TERMINATED BY &amp;amp;apos;&amp;lt;&amp;gt;&amp;amp;apos;                                     |
|   LINES TERMINATED BY &amp;amp;apos;\n&amp;amp;apos;                                      |
| STORED AS INPUTFORMAT                                           |
|   &amp;amp;apos;org.apache.hadoop.mapred.TextInputFormat&amp;amp;apos;                    |
| OUTPUTFORMAT                                                    |
|   &amp;amp;apos;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&amp;amp;apos;  |
| LOCATION                                                        |
|   &amp;amp;apos;hdfs://epsdatalake/user/hive/test_muldelim&amp;amp;apos;              |
| TBLPROPERTIES (                                                 |
|   &amp;amp;apos;transient_lastDdlTime&amp;amp;apos;=&amp;amp;apos;1476727100&amp;amp;apos;)                         |
+-----------------------------------------------------------------+--+
15 rows selected (0.286 seconds)

</description>
			<version>0.13.0</version>
			<fixedVersion>0.14.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyStruct.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyBinary.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.lazy.TestLazyPrimitive.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">5871</link>
			<link type="Reference" description="relates to">5871</link>
		</links>
	</bug>
	<bug id="15004" opendate="2016-10-18 18:22:41" fixdate="2016-10-18 18:25:12" resolution="Duplicate">
		<buginformation>
			<summary>Query for parquet tables failing with java.lang.IllegalArgumentException: FilterPredicate column: f&amp;apos;s declared type (java.lang.Double) does not match the schema found in file metadata. </summary>
			<description>Queries involving float data type, run against parquet tables failing
hive&amp;gt; desc extended all100k;
OK
t tinyint
sismallint
i int
b bigint
f float
d double
s string
dcdecimal(38,18)
boboolean
v varchar(25)
c char(25)
tstimestamp
dtdate
Detailed Table InformationTable(tableName:all100k, dbName:default, owner:hrt_qa, createTime:1476765150, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:t, type:tinyint, comment:null), FieldSchema(name:si, type:smallint, comment:null), FieldSchema(name:i, type:int, comment:null), FieldSchema(name:b, type:bigint, comment:null), FieldSchema(name:f, type:float, comment:null), FieldSchema(name:d, type:double, comment:null), FieldSchema(name:s, type:string, comment:null), FieldSchema(name:dc, type:decimal(38,18), comment:null), FieldSchema(name:bo, type:boolean, comment:null), FieldSchema(name:v, type:varchar(25), comment:null), FieldSchema(name:c, type:char(25), comment:null), FieldSchema(name:ts, type:timestamp, comment:null), FieldSchema(name:dt, type:date, comment:null)], location:hdfs://ctr-e45-1475874954070-9012-01-000008.hwx.site:8020/apps/hive/warehouse/all100k, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:
{serialization.format=1}
), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{numFiles=1, transient_lastDdlTime=1476765184, COLUMN_STATS_ACCURATE={"COLUMN_STATS":
{"t":"true","si":"true","i":"true","b":"true","f":"true","d":"true","s":"true","dc":"true","bo":"true","v":"true","c":"true","ts":"true"}
,"BASIC_STATS":"true"}, totalSize=6564143, numRows=100000, rawDataSize=1300000}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
Time taken: 0.54 seconds, Fetched: 15 row(s)
hive&amp;gt; select t from all100k
&amp;gt; where t&amp;lt;&amp;gt;0 and s&amp;lt;&amp;gt;0 and b&amp;lt;&amp;gt;0 and (f&amp;lt;&amp;gt;0 or d&amp;lt;&amp;gt;0);
OK
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
Failed with exception java.io.IOException:java.lang.IllegalArgumentException: FilterPredicate column: f&amp;amp;apos;s declared type (java.lang.Double) does not match the schema found in file metadata. Column f is of type: FLOAT
Valid types for this column are: [class java.lang.Float]
Time taken: 0.919 seconds</description>
			<version>1.2.1</version>
			<fixedVersion>1.2.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.parquet.read.TestParquetFilterPredicate.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.parquet.LeafFilterFactory.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">11504</link>
		</links>
	</bug>
	<bug id="13423" opendate="2016-04-05 13:49:08" fixdate="2016-10-20 13:40:06" resolution="Fixed">
		<buginformation>
			<summary>Handle the overflow case for decimal datatype for sum()</summary>
			<description>When a column col1 defined as decimal and if the sum of the column overflows, we will try to increase the decimal precision by 10. But if it&amp;amp;apos;s reaching 38 (the max precision), the overflow still could happen. Right now, if such case happens, the following exception will throw since hive is writing incorrect data.
Follow the following steps to repro. 

CREATE TABLE DECIMAL_PRECISION(dec decimal(38,18));
INSERT INTO DECIMAL_PRECISION VALUES(98765432109876543210.12345), (98765432109876543210.12345);
SELECT SUM(dec) FROM DECIMAL_PRECISION;



Caused by: java.lang.ArrayIndexOutOfBoundsException: 1
        at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils.readVInt(LazyBinaryUtils.java:314) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils.checkObjectByteInfo(LazyBinaryUtils.java:219) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryStruct.parse(LazyBinaryStruct.java:142) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]

</description>
			<version>2.0.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">6459</link>
		</links>
	</bug>
	<bug id="7483" opendate="2014-07-23 03:02:34" fixdate="2016-10-21 00:48:43" resolution="Duplicate">
		<buginformation>
			<summary>hive insert overwrite table select from self dead lock</summary>
			<description>CREATE TABLE test(
  id int, 
  msg string)
PARTITIONED BY ( 
  continent string, 
  country string)
CLUSTERED BY (id) 
INTO 10 BUCKETS
STORED AS ORC;
alter table test add partition(continent=&amp;amp;apos;Asia&amp;amp;apos;,country=&amp;amp;apos;India&amp;amp;apos;);
in hive-site.xml:
hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
hive.support.concurrency=true;
in hive shell:
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
insert into test table some records first.
then execute sql:
insert overwrite table test partition(continent=&amp;amp;apos;Asia&amp;amp;apos;,country=&amp;amp;apos;India&amp;amp;apos;) select id,msg from test;
the log stop at :
INFO log.PerfLogger: &amp;lt;PERFLOG method=acquireReadWriteLocks from=org.apache.hadoop.hive.ql.Driver&amp;gt;
i think it has dead lock when insert overwrite table from it self.</description>
			<version>0.13.1</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.DbLockManager.java</file>
			<file type="M">org.apache.hadoop.hive.common.JavaUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">10483</link>
		</links>
	</bug>
	<bug id="15029" opendate="2016-10-21 11:29:41" fixdate="2016-10-25 11:15:33" resolution="Fixed">
		<buginformation>
			<summary>Add logic to estimate stats for BETWEEN operator</summary>
			<description>Currently, BETWEEN operator is considered in the default case: reduces the input rows to the half. This may lead to wrong estimates for the number of rows produced by Filter operators.</description>
			<version>2.1.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
		</fixedFiles>
	</bug>
	<bug id="15030" opendate="2016-10-21 12:08:24" fixdate="2016-10-25 11:18:55" resolution="Fixed">
		<buginformation>
			<summary>Fixes in inference of collation for Tez cost model</summary>
			<description>Tez cost model might get NPE if collation returned by join algorithm is null.</description>
			<version>2.1.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.cost.HiveOnTezCostModel.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.cost.HiveAlgorithmsUtil.java</file>
		</fixedFiles>
	</bug>
	<bug id="14964" opendate="2016-10-14 17:35:44" fixdate="2016-10-27 03:39:03" resolution="Fixed">
		<buginformation>
			<summary>Failing Test: Fix TestBeelineArgParsing tests</summary>
			<description>Failing last several builds:

 org.apache.hive.beeline.TestBeelineArgParsing.testAddLocalJar[0]	0.12 sec	12
 org.apache.hive.beeline.TestBeelineArgParsing.testAddLocalJarWithoutAddDriverClazz[0]	29 ms	12
 org.apache.hive.beeline.TestBeelineArgParsing.testAddLocalJar[1]	42 ms	12

</description>
			<version>1.1.1</version>
			<fixedVersion>1.3.0, 1.2.2, 2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.beeline.TestBeelineArgParsing.java</file>
			<file type="M">org.apache.hive.beeline.ClassNameCompleter.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">15053</link>
			<link type="Duplicate" description="is duplicated by">15133</link>
			<link type="Duplicate" description="is duplicated by">14975</link>
			<link type="Reference" description="is related to">15053</link>
		</links>
	</bug>
	<bug id="15046" opendate="2016-10-24 21:16:19" fixdate="2016-10-27 11:11:17" resolution="Fixed">
		<buginformation>
			<summary>Multiple fixes for Druid handler</summary>
			<description>
Druid query type not recognized after Calcite upgrade; introduced by HIVE-13316.
Fix handling of NULL values for GroupBy queries.
Fix handling of dimension/metrics names, as those names in Druid are case sensitive.
Select Druid query to effectively return no rows when the result is empty (previously returning a single row).
When it is split, each of the parts of a Select query might return more results than threshold; set threshold to max integer in query so we do not face this problem.

</description>
			<version>2.2.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.druid.HiveDruidQueryBasedInputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.java</file>
			<file type="M">org.apache.hadoop.hive.druid.serde.DruidSelectQueryRecordReader.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTBuilder.java</file>
			<file type="M">org.apache.hadoop.hive.druid.serde.DruidGroupByQueryRecordReader.java</file>
			<file type="M">org.apache.hadoop.hive.druid.serde.DruidTopNQueryRecordReader.java</file>
		</fixedFiles>
	</bug>
	<bug id="15065" opendate="2016-10-26 01:10:06" fixdate="2016-10-27 21:23:33" resolution="Fixed">
		<buginformation>
			<summary>SimpleFetchOptimizer should decide based on metastore stats when available</summary>
			<description>Currently the decision to use fetch optimizer or not is based on scanning the filesystem for file lengths and see if the aggregated size is less the fetch task threshold. This can be very expensive for cloud environment. This issue is mitigated to some extent by HIVE-14920 but still that requires file system scan. We can make decision based on the stats from metastore and falling back when stats is not available. Since fast stats (numRows and fileSize) is always available this should work most of the time. </description>
			<version>2.2.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer.java</file>
		</fixedFiles>
	</bug>
	<bug id="14883" opendate="2016-10-03 21:59:14" fixdate="2016-10-28 18:02:22" resolution="Duplicate">
		<buginformation>
			<summary>Checks for Acid operation/bucket table write are in the wrong place</summary>
			<description>The following code in 
 in SemanticAnalyzer.getMetaData(QB qb, ReadEntity parentInput) 

      // Disallow INSERT INTO on bucketized tables
      boolean isAcid = AcidUtils.isAcidTable(tab);
      boolean isTableWrittenTo = qb.getParseInfo().isInsertIntoTable(tab.getDbName(), tab.getTableName());
      if (isTableWrittenTo &amp;amp;&amp;amp;
          tab.getNumBuckets() &amp;gt; 0 &amp;amp;&amp;amp; !isAcid) {
        throw new SemanticException(ErrorMsg.INSERT_INTO_BUCKETIZED_TABLE.
            getMsg("Table: " + tabName));
      }
      // Disallow update and delete on non-acid tables
      if ((updating() || deleting()) &amp;amp;&amp;amp; !isAcid &amp;amp;&amp;amp; isTableWrittenTo) {
        //isTableWrittenTo: delete from acidTbl where a in (select id from nonAcidTable)
        //so only assert this if we are actually writing to this table
        // Whether we are using an acid compliant transaction manager has already been caught in
        // UpdateDeleteSemanticAnalyzer, so if we are updating or deleting and getting nonAcid
        // here, it means the table itself doesn&amp;amp;apos;t support it.
        throw new SemanticException(ErrorMsg.ACID_OP_ON_NONACID_TABLE, tabName);
      }


is done in the loop "    for (String alias : tabAliases) {" which is over tables being read.
Should be done in "    for (String name : qbp.getClauseNamesForDest()) {" loop</description>
			<version>1.2.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.ASTNode.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.java</file>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager.java</file>
			<file type="M">org.apache.hadoop.hive.ql.hooks.WriteEntity.java</file>
			<file type="M">org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.LockComponentBuilder.java</file>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.HiveTxnManager.java</file>
			<file type="M">org.apache.hadoop.hive.ql.TestTxnCommands2WithSplitUpdate.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.TestIUD.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.CreateTableDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.hooks.ReadEntity.java</file>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
			<file type="M">org.apache.hadoop.hive.ql.ErrorMsg.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.BucketingSortingReduceSinkOptimizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TestCompactionTxnHandler.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Context.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="blocks">10924</link>
			<link type="Duplicate" description="duplicates">14943</link>
		</links>
	</bug>
	<bug id="15061" opendate="2016-10-25 22:12:45" fixdate="2016-10-31 21:09:22" resolution="Fixed">
		<buginformation>
			<summary>Metastore types are sometimes case sensitive</summary>
			<description>Impala recently encountered an issue with the metastore (IMPALA-4260 ) where column stats would get dropped when adding a column to a table.
The reason seems to be that Hive does a case sensitive check on the column stats types during an "alter table" and expects the types to be all lower case. This case sensitive check doesn&amp;amp;apos;t appear to happen when the stats are set in the first place.
We&amp;amp;apos;re solving this on the Impala end by storing types in the metastore as all lower case, but Hive&amp;amp;apos;s behavior here is very confusing. It should either always be case sensitive, so that you can&amp;amp;apos;t create column stats with types that Hive considers invalid, or it should never be case sensitive.</description>
			<version>1.1.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
		</fixedFiles>
	</bug>
	<bug id="15099" opendate="2016-10-31 18:07:29" fixdate="2016-11-01 00:36:28" resolution="Fixed">
		<buginformation>
			<summary>PTFOperator.PTFInvocation didn&amp;apos;t properly reset the input partition</summary>
			<description>There is an issue with PTFOperator.PTFInvocation where the inputPart is not reset properly. The inputPart has been closed and its content (member variables) has been cleaned up, but since itself is not nullified, it&amp;amp;apos;s reused in the next round and caused NPE issue.</description>
			<version>1.2.1</version>
			<fixedVersion>1.3.0, 2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.PTFOperator.java</file>
		</fixedFiles>
	</bug>
	<bug id="15133" opendate="2016-11-04 22:46:34" fixdate="2016-11-06 22:04:43" resolution="Fixed">
		<buginformation>
			<summary>Branch-1.2: Investigate TestBeelineArgParsing</summary>
			<description></description>
			<version>1.2.2</version>
			<fixedVersion>1.3.0, 1.2.2</fixedVersion>
			<type>Sub-task</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.beeline.TestBeelineArgParsing.java</file>
			<file type="M">org.apache.hive.beeline.ClassNameCompleter.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">14964</link>
		</links>
	</bug>
	<bug id="15054" opendate="2016-10-25 15:53:42" fixdate="2016-11-07 03:27:09" resolution="Fixed">
		<buginformation>
			<summary>Hive insertion query execution fails on Hive on Spark</summary>
			<description>The query of insert overwrite table tbl1 sometimes will fail with the following errors. Seems we are constructing taskAttemptId with partitionId which is not unique if there are multiple attempts.

ava.lang.IllegalStateException: Hit error while closing operators - failing tree: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to rename output from: hdfs://table1/.hive-staging_hive_2016-06-14_01-53-17_386_3231646810118049146-9/_task_tmp.-ext-10002/_tmp.002148_0 to: hdfs://table1/.hive-staging_hive_2016-06-14_01-53-17_386_3231646810118049146-9/_tmp.-ext-10002/002148_0
at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.close(SparkMapRecordHandler.java:202)
at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.closeRecordProcessor(HiveMapFunctionResultList.java:58)
at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList$ResultIterator.hasNext(HiveBaseFunctionResultList.java:106)
at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:41)
at scala.collection.Iterator$class.foreach(Iterator.scala:727)
at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$15.apply(AsyncRDDActions.scala:120)


</description>
			<version>2.0.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.HivePairFlatMapFunction.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="is duplicated by">13066</link>
		</links>
	</bug>
	<bug id="13066" opendate="2016-02-17 06:47:59" fixdate="2016-11-07 03:28:24" resolution="Duplicate">
		<buginformation>
			<summary>Hive on Spark gives incorrect results when speculation is on</summary>
			<description>The issue is reported by users. One possible reason is that we always append 0 as the attempt ID for each task so that hive won&amp;amp;apos;t be able to distinguish between speculative tasks and original ones.</description>
			<version>2.0.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.HivePairFlatMapFunction.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">15054</link>
		</links>
	</bug>
	<bug id="13947" opendate="2016-06-04 09:58:36" fixdate="2016-11-07 14:17:59" resolution="Fixed">
		<buginformation>
			<summary>HoS print wrong number for hash table size in map join scenario</summary>
			<description>In sparkHashTableSinkOperator, when flushToFile, before close output stream, it try to get the file length, and will get 0 for it,  take hashTableSinkOperator for ref, it should get length after output stream closed</description>
			<version>1.2.1</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.exec.SparkHashTableSinkOperator.java</file>
		</fixedFiles>
	</bug>
	<bug id="14924" opendate="2016-10-10 22:40:22" fixdate="2016-11-08 07:17:02" resolution="Fixed">
		<buginformation>
			<summary>MSCK REPAIR table with single threaded is throwing null pointer exception</summary>
			<description>MSCK REPAIR TABLE is throwing Null Pointer Exception while running on single threaded mode (hive.mv.files.thread=0)
Error:
2016-10-10T22:27:13,564 ERROR [e9ce04a8-2a84-426d-8e79-a2d15b8cee09 main([])]: exec.DDLTask (DDLTask.java:failed(581)) - java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.checkPartitionDirs(HiveMetaStoreChecker.java:423)
	at org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.findUnknownPartitions(HiveMetaStoreChecker.java:315)
	at org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.checkTable(HiveMetaStoreChecker.java:291)
	at org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.checkTable(HiveMetaStoreChecker.java:236)
	at org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.checkMetastore(HiveMetaStoreChecker.java:113)
	at org.apache.hadoop.hive.ql.exec.DDLTask.msck(DDLTask.java:1834)
In order to reproduce:
set hive.mv.files.thread=0 and run MSCK REPAIR TABLE command</description>
			<version>2.1.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="is related to">13984</link>
		</links>
	</bug>
	<bug id="14984" opendate="2016-10-17 08:48:48" fixdate="2016-11-08 13:34:03" resolution="Fixed">
		<buginformation>
			<summary>Hive-WebUI access results in Request is a replay (34) attack</summary>
			<description>When trying to access kerberized webui of HS2, The following error is received
GSSException: Failure unspecified at GSS-API level (Mechanism level: Request is a replay (34))
While this is not happening for RM webui (checked if kerberos webui is enabled)
To reproduce the issue 
Try running
curl --negotiate -u : -b ~/cookiejar.txt -c ~/cookiejar.txt http://&amp;lt;hostname&amp;gt;:10002/
from any cluster nodes
or 
Try accessing the URL from a VM with windows machine and firefox browser to replicate the issue
The following workaround helped, but need a permanent solution for the bug
Workaround:
=========
First access the index.html directly and then actual URL of webui
curl --negotiate -u : -b ~/cookiejar.txt -c ~/cookiejar.txt http://&amp;lt;hostname&amp;gt;:10002/index.html
curl --negotiate -u : -b ~/cookiejar.txt -c ~/cookiejar.txt http://&amp;lt;hostname&amp;gt;:10002
In browser:
First access
http://&amp;lt;hostname&amp;gt;:10002/index.html
then
http://&amp;lt;hostname&amp;gt;:10002</description>
			<version>1.2.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.http.HttpServer.java</file>
			<file type="M">org.apache.hive.service.server.TestHS2HttpServer.java</file>
		</fixedFiles>
		<links>
			<link type="Regression" description="breaks">15196</link>
		</links>
	</bug>
	<bug id="14975" opendate="2016-10-15 06:19:33" fixdate="2016-11-09 19:44:38" resolution="Duplicate">
		<buginformation>
			<summary>Flaky Test: TestBeelineArgParsing.testAddLocalJarWithoutAddDriverClazz</summary>
			<description>

2016-10-14T22:51:32,947  INFO [main] beeline.TestBeelineArgParsing: Add /home/hiveptest/104.155.175.228-hiveptest-0/maven/postgresql/postgresql/9.1-901.jdbc4/postgresql-9.1-901.jdbc4.jar for the driver class org.postgresql.Driver
Fail to add local jar due to the exception:java.util.zip.ZipException: error in opening zip file
error in opening zip file

</description>
			<version>2.2.0</version>
			<fixedVersion></fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.beeline.TestBeelineArgParsing.java</file>
			<file type="M">org.apache.hive.beeline.ClassNameCompleter.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">14964</link>
		</links>
	</bug>
	<bug id="15143" opendate="2016-11-07 20:56:36" fixdate="2016-11-10 20:22:55" resolution="Fixed">
		<buginformation>
			<summary>add logging for HIVE-15024</summary>
			<description>

Caused by: java.io.IOException: java.lang.ClassCastException: org.apache.hadoop.hive.common.io.DiskRangeList cannot be cast to org.apache.orc.impl.BufferChunk
        at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat$LlapRecordReader.rethrowErrorIfAny(LlapInputFormat.java:383)
        at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat$LlapRecordReader.nextCvb(LlapInputFormat.java:338)
        at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat$LlapRecordReader.next(LlapInputFormat.java:278)
        at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat$LlapRecordReader.next(LlapInputFormat.java:167)
        at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:350)
        ... 23 more
Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.common.io.DiskRangeList cannot be cast to org.apache.orc.impl.BufferChunk
        at org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.prepareRangesForCompressedRead(EncodedReaderImpl.java:728)
        at org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.readEncodedStream(EncodedReaderImpl.java:616)
        at org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.readEncodedColumns(EncodedReaderImpl.java:397)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.performDataRead(OrcEncodedDataReader.java:424)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$4.run(OrcEncodedDataReader.java:227)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$4.run(OrcEncodedDataReader.java:224)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:224)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:93)
        ... 6 more

2016-10-20T00:48:45,354 WARN  [TezTaskRunner (1475017598908_0410_15_00_000020_0)] org.apache.tez.runtime.LogicalIOProcessorRuntimeTask: Ignoring exception when closing input part(cleanup). Exception class
=java.io.IOException, message=java.lang.ClassCastException: org.apache.hadoop.hive.common.io.DiskRangeList cannot be cast to org.apache.orc.impl.BufferChunk
2016-10-20T00:48:45,416 WARN  [TaskHeartbeatThread (1475017598908_0410_15_00_000020_0)] org.apache.hadoop.hive.llap.daemon.impl.LlapTaskReporter: Exiting TaskReporter thread with pending queue size=2

</description>
			<version>2.2.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
		</fixedFiles>
		<links>
			<link type="Cloners" description="is a clone of">15024</link>
		</links>
	</bug>
	<bug id="15137" opendate="2016-11-05 04:21:33" fixdate="2016-11-11 23:38:54" resolution="Fixed">
		<buginformation>
			<summary>metastore add partitions background thread should use current username</summary>
			<description>The background thread used in HIVE-13901 for adding partitions needs to be reinitialized with current UGI for each invocation. Otherwise the user in context while thread was created would be the current UGI during the actions in the thread.</description>
			<version>2.1.1</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
		</fixedFiles>
		<links>
			<link type="Regression" description="is broken by">13901</link>
		</links>
	</bug>
	<bug id="14943" opendate="2016-10-13 17:43:35" fixdate="2016-11-12 20:26:41" resolution="Fixed">
		<buginformation>
			<summary>Base Implementation</summary>
			<description>Create the 1st pass functional implementation of MERGE
This should run e2e and produce correct results.  </description>
			<version>1.2.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Sub-task</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.ASTNode.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.java</file>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager.java</file>
			<file type="M">org.apache.hadoop.hive.ql.hooks.WriteEntity.java</file>
			<file type="M">org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.LockComponentBuilder.java</file>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.HiveTxnManager.java</file>
			<file type="M">org.apache.hadoop.hive.ql.TestTxnCommands2WithSplitUpdate.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.TestIUD.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.CreateTableDesc.java</file>
			<file type="M">org.apache.hadoop.hive.ql.hooks.ReadEntity.java</file>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
			<file type="M">org.apache.hadoop.hive.ql.ErrorMsg.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.BucketingSortingReduceSinkOptimizer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.java</file>
			<file type="M">org.apache.hadoop.hive.metastore.txn.TestCompactionTxnHandler.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
			<file type="M">org.apache.hadoop.hive.ql.Context.java</file>
		</fixedFiles>
		<links>
			<link type="Blocker" description="is blocked by">14993</link>
			<link type="Blocker" description="is blocked by">15010</link>
			<link type="Duplicate" description="is duplicated by">14948</link>
			<link type="Duplicate" description="is duplicated by">14944</link>
			<link type="Duplicate" description="is duplicated by">14945</link>
			<link type="Duplicate" description="is duplicated by">14952</link>
			<link type="Duplicate" description="is duplicated by">15011</link>
			<link type="Duplicate" description="is duplicated by">14883</link>
			<link type="dependent" description="is depended upon by">13795</link>
		</links>
	</bug>
	<bug id="15096" opendate="2016-10-29 03:00:20" fixdate="2016-11-13 03:17:53" resolution="Fixed">
		<buginformation>
			<summary>hplsql registerUDF conflicts with pom.xml</summary>
			<description>in hplsql code, registerUDF code is
    sql.add("ADD JAR " + dir + "hplsql.jar");
    sql.add("ADD JAR " + dir + "antlr-runtime-4.5.jar");
    sql.add("ADD FILE " + dir + Conf.SITE_XML);
but pom configufation is
  &amp;lt;parent&amp;gt;
    &amp;lt;groupId&amp;gt;org.apache.hive&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;hive&amp;lt;/artifactId&amp;gt;
    &amp;lt;version&amp;gt;2.2.0-SNAPSHOT&amp;lt;/version&amp;gt;
    &amp;lt;relativePath&amp;gt;../pom.xml&amp;lt;/relativePath&amp;gt;
  &amp;lt;/parent&amp;gt;
  &amp;lt;artifactId&amp;gt;hive-hplsql&amp;lt;/artifactId&amp;gt;
  &amp;lt;packaging&amp;gt;jar&amp;lt;/packaging&amp;gt;
  &amp;lt;name&amp;gt;Hive HPL/SQL&amp;lt;/name&amp;gt;
    &amp;lt;dependency&amp;gt;
       &amp;lt;groupId&amp;gt;org.antlr&amp;lt;/groupId&amp;gt;
       &amp;lt;artifactId&amp;gt;antlr4-runtime&amp;lt;/artifactId&amp;gt;
       &amp;lt;version&amp;gt;4.5&amp;lt;/version&amp;gt;
    &amp;lt;/dependency&amp;gt;
when run hplsql , errors occur as below
 Error while processing statement: /opt/apps/apache-hive-2.0.0-bin/lib/hplsql.jar does not exist</description>
			<version>2.0.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hive.hplsql.Exec.java</file>
		</fixedFiles>
	</bug>
	<bug id="11208" opendate="2015-07-08 14:06:37" fixdate="2016-11-17 17:02:46" resolution="Fixed">
		<buginformation>
			<summary>Can not drop a default partition __HIVE_DEFAULT_PARTITION__ which is not a "string" type</summary>
			<description>When partition is not a string type, for example, if it is a int type, when drop the default partition _HIVE_DEFAULT_PARTITION_, you will get:
SemanticException Unexpected unknown partitions
Reproduce:

SET hive.exec.dynamic.partition=true;
SET hive.exec.dynamic.partition.mode=nonstrict;
set hive.exec.max.dynamic.partitions.pernode=10000;

DROP TABLE IF EXISTS test;
CREATE TABLE test (col1 string) PARTITIONED BY (p1 int) ROW FORMAT DELIMITED FIELDS TERMINATED BY &amp;amp;apos;\001&amp;amp;apos; STORED AS TEXTFILE;
INSERT OVERWRITE TABLE test PARTITION (p1) SELECT code, IF(salary &amp;gt; 600, 100, null) as p1 FROM jsmall;

hive&amp;gt; SHOW PARTITIONS test;
OK
p1=100
p1=__HIVE_DEFAULT_PARTITION__
Time taken: 0.124 seconds, Fetched: 2 row(s)

hive&amp;gt; ALTER TABLE test DROP partition (p1 = &amp;amp;apos;__HIVE_DEFAULT_PARTITION__&amp;amp;apos;);
FAILED: SemanticException Unexpected unknown partitions for (p1 = null)


</description>
			<version>1.1.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.java</file>
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ppr.PartExprEvalUtils.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqual.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNotEqual.java</file>
		</fixedFiles>
		<links>
			<link type="Duplicate" description="duplicates">11237</link>
		</links>
	</bug>
	<bug id="15090" opendate="2016-10-28 12:11:18" fixdate="2016-11-17 17:06:31" resolution="Fixed">
		<buginformation>
			<summary>Temporary DB failure can stop ExpiredTokenRemover thread</summary>
			<description>In HIVE-13090 we decided that we should not close the metastore if there is an unexpected exception during the expired token removal process, but that fix leaves a running metastore without ExpiredTokenRemover thread.
To fix this I will move the catch inside the running loop, and hope the thread could recover from the exception</description>
			<version>1.3.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">15184</link>
			<link type="Regression" description="is broken by">13090</link>
		</links>
	</bug>
	<bug id="15231" opendate="2016-11-17 17:40:17" fixdate="2016-11-18 14:09:34" resolution="Fixed">
		<buginformation>
			<summary>query on view with CTE and alias fails with table not found error</summary>
			<description>HIVE-10698 fixed one issue of the query on view with CTE, but it seems to break another case if a alias is given for the CTE.

use bugtest;
create table basetb(id int, name string);
create view testv1 as
with subtb as (select id, name from bugtest.basetb)
select id from subtb a;
use castest;
explain select * from bugtest.testv1;
hive&amp;gt; explain select * from bugtest.testv1;

FAILED: SemanticException Line 2:21 Table not found &amp;amp;apos;subtb&amp;amp;apos; in definition of VIEW testv1 [
with subtb as (select `basetb`.`id`, `basetb`.`name` from `bugtest`.`basetb`)
select `a`.`id` from `bugtest`.`subtb` `a`
] used as testv1 at Line 1:14

</description>
			<version>1.3.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">10698</link>
		</links>
	</bug>
	<bug id="15233" opendate="2016-11-17 19:54:56" fixdate="2016-11-18 17:25:33" resolution="Fixed">
		<buginformation>
			<summary>UDF UUID() should be non-deterministic</summary>
			<description>The UUID() function should be non-deterministic.</description>
			<version>2.2.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.udf.TestUDFUUID.java</file>
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFUUID.java</file>
		</fixedFiles>
		<links>
			<link type="Reference" description="relates to">12721</link>
		</links>
	</bug>
	<bug id="14089" opendate="2016-06-24 08:17:45" fixdate="2016-11-18 19:08:26" resolution="Fixed">
		<buginformation>
			<summary>complex type support in LLAP IO is broken </summary>
			<description>HIVE-13617 is causing MiniLlapCliDriver following test failures


org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver_vector_complex_all
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver_vector_complex_join


</description>
			<version>2.2.0</version>
			<fixedVersion>2.2.0</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReader.java</file>
			<file type="M">org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
			<file type="M">org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.java</file>
			<file type="M">org.apache.hadoop.hive.ql.plan.MapWork.java</file>
			<file type="M">org.apache.orc.impl.TreeReaderFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.encoded.EncodedTreeReaderFactory.java</file>
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			<file type="M">org.apache.hadoop.hive.ql.io.orc.encoded.Reader.java</file>
			<file type="M">org.apache.hadoop.hive.llap.io.metadata.OrcFileMetadata.java</file>
			<file type="M">org.apache.hadoop.hive.common.io.encoded.EncodedColumnBatch.java</file>
			<file type="M">org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.java</file>
			<file type="M">org.apache.orc.impl.ConvertTreeReaderFactory.java</file>
		</fixedFiles>
	</bug>
	<bug id="13539" opendate="2016-04-18 19:16:27" fixdate="2016-11-18 21:07:06" resolution="Fixed">
		<buginformation>
			<summary>HiveHFileOutputFormat searching the wrong directory for HFiles</summary>
			<description>When creating HFiles for a bulkload in HBase I believe it is looking in the wrong directory to find the HFiles, resulting in the following exception:


Error: java.lang.RuntimeException: Hive Runtime Error while closing operators: java.io.IOException: Multiple family directories found in hdfs://c1n1.gbif.org:8020/user/hive/warehouse/tim.db/coords_hbase/_temporary/2/_temporary
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.close(ExecReducer.java:295)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:453)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: Multiple family directories found in hdfs://c1n1.gbif.org:8020/user/hive/warehouse/tim.db/coords_hbase/_temporary/2/_temporary
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator$FSPaths.closeWriters(FileSinkOperator.java:188)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.closeOp(FileSinkOperator.java:958)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:598)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:610)
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.close(ExecReducer.java:287)
	... 7 more
Caused by: java.io.IOException: Multiple family directories found in hdfs://c1n1.gbif.org:8020/user/hive/warehouse/tim.db/coords_hbase/_temporary/2/_temporary
	at org.apache.hadoop.hive.hbase.HiveHFileOutputFormat$1.close(HiveHFileOutputFormat.java:158)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator$FSPaths.closeWriters(FileSinkOperator.java:185)
	... 11 more


The issue is that is looks for the HFiles in hdfs://c1n1.gbif.org:8020/user/hive/warehouse/tim.db/coords_hbase/_temporary/2/_temporary when I believe it should be looking in the task attempt subfolder, such as hdfs://c1n1.gbif.org:8020/user/hive/warehouse/tim.db/coords_hbase/_temporary/2/_temporary/attempt_1461004169450_0002_r_000000_1000.
This can be reproduced in any HFile creation such as:


CREATE TABLE coords_hbase(id INT, x DOUBLE, y DOUBLE)
STORED BY &amp;amp;apos;org.apache.hadoop.hive.hbase.HBaseStorageHandler&amp;amp;apos;
WITH SERDEPROPERTIES (
  &amp;amp;apos;hbase.columns.mapping&amp;amp;apos; = &amp;amp;apos;:key,o:x,o:y&amp;amp;apos;,
  &amp;amp;apos;hbase.table.default.storage.type&amp;amp;apos; = &amp;amp;apos;binary&amp;amp;apos;);

SET hfile.family.path=/tmp/coords_hfiles/o; 
SET hive.hbase.generatehfiles=true;

INSERT OVERWRITE TABLE coords_hbase 
SELECT id, decimalLongitude, decimalLatitude
FROM source
CLUSTER BY id; 


Any advice greatly appreciated</description>
			<version>1.1.0</version>
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			<type>Bug</type>
		</buginformation>
		<fixedFiles>
			<file type="M">org.apache.hadoop.hive.hbase.HiveHFileOutputFormat.java</file>
		</fixedFiles>
	</bug>
</bugrepository>