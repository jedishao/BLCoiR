<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 02 19:02:02 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-138/HBASE-138.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-138] [hbase] Under load, regions become extremely large and eventually cause region servers to become unresponsive</title>
                <link>https://issues.apache.org/jira/browse/HBASE-138</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;When attempting to write to HBase as fast as possible, HBase accepts puts at a reasonably high rate for a while, and then the rate begins to drop off, ultimately culminating in exceptions reaching client code. In my testing, I was able to write about 370 10KB records a second to HBase until I reach around 1 million rows written. At that point, a moderate to large number of exceptions - NotServingRegionException, WrongRegionException, region offline, etc - begin reaching the client code. This appears to be because the retry-and-wait logic in HTable runs out of retries and fails. &lt;/p&gt;

&lt;p&gt;Looking at mapfiles for the regions from the command line shows that some of the mapfiles are between 1 and 2 GB in size, much more than the stated file size limit. Talking with Stack, one possible explanation for this is that the RegionServer is not choosing to compact files often enough, leading to many small mapfiles, which in turn leads to a few overlarge mapfiles. Then, when the time comes to do a split or &quot;major&quot; compaction, it takes an unexpectedly long time to complete these operations. This translates into errors for the client application.&lt;/p&gt;

&lt;p&gt;If I back off the import process and give the cluster some quiet time, some splits and compactions clearly do take place, because the number of regions go up and the number of mapfiles/region goes down. I can then begin writing again in earnest for a short period of time until the problem begins again.&lt;/p&gt;

&lt;p&gt;Both Marc Harris and myself have seen this behavior.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12387412">HBASE-138</key>
            <summary>[hbase] Under load, regions become extremely large and eventually cause region servers to become unresponsive</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.png">Blocker</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="bryanduxbury">Bryan Duxbury</reporter>
                        <labels>
                    </labels>
                <created>Tue, 29 Jan 2008 21:54:02 +0000</created>
                <updated>Fri, 22 Aug 2008 21:34:50 +0000</updated>
                            <resolved>Sat, 2 Feb 2008 00:43:58 +0000</resolved>
                                                                        <due></due>
                            <votes>0</votes>
                                    <watches>1</watches>
                                                                <comments>
                            <comment id="12563797" author="viper799" created="Wed, 30 Jan 2008 01:55:21 +0000"  >&lt;p&gt;I am talking on a theory I have about hot spots like this over at&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-64&quot; title=&quot;Add max number of mapfiles to compact at one time giveing us a minor &amp;amp; major compaction&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-64&quot;&gt;&lt;del&gt;HADOOP-2615&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Take a look and see what you thank about the idea on the New idea on compaction process I had and comment if you like it or have anything better.&lt;br/&gt;
I thank this would solve this problem if we only compacted a few newest map files at a time the splitter would check the region more often to see if it needs split and do so if needed.&lt;/p&gt;</comment>
                            <comment id="12563811" author="bryanduxbury" created="Wed, 30 Jan 2008 02:55:52 +0000"  >&lt;p&gt;I&apos;ve been taking a look at the RegionServer side of things to try and understand why a split wouldn&apos;t occur. Some code:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (e.getRegion().compactIfNeeded()) {
  splitter.splitRequested(e);
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We only queue a region to be split if it&apos;s just been compacted. I assume the rationale here is that unless a compaction occurred, there&apos;d be no reason to split in the first place. I&apos;m not convinced that&apos;s true, however. A store will only compact if it has more mapfiles than the compaction threshold, which in the case of some of my regions, wasn&apos;t the case - the individual mapfiles were 1.5GiB, but there were only 2. As a result, compaction and thus splitting was skipped. Shouldn&apos;t we be testing to see if the overall size of the mapfiles make splitting necessary, rather than letting the compaction determine whether we do anything?&lt;/p&gt;

&lt;p&gt;Perhaps we should add an optional compaction. Instead of testing HStore.needsCompaction, which only checks if it is above the compaction threshold, maybe we should also have a isCompactable, which just checks if there is more than one mapfile. The optional compacts could happen behind mandatory, threshold-based compacts. Then, we could always put an HStore on the compact queue whenever there is an event that would cause a change to the number of mapfiles, with the constraint that if the store is already on the compact queue, we don&apos;t re-add it.&lt;/p&gt;

&lt;p&gt;If we did all of that, then it would probably put us in the right state to keep the split thread doing exactly what it is doing right now, but splits will also happen in downtime.&lt;/p&gt;</comment>
                            <comment id="12563853" author="stack" created="Wed, 30 Jan 2008 06:35:27 +0000"  >&lt;p&gt;One problem I&apos;ve seen is that the compaction runs, a split is queued and needs to run because we exceed thresholds but during the compaction, we dumped a bunch of flushes &amp;#8211; so many we exceed the compaction threshold and it happens to run again before the split has had a chance to run (2712 made it so this situation does not cascade &amp;#8211; splitter will run after the second split but its not enough).  &lt;/p&gt;

&lt;p&gt;Patch that puts the compactor and splitter threads back together again so that a new compaction will not run if a split is needed.&lt;/p&gt;

&lt;p&gt;Testing, it needs more work.  Putting aside for the moment to see if &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-69&quot; title=&quot;[hbase] Make cache flush triggering less simplistic&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-69&quot;&gt;&lt;del&gt;HADOOP-2636&lt;/del&gt;&lt;/a&gt; helps with this issue.&lt;/p&gt;</comment>
                            <comment id="12563869" author="stack" created="Wed, 30 Jan 2008 07:18:59 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-69&quot; title=&quot;[hbase] Make cache flush triggering less simplistic&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-69&quot;&gt;&lt;del&gt;HADOOP-2636&lt;/del&gt;&lt;/a&gt; doesn&apos;t seem to help with this problem in particuar.&lt;/p&gt;

&lt;p&gt;Running 8 clients doing PerformanceEvaluation, what I&apos;m looking for is steady number of files to compact on each run.  &lt;/p&gt;

&lt;p&gt;Here are first four compactions before the patch:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2008-01-30 07:03:06,803 DEBUG org.apache.hadoop.hbase.HStore: started compaction of 3 files using hdfs:&lt;span class=&quot;code-comment&quot;&gt;//12.:9123/hbase123/TestTable/compaction.dir &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 530893190/info
&lt;/span&gt;2008-01-30 07:04:25,345 DEBUG org.apache.hadoop.hbase.HStore: started compaction of 3 files using hdfs:&lt;span class=&quot;code-comment&quot;&gt;//12.:9123/hbase123/TestTable/compaction.dir &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 530893190/info
&lt;/span&gt;2008-01-30 07:06:35,573 DEBUG org.apache.hadoop.hbase.HStore: started compaction of 3 files using hdfs:&lt;span class=&quot;code-comment&quot;&gt;//12.:9123/hbase123/TestTable/compaction.dir &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 530893190/info
&lt;/span&gt;2008-01-30 07:11:14,999 DEBUG org.apache.hadoop.hbase.HStore: started compaction of 9 files using hdfs:&lt;span class=&quot;code-comment&quot;&gt;//12.:9123/hbase123/TestTable/compaction.dir &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 560724365/info&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;A split ran in between the 3rd and 4th compaction.&lt;/p&gt;

&lt;p&gt;Here are first four compactions after application of patch:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2008-01-30 06:43:17,972 DEBUG org.apache.hadoop.hbase.HStore: started compaction of 4 files using hdfs:&lt;span class=&quot;code-comment&quot;&gt;//12.:9123/hbase123/TestTable/compaction.dir &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 1984834473/info
&lt;/span&gt;2008-01-30 06:44:54,734 DEBUG org.apache.hadoop.hbase.HStore: started compaction of 3 files using hdfs:&lt;span class=&quot;code-comment&quot;&gt;//12.:9123/hbase123/TestTable/compaction.dir &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 1984834473/info
&lt;/span&gt;2008-01-30 06:48:53,389 DEBUG org.apache.hadoop.hbase.HStore: started compaction of 7 files using hdfs:&lt;span class=&quot;code-comment&quot;&gt;//12.:9123/hbase123/TestTable/compaction.dir &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 712183868/info
&lt;/span&gt;2008-01-30 06:53:25,746 DEBUG org.apache.hadoop.hbase.HStore: started compaction of 9 files using hdfs:&lt;span class=&quot;code-comment&quot;&gt;//12.:9123/hbase123/TestTable/compaction.dir &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 712183868/info&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="12564429" author="stack" created="Thu, 31 Jan 2008 17:59:38 +0000"  >&lt;p&gt;Problem: We have no governor on flushes so its possible &amp;#8211; especially after all the performance improvements since 0.15.x &amp;#8211; to flush at a rate that overwhelms the rate at which we compact store files&lt;/p&gt;

&lt;p&gt;The attached patch does not solve the overrun problem.  It does its best at ameliorating the problem by making splits happen more promptly and then post split, makes it so when region is quiescent, if store files of &amp;gt; 256M &amp;#8211; even if only 1 of them &amp;#8211; we&apos;ll split.&lt;/p&gt;

&lt;p&gt;More detail on patch:&lt;/p&gt;

&lt;p&gt;+ &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-247&quot; title=&quot;[hbase] under load, regions won&amp;#39;t split&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-247&quot;&gt;&lt;del&gt;HADOOP-2712&lt;/del&gt;&lt;/a&gt; set a flag such that if a split was queued, we&apos;d stop further compactions.  Turns out that more often than not, it was possible for one last compaction to start before the split ran.  This patch puts back together the compacting and splitting thread; a check if split is needed will always follow a com&lt;br/&gt;
paction check (event-driven this was not guaranteed when the threads were distinct).  Also made it possible to split though no compaction was run (Removed the 2172 addition.  Was too subtle).&lt;br/&gt;
+ Flushes could also get in the way of a split so now flushes are blocked too when split is queued.&lt;br/&gt;
+ On open, check if region needs to be compacted (Previous this check was only done after first flush &amp;#8211; which could be 20/30s out&lt;br/&gt;
+ Made it so we split if &amp;gt; 256M, not if &amp;gt; 1.5 * 256M. Set the multiplier on flushes to be 1 instead of 2 so we flush at 64M, not 64M plus some slop.  Regularizes split and flushes.&lt;br/&gt;
+ Make it so we&apos;ll split if only one file &amp;gt; 256M and that we&apos;ll compact if only one file but it has references to parent region&lt;/p&gt;

&lt;p&gt;I tried Billy&apos;s suggestion of putting a cap on number of mapfiles to compact in the one go.  We need to do more work though before we can make use of this suggested technique because regions that hold references are not splittable: I was compacting the N oldest, then on second compactions, would do N oldest again. but the remainder could have references to parent regions and so couldn&apos;t be split.  Meantime we&apos;d accumulate flush files &amp;#8211; the region would never split and the count of flush files would overwhelm the compactor.  Need to be smarter and do as Billy suggests and pick up the small files)&lt;/p&gt;</comment>
                            <comment id="12564447" author="stack" created="Thu, 31 Jan 2008 18:31:33 +0000"  >&lt;p&gt;Load doesn&apos;t have to be extreme.&lt;/p&gt;</comment>
                            <comment id="12564490" author="stack" created="Thu, 31 Jan 2008 20:31:58 +0000"  >&lt;p&gt;v9 adds one line to TestCompaction, setting old config. so it will pass.&lt;/p&gt;</comment>
                            <comment id="12564491" author="stack" created="Thu, 31 Jan 2008 20:34:47 +0000"  >&lt;p&gt;Passes tests locally.&lt;/p&gt;

&lt;p&gt;I think this issue a blocker but won&apos;t mark it so until Bryan Duxbury test says this patch is an improvement over old behavior (and jimk said he&apos;d review).  Meantime, moving it to hudson to make sure its ok w/ him in case we end up committing. &lt;/p&gt;</comment>
                            <comment id="12564518" author="bryanduxbury" created="Thu, 31 Jan 2008 21:52:13 +0000"  >&lt;p&gt;After about 45% of 1 million 10KB rows imported, the import started to slow down markedly. I did a little DFS digging to get a sense of the size of mapfiles:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
[rapleaf@tf1 hadoop]$ bin/hadoop dfs -lsr / | grep test_table | grep &lt;span class=&quot;code-quote&quot;&gt;&quot;mapfiles/[^/]*/data&quot;&lt;/span&gt; | grep -v compaction.dir | awk &apos;{print $4}&apos; | sort -n | awk &apos;{print $1 / 1024 / 1024}&apos;
0
0.589743
21.5422
29.4829
36.4409
36.834
54.6908
56.6071
60.0075
61.7568
64
64.2137
64.2137
64.2137
64.2137
64.2137
64.2137
64.2137
64.2137
64.2137
64.3218
65.3046
68.1251
68.9211
71.2503
73.2158
73.9037
77.5301
82.1786
83.0631
83.1417
88.94
92.9497
98.2762
111.76
112.399
116.162
119.337
127.572
128.496
657.9
760.569
1261.14
1564.22
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;(If you can&apos;t read awk, that&apos;s size in megabytes of each mapfile in the DFS for my test table).&lt;/p&gt;

&lt;p&gt;There&apos;s only 7 regions, and the biggest is almost 1.5 GiB. I will report again when the job has completed and the cluster has had a chance to cool down.&lt;/p&gt;</comment>
                            <comment id="12564527" author="bryanduxbury" created="Thu, 31 Jan 2008 22:26:09 +0000"  >&lt;p&gt;I may have spoken too soon. After a bit of a slowdown around 45%, some splits burst through and the writing rate increased back to what I expected it would be.&lt;/p&gt;

&lt;p&gt;Right at the end of the job, I still have a bunch of big mapfiles:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
[rapleaf@tf1 hadoop]$ bin/hadoop dfs -lsr / | grep test_table | grep &lt;span class=&quot;code-quote&quot;&gt;&quot;mapfiles/[^/]*/data&quot;&lt;/span&gt; | grep -v compaction.dir | awk &apos;{print $4}&apos; | sort -n | awk &apos;{print $1 / 1024 / 1024}&apos;
18.6529
18.987
20.3924
25.5912
30.4755
32.5393
57.0985
60.0075
60.2728
61.7568
64.2137
64.2137
64.2137
64.2137
64.2137
64.2137
64.2137
64.2137
64.2137
64.2137
64.2137
64.2137
64.2137
64.2137
64.2137
64.2137
64.2137
64.2137
64.2137
64.2137
64.2137
64.2137
64.2137
64.2235
64.2235
64.2432
64.2432
69.8449
75.3975
76.5179
77.766
79.3581
81.8543
82.6503
83.0631
88.94
90.8564
92.5664
97.2247
101.814
104.703
105.116
110.62
113.814
127.543
128.427
128.427
128.427
128.427
128.516
353.175
367.907
471.401
474.664
575.348
657.9
906.067
921.349
1578.89
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;25 minutes after, I&apos;ve had a few more splits, getting me up to 23 regions overall, with only 40 mapfiles. Some of the files are still much larger than they should be.&lt;/p&gt;

&lt;p&gt;I definitely see this having been an improvement. I don&apos;t think it&apos;s the whole way yet.&lt;/p&gt;</comment>
                            <comment id="12564529" author="hadoopqa" created="Thu, 31 Jan 2008 22:36:23 +0000"  >&lt;p&gt;-1 overall.  Here are the results of testing the latest attachment &lt;br/&gt;
&lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12374498/split-v9.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12374498/split-v9.patch&lt;/a&gt;&lt;br/&gt;
against trunk revision 616796.&lt;/p&gt;

&lt;p&gt;    @author +1.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    javadoc +1.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    javac +1.  The applied patch does not generate any new javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    release audit +1.  The applied patch does not generate any new release audit warnings.&lt;/p&gt;

&lt;p&gt;    findbugs +1.  The patch does not introduce any new Findbugs warnings.&lt;/p&gt;

&lt;p&gt;    core tests +1.  The patch passed core unit tests.&lt;/p&gt;

&lt;p&gt;    contrib tests -1.  The patch failed contrib unit tests.&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1717/testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1717/testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1717/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1717/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html&lt;/a&gt;&lt;br/&gt;
Checkstyle results: &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1717/artifact/trunk/build/test/checkstyle-errors.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1717/artifact/trunk/build/test/checkstyle-errors.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1717/console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1717/console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="12564632" author="stack" created="Fri, 1 Feb 2008 04:30:13 +0000"  >&lt;p&gt;This version makes use of old property that used to hold the splitcompactor threads wait interval.  Using it w/ 20 seconds instead of 15.  Means we take on writes slower, but doing math (times flushes take, intervals at which they run, how long it takes a compaction to run generally, etc.), should make things more robust.&lt;/p&gt;</comment>
                            <comment id="12564644" author="stack" created="Fri, 1 Feb 2008 05:43:48 +0000"  >&lt;p&gt;Patch that passes TTMR and TTI.  Splits get cleaned up real fast now.  Changed the mutl-region maker so it will work even if the parent region has been deleted already by the time it goes looking for it.&lt;/p&gt;

&lt;p&gt;Also, chatting w/ Bryan, the numbers he pastes above were from a run that did not have split-v8.patch in place.&lt;/p&gt;</comment>
                            <comment id="12564645" author="stack" created="Fri, 1 Feb 2008 05:44:09 +0000"  >&lt;p&gt;Trying hudson again&lt;/p&gt;</comment>
                            <comment id="12564679" author="hadoopqa" created="Fri, 1 Feb 2008 08:15:40 +0000"  >&lt;p&gt;-1 overall.  Here are the results of testing the latest attachment &lt;br/&gt;
&lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12374528/split-v11.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12374528/split-v11.patch&lt;/a&gt;&lt;br/&gt;
against trunk revision 616796.&lt;/p&gt;

&lt;p&gt;    @author +1.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    javadoc +1.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    javac +1.  The applied patch does not generate any new javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    release audit +1.  The applied patch does not generate any new release audit warnings.&lt;/p&gt;

&lt;p&gt;    findbugs +1.  The patch does not introduce any new Findbugs warnings.&lt;/p&gt;

&lt;p&gt;    core tests +1.  The patch passed core unit tests.&lt;/p&gt;

&lt;p&gt;    contrib tests -1.  The patch failed contrib unit tests.&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1720/testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1720/testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1720/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1720/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html&lt;/a&gt;&lt;br/&gt;
Checkstyle results: &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1720/artifact/trunk/build/test/checkstyle-errors.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1720/artifact/trunk/build/test/checkstyle-errors.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1720/console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1720/console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="12564936" author="jimk" created="Fri, 1 Feb 2008 21:14:35 +0000"  >&lt;p&gt;Reviewed patch. +1&lt;/p&gt;</comment>
                            <comment id="12564940" author="stack" created="Fri, 1 Feb 2008 21:22:57 +0000"  >&lt;p&gt;I don&apos;t know why failed in TTI.  Tried it locally again and its fine. Enabled mapred logging so can see better why it failed.&lt;/p&gt;</comment>
                            <comment id="12564941" author="stack" created="Fri, 1 Feb 2008 21:23:41 +0000"  >&lt;p&gt;Patch also includes accomodation of jim review comments removing HConstant change and fixing bad method name.&lt;/p&gt;</comment>
                            <comment id="12564942" author="stack" created="Fri, 1 Feb 2008 21:23:57 +0000"  >&lt;p&gt;Retrying hudson to get more info on why failed.&lt;/p&gt;</comment>
                            <comment id="12564969" author="hadoopqa" created="Fri, 1 Feb 2008 23:00:14 +0000"  >&lt;p&gt;+1 overall.  Here are the results of testing the latest attachment &lt;br/&gt;
&lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12374583/split-v12.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12374583/split-v12.patch&lt;/a&gt;&lt;br/&gt;
against trunk revision 616796.&lt;/p&gt;

&lt;p&gt;    @author +1.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    javadoc +1.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    javac +1.  The applied patch does not generate any new javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    release audit +1.  The applied patch does not generate any new release audit warnings.&lt;/p&gt;

&lt;p&gt;    findbugs +1.  The patch does not introduce any new Findbugs warnings.&lt;/p&gt;

&lt;p&gt;    core tests +1.  The patch passed core unit tests.&lt;/p&gt;

&lt;p&gt;    contrib tests +1.  The patch passed contrib unit tests.&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1723/testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1723/testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1723/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1723/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html&lt;/a&gt;&lt;br/&gt;
Checkstyle results: &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1723/artifact/trunk/build/test/checkstyle-errors.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1723/artifact/trunk/build/test/checkstyle-errors.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1723/console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1723/console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="12564981" author="bryanduxbury" created="Fri, 1 Feb 2008 23:55:23 +0000"  >&lt;p&gt;I&apos;ve applied the latest patch and tested again. My import job finished with a minimum of errors. and all my mapfiles are significantly smaller. On top of that, splits are happening much more frequently - 2-4x as often I would say.&lt;/p&gt;

&lt;p&gt;There may still be other issues lurking around this area of functionality, but I would commit this issue. &lt;/p&gt;

&lt;p&gt;+1&lt;/p&gt;</comment>
                            <comment id="12564990" author="stack" created="Sat, 2 Feb 2008 00:29:08 +0000"  >&lt;p&gt;Marking as a blocker because without it, folks&apos; initial impression of hbase will be extremely negative (without this patch hbase under even moderate load struggles; if load is sustained, hbase can go unresponsive; if the loading completes, hbase cluster will have a few big regions often way in excess of the configured maximum size that will never be broken down).&lt;/p&gt;</comment>
                            <comment id="12564997" author="stack" created="Sat, 2 Feb 2008 00:43:58 +0000"  >&lt;p&gt;Committed to TRUNK and backported to 0.16.0.&lt;/p&gt;</comment>
                            <comment id="12565068" author="hudson" created="Sat, 2 Feb 2008 12:38:14 +0000"  >&lt;p&gt;Integrated in Hadoop-trunk #387 (See &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/387/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/387/&lt;/a&gt;)&lt;/p&gt;</comment>
                            <comment id="12565209" author="viper799" created="Sun, 3 Feb 2008 20:17:46 +0000"  >&lt;p&gt;A+ Job on this patch guys! This makes hbase much more stable then it was, good job&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12386311">HBASE-64</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12374526" name="split-v10.patch" size="18802" author="stack" created="Fri, 1 Feb 2008 04:28:12 +0000"/>
                            <attachment id="12374528" name="split-v11.patch" size="24603" author="stack" created="Fri, 1 Feb 2008 05:43:48 +0000"/>
                            <attachment id="12374583" name="split-v12.patch" size="25509" author="stack" created="Fri, 1 Feb 2008 21:22:57 +0000"/>
                            <attachment id="12374480" name="split-v8.patch" size="17159" author="stack" created="Thu, 31 Jan 2008 17:59:37 +0000"/>
                            <attachment id="12374498" name="split-v9.patch" size="17713" author="stack" created="Thu, 31 Jan 2008 20:31:58 +0000"/>
                            <attachment id="12374334" name="split.patch" size="12733" author="stack" created="Wed, 30 Jan 2008 06:35:27 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>6.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Wed, 30 Jan 2008 01:55:21 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>24922</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 44 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0h4qf:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>98029</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>