<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sat Dec 03 17:10:52 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-2902/HBASE-2902.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-2902] Improve our default shipping GC config. and doc -- along the way do a bit of GC myth-busting</title>
                <link>https://issues.apache.org/jira/browse/HBASE-2902</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;This issue is about improving the near-term story, working with our current lot, the slowly evolving &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/help_16.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; 1.6x JVMs and CMS (Longer-term, another issue in hbase tracks the G1 story and longer term, Todd is making a bit of traction over on the GC hotspot list).  &lt;/p&gt;

&lt;p&gt;At the moment we ship with CMS and i-CMS enabled by default.   At a minimum, i-cms does not apply on most hw hbase is deployed on &amp;#8211; i-cms is for hw w/ 2 or less processors &amp;#8211; and it seems as though we do not use multiple threads doing YG collections; i.e. -XX:UseParNewGC &quot;Use parallel threads in the new generation&quot; (Here&apos;s what I see...it seems to be off in jdk6 according to &lt;a href=&quot;http://www.md.pp.ru/~eu/jdk6options.html#UseParNewGC&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.md.pp.ru/~eu/jdk6options.html#UseParNewGC&lt;/a&gt;  but then this says its on by default when use CMS -&amp;gt; &lt;a href=&quot;http://blogs.sun.com/jonthecollector/category/Java&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://blogs.sun.com/jonthecollector/category/Java&lt;/a&gt; ... but then this says enable it &lt;a href=&quot;http://www.austinjug.org/presentations/JDK6PerfUpdate_Dec2009.pdf&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.austinjug.org/presentations/JDK6PerfUpdate_Dec2009.pdf&lt;/a&gt;.  I see this when its enabled: [Rescan (parallel) ... so it seems like its off.  Need to review the src code).&lt;/p&gt;

&lt;p&gt;We should make the above changes or at least doc them.&lt;/p&gt;

&lt;p&gt;We should consider enabling GC logging by default.  Its low cost apparently (citation below).  We&apos;d just need to do something about the log management.  Not sure you can roll them &amp;#8211; investigate &amp;#8211; and anyways we should roll on startup at least so we don&apos;t lose GC logs across restarts.&lt;/p&gt;

&lt;p&gt;We should play with initiating ratios; maybe starting CMS earlier will push out the fragmented heap that brings on the killer stop-the-world collection.&lt;/p&gt;

&lt;p&gt;I read somewhere recently that invoking System.gc will run a CMS GC if CMS is enabled.  We should investigate.  If it ran the serial collector, we could at least doc. that users could run a defragmenting stop-the-world serial collection on &apos;off&apos; times or at least make it so the stop-the-world happened when expected instead of at some random time.&lt;/p&gt;

&lt;p&gt;While here, lets do a bit of myth-busting.  Here&apos;s a few postulates:&lt;/p&gt;

&lt;p&gt;+ Keep the young generation small or at least, cap its size else it grows to occupy a large part of the heap&lt;/p&gt;

&lt;p&gt;The above is a Ryanism.  Doing the above &amp;#8211; along w/ massive heap size &amp;#8211; has put off the fragmentation that others run into at SU at least.&lt;/p&gt;

&lt;p&gt;Interestingly, this document &amp;#8211; &lt;a href=&quot;http://www.google.com/url?sa=t&amp;amp;source=web&amp;amp;cd=1&amp;amp;ved=0CBcQFjAA&amp;amp;url=http%3A%2F%2Fmediacast.sun.com%2Fusers%2FLudovic%2Fmedia%2FGCTuningPresentationFISL10.pdf&amp;amp;ei=ZPtaTOiLL5bcsAa7gsl1&amp;amp;usg=AFQjCNHP691SIIE-6NSKccM4mZtm1U6Ahw&amp;amp;sig2=2cjvcaeyn1aISL2THEENjQ&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.google.com/url?sa=t&amp;amp;source=web&amp;amp;cd=1&amp;amp;ved=0CBcQFjAA&amp;amp;url=http%3A%2F%2Fmediacast.sun.com%2Fusers%2FLudovic%2Fmedia%2FGCTuningPresentationFISL10.pdf&amp;amp;ei=ZPtaTOiLL5bcsAa7gsl1&amp;amp;usg=AFQjCNHP691SIIE-6NSKccM4mZtm1U6Ahw&amp;amp;sig2=2cjvcaeyn1aISL2THEENjQ&lt;/a&gt; &amp;#8211; would seem to recommend near the opposite in that it suggests that when using CMS, do all you can to keep stuff in the YG.  Avoid having stuff age up to the tenured heap if you can.  This would seem imply using a larger YG.&lt;/p&gt;

&lt;p&gt;Chatting w/ Ryan, the reason to keep the YG small is so we don&apos;t have long pauses doing YG collections.  According to the above citation, its not big YGs that cause long YG pauses but the copying of data (not sure if its copying of data inside the YG or if it meant copying up to tenured &amp;#8211; chatting w/ Ryan we thought there&apos;d be no difference &amp;#8211; but we should investigate)&lt;/p&gt;

&lt;p&gt;I look a look at a running upload with a small heap admittedly.  What I was seeing was that using our defaults, rare was anything in YG of age &amp;gt; 1 GC; i.e. near everything in YG was being promoted.  This may have been a symptom of my small (default) heap but we should look into this and try and ensure objects are promoted because they are old, not because there is not enough space in YG. &lt;/p&gt;

&lt;p&gt;+ We should write a slab allocator or allocate memory outside of the JVM heap&lt;/p&gt;

&lt;p&gt;Thinking on this, slab allocator, while a lot of work, I can see it helping us w/ block cache, but what if memstore is the fragmented-heap maker?  In this case, slab-allocator is only part of the fix.  It should be easy to see which is the fragmented heap maker since we can turn off the cache easy enough (though it seems like its accessed anyways even if disabled &amp;#8211; need to make sure its not doing allocations to the cache in this case)&lt;/p&gt;

&lt;p&gt;Other things while on this topic.  We need to come up w/ a loading that brings on the CMS fault that comes of a fragmented heap (CMS is non-compacting but apparently it will join together free blocks to make bigger ones so there is some anti-fragmenting behavior going on).  Apparently lots of large irregular sized items is the ticket. &lt;/p&gt;




</description>
                <environment></environment>
        <key id="12470927">HBASE-2902</key>
            <summary>Improve our default shipping GC config. and doc -- along the way do a bit of GC myth-busting</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="3">Duplicate</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="stack">stack</reporter>
                        <labels>
                    </labels>
                <created>Thu, 5 Aug 2010 18:13:56 +0000</created>
                <updated>Tue, 12 Aug 2014 19:21:23 +0000</updated>
                            <resolved>Tue, 12 Aug 2014 19:21:23 +0000</resolved>
                                                                    <component>Performance</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>2</watches>
                                                                <comments>
                            <comment id="12895799" author="tlipcon" created="Thu, 5 Aug 2010 20:56:29 +0000"  >&lt;p&gt;If you enable the CMS GC, it should automatically enable UseParNewGC for you. But I agree on getting rid of incremental mode.&lt;/p&gt;

&lt;p&gt;Tuning down the CMS initiating occupancy helps a bit, but just &quot;puts off&quot; the problem for another 24 hours of high load for me. But we should probably turn it on anyway.&lt;/p&gt;

&lt;p&gt;Big +1 on having some kind of standalone workload that uses Memstore and block cache classes to bring on this pause, especially if we can do it without actually doing any disk IO. Having such a jar to send to the JVM developers would get them to pay a lot more attention to us - a bug that they can&apos;t reproduce is too easy to ignore.&lt;/p&gt;</comment>
                            <comment id="12895812" author="stack" created="Thu, 5 Aug 2010 21:21:13 +0000"  >&lt;p&gt;.bq If you enable the CMS GC, it should automatically enable UseParNewGC for you. But I agree on getting rid of incremental mode. &lt;/p&gt;

&lt;p&gt;Yeah, should, but I seem to be saying above that adding the flag made for different output in GC logs, an explicit message that we were doing parallel collections where w/o the flag, this seemed to be missing (I&apos;m quoting an old log of mine from a while back so could be off)&lt;/p&gt;

&lt;p&gt;.bq Tuning down the CMS initiating occupancy helps a bit, but just &quot;puts off&quot; the problem for another 24 hours of high load for me. But we should probably turn it on anyway.&lt;/p&gt;

&lt;p&gt;Yeah, there is turning down the initiating occupancy and then there is that flag which says pay attention to the initating occupancy flag ONLY (these flags are crazy) because otherwise other factors may put off the CMS past whatever the initiating occupancy says.&lt;/p&gt;

&lt;p&gt;I like the idea of a jar w/ just memstore or LRU cache that can quickly bring on fragmentation..... (Let me see if I can make one as part of this issue).&lt;/p&gt;</comment>
                            <comment id="12896733" author="ryanobjc" created="Mon, 9 Aug 2010 23:47:57 +0000"  >&lt;p&gt;From the Azul guys, Fragger which is in the public domain.  A tool to induce fragmentation in a heap to trigger a full heap compaction.&lt;/p&gt;</comment>
                            <comment id="12896737" author="ryanobjc" created="Mon, 9 Aug 2010 23:56:48 +0000"  >&lt;p&gt;So to back up my small young gen, back a year ago I was working on making HBase perform, and I found young gen collections taking a huge amount of time.  I&apos;m talking at least 800ms and possibly 1-2 seconds.  I pinged a friend who said that young-gen should always be fast because it should be about the size of L3.  Hence the inception of the 6M and later the 64M parnew heaps.&lt;/p&gt;</comment>
                            <comment id="12896758" author="stack" created="Tue, 10 Aug 2010 02:22:58 +0000"  >&lt;p&gt;Sure, but see the cited slide deck above.  Its not the size of the YG but the amount of copying done (Not sure which kinda copying &amp;#8211; to be elicited).  Might be worth playing w/ bigger YG making it so stuff is aged having made it through 4 or 5 or even 10 YGGCs before it gets tenured.&lt;/p&gt;

&lt;p&gt;St.Ack&lt;/p&gt;</comment>
                            <comment id="12904678" author="stack" created="Tue, 31 Aug 2010 16:48:19 +0000"  >&lt;p&gt;Above proposal is to enable GC logging as default.  Unfortunately, we can&apos;t just yet.  While GC logging is apparently near friction-free, the lack of a rotation of live logs makes enabling untenable (Here is latest on rotating logs &lt;a href=&quot;http://mail.openjdk.java.net/pipermail/hotspot-gc-use/2010-May/000597.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://mail.openjdk.java.net/pipermail/hotspot-gc-use/2010-May/000597.html&lt;/a&gt;).&lt;/p&gt;</comment>
                            <comment id="14094545" author="apurtell" created="Tue, 12 Aug 2014 19:21:23 +0000"  >&lt;p&gt;Stale issue. Superseded by recent blockcache / bucket cache related work.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12451634" name="Fragger.java" size="25245" author="ryanobjc" created="Mon, 9 Aug 2010 23:47:57 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 5 Aug 2010 20:56:29 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>32825</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            2 years, 16 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i02fdr:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>12093</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>