<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 02 20:19:51 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-514/HBASE-514.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-514] table &apos;does not exist&apos; when it does</title>
                <link>https://issues.apache.org/jira/browse/HBASE-514</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;This one I&apos;ve seen a few times.  In hql, I do show tables and it shows my table.  I then try to do a select against the table and hql reports table does not exist.  Digging, whats happening is that the getClosest facility is failing to find the first table region in the .META. table.  I hacked up a region reading tool &amp;#8211; attached (for 0.1 branch) &amp;#8211; and tried it against but a copy and the actual instance of the region and it could do the getClosest fine.  I&apos;m pretty sure I restarted the HRS and when it came up again, the master had given it again the .META. and again was failing to find the first region in the table (Looked around in server logs and it seemed &apos;healthy&apos;).&lt;/p&gt;</description>
                <environment></environment>
        <key id="12390988">HBASE-514</key>
            <summary>table &apos;does not exist&apos; when it does</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="bryanduxbury">Bryan Duxbury</assignee>
                                    <reporter username="stack">stack</reporter>
                        <labels>
                    </labels>
                <created>Thu, 13 Mar 2008 18:21:27 +0000</created>
                <updated>Fri, 22 Aug 2008 21:17:40 +0000</updated>
                            <resolved>Tue, 18 Mar 2008 19:43:56 +0000</resolved>
                                    <version>0.16.0</version>
                                    <fixVersion>0.1.0</fixVersion>
                                    <component>Client</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>1</watches>
                                                                <comments>
                            <comment id="12578441" author="bryanduxbury" created="Thu, 13 Mar 2008 18:47:35 +0000"  >&lt;p&gt;I&apos;m not sure if this is the root or even part of the problem, but looking back at the code I wrote for getClosestRowBefore, the Memcache branch of the code isn&apos;t correct. It&apos;s pretty close, but I can envision some scenarios where it might produce bad results.&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;internalGetRowKeyAtOrBefore doesn&apos;t take into account potentially deleted cells. This probably isn&apos;t a big issue when dealing with region location stuff, but you never know.&lt;/li&gt;
	&lt;li&gt;The check at the end of the tailMap branch of code in internalGetRowKeyAtOrBefore didn&apos;t make sure that the found row was still a match to the row we&apos;re searching for, meaning that in situations when the timestamp never matched for the found row but there were following rows, if the first following row&apos;s timestamp was less than the search timestamp, you&apos;d get that row. Note that this means you&apos;d actually get the first row AFTER the row you were looking for, which is the wrong behavior.&lt;/li&gt;
	&lt;li&gt;The handling of the headMap branch of internalGetRowKeyAtOrBefore is potentially incorrect. I assumed that the last cell in the headmap would contain the row we&apos;re looking for, but again don&apos;t check for possibly deleted cells. This means you could potentially get a row key for a region that had been deleted, say after a region split gets finalized in META.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I&apos;ll whip up a patch to address these problems at least, so that we can feel a little more confident that it&apos;s doing the right thing.&lt;/p&gt;</comment>
                            <comment id="12578485" author="bryanduxbury" created="Thu, 13 Mar 2008 20:45:30 +0000"  >&lt;p&gt;One additional issue i discovered - deletes in the memcache are not accounted for in the map files, either. So a flushed edit goes to disk, it could possibly be picked up by rowAtOrBeforeFromMapFile.&lt;/p&gt;</comment>
                            <comment id="12578509" author="bryanduxbury" created="Thu, 13 Mar 2008 21:56:54 +0000"  >&lt;p&gt;I&apos;ve been able to pretty easily fix the problem in the Memcache. However, I think I&apos;ve hit something of an algorithmic snag when it comes to sorting out the closest row in the store files.&lt;/p&gt;

&lt;p&gt;The problem lies in the fact that there&apos;s no guarantee newer store files are written with data that comes after data in older store files. For instance, a very recent store file could have data written from 10 minutes ago, but a store file written 10 minutes ago could have data timestamped an hour in the future. This means that you don&apos;t just need to check every store file for what it thinks is the closest matching row. You actually have to ask every store file what it thinks and then check it against &lt;b&gt;every other store file&lt;/b&gt; to make sure it isn&apos;t deleted! Basically, figuring out the closest row with the current strategy is n^2 where n is the number of store files. &lt;/p&gt;

&lt;p&gt;Maybe there&apos;s a better strategy to investigate. Perhaps if we ask every store file what it thinks the best candidate is, then we sort the results by row, and then scan all the store files from the first candidate to the last candidate, noting any deletes as we go, and then ultimately choosing the maximum row remaining in that list. Would that actually even help?&lt;/p&gt;</comment>
                            <comment id="12578540" author="bryanduxbury" created="Thu, 13 Mar 2008 23:58:40 +0000"  >&lt;p&gt;So perhaps the most palatable solution is to clearly specify that getClosestRowBefore is only meant to be used on the .META. table, since it has very simple characteristics. &lt;/p&gt;

&lt;p&gt;In particular, writes in .META. (or &lt;del&gt;ROOT&lt;/del&gt;, used interchangeably) will always be written with the latest timestamp, meaning that older map files will never have future-dated information. Then, we can start with the memcache, then begin with the oldest map file and advance forward searching for better qualified rows and excluding rows that have been deleted in later map files. &lt;/p&gt;</comment>
                            <comment id="12578541" author="bryanduxbury" created="Fri, 14 Mar 2008 00:02:00 +0000"  >&lt;p&gt;In looking at the code, I&apos;ve found one further problem with the way its original implemented. In HStore#getRowKeyAtOrBefore, if the Memcache returns a non-null key, then we will automatically use that without looking in the mapfiles at all. This means that until there is a flush, things are fine. However, as soon as the memcache flushes once, and there are some writes into memcache, then the Memcache will return incorrect answers about the closest row. This actually seems like it could explain difficulties with WREs because when your HBase gets reasonably large, there will be a .META. region flush, and then you&apos;re hosed for a while.&lt;/p&gt;

&lt;p&gt;Instead we need to take the answer the memcache gives us and merge it with the results we get from the map files. Should be a fairly easy fix.&lt;/p&gt;</comment>
                            <comment id="12578877" author="bryanduxbury" created="Fri, 14 Mar 2008 18:11:31 +0000"  >&lt;p&gt;Here&apos;s a go at it. The logic is much more complicated, though it shouldn&apos;t be too impossible to follow. The unit tests pass locally&lt;/p&gt;</comment>
                            <comment id="12578879" author="bryanduxbury" created="Fri, 14 Mar 2008 18:12:11 +0000"  >&lt;p&gt;Please review the 0.1 patch. I will put together a trunk patch when I am convinced the 0.1 is sufficient.&lt;/p&gt;</comment>
                            <comment id="12578883" author="bryanduxbury" created="Fri, 14 Mar 2008 18:21:11 +0000"  >&lt;p&gt;Jim couldn&apos;t get the patch to apply cleanly, here&apos;s a new version.&lt;/p&gt;</comment>
                            <comment id="12578914" author="stack" created="Fri, 14 Mar 2008 20:01:14 +0000"  >&lt;p&gt;This complicated patch is missing commit message.&lt;/p&gt;

&lt;p&gt;I like the way you removed it from HRegionInterface and removed ts.  Should it be package private in HRegion?&lt;/p&gt;

&lt;p&gt;HashSet is not used in HStore&lt;/p&gt;

&lt;p&gt;Why are we not allows searching w/ &apos;key&apos;?  Why are we using the earliest key?&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
      &lt;span class=&quot;code-comment&quot;&gt;// we want the earliest possible to start searching from                                                                                                |       }                                                                                                                                                     
&lt;/span&gt;      HStoreKey search_key = candidateKeys.isEmpty() ?                                                                                                        |     }                                                                                                                                                       
        &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; HStoreKey(key) : &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; HStoreKey(candidateKeys.first().getRow());     
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Regards your strip timestamp method, creating a new HSK is safest but do you need to create a new instance?  (Could setVersion on current hsk).&lt;/p&gt;

&lt;p&gt;Do you think candidateKeys size could be large or the aggregate of the values would blow memory?  Do we have to carry the values?  Can we get away with just yes no on whether a delete value or not?&lt;/p&gt;

&lt;p&gt;Whas happening here?&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
          &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (found_key.getRow().compareTo(key) &amp;lt;= 0) {
            &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (HLogEdit.isDeleted(tailMap.get(found_key))) {
              candidateKeys.remove(stripTimestamp(found_key));
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Deletes cells only delete cells of same r/c/ts combo.  Here you are deleting any key that has same r/c without regard for ts &amp;#8211; including cells newer than the found_key.  Is that intentional? (Happens in two places)&lt;/p&gt;

&lt;p&gt;Incomplete comment: +        // empty. examine all the keys of the&lt;/p&gt;

&lt;p&gt;In this code:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
              &lt;span class=&quot;code-comment&quot;&gt;// &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; the last row we found a candidate key &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; is different than
&lt;/span&gt;              &lt;span class=&quot;code-comment&quot;&gt;// the row of the current candidate, we can stop looking.
&lt;/span&gt;              &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (lastRowFound != &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt; &amp;amp;&amp;amp; !lastRowFound.equals(thisKey.getRow())) {
                &lt;span class=&quot;code-keyword&quot;&gt;break&lt;/span&gt;;
              }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Why break, why not keeping moving forward till we get even closer to the passed in key?&lt;/p&gt;

&lt;p&gt;Remove commented out code.&lt;/p&gt;

&lt;p&gt;Is the return premature in below or is rationale that because mapfiles are sorted, we look at first one and its last key is &apos;highest&apos; for this store?&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (finalKey.getRow().compareTo(row) &amp;lt; 0) {
          candidateKeys.add(stripTimestamp(finalKey));
          &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt;;
        } 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Unit tests in branch all test.  I ran a randomWrite &amp;#8211; thinking randomWriting would exercise this patch &amp;#8211;  PE with 4 clients and it succeeded (8 clients OOME&apos;d though heap of 1.6G).  Means that you haven&apos;t broken anything it seems.&lt;/p&gt;



</comment>
                            <comment id="12578922" author="bryanduxbury" created="Fri, 14 Mar 2008 20:37:46 +0000"  >&lt;p&gt;&amp;gt; Why are we not allows searching w/ &apos;key&apos;? Why are we using the earliest key?&lt;br/&gt;
If there are any keys in the candidateKeys, then we want to start with the first one in there. If there are keys in candidateKeys, then they are guaranteed to be &amp;lt;= the search row&apos;s key. We need to start as early as possible because we also want to scan over regions where there might be deletes for candidates from earlier on. If we always just started at the search row, we wouldn&apos;t know if earlier candidate keys had been suppressed later on. &lt;/p&gt;

&lt;p&gt;&amp;gt; Regards your strip timestamp method, creating a new HSK is safest but do you need to create a new instance? (Could setVersion on current hsk).&lt;br/&gt;
I have no problem changing this method. Figuring cut down on object creations?&lt;/p&gt;

&lt;p&gt;&amp;gt; Do you think candidateKeys size could be large or the aggregate of the values would blow memory? Do we have to carry the values? Can we get away with just yes no on whether a delete value or not?&lt;br/&gt;
We do not carry the values, just the keys. If the keys happen to be very large, then there is the possibility of using up lots of memory. Moreover, the idea is that when we encounter a delete, we&apos;ll use that event to remove the key that corresponds to it from the set of candidate keys. The candidate keys set should grow and shrink as we find and entertain more candidate keys. &lt;/p&gt;

&lt;p&gt;&amp;gt; In this code: ...&lt;br/&gt;
Actually, in this case, we&apos;re iterating through the map backwards, so once we&apos;ve found one row that survives the whole way, then we&apos;ve found the closest row already, and should stop looking.&lt;/p&gt;

&lt;p&gt;&amp;gt; Is the return premature in below or is rationale that because mapfiles are sorted, we look at first one and its last key is &apos;highest&apos; for this store?&lt;br/&gt;
Yes.&lt;/p&gt;</comment>
                            <comment id="12578950" author="jimk" created="Fri, 14 Mar 2008 22:25:22 +0000"  >&lt;p&gt;I found instances where we delete cells using TIMESTAMP_LATEST, hence&lt;br/&gt;
stripping timestamps is absolutely  the right thing to do.&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;HMaster$ChangeTableState.postProcessMeta&lt;/li&gt;
	&lt;li&gt;HRegion.offlineRegionInMETA&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The following should call deleteAll:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;HMerge${OfflineMerger,OnlineMerger).updateMeta&lt;/li&gt;
	&lt;li&gt;DisabledTestScanner2.removeRegionFromMETA&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;HStore:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public method HStore$Memcache.getRowKeyAtOrBefore is missing javadoc for parameters and exceptions&lt;/li&gt;
	&lt;li&gt;stripTimestamp should be package private and not private because it causes synthetic accessors to be generate so Memcache can access it. Will also make port to trunk easier&lt;/li&gt;
	&lt;li&gt;public method HStore.getRowKeyAtOrBefore is missing javadoc for parameters, return value and exceptions thrown, also remove commented code.&lt;/li&gt;
	&lt;li&gt;unnecessary else at line 1869&lt;/li&gt;
	&lt;li&gt;comments should include examples to show why scanning from oldest to newest is important. We&apos;ll all have forgotten why 6 months from now.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;&amp;gt; stack - 14/Mar/08 01:01 PM&lt;br/&gt;
&amp;gt; This complicated patch is missing commit message.&lt;br/&gt;
+1&lt;/p&gt;

&lt;p&gt;&amp;gt; HashSet is not used in HStore&lt;/p&gt;

&lt;p&gt;Neither is Set&lt;/p&gt;

&lt;p&gt;&amp;gt; Regards your strip timestamp method, creating a new HSK is safest&lt;br/&gt;
&amp;gt; but do you need to create a new instance? (Could setVersion on current&lt;br/&gt;
&amp;gt; hsk). &lt;/p&gt;

&lt;p&gt;If you did setVersion on current HSK, then you would be invalidating&lt;br/&gt;
the map that it was being read from (memcache or snapshot)&lt;/p&gt;
</comment>
                            <comment id="12578982" author="bryanduxbury" created="Sat, 15 Mar 2008 00:43:45 +0000"  >&lt;p&gt;Here&apos;s another patch. It incorporates a lot of the fixes suggested by Jim and Stack.&lt;/p&gt;

&lt;p&gt;Before it gets committed, I will definitely take the time to write up examples and rationale.&lt;/p&gt;

&lt;p&gt;Review please.&lt;/p&gt;</comment>
                            <comment id="12580006" author="stack" created="Tue, 18 Mar 2008 19:02:24 +0000"  >&lt;p&gt;+1&lt;/p&gt;

&lt;p&gt;Reviewed till my head hurt and then went and ran load test.  Completed w/o error (no empty cell stragglers in master scans of meta regions).  Unit tests also passed.&lt;/p&gt;</comment>
                            <comment id="12580028" author="bryanduxbury" created="Tue, 18 Mar 2008 19:43:56 +0000"  >&lt;p&gt;I just committed this to 0.1 and cloned the issue for trunk.&lt;/p&gt;</comment>
                            <comment id="12600724" author="pratyushb" created="Thu, 29 May 2008 10:09:16 +0000"  >&lt;p&gt;I have been using Hbase for some time now. We have set up a web-crawler and we are downloading the data into two tables in Hbase. I am using Hbase 0.1.1 for our purpose.&lt;/p&gt;

&lt;p&gt;There is a particular table web_content having the following structure...&lt;/p&gt;

&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; Column Family Descriptor                                                                                &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; name: content, max versions: 1, compression: BLOCK, in memory: false, max length: 2147483647, &lt;br/&gt;
bloom filter: none                                                                 &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; name: content_length, max versions: 1, compression: BLOCK, in memory: false,max length: 32, &lt;br/&gt;
bloom filter: none                                                                    &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; name: content_type, max versions: 1, compression: BLOCK, in memory: false, max length: 100, &lt;br/&gt;
bloom filter: none                                                                       &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; name: crawl_date, max versions: 1, compression: BLOCK, in memory: false, max length: 1000,&lt;br/&gt;
bloom filter: none                                                                         &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; name: http_headers, max versions: 1, compression: BLOCK, in memory: false, max length: 10000,&lt;br/&gt;
bloom filter: none                                                                    &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; name: last_modified_date, max versions: 1, compression: BLOCK, in memory: false, max length: 100,&lt;br/&gt;
bloom filter: none                                                               &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; name: outlinks_count, max versions: 1, compression: BLOCK, in memory: false, max length: 100,&lt;br/&gt;
bloom filter: none                                                                    &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; name: parsed_text, max versions: 1, compression: BLOCK, in memory: false, max length: 2147483647,&lt;br/&gt;
bloom filter: none                                                              &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; name: title, max versions: 1, compression: BLOCK, in memory: false, max length: 1000&lt;br/&gt;
, bloom filter: none                                                                                 &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;We are using Heritrix-2.0.1 as our crawling engine and we have created a Hbase writer which writes the contents of the downloaded pages into the above table.&lt;br/&gt;
Initially the crawler runs fine and i often query the web_conten table with &quot;Select count&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/star_yellow.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; from web_content&quot; to get the rate at which URLs are written in the table. However wehn the crawler runs for hours, and we have nearly 40K-50K urls in the table, the table suddenly seems to dissappear. Querying with the above query returns &quot;web_content&quot; is an non-existant table.&lt;/p&gt;

&lt;p&gt;This has occured multiple times to me, and i found out that there was already a JIRA(&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-514&quot; title=&quot;table &amp;#39;does not exist&amp;#39; when it does&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-514&quot;&gt;&lt;del&gt;HBASE-514&lt;/del&gt;&lt;/a&gt;) on this.  It says that this issue has been fixed in version 0.1.0. We are using 0.1.1 which is a later version but the problem seems to exist.&lt;/p&gt;

&lt;p&gt;we have a small cluster of 6-10 machines wherein we are running hadoop-0.16.3 and hbase 0.1.1. One machine constitutes the NameNode, SecondaryNameNode, HMaster. While other machines form regionservers and datanodes.&lt;br/&gt;
I had run the system with DEBUG enabled and i am attaching the log files for help.&lt;/p&gt;

&lt;p&gt;this is a screenshot of the error thati am facing...&lt;/p&gt;

&lt;p&gt;hql &amp;gt; select count&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/star_yellow.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; from web_content;&lt;br/&gt;
08/05/29 09:38:13 INFO hbase.HTable: Creating scanner over web_content starting at key&lt;br/&gt;
08/05/29 09:38:13 DEBUG hbase.HTable: Advancing internal scanner to startKey&lt;br/&gt;
08/05/29 09:38:13 DEBUG hbase.HTable: New region: address: 10.178.87.27:60020, regioninfo: regionname: web_content,,1212056557578, startKey: &amp;lt;&amp;gt;, endKey: &amp;lt;&amp;gt;, encodedName: 1099787575, tableDesc: {name: web_content, families: {content:=&lt;/p&gt;
{name: content, max versions: 1, compression: BLOCK, in memory: false, max length: 2147483647, bloom filter: none}
&lt;p&gt;, content_length:=&lt;/p&gt;
{name: content_length, max versions: 1, compression: BLOCK, in memory: false, max length: 32, bloom filter: none}
&lt;p&gt;, content_type:=&lt;/p&gt;
{name: content_type, max versions: 1, compression: BLOCK, in memory: false, max length: 100, bloom filter: none}
&lt;p&gt;, crawl_date:=&lt;/p&gt;
{name: crawl_date, max versions: 1, compression: BLOCK, in memory: false, max length: 1000, bloom filter: none}
&lt;p&gt;, http_headers:=&lt;/p&gt;
{name: http_headers, max versions: 1, compression: BLOCK, in memory: false, max length: 10000, bloom filter: none}
&lt;p&gt;, last_modified_date:=&lt;/p&gt;
{name: last_modified_date, max versions: 1, compression: BLOCK, in memory: false, max length: 100, bloom filter: none}
&lt;p&gt;, outlinks_count:=&lt;/p&gt;
{name: outlinks_count, max versions: 1, compression: BLOCK, in memory: false, max length: 100, bloom filter: none}
&lt;p&gt;, parsed_text:=&lt;/p&gt;
{name: parsed_text, max versions: 1, compression: BLOCK, in memory: false, max length: 2147483647, bloom filter: none}
&lt;p&gt;, title:=&lt;/p&gt;
{name: title, max versions: 1, compression: BLOCK, in memory: false, max length: 1000, bloom filter: none}
&lt;p&gt;}}&lt;br/&gt;
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hbase.UnknownScannerException: Name: -7311630080500504399&lt;br/&gt;
        at org.apache.hadoop.hbase.HRegionServer.next(HRegionServer.java:1425)&lt;br/&gt;
        at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)&lt;br/&gt;
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)&lt;br/&gt;
        at java.lang.reflect.Method.invoke(Method.java:597)&lt;br/&gt;
        at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:413)&lt;br/&gt;
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:901)&lt;/p&gt;

&lt;p&gt;        at org.apache.hadoop.ipc.Client.call(Client.java:512)&lt;br/&gt;
        at org.apache.hadoop.hbase.ipc.HbaseRPC$Invoker.invoke(HbaseRPC.java:210)&lt;br/&gt;
        at $Proxy1.next(Unknown Source)&lt;br/&gt;
        at org.apache.hadoop.hbase.HTable$ClientScanner.next(HTable.java:914)&lt;br/&gt;
        at org.apache.hadoop.hbase.hql.SelectCommand.scanPrint(SelectCommand.java:233)&lt;br/&gt;
        at org.apache.hadoop.hbase.hql.SelectCommand.execute(SelectCommand.java:100)&lt;br/&gt;
        at org.apache.hadoop.hbase.hql.HQLClient.executeQuery(HQLClient.java:50)&lt;br/&gt;
        at org.apache.hadoop.hbase.Shell.main(Shell.java:114)&lt;br/&gt;
8797 row(s) in set. (140.71 sec)&lt;br/&gt;
hql &amp;gt; select count&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/star_yellow.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; from web_content;&lt;br/&gt;
08/05/29 09:45:34 INFO hbase.HTable: Creating scanner over web_content starting at key&lt;br/&gt;
08/05/29 09:45:34 DEBUG hbase.HTable: Advancing internal scanner to startKey&lt;br/&gt;
08/05/29 09:45:34 DEBUG hbase.HTable: New region: address: 10.178.87.27:60020, regioninfo: regionname: web_content,,1212056557578, startKey: &amp;lt;&amp;gt;, endKey: &amp;lt;&amp;gt;, encodedName: 1099787575, tableDesc: {name: web_content, families: {content:=&lt;/p&gt;
{name: content, max versions: 1, compression: BLOCK, in memory: false, max length: 2147483647, bloom filter: none}
&lt;p&gt;, content_length:=&lt;/p&gt;
{name: content_length, max versions: 1, compression: BLOCK, in memory: false, max length: 32, bloom filter: none}
&lt;p&gt;, content_type:=&lt;/p&gt;
{name: content_type, max versions: 1, compression: BLOCK, in memory: false, max length: 100, bloom filter: none}
&lt;p&gt;, crawl_date:=&lt;/p&gt;
{name: crawl_date, max versions: 1, compression: BLOCK, in memory: false, max length: 1000, bloom filter: none}
&lt;p&gt;, http_headers:=&lt;/p&gt;
{name: http_headers, max versions: 1, compression: BLOCK, in memory: false, max length: 10000, bloom filter: none}
&lt;p&gt;, last_modified_date:=&lt;/p&gt;
{name: last_modified_date, max versions: 1, compression: BLOCK, in memory: false, max length: 100, bloom filter: none}
&lt;p&gt;, outlinks_count:=&lt;/p&gt;
{name: outlinks_count, max versions: 1, compression: BLOCK, in memory: false, max length: 100, bloom filter: none}
&lt;p&gt;, parsed_text:=&lt;/p&gt;
{name: parsed_text, max versions: 1, compression: BLOCK, in memory: false, max length: 2147483647, bloom filter: none}
&lt;p&gt;, title:=&lt;/p&gt;
{name: title, max versions: 1, compression: BLOCK, in memory: false, max length: 1000, bloom filter: none}
&lt;p&gt;}}&lt;br/&gt;
08/05/29 09:45:44 DEBUG hbase.HTable: reloading table servers because: org.apache.hadoop.hbase.NotServingRegionException: web_content,,1212056557578&lt;br/&gt;
        at org.apache.hadoop.hbase.HRegionServer.getRegion(HRegionServer.java:1639)&lt;br/&gt;
        at org.apache.hadoop.hbase.HRegionServer.getRegion(HRegionServer.java:1611)&lt;br/&gt;
        at org.apache.hadoop.hbase.HRegionServer.openScanner(HRegionServer.java:1480)&lt;br/&gt;
        at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)&lt;br/&gt;
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)&lt;br/&gt;
        at java.lang.reflect.Method.invoke(Method.java:597)&lt;br/&gt;
        at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:413)&lt;br/&gt;
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:901)&lt;/p&gt;

&lt;p&gt;08/05/29 09:45:44 DEBUG hbase.HConnectionManager$TableServers: reloading table servers because: region offline: web_content,,1212056557578&lt;br/&gt;
08/05/29 09:45:54 DEBUG hbase.HConnectionManager$TableServers: reloading table servers because: HRegionInfo was null or empty in .META.&lt;br/&gt;
08/05/29 09:46:04 DEBUG hbase.HConnectionManager$TableServers: reloading table servers because: HRegionInfo was null or empty in .META.&lt;br/&gt;
org.apache.hadoop.hbase.TableNotFoundException: Table &apos;web_content&apos; does not exist.&lt;br/&gt;
        at org.apache.hadoop.hbase.HConnectionManager$TableServers.locateRegionInMeta(HConnectionManager.java:418)&lt;br/&gt;
        at org.apache.hadoop.hbase.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:350)&lt;br/&gt;
        at org.apache.hadoop.hbase.HConnectionManager$TableServers.relocateRegion(HConnectionManager.java:318)&lt;br/&gt;
        at org.apache.hadoop.hbase.HTable.getRegionLocation(HTable.java:114)&lt;br/&gt;
        at org.apache.hadoop.hbase.HTable$ClientScanner.nextScanner(HTable.java:889)&lt;br/&gt;
        at org.apache.hadoop.hbase.HTable$ClientScanner.&amp;lt;init&amp;gt;(HTable.java:817)&lt;br/&gt;
        at org.apache.hadoop.hbase.HTable.obtainScanner(HTable.java:522)&lt;br/&gt;
        at org.apache.hadoop.hbase.HTable.obtainScanner(HTable.java:411)&lt;br/&gt;
        at org.apache.hadoop.hbase.hql.SelectCommand.scanPrint(SelectCommand.java:219)&lt;br/&gt;
        at org.apache.hadoop.hbase.hql.SelectCommand.execute(SelectCommand.java:100)&lt;br/&gt;
        at org.apache.hadoop.hbase.hql.HQLClient.executeQuery(HQLClient.java:50)&lt;br/&gt;
        at org.apache.hadoop.hbase.Shell.main(Shell.java:114)&lt;br/&gt;
0 row(s) in set. (40.37 sec)&lt;/p&gt;

&lt;p&gt;Interestingly after this if I query the Hbase using show tables, it still shows me two tables.&lt;/p&gt;

&lt;p&gt;If anybody can tell me what exactly is going wrong or what is the intended fix, it will be of great help.&lt;/p&gt;

&lt;p&gt;thanks&lt;/p&gt;

&lt;p&gt;Pratyush&lt;/p&gt;</comment>
                            <comment id="12600838" author="stack" created="Thu, 29 May 2008 16:59:01 +0000"  >&lt;p&gt;Would suggest you update to 0.1.2.  Lots of critical fixes including fixes for what you are seeing above, I&apos;d guess.  No migration needed going between 0.1.1 and 0.1.2 so upgrade is just matter of replacing the software.&lt;/p&gt;

&lt;p&gt;Regards your schema, why have so many families?  Would suggest you put the content into one family and then all other attributes into another family.&lt;/p&gt;

&lt;p&gt;Good to hear you have a writer for Heritrix (Is it based off the work by Ankur Goel &amp;#8211; and the code I passed him)?&lt;/p&gt;

&lt;p&gt;FYI, rather than stick your question into a closed issue, you might do better next time posting to the list.&lt;/p&gt;






</comment>
                            <comment id="12601021" author="pratyushb" created="Fri, 30 May 2008 06:11:46 +0000"  >
&lt;p&gt; Dear Stack,&lt;/p&gt;

&lt;p&gt;Sorry about the inconvenience caused. Actually putting the log data in the mail made my office server detect it as a spam and hence i was unable to send a mail to the group.? &lt;/p&gt;

&lt;p&gt;Thanks for your suggestions. I will definitely upgrade to 0.1.2. Your suggestions regrading the schema is also very helpful, we would look into it to make necessary adjustments. &lt;/p&gt;

&lt;p&gt;Yes this is indeed the code that was worked upon by Ankur Goel, he happens to be my teammate !!! &lt;/p&gt;

&lt;p&gt;thanks and regards&lt;/p&gt;

&lt;p&gt;Pratyush&lt;/p&gt;






</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10020">
                    <name>Cloners</name>
                                                                <inwardlinks description="is cloned by">
                                        <issuelink>
            <issuekey id="12391779">HBASE-528</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12377933" name="514-0.1-v2.patch" size="28923" author="bryanduxbury" created="Fri, 14 Mar 2008 18:21:10 +0000"/>
                            <attachment id="12377957" name="514-0.1-v3.patch" size="30831" author="bryanduxbury" created="Sat, 15 Mar 2008 00:43:45 +0000"/>
                            <attachment id="12377932" name="514-0.1.patch" size="28923" author="bryanduxbury" created="Fri, 14 Mar 2008 18:11:31 +0000"/>
                            <attachment id="12377818" name="region-v2.patch" size="5019" author="stack" created="Thu, 13 Mar 2008 18:21:44 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>4.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 13 Mar 2008 18:47:35 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>25241</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 28 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0h7rj:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>98520</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>