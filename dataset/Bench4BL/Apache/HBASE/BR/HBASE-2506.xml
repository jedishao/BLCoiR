<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sat Dec 03 16:24:16 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-2506/HBASE-2506.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-2506] Too easy to OOME a RS</title>
                <link>https://issues.apache.org/jira/browse/HBASE-2506</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Testing a cluster with 1GB heap, I found that we are letting the region servers kill themselves too easily when scanning using pre-fetching. To reproduce, get 10-20M rows using PE and run a count in the shell using CACHE =&amp;gt; 30000 or any other very high number. For good measure, here&apos;s the stack trace:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2010-04-30 13:20:23,241 FATAL org.apache.hadoop.hbase.regionserver.HRegionServer: OutOfMemoryError, aborting.
java.lang.OutOfMemoryError: Java heap space
        at java.util.Arrays.copyOf(Arrays.java:2786)
        at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:94)
        at java.io.DataOutputStream.write(DataOutputStream.java:90)
        at org.apache.hadoop.hbase.client.Result.writeArray(Result.java:478)
        at org.apache.hadoop.hbase.io.HbaseObjectWritable.writeObject(HbaseObjectWritable.java:312)
        at org.apache.hadoop.hbase.io.HbaseObjectWritable.write(HbaseObjectWritable.java:229)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:941)
2010-04-30 13:20:23,241 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Dump of metrics: request=0.0, regions=29, stores=29, storefiles=44, storefileIndexSize=6, memstoreSize=255,
 compactionQueueSize=0, usedHeap=926, maxHeap=987, blockCacheSize=1700064, blockCacheFree=205393696, blockCacheCount=0, blockCacheHitRatio=0
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I guess the same could happen with largish write buffers. We need something better than OOME.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12463460">HBASE-2506</key>
            <summary>Too easy to OOME a RS</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="3">Duplicate</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="jdcryans">Jean-Daniel Cryans</reporter>
                        <labels>
                            <label>moved_from_0_20_5</label>
                    </labels>
                <created>Fri, 30 Apr 2010 20:45:33 +0000</created>
                <updated>Mon, 9 Feb 2015 06:08:16 +0000</updated>
                            <resolved>Mon, 9 Feb 2015 06:08:16 +0000</resolved>
                                                                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                                                                                            <comments>
                            <comment id="12862867" author="jdcryans" created="Fri, 30 Apr 2010 21:11:05 +0000"  >&lt;p&gt;Well it seems that for scans we already have everything in place:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  /**
   * Maximum number of bytes returned when calling a scanner&apos;s next method.
   * Note that when a single row is larger than &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; limit the row is still
   * returned completely.
   * 
   * The &lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt; value is unlimited.
   */
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; DEFAULT_HBASE_CLIENT_SCANNER_MAX_RESULT_SIZE = &lt;span class=&quot;code-object&quot;&gt;Long&lt;/span&gt;.MAX_VALUE;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Instead of keeping it to that default, we could just set it to a reasonable value like 64MB.&lt;/p&gt;</comment>
                            <comment id="12862890" author="jdcryans" created="Fri, 30 Apr 2010 21:48:43 +0000"  >&lt;p&gt;64MB still OOMEs, but with 32MB I&apos;m able to scan a 16M rows table whatever the CACHE value I&apos;m using.&lt;/p&gt;</comment>
                            <comment id="12862916" author="stack" created="Fri, 30 Apr 2010 22:38:24 +0000"  >&lt;p&gt;+1 but make it smaller than 64M to allow for concurrent scans.&lt;/p&gt;</comment>
                            <comment id="12866797" author="stack" created="Wed, 12 May 2010 23:48:09 +0000"  >&lt;p&gt;Bulk move of 0.20.5 issues into 0.21.0 after vote that we merge branch into TRUNK up on list.&lt;/p&gt;</comment>
                            <comment id="12918204" author="streamy" created="Tue, 5 Oct 2010 21:21:21 +0000"  >&lt;p&gt;We going to get this in for 0.90?&lt;/p&gt;</comment>
                            <comment id="12918206" author="streamy" created="Tue, 5 Oct 2010 21:22:12 +0000"  >&lt;p&gt;If we&apos;re going to make a low default, we should make a note in default scanner caching config and scanner cache javadoc method about the interplay with this parameter.&lt;/p&gt;</comment>
                            <comment id="12923229" author="stack" created="Wed, 20 Oct 2010 23:26:20 +0000"  >&lt;p&gt;Moving out for now.  A nice-to-have.  Users just need to keep their write buffers reasonably sized for now.&lt;/p&gt;</comment>
                            <comment id="12964485" author="apurtell" created="Sun, 28 Nov 2010 02:02:29 +0000"  >&lt;p&gt;Up on dev@hbase, Ted Yu said:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I am wondering if HBase can utilize the following to  balance load across region servers for reduced GC pause: &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://download.oracle.com/javase/1.5.0/docs/api/java/lang/management/MemoryPoolMXBean.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://download.oracle.com/javase/1.5.0/docs/api/java/lang/management/MemoryPoolMXBean.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;especially Usage Threshold Notifications.&lt;/p&gt;&lt;/blockquote&gt;


&lt;p&gt;I&apos;ve also been thinking the regionservers should take some action &amp;#8211; exactly what surely will be subject to debate &amp;#8211; to protect themselves should they enter a low memory condition, such as one or more of:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Defer compactions&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Defer splits&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Slow down writers&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Randomly choose a region to abandon&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;In a nasty test case of mine I observe cascading failure where enough really aggressive writers can cause a regionserver to OOME, (4GB or 8GB heaps, all config settings set to defaults), then after regions are redeployed cause the next victim to OOME, and on and on until all regionservers are dead. OOMEs typically happen during splits or compactions. This happens up on EC2 so I think the relatively slow I/O causes HBase, eventually, to overcommit heap.&lt;/p&gt;

&lt;p&gt;In general, there is a lack of a back-pressure mechanism to slow down writers that are overcommitting the system.&lt;/p&gt;</comment>
                            <comment id="12964486" author="apurtell" created="Sun, 28 Nov 2010 02:05:10 +0000"  >&lt;p&gt;Raising priority to blocker. I think we need some sort of answer for this for 0.92 other than, &quot;users: control your writers&quot;. Currently a regionserver gives its clients no indication it may be about to overcommit heap.&lt;/p&gt;</comment>
                            <comment id="12964493" author="yuzhihong@gmail.com" created="Sun, 28 Nov 2010 03:28:46 +0000"  >&lt;p&gt;The original intention of my suggestion was to avoid region server over-sleeping due to long GC pause.&lt;br/&gt;
From our staging cluster, I found there was a threshold for number of mappers per node (we run map/reduce along side region servers) above which some region server(s) would sleep due to long GC pause.&lt;br/&gt;
I think the load balancer should do the following in order to alleviate GC pause:&lt;br/&gt;
. keep track of most frequently accessed regions so that they can spread/move to more region servers&lt;br/&gt;
. consider memory pressure when making region move decision&lt;/p&gt;</comment>
                            <comment id="12964499" author="yuzhihong@gmail.com" created="Sun, 28 Nov 2010 06:06:17 +0000"  >&lt;p&gt;Currently load balancer defines server load by adding number of regions.&lt;/p&gt;

&lt;p&gt;Another definition of server load can be the summation of number of (write) accesses in the last N minutes for each region hosted by the server. HRegionInfo can use a rolling buffer of length N to record the number of accesses in each minute.&lt;br/&gt;
Under this definition, load balancer gives low weight to regions that are rarely accessed in the near past and tries to balance &apos;hot&apos; regions.&lt;/p&gt;</comment>
                            <comment id="12964539" author="yuzhihong@gmail.com" created="Sun, 28 Nov 2010 11:47:58 +0000"  >&lt;p&gt;Andrew mentioned deferring splits as one of the reaction to low memory condition.&lt;br/&gt;
One scenario for performing split is that there&apos;re few regions heavily written to and they all reside on the same region server. In this case we can move them to less loaded servers and split them to distribute load.&lt;br/&gt;
If the load balancer is aware of server load in terms of number of accesses, this scenario could at least be delayed.&lt;/p&gt;</comment>
                            <comment id="13002282" author="jdcryans" created="Thu, 3 Mar 2011 22:26:31 +0000"  >&lt;p&gt;I created this jira a long time ago but it&apos;s still very true today. I tried doing an Export of a table with very fat rows, I started with 2 maps per machine on 17 machines and scanner caching of 100, OOME on half the servers (8GB of heap btw). Scanner caching set to 10, still OOME. 1 map per machine, still OOME. I even restarted after the second job thinking that clearing the memstores would help but it didn&apos;t.&lt;/p&gt;

&lt;p&gt;They usually die with this:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;

java.lang.OutOfMemoryError: Java heap space
	at java.util.Arrays.copyOf(Arrays.java:2786)
	at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:94)
	at java.io.DataOutputStream.write(DataOutputStream.java:90)
	at org.apache.hadoop.hbase.client.Result.writeArray(Result.java:478)
	at org.apache.hadoop.hbase.io.HbaseObjectWritable.writeObject(HbaseObjectWritable.java:314)
	at org.apache.hadoop.hbase.io.HbaseObjectWritable.write(HbaseObjectWritable.java:231)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1049)

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13002294" author="jdcryans" created="Thu, 3 Mar 2011 22:51:12 +0000"  >&lt;p&gt;Ah turns out the scanner caching value I was thinking I was using was actually picked up from somewhere else (10k instead of 10). Still, it really was easy.&lt;/p&gt;</comment>
                            <comment id="13002297" author="ryanobjc" created="Thu, 3 Mar 2011 22:57:37 +0000"  >&lt;p&gt;we could catch the oom in this case and instead return an error to the&lt;br/&gt;
client.  if you are unable to allocate a 500MB buffer to send a rpc&lt;br/&gt;
response it might not actually need to kill the RS, because if we are&lt;br/&gt;
truly out of memory different threads will catch that.  So catch that&lt;br/&gt;
OOM then send an exception response instead.&lt;/p&gt;

&lt;p&gt;Does that sound good?&lt;/p&gt;</comment>
                            <comment id="13002376" author="apurtell" created="Fri, 4 Mar 2011 00:56:04 +0000"  >&lt;blockquote&gt;&lt;p&gt;if you are unable to allocate a 500MB buffer to send a rpc response it might not actually need to kill the RS, because if we are truly out of memory different threads will catch that&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This sounds reasonable to me.&lt;br/&gt;
And/or throw a ResultTooLargeException or similar if the BAOS size exceeds some reasonable value, with exception text that points to doc about intra-row scanning?&lt;/p&gt;</comment>
                            <comment id="13002378" author="ryanobjc" created="Fri, 4 Mar 2011 00:58:36 +0000"  >&lt;p&gt;We have warning logs for large results, but the issue here is what&lt;br/&gt;
happens when the malloc fails?  We can catch and construct an&lt;br/&gt;
exceptional response.&lt;/p&gt;</comment>
                            <comment id="13052811" author="jdcryans" created="Tue, 21 Jun 2011 20:33:14 +0000"  >&lt;p&gt;Bumping to 0.94, this jira is more like a wish rather than an actual bug with a specific plan.&lt;/p&gt;</comment>
                            <comment id="13234513" author="lhofhansl" created="Wed, 21 Mar 2012 17:07:27 +0000"  >&lt;p&gt;Rebumping to 0.96&lt;/p&gt;</comment>
                            <comment id="13480222" author="yuzhihong@gmail.com" created="Fri, 19 Oct 2012 18:31:18 +0000"  >&lt;p&gt;I read through this JIRA again.&lt;br/&gt;
Looking at HBaseServer.java, we already have code such as the following:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
        } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (OutOfMemoryError e) {
          &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (errorHandler != &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) {
            &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (errorHandler.checkOOME(e)) {
              LOG.info(getName() + &lt;span class=&quot;code-quote&quot;&gt;&quot;: exiting on OOME&quot;&lt;/span&gt;);
              &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt;;
            }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Meaning IPC has some way to clean up itself.&lt;br/&gt;
My suggestion above about considering accesses to region in server load has been implemented by stochastic load balancer.&lt;br/&gt;
I feel we can lower the priority of this JIRA.&lt;/p&gt;</comment>
                            <comment id="13586195" author="stack" created="Mon, 25 Feb 2013 19:40:47 +0000"  >&lt;p&gt;Lowering priority.  Not being worked on.&lt;/p&gt;</comment>
                            <comment id="14311841" author="lhofhansl" created="Mon, 9 Feb 2015 06:08:16 +0000"  >&lt;p&gt;Done in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-12976&quot; title=&quot;Set default value for hbase.client.scanner.max.result.size&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-12976&quot;&gt;&lt;del&gt;HBASE-12976&lt;/del&gt;&lt;/a&gt; (and continued in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11544&quot; title=&quot;[Ergonomics] hbase.client.scanner.caching is dogged and will try to return batch even if it means OOME&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11544&quot;&gt;&lt;del&gt;HBASE-11544&lt;/del&gt;&lt;/a&gt;)&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12606451">HBASE-6728</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12612851">HBASE-7023</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                            <subtask id="12624406">HBASE-7379</subtask>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Fri, 30 Apr 2010 22:38:24 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>26340</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            1 year, 42 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i02cu7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>11681</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>