<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 02 19:05:38 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-1039/HBASE-1039.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-1039] Compaction fails if bloomfilters are enabled</title>
                <link>https://issues.apache.org/jira/browse/HBASE-1039</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;From Thibaut up on the list.&lt;/p&gt;

&lt;p&gt;As soon as hbase tries to compact the table, the following exception appears in the logfile: (Other compactations also work fine without any errors)&lt;/p&gt;

&lt;p&gt;2008-11-30 00:55:57,769 ERROR&lt;br/&gt;
org.apache.hadoop.hbase.regionserver.CompactSplitThread: Compaction failed for region mytable,,1228002541526&lt;br/&gt;
java.lang.IllegalArgumentException: maxValue must be &amp;gt; 0&lt;br/&gt;
    at org.onelab.filter.HashFunction.&amp;lt;init&amp;gt;(HashFunction.java:84)&lt;br/&gt;
    at org.onelab.filter.Filter.&amp;lt;init&amp;gt;(Filter.java:97)&lt;br/&gt;
    at org.onelab.filter.BloomFilter.&amp;lt;init&amp;gt;(BloomFilter.java:102)&lt;br/&gt;
    at org.apache.hadoop.hbase.regionserver.HStoreFile$BloomFilterMapFile$Writer.&amp;lt;init&amp;gt;(HStoreFile.java:829)&lt;br/&gt;
    at org.apache.hadoop.hbase.regionserver.HStoreFile.getWriter(HStoreFile.java:436)&lt;br/&gt;
    at org.apache.hadoop.hbase.regionserver.HStore.compact(HStore.java:889)&lt;br/&gt;
    at org.apache.hadoop.hbase.regionserver.HRegion.compactStores(HRegion.java:902)&lt;br/&gt;
    at org.apache.hadoop.hbase.regionserver.HRegion.compactStores(HRegion.java:860)&lt;br/&gt;
    at org.apache.hadoop.hbase.regionserver.CompactSplitThread.run(CompactSplitThread.java:83)&lt;/p&gt;

&lt;p&gt;Because the region cannot compact and/or split, it is soon dead after (re)assignment.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12409512">HBASE-1039</key>
            <summary>Compaction fails if bloomfilters are enabled</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="stack">stack</assignee>
                                    <reporter username="apurtell">Andrew Purtell</reporter>
                        <labels>
                    </labels>
                <created>Sun, 30 Nov 2008 18:44:38 +0000</created>
                <updated>Sun, 13 Sep 2009 22:26:34 +0000</updated>
                            <resolved>Wed, 3 Dec 2008 23:14:28 +0000</resolved>
                                    <version>0.18.1</version>
                                    <fixVersion>0.19.0</fixVersion>
                                    <component>regionserver</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>0</watches>
                                                                <comments>
                            <comment id="12651852" author="apurtell" created="Sun, 30 Nov 2008 18:51:25 +0000"  >&lt;p&gt;Getting more information from Thibaut via email...&lt;/p&gt;</comment>
                            <comment id="12651870" author="midnightcoder" created="Sun, 30 Nov 2008 21:11:59 +0000"  >&lt;p&gt;There seems to be some talk that HFS will incorporate bloom filter code. Anyone know the status on this or how it will impact the need for hbase to implement this?&lt;/p&gt;</comment>
                            <comment id="12652014" author="apurtell" created="Mon, 1 Dec 2008 14:43:27 +0000"  >&lt;p&gt;One crucial detail it seems is that the bloomfilter related exception happens even when no bloomfilters are enabled in the schema.  There are also DFS related exceptions. &lt;/p&gt;

&lt;p&gt;From Thibaut:&lt;/p&gt;

&lt;p&gt;I created all the tables from scratch and didn&apos;t change them at run time. The schema for all the tables right now is as followed. (data is a bytearray of a serialized google buffer object)&lt;/p&gt;

&lt;p&gt;    {NAME =&amp;gt; &apos;entries&apos;, IS_ROOT =&amp;gt; &apos;false&apos;, IS_META =&amp;gt; &apos;false&apos;, FAMILIES =&amp;gt; [&lt;/p&gt;
{NAME =&amp;gt; &apos;data&apos;, BLOOMFILTER =&amp;gt; &apos;false&apos;, COMPRESSION =&amp;gt; &apos;NONE&apos;, VERSIONS =&amp;gt; &apos;3&apos;, LENGTH =&amp;gt; &apos;2147483647&apos;, TTL =&amp;gt; &apos;-1&apos;, IN_MEMORY =&amp;gt; &apos;false&apos;, BLOCKCACHE =&amp;gt; &apos;false&apos;}
&lt;p&gt;]}&lt;/p&gt;

&lt;p&gt;I reran everything from scratch with the new table scheme and got the same exception again, just on a different table this time: (Disabling the bloomfilter, compression and the blockcache doesn&apos;t seem to have any effect)&lt;/p&gt;

&lt;p&gt;2008-11-30 23:22:20,774 ERROR org.apache.hadoop.hbase.regionserver.CompactSplitThread: Compaction failed for region entries,,1228075277421&lt;br/&gt;
java.lang.IllegalArgumentException: maxValue must be &amp;gt; 0&lt;br/&gt;
    at org.onelab.filter.HashFunction.&amp;lt;init&amp;gt;(HashFunction.java:84)&lt;br/&gt;
    at org.onelab.filter.Filter.&amp;lt;init&amp;gt;(Filter.java:97)&lt;br/&gt;
    at org.onelab.filter.BloomFilter.&amp;lt;init&amp;gt;(BloomFilter.java:102)&lt;br/&gt;
    at org.apache.hadoop.hbase.regionserver.HStoreFile$BloomFilterMapFile$Writer.&amp;lt;init&amp;gt;(HStoreFile.java:829)&lt;br/&gt;
    at org.apache.hadoop.hbase.regionserver.HStoreFile.getWriter(HStoreFile.java:436)&lt;br/&gt;
    at org.apache.hadoop.hbase.regionserver.HStore.compact(HStore.java:889)&lt;br/&gt;
    at org.apache.hadoop.hbase.regionserver.HRegion.compactStores(HRegion.java:902)&lt;br/&gt;
    at org.apache.hadoop.hbase.regionserver.HRegion.compactStores(HRegion.java:860)&lt;br/&gt;
    at org.apache.hadoop.hbase.regionserver.CompactSplitThread.run(CompactSplitThread.java:83)&lt;/p&gt;

&lt;p&gt;The log file is also full of these kind of errors: (before and after)&lt;/p&gt;

&lt;p&gt;2008-11-30 23:22:44,500 INFO org.apache.hadoop.ipc.Server: IPC Server handler 16 on 60020, call next(8976385860586379110) from x.x.x.203:52747: error: org.apache.hadoop.hbase.UnknownScannerException: Name: 8976385860586379110&lt;br/&gt;
    at org.apache.hadoop.hbase.regionserver.HRegionServer.next(HRegionServer.java:1077)&lt;br/&gt;
    at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)&lt;br/&gt;
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)&lt;br/&gt;
    at java.lang.reflect.Method.invoke(Method.java:585)&lt;br/&gt;
    at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:554)&lt;br/&gt;
    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:888)&lt;/p&gt;

&lt;p&gt;Shortly afterwards I got the dfs error on the regionserver again, on a different table though (and might be completely unreleated and not important?):&lt;/p&gt;

&lt;p&gt;2008-11-30 23:26:55,885 WARN org.apache.hadoop.dfs.DFSClient: Exception while reading from blk_-9066140877711029349_706715 of /hbase/webrequestscache/2091560474/data/mapfiles/1510543474646532027/data from x.x.x.204:50010: java.io.IOException: Premeture EOF from inputStream&lt;br/&gt;
    at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:102)&lt;br/&gt;
    at org.apache.hadoop.dfs.DFSClient$BlockReader.readChunk(DFSClient.java:996)&lt;br/&gt;
    at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:236)&lt;br/&gt;
    at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:191)&lt;br/&gt;
    at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:159)&lt;br/&gt;
    at org.apache.hadoop.dfs.DFSClient$BlockReader.read(DFSClient.java:858)&lt;br/&gt;
    at org.apache.hadoop.dfs.DFSClient$DFSInputStream.readBuffer(DFSClient.java:1384)&lt;br/&gt;
    at org.apache.hadoop.dfs.DFSClient$DFSInputStream.read(DFSClient.java:1420)&lt;br/&gt;
    at java.io.DataInputStream.readFully(DataInputStream.java:176)&lt;br/&gt;
    at org.apache.hadoop.io.DataOutputBuffer$Buffer.write(DataOutputBuffer.java:64)&lt;br/&gt;
    at org.apache.hadoop.io.DataOutputBuffer.write(DataOutputBuffer.java:102)&lt;br/&gt;
    at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1933)&lt;br/&gt;
    at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1833)&lt;br/&gt;
    at org.apache.hadoop.io.MapFile$Reader.seekInternal(MapFile.java:463)&lt;br/&gt;
    at org.apache.hadoop.io.MapFile$Reader.getClosest(MapFile.java:558)&lt;br/&gt;
    at org.apache.hadoop.io.MapFile$Reader.getClosest(MapFile.java:541)&lt;br/&gt;
    at org.apache.hadoop.hbase.regionserver.HStoreFile$BloomFilterMapFile$Reader.getClosest(HStoreFile.java:761)&lt;br/&gt;
    at org.apache.hadoop.hbase.regionserver.HStore.get(HStore.java:1291)&lt;br/&gt;
    at org.apache.hadoop.hbase.regionserver.HRegion.get(HRegion.java:1154)&lt;br/&gt;
    at org.apache.hadoop.hbase.regionserver.HRegionServer.get(HRegionServer.java:1020)&lt;br/&gt;
    at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)&lt;br/&gt;
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)&lt;br/&gt;
    at java.lang.reflect.Method.invoke(Method.java:585)&lt;br/&gt;
    at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:554)&lt;br/&gt;
    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:888)&lt;/p&gt;

&lt;p&gt;Datnode entries related to that block:&lt;/p&gt;

&lt;p&gt;08/11/30 22:44:19 INFO dfs.DataNode: Receiving block blk_-9066140877711029349_706715 src: /x.x.x.204:44313 dest: /x.x.x.204:50010&lt;br/&gt;
08/11/30 22:44:41 INFO dfs.DataNode: Received block blk_-9066140877711029349_706715 of size 33554432 from /x.x.x.204&lt;br/&gt;
08/11/30 22:44:41 INFO dfs.DataNode: PacketResponder 3 for block blk_-9066140877711029349_706715 terminating&lt;br/&gt;
08/11/30 22:53:18 WARN dfs.DataNode: DatanodeRegistration(x.x.x.204:50010, storageID=DS-364968361-x.x.x.204-50010-1220223683238, infoPort=50075, ipcPort=50020):Got exception while serving blk_-9066140877711029349_706715 to /x.x.x.204: java.net.SocketTimeoutException: 480000 millis timeout while waiting for channel to be ready for write. ch :java.nio.channels.SocketChannel&lt;span class=&quot;error&quot;&gt;&amp;#91;connected local=/x.x.x.204:50010 remote=/x.x.x.204:46220&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:185)&lt;br/&gt;
        at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:159)&lt;br/&gt;
        at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:198)&lt;br/&gt;
        at org.apache.hadoop.dfs.DataNode$BlockSender.sendChunks(DataNode.java:1873)&lt;br/&gt;
        at org.apache.hadoop.dfs.DataNode$BlockSender.sendBlock(DataNode.java:1967)&lt;br/&gt;
        at org.apache.hadoop.dfs.DataNode$DataXceiver.readBlock(DataNode.java:1109)&lt;br/&gt;
        at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:1037)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:595)&lt;/p&gt;</comment>
                            <comment id="12652069" author="jimk" created="Mon, 1 Dec 2008 17:30:33 +0000"  >&lt;p&gt;You can neither enable nor disable bloomfilters once a column has data in it.&lt;/p&gt;

&lt;p&gt;If you enable it on a table with existing data, compact assumes all stores have a bloom filter and will NPE because it cannot read it.&lt;/p&gt;

&lt;p&gt;If you disable bloom filters on a table that has data in it, compact will fail because the store file knows it has a bloom filter.&lt;/p&gt;

&lt;p&gt;It is easier to fix the latter than the former by going through the store files and deleting the bloom filter file.&lt;/p&gt;

&lt;p&gt;Enabling bloom filters after the table has data in it is much harder as all store files must be read and a bloom filter created for each.&lt;/p&gt;

&lt;p&gt;It would be better to disallow the enabling of bloom filters once the table has been created. This would at least prevent shooting yourself in the foot.&lt;/p&gt;</comment>
                            <comment id="12652094" author="apurtell" created="Mon, 1 Dec 2008 18:54:00 +0000"  >&lt;p&gt;According to the reporter (Thibaut, on hbase-user@)), the table schema never uses bloomfilters yet the bloomfilter related exceptions occur. &lt;/p&gt;</comment>
                            <comment id="12652099" author="stack" created="Mon, 1 Dec 2008 19:03:13 +0000"  >&lt;p&gt;Why not have the bloom Writer just not build a bloom filter if ALL inputs don&apos;t already have blooms rather than NPE (in getReaders, if an input doesn&apos;t have nrows, set it to -1)?   Could output a warning and just carry on.  New flushes will include bloom filters so subsequent compactions will have bloom filters to hand.  Eventually all inputs will have bloom filters and only then on compaction, write out compacted file with blooms.&lt;/p&gt;

&lt;p&gt;Adding disallow set/unset would be awkward in implementation; i.e. providing the appropriate context that determines when a flag is settable or not in HTD.&lt;/p&gt;




</comment>
                            <comment id="12652103" author="stack" created="Mon, 1 Dec 2008 19:12:37 +0000"  >&lt;blockquote&gt;&lt;p&gt;There seems to be some talk that HFS will incorporate bloom filter code. Anyone know the status on this or how it will impact the need for hbase to implement this?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Do you have an issue id where this is discussed Bruce?&lt;/p&gt;

&lt;p&gt;That you&apos;d get the bloom filter exception on table that doesn&apos;t have it enabled &amp;#8211; or that never had it enabled in the past is odd... difficult to explain.&lt;/p&gt;</comment>
                            <comment id="12652104" author="streamy" created="Mon, 1 Dec 2008 19:14:28 +0000"  >&lt;p&gt;Correction from Thibaut on list:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;You are right. I just saw that that table (the only table) has indeed the bloomfilters and compression enabled (blockcache is disabled).&lt;br/&gt;
{NAME =&amp;gt; &apos;entries&apos;, IS_ROOT =&amp;gt; &apos;false&apos;, IS_META =&amp;gt; &apos;false&apos;, FAMILIES =&amp;gt; [&lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {NAME =&amp;gt; &amp;#39;data&amp;#39;, BLOOMFILTER =&amp;gt; &amp;#39;true&amp;#39;, COMPRESSION =&amp;gt; &amp;#39;BLOCK&amp;#39;, VERSIONS =&amp;gt; &amp;#39;3&amp;#39;, LENGTH =&amp;gt; &amp;#39;2147483647&amp;#39;, TTL =&amp;gt; &amp;#39;-1&amp;#39;, IN_MEMORY =&amp;gt; &amp;#39;false&amp;#39;, BLOCKCACHE =&amp;gt; &amp;#39;false&amp;#39;}&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;]}&lt;/p&gt;

&lt;p&gt;As for the load/dfs errors, thanks for explaining this. I was only starting up one region server to have all the log entries at one place, and it was indeed under heavy load. (Multiple threads setting/getting keys)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;He originally said 8 node cluster, but now makes mention of only single regionserver.  So one problem seems related to blooms, probably also seeing load-related issues.&lt;/p&gt;</comment>
                            <comment id="12652232" author="apurtell" created="Tue, 2 Dec 2008 01:56:27 +0000"  >&lt;p&gt;+1 on the idea of allowing alteration of table schema to enable bloomfilters after the fact. Current situation with &quot;java.lang.IllegalArgumentException: maxValue must be &amp;gt; 0&quot; exceptions in that case violate principle of least surprise. &lt;/p&gt;</comment>
                            <comment id="12652982" author="stack" created="Wed, 3 Dec 2008 21:31:16 +0000"  >&lt;p&gt;Writing, if illegal filter parameters passed, continue with a warning.  If reading, if can&apos;t find the filter file, continue with warning.&lt;/p&gt;</comment>
                            <comment id="12652983" author="stack" created="Wed, 3 Dec 2008 21:32:33 +0000"  >&lt;p&gt;Bring into 0.19.0.  Effects apurtell.  Compactions are failing and so his regions continue to grow.  Fellas can&apos;t undo bloomfilters w/o throwing away data w/o this patch.&lt;/p&gt;</comment>
                            <comment id="12653011" author="apurtell" created="Wed, 3 Dec 2008 22:35:05 +0000"  >&lt;p&gt;+1 on the 1039 patch. I&apos;ll try keeping my data and seeing if this will clear the problem. At least if the compactions succeed, that&apos;s a good step forward.&lt;/p&gt;</comment>
                            <comment id="12653018" author="stack" created="Wed, 3 Dec 2008 22:42:45 +0000"  >&lt;p&gt;Committed.  Waiting on confirmation that this patch actually works before closing.&lt;/p&gt;</comment>
                            <comment id="12653033" author="apurtell" created="Wed, 3 Dec 2008 23:02:35 +0000"  >&lt;p&gt;Confirmed. I see the warning in the regionserver logs, then messages that indicate compaction completed successfully. &lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310010">
                    <name>Incorporates</name>
                                                                <inwardlinks description="is part of">
                                        <issuelink>
            <issuekey id="12409996">HBASE-1047</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12395215" name="1039.patch" size="2134" author="stack" created="Wed, 3 Dec 2008 21:31:16 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Sun, 30 Nov 2008 21:11:59 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>25535</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 1 week, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0hayf:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>99037</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>