<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 02 19:05:53 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-15900/HBASE-15900.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-15900] RS stuck in get lock of HStore</title>
                <link>https://issues.apache.org/jira/browse/HBASE-15900</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;It happens on my production cluster when i run MR job.  I save the dump.txt from this RS webUI.&lt;/p&gt;

&lt;p&gt;Many threads stuck here:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt; 133 (B.defaultRpcServer.handler=94,queue=4,port=16020):
   32   State: WAITING
   31   Blocked count: 477816
   30   Waited count: 535255
   29   Waiting on java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync@6447ba67
   28   Stack:
   27     sun.misc.Unsafe.park(Native Method)
   26     java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
   25     java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
   24     java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireShared(AbstractQueuedSynchronizer.java:967)
   23     java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireShared(AbstractQueuedSynchronizer.java:1283)
   22     java.util.concurrent.locks.ReentrantReadWriteLock$ReadLock.lock(ReentrantReadWriteLock.java:727)
   21     org.apache.hadoop.hbase.regionserver.HStore.add(HStore.java:666)
   20     org.apache.hadoop.hbase.regionserver.HRegion.applyFamilyMapToMemstore(HRegion.java:3621)
   19     org.apache.hadoop.hbase.regionserver.HRegion.doMiniBatchMutation(HRegion.java:3038)
   18     org.apache.hadoop.hbase.regionserver.HRegion.batchMutate(HRegion.java:2793)
   17     org.apache.hadoop.hbase.regionserver.HRegion.batchMutate(HRegion.java:2735)
   16     org.apache.hadoop.hbase.regionserver.RSRpcServices.doBatchOp(RSRpcServices.java:692)
   15     org.apache.hadoop.hbase.regionserver.RSRpcServices.doNonAtomicRegionMutation(RSRpcServices.java:654)
   14     org.apache.hadoop.hbase.regionserver.RSRpcServices.multi(RSRpcServices.java:2029)
   13     org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:32213)
   12     org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2112)
   11     org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:101)
   10     org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop(RpcExecutor.java:130)
    9     org.apache.hadoop.hbase.ipc.RpcExecutor$1.run(RpcExecutor.java:107)
    8     java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment></environment>
        <key id="12973328">HBASE-15900</key>
            <summary>RS stuck in get lock of HStore</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="3">Duplicate</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="chenheng">Heng Chen</reporter>
                        <labels>
                    </labels>
                <created>Fri, 27 May 2016 07:37:25 +0000</created>
                <updated>Thu, 23 Jun 2016 10:15:36 +0000</updated>
                            <resolved>Thu, 23 Jun 2016 06:23:06 +0000</resolved>
                                    <version>1.1.1</version>
                    <version>1.3.0</version>
                                                        <due></due>
                            <votes>0</votes>
                                    <watches>11</watches>
                                                                <comments>
                            <comment id="15303712" author="chenheng" created="Fri, 27 May 2016 07:38:03 +0000"  >&lt;p&gt;Upload the dump.txt&lt;/p&gt;</comment>
                            <comment id="15303748" author="chenheng" created="Fri, 27 May 2016 08:27:06 +0000"  >&lt;p&gt;Check code on HStore.java,  the method which may hold write lock list below:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;bulkLoadHFile&lt;/li&gt;
	&lt;li&gt;close&lt;/li&gt;
	&lt;li&gt;replaceStoreFiles&lt;/li&gt;
	&lt;li&gt;snapshot&lt;/li&gt;
	&lt;li&gt;updateStorefiles&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I can&apos;t find any thread which hold the write lock in jstack, but i think it has relates with compaction. Because there are lots of logs in rs.log like below, it seems compaction stuck or not run, so flush always be requested. &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2016-05-27 15:25:50,734 INFO  [dx-pipe-regionserver14-online,16020,1464166068185_ChoreService_1] regionserver.HRegionServer: dx-pipe-regionserver14-online,16020,1464166068185-MemstoreFlusherChore requesting flush &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region frog_stastic,J\x08\x7F\x04_{211}_1455521065789,1460472441110.d13dc38891b807c97bfc2cab7fa60f86. after a delay of 11759
2016-05-27 15:25:50,734 INFO  [dx-pipe-regionserver14-online,16020,1464166068185_ChoreService_1] regionserver.HRegionServer: dx-pipe-regionserver14-online,16020,1464166068185-MemstoreFlusherChore requesting flush &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region frog_stastic,\x0F&amp;lt;\x80\x00_211_1453625257737,1464218845975.56a21606788d0eeff0b268b0bb670841. after a delay of 10261
2016-05-27 15:25:50,734 INFO  [dx-pipe-regionserver14-online,16020,1464166068185_ChoreService_1] regionserver.HRegionServer: dx-pipe-regionserver14-online,16020,1464166068185-MemstoreFlusherChore requesting flush &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region frog_stastic,\x81n\x8D\x00_{311}_1455203248372,1461067416754.248e5726c8fdd029c61433c7f291eed3. after a delay of 11702
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


</comment>
                            <comment id="15303751" author="mantonov" created="Fri, 27 May 2016 08:31:24 +0000"  >&lt;p&gt;I&apos;ve seen that as well, for sure (w/o running MR jobs), I&apos;ve looked at the locks in HStore and didn&apos;t see anything obviously wrong.. In my case the visible effect was that flushes were blocked/delayed for a very long time, is that what you see as well, or you saw other effects?&lt;/p&gt;</comment>
                            <comment id="15303759" author="chenheng" created="Fri, 27 May 2016 08:44:11 +0000"  >&lt;p&gt;Yes, something like this.  I extract one region log which always be delayed flush.  And upload it &lt;/p&gt;</comment>
                            <comment id="15303771" author="chenheng" created="Fri, 27 May 2016 08:50:07 +0000"  >&lt;p&gt;In the log, we can see,  before request flush of region &apos;81194e359707acadee2906ffe36ab130&apos;,  all compaction about it never finish, it seems stuck in somewhere.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2016-05-26 06:41:21,150 DEBUG [regionserver/dx-pipe-regionserver14-online/10.11.52.77:16020-shortCompactions-1464166647562] regionserver.HStore: 81194e359707acadee2906ffe36ab130 - cf: Initiating major compaction (all files)
2016-05-26 09:28:30,390 DEBUG [regionserver/dx-pipe-regionserver14-online/10.11.52.77:16020-shortCompactions-1464166647562] regionserver.HStore: 81194e359707acadee2906ffe36ab130 - cf: Initiating major compaction (all files)
2016-05-26 12:14:41,707 DEBUG [regionserver/dx-pipe-regionserver14-online/10.11.52.77:16020-shortCompactions-1464166647562] regionserver.HStore: 81194e359707acadee2906ffe36ab130 - cf: Initiating major compaction (all files)
2016-05-26 15:01:56,362 DEBUG [regionserver/dx-pipe-regionserver14-online/10.11.52.77:16020-shortCompactions-1464166647562] regionserver.HStore: 81194e359707acadee2906ffe36ab130 - cf: Initiating major compaction (all files)
2016-05-26 17:48:01,407 DEBUG [regionserver/dx-pipe-regionserver14-online/10.11.52.77:16020-shortCompactions-1464166647562] regionserver.HStore: 81194e359707acadee2906ffe36ab130 - cf: Initiating major compaction (all files)
2016-05-26 20:34:41,771 DEBUG [regionserver/dx-pipe-regionserver14-online/10.11.52.77:16020-shortCompactions-1464166647562] regionserver.HStore: 81194e359707acadee2906ffe36ab130 - cf: Initiating major compaction (all files)
2016-05-26 23:21:21,708 DEBUG [regionserver/dx-pipe-regionserver14-online/10.11.52.77:16020-shortCompactions-1464166647562] regionserver.HStore: 81194e359707acadee2906ffe36ab130 - cf: Initiating major compaction (all files)
2016-05-27 02:08:42,414 DEBUG [regionserver/dx-pipe-regionserver14-online/10.11.52.77:16020-shortCompactions-1464166647562] regionserver.HStore: 81194e359707acadee2906ffe36ab130 - cf: Initiating major compaction (all files)
2016-05-27 04:54:41,549 DEBUG [regionserver/dx-pipe-regionserver14-online/10.11.52.77:16020-shortCompactions-1464166647562] regionserver.HStore: 81194e359707acadee2906ffe36ab130 - cf: Initiating major compaction (all files)
2016-05-27 07:41:56,722 DEBUG [regionserver/dx-pipe-regionserver14-online/10.11.52.77:16020-shortCompactions-1464166647562] regionserver.HStore: 81194e359707acadee2906ffe36ab130 - cf: Initiating major compaction (all files)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15303921" author="sergey.soldatov" created="Fri, 27 May 2016 10:53:59 +0000"  >&lt;p&gt;I would highly recommend to try get dump using &lt;tt&gt;jstack -l&lt;/tt&gt; while you see delayed flush.  It should be run under the same user as RS (&lt;tt&gt;hbase&lt;/tt&gt; usually). Find the corresponding process id using &lt;tt&gt;jps&lt;/tt&gt; and dump thread dump using &lt;tt&gt;jstack -l &amp;lt;pid&amp;gt;&lt;/tt&gt;&lt;/p&gt;</comment>
                            <comment id="15306119" author="stack" created="Mon, 30 May 2016 00:07:42 +0000"  >&lt;p&gt;I didn&apos;t know about the &apos;-l&apos;. It is not on mac os x it seems, only on linux. Looks useful. Any luck &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=chenheng&quot; class=&quot;user-hover&quot; rel=&quot;chenheng&quot;&gt;Heng Chen&lt;/a&gt;?&lt;/p&gt;</comment>
                            <comment id="15306187" author="chenheng" created="Mon, 30 May 2016 03:09:21 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt;&lt;br/&gt;
I have found something&lt;br/&gt;
HStore.lock.readLock was hold by this thread below.  So compaction could not acquire the lock.writeLock in HStore.replaceStoreFiles,  so all compaction was blocked and memstore could not be flushed because of so many store files exist.  &lt;/p&gt;

&lt;p&gt;Now, i am trying to figure out why scan was blocked in IdLock.getLockEntry,  it happened many times. Maybe it was &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-14178&quot; title=&quot;regionserver blocks because of waiting for offsetLock&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-14178&quot;&gt;&lt;del&gt;HBASE-14178&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;And there is another point i can&apos;t understand,  only readLock was held by one thread, why there were so many threads waiting for readLock?&lt;/p&gt;

&lt;p&gt;BTW. scan operation in my cluster is only called by phoenix, not sure it has relates with the problem.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt; 43 (B.defaultRpcServer.handler=4,queue=4,port=16020):
  State: WAITING
  Blocked count: 224987
  Waited count: 253413
  Waiting on org.apache.hadoop.hbase.util.IdLock$Entry@48148720
  Stack:
    java.lang.&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;.wait(Native Method)
    java.lang.&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;.wait(&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;.java:502)
    org.apache.hadoop.hbase.util.IdLock.getLockEntry(IdLock.java:81)
    org.apache.hadoop.hbase.io.hfile.HFileReaderV2.readBlock(HFileReaderV2.java:397)
    org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexReader.loadDataBlockWithScanInfo(HFileBlockIndex.java:259)
    org.apache.hadoop.hbase.io.hfile.HFileReaderV2$AbstractScannerV2.seekTo(HFileReaderV2.java:634)
    org.apache.hadoop.hbase.io.hfile.HFileReaderV2$AbstractScannerV2.seekTo(HFileReaderV2.java:584)
    org.apache.hadoop.hbase.regionserver.StoreFileScanner.seekAtOrAfter(StoreFileScanner.java:247)
    org.apache.hadoop.hbase.regionserver.StoreFileScanner.seek(StoreFileScanner.java:156)
    org.apache.hadoop.hbase.regionserver.StoreScanner.seekScanners(StoreScanner.java:363)
    org.apache.hadoop.hbase.regionserver.StoreScanner.&amp;lt;init&amp;gt;(StoreScanner.java:217)
    org.apache.hadoop.hbase.regionserver.HStore.getScanner(HStore.java:2003)
    org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.&amp;lt;init&amp;gt;(HRegion.java:5294)
    org.apache.hadoop.hbase.regionserver.HRegion.instantiateRegionScanner(HRegion.java:2486)
    org.apache.hadoop.hbase.regionserver.HRegion.getScanner(HRegion.java:2472)
    org.apache.hadoop.hbase.regionserver.HRegion.getScanner(HRegion.java:2454)
    org.apache.hadoop.hbase.regionserver.RSRpcServices.scan(RSRpcServices.java:2253)
    org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:32205)
    org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2112)
    org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:101)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15306192" author="sergey.soldatov" created="Mon, 30 May 2016 03:18:01 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=chenheng&quot; class=&quot;user-hover&quot; rel=&quot;chenheng&quot;&gt;Heng Chen&lt;/a&gt; yet again, could you use jstack with -l  and attach the dump? We have seen a similar issue with blocked scanner and flusher but it had nothing with compaction as far as I remember. &lt;/p&gt;</comment>
                            <comment id="15306194" author="chenheng" created="Mon, 30 May 2016 03:20:27 +0000"  >&lt;p&gt;Sorry,  &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sergey.soldatov&quot; class=&quot;user-hover&quot; rel=&quot;sergey.soldatov&quot;&gt;Sergey Soldatov&lt;/a&gt; My RS has been restart,  but it always happens recently,  let me do it next time.    &lt;/p&gt;</comment>
                            <comment id="15306195" author="sergey.soldatov" created="Mon, 30 May 2016 03:22:10 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=chenheng&quot; class=&quot;user-hover&quot; rel=&quot;chenheng&quot;&gt;Heng Chen&lt;/a&gt; that would be great. Thank you!&lt;/p&gt;</comment>
                            <comment id="15306196" author="chenheng" created="Mon, 30 May 2016 03:24:31 +0000"  >&lt;p&gt;Lucky!  it was saved last time by my workmate. &lt;/p&gt;

&lt;p&gt;Upload it now!&lt;/p&gt;</comment>
                            <comment id="15306264" author="sergey.soldatov" created="Mon, 30 May 2016 05:58:57 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=chenheng&quot; class=&quot;user-hover&quot; rel=&quot;chenheng&quot;&gt;Heng Chen&lt;/a&gt; Thank you. Quite interesting. A bunch of threads are waiting for RRW locks, but none of threads is own lock on it, but at least one is supposed to.  Which JDK you are using? &lt;/p&gt;</comment>
                            <comment id="15306269" author="chenheng" created="Mon, 30 May 2016 06:02:43 +0000"  >&lt;p&gt;1.8.0_20&lt;/p&gt;

&lt;p&gt;I can find threads which own the read lock,  but can&apos;t figure out who holds the write lock too.&lt;/p&gt;</comment>
                            <comment id="15307438" author="chenheng" created="Tue, 31 May 2016 08:30:02 +0000"  >&lt;p&gt;I open DEBUG Level log in RS,  and found the problem which i describe in &lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/hbase-user/201605.mbox/%3CCAKKbPbKS5jWYvUehaeBm1412WgcEN542Xkf3CJwYOgfLK_a7dw%40mail.gmail.com%3E&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://mail-archives.apache.org/mod_mbox/hbase-user/201605.mbox/%3CCAKKbPbKS5jWYvUehaeBm1412WgcEN542Xkf3CJwYOgfLK_a7dw%40mail.gmail.com%3E&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It has different jstack with this issue, but the problem looks similar,  one region could not be closed because of there is one compaction left,  the region always be requested to flush, but flush cound not run because writestate.writeEnabled has been set to be false.&lt;/p&gt;

&lt;p&gt;I upload the jstack and debug level rs log about one region &apos;0d32a6bab354e6cc170cd59a2d485797&apos;.  &lt;/p&gt;</comment>
                            <comment id="15307450" author="chenheng" created="Tue, 31 May 2016 08:42:50 +0000"  >&lt;p&gt;We could see in 0d32a6bab354e6cc170cd59a2d485797.rs.log,  the region could not be closed due to there is one compaction left&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2016-05-31 14:18:18,414 INFO  [PriorityRpcServer.handler=19,queue=1,port=16020] regionserver.RSRpcServices: Close 0d32a6bab354e6cc170cd59a2d485797, moving to dx-pipe-regionserver8-online,16020,1464675204532
2016-05-31 14:18:18,443 DEBUG [RS_CLOSE_REGION-dx-pipe-regionserver20-online:16020-1] handler.CloseRegionHandler: Processing close of frog_stastic,$+\xD6\x05_{211}_1454335203,1462760599763.0d32a6bab354e6cc170cd59a2d485797.
2016-05-31 14:18:18,443 DEBUG [RS_CLOSE_REGION-dx-pipe-regionserver20-online:16020-1] regionserver.HRegion: Closing frog_stastic,$+\xD6\x05_{211}_1454335203,1462760599763.0d32a6bab354e6cc170cd59a2d485797.: disabling compactions &amp;amp; flushes
2016-05-31 14:18:18,443 DEBUG [RS_CLOSE_REGION-dx-pipe-regionserver20-online:16020-1] regionserver.HRegion: waiting &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 1 compactions to complete &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region frog_stastic,$+\xD6\x05_{211}_1454335203,1462760599763.0d32a6bab354e6cc170cd59a2d485797.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The left compaction is one  major compaction run in longCompaction thread,  the progress of it reached 81.23%, then it has no response after that.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2016-05-31 05:28:29,476 DEBUG [regionserver/dx-pipe-regionserver20-online/10.11.52.80:16020-longCompactions-1464599181827] regionserver.HStore: 0d32a6bab354e6cc170cd59a2d485797 - info: Initiating major compaction (all files)
2016-05-31 05:28:29,476 INFO  [regionserver/dx-pipe-regionserver20-online/10.11.52.80:16020-longCompactions-1464599181827] regionserver.HRegion: Starting compaction on info in region frog_stastic,$+\xD6\x05_{211}_1454335203,1462760599763.0d32a6bab354e6cc170cd59a2d485797.
2016-05-31 05:28:29,476 INFO  [regionserver/dx-pipe-regionserver20-online/10.11.52.80:16020-longCompactions-1464599181827] regionserver.HStore: Starting compaction of 9 file(s) in info of frog_stastic,$+\xD6\x05_{211}_1454335203,1462760599763.0d32a6bab354e6cc170cd59a2d485797. into tmpdir=hdfs:&lt;span class=&quot;code-comment&quot;&gt;//f04/hbase/data/&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;/frog_stastic/0d32a6bab354e6cc170cd59a2d485797/.tmp, totalSize=5.7 G
&lt;/span&gt;2016-05-31 05:28:29,476 DEBUG [regionserver/dx-pipe-regionserver20-online/10.11.52.80:16020-longCompactions-1464599181827] compactions.Compactor: Compacting hdfs:&lt;span class=&quot;code-comment&quot;&gt;//f04/hbase/data/&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;/frog_stastic/0d32a6bab354e6cc170cd59a2d485797/info/c31e84594c444d3cb8de00029e24aa34, keycount=78408609, bloomtype=ROW, size=4.9 G, encoding=NONE, seqNum=77065948, earliestPutTs=1453899210588
&lt;/span&gt;2016-05-31 05:28:29,476 DEBUG [regionserver/dx-pipe-regionserver20-online/10.11.52.80:16020-longCompactions-1464599181827] compactions.Compactor: Compacting hdfs:&lt;span class=&quot;code-comment&quot;&gt;//f04/hbase/data/&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;/frog_stastic/0d32a6bab354e6cc170cd59a2d485797/info/d3310d547e494e7db2b2e952e5fb412e, keycount=6356215, bloomtype=ROW, size=428.0 M, encoding=NONE, seqNum=78270525, earliestPutTs=1463613139775
&lt;/span&gt;2016-05-31 05:28:29,477 DEBUG [regionserver/dx-pipe-regionserver20-online/10.11.52.80:16020-longCompactions-1464599181827] compactions.Compactor: Compacting hdfs:&lt;span class=&quot;code-comment&quot;&gt;//f04/hbase/data/&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;/frog_stastic/0d32a6bab354e6cc170cd59a2d485797/info/8b11d48ac6a241f1b3ace1a822f087b0, keycount=1850675, bloomtype=ROW, size=124.6 M, encoding=NONE, seqNum=78624661, earliestPutTs=1464048401491
&lt;/span&gt;2016-05-31 05:28:29,477 DEBUG [regionserver/dx-pipe-regionserver20-online/10.11.52.80:16020-longCompactions-1464599181827] compactions.Compactor: Compacting hdfs:&lt;span class=&quot;code-comment&quot;&gt;//f04/hbase/data/&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;/frog_stastic/0d32a6bab354e6cc170cd59a2d485797/info/514ea98b95eb4b5fa4e122ebdd32d004, keycount=443135, bloomtype=ROW, size=29.9 M, encoding=NONE, seqNum=78710458, earliestPutTs=1464306574727
&lt;/span&gt;2016-05-31 05:28:29,478 DEBUG [regionserver/dx-pipe-regionserver20-online/10.11.52.80:16020-longCompactions-1464599181827] compactions.Compactor: Compacting hdfs:&lt;span class=&quot;code-comment&quot;&gt;//f04/hbase/data/&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;/frog_stastic/0d32a6bab354e6cc170cd59a2d485797/info/bf4877dbdf804492878750c30f3cfd8e, keycount=780345, bloomtype=ROW, size=52.6 M, encoding=NONE, seqNum=78852324, earliestPutTs=1464307875726
&lt;/span&gt;2016-05-31 05:28:29,478 DEBUG [regionserver/dx-pipe-regionserver20-online/10.11.52.80:16020-longCompactions-1464599181827] compactions.Compactor: Compacting hdfs:&lt;span class=&quot;code-comment&quot;&gt;//f04/hbase/data/&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;/frog_stastic/0d32a6bab354e6cc170cd59a2d485797/info/79ae0770fb58484e866c7b48f84e16a4, keycount=357040, bloomtype=ROW, size=24.1 M, encoding=NONE, seqNum=78916832, earliestPutTs=1464391849632
&lt;/span&gt;2016-05-31 05:28:29,478 DEBUG [regionserver/dx-pipe-regionserver20-online/10.11.52.80:16020-longCompactions-1464599181827] compactions.Compactor: Compacting hdfs:&lt;span class=&quot;code-comment&quot;&gt;//f04/hbase/data/&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;/frog_stastic/0d32a6bab354e6cc170cd59a2d485797/info/e7708a72734448c5a86305a0224af683, keycount=1725115, bloomtype=ROW, size=116.1 M, encoding=NONE, seqNum=79190322, earliestPutTs=1464393070342
&lt;/span&gt;2016-05-31 05:28:29,479 DEBUG [regionserver/dx-pipe-regionserver20-online/10.11.52.80:16020-longCompactions-1464599181827] compactions.Compactor: Compacting hdfs:&lt;span class=&quot;code-comment&quot;&gt;//f04/hbase/data/&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;/frog_stastic/0d32a6bab354e6cc170cd59a2d485797/info/1dcd54598541492bb20c279716488b5a, keycount=869015, bloomtype=ROW, size=58.6 M, encoding=NONE, seqNum=79334888, earliestPutTs=1464588026997
&lt;/span&gt;2016-05-31 05:28:29,479 DEBUG [regionserver/dx-pipe-regionserver20-online/10.11.52.80:16020-longCompactions-1464599181827] compactions.Compactor: Compacting hdfs:&lt;span class=&quot;code-comment&quot;&gt;//f04/hbase/data/&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;/frog_stastic/0d32a6bab354e6cc170cd59a2d485797/info/0a76eb5184dd4d42bdd16a588d295d0e, keycount=53330, bloomtype=ROW, size=3.7 M, encoding=NONE, seqNum=79346806, earliestPutTs=1464591201410
&lt;/span&gt;2016-05-31 05:29:29,684 DEBUG [regionserver/dx-pipe-regionserver20-online/10.11.52.80:16020-longCompactions-1464599181827] compactions.Compactor: Compaction progress: frog_stastic,$+\xD6\x05_{211}_1454335203,1462760599763.0d32a6bab354e6cc170cd59a2d485797.#info 14047066/90843479 (15.46%), rate=14910.65 kB/sec, throughputController is NoLimitCompactionThroughputController
2016-05-31 05:30:29,684 DEBUG [regionserver/dx-pipe-regionserver20-online/10.11.52.80:16020-longCompactions-1464599181827] compactions.Compactor: Compaction progress: frog_stastic,$+\xD6\x05_{211}_1454335203,1462760599763.0d32a6bab354e6cc170cd59a2d485797.#info 23487276/90843479 (25.85%), rate=10021.16 kB/sec, throughputController is NoLimitCompactionThroughputController
2016-05-31 05:31:29,684 DEBUG [regionserver/dx-pipe-regionserver20-online/10.11.52.80:16020-longCompactions-1464599181827] compactions.Compactor: Compaction progress: frog_stastic,$+\xD6\x05_{211}_1454335203,1462760599763.0d32a6bab354e6cc170cd59a2d485797.#info 39884696/90843479 (43.90%), rate=17406.01 kB/sec, throughputController is NoLimitCompactionThroughputController
2016-05-31 05:32:29,686 DEBUG [regionserver/dx-pipe-regionserver20-online/10.11.52.80:16020-longCompactions-1464599181827] compactions.Compactor: Compaction progress: frog_stastic,$+\xD6\x05_{211}_1454335203,1462760599763.0d32a6bab354e6cc170cd59a2d485797.#info 45639621/90843479 (50.24%), rate=6103.74 kB/sec, throughputController is NoLimitCompactionThroughputController
2016-05-31 05:33:30,012 DEBUG [regionserver/dx-pipe-regionserver20-online/10.11.52.80:16020-longCompactions-1464599181827] compactions.Compactor: Compaction progress: frog_stastic,$+\xD6\x05_{211}_1454335203,1462760599763.0d32a6bab354e6cc170cd59a2d485797.#info 61288969/90843479 (67.47%), rate=16531.36 kB/sec, throughputController is NoLimitCompactionThroughputController
2016-05-31 05:34:30,225 DEBUG [regionserver/dx-pipe-regionserver20-online/10.11.52.80:16020-longCompactions-1464599181827] compactions.Compactor: Compaction progress: frog_stastic,$+\xD6\x05_{211}_1454335203,1462760599763.0d32a6bab354e6cc170cd59a2d485797.#info 73794589/90843479 (81.23%), rate=13231.85 kB/sec, throughputController is NoLimitCompactionThroughputController
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And in jstack,  we could see the longCompaction thread is in RUNNABLE state, but it seems stuck.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-quote&quot;&gt;&quot;regionserver/dx-pipe-regionserver20-online/10.11.52.80:16020-longCompactions-1464599181827&quot;&lt;/span&gt; #727 daemon prio=5 os_prio=0 tid=0x00007f5324412000 nid=0x3796 runnable [0x00007f52fc5a8000]
   java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
	- locked &amp;lt;0x00000006b5900d70&amp;gt; (a sun.nio.ch.Util$2)
	- locked &amp;lt;0x00000006b58fa2a8&amp;gt; (a java.util.Collections$UnmodifiableSet)
	- locked &amp;lt;0x00000006b58d4400&amp;gt; (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:335)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.readChannelFully(PacketReceiver.java:258)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:209)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:171)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:102)
	at org.apache.hadoop.hdfs.RemoteBlockReader2.readNextPacket(RemoteBlockReader2.java:186)
	at org.apache.hadoop.hdfs.RemoteBlockReader2.read(RemoteBlockReader2.java:146)
	- locked &amp;lt;0x00000006b597e8f8&amp;gt; (a org.apache.hadoop.hdfs.RemoteBlockReader2)
	at org.apache.hadoop.hdfs.DFSInputStream$ByteArrayStrategy.doRead(DFSInputStream.java:686)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:742)
	- eliminated &amp;lt;0x000000062d8aeed8&amp;gt; (a org.apache.hadoop.hdfs.DFSInputStream)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:799)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:840)
	- locked &amp;lt;0x000000062d8aeed8&amp;gt; (a org.apache.hadoop.hdfs.DFSInputStream)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.hbase.io.hfile.HFileBlock.readWithExtra(HFileBlock.java:678)
	at org.apache.hadoop.hbase.io.hfile.HFileBlock$AbstractFSReader.readAtOffset(HFileBlock.java:1372)
	at org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderImpl.readBlockDataInternal(HFileBlock.java:1591)
	at org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderImpl.readBlockData(HFileBlock.java:1470)
	at org.apache.hadoop.hbase.io.hfile.HFileReaderV2.readBlock(HFileReaderV2.java:437)
	at org.apache.hadoop.hbase.io.hfile.HFileReaderV2$AbstractScannerV2.readNextDataBlock(HFileReaderV2.java:708)
	at org.apache.hadoop.hbase.io.hfile.HFileReaderV2$ScannerV2.isNextBlock(HFileReaderV2.java:833)
	at org.apache.hadoop.hbase.io.hfile.HFileReaderV2$ScannerV2.positionForNextBlock(HFileReaderV2.java:828)
	at org.apache.hadoop.hbase.io.hfile.HFileReaderV2$ScannerV2._next(HFileReaderV2.java:845)
	at org.apache.hadoop.hbase.io.hfile.HFileReaderV2$ScannerV2.next(HFileReaderV2.java:865)
	at org.apache.hadoop.hbase.regionserver.StoreFileScanner.next(StoreFileScanner.java:139)
	at org.apache.hadoop.hbase.regionserver.KeyValueHeap.next(KeyValueHeap.java:108)
	at org.apache.hadoop.hbase.regionserver.StoreScanner.next(StoreScanner.java:596)
	at org.apache.hadoop.hbase.regionserver.compactions.Compactor.performCompaction(Compactor.java:255)
	at org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor.compact(DefaultCompactor.java:106)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreEngine$DefaultCompactionContext.compact(DefaultStoreEngine.java:112)
	at org.apache.hadoop.hbase.regionserver.HStore.compact(HStore.java:1212)
	at org.apache.hadoop.hbase.regionserver.HRegion.compact(HRegion.java:1798)
	at org.apache.hadoop.hbase.regionserver.CompactSplitThread$CompactionRunner.run(CompactSplitThread.java:519)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)

   Locked ownable synchronizers:
	- &amp;lt;0x000000062afa5528&amp;gt; (a java.util.concurrent.ThreadPoolExecutor$Worker)
	- &amp;lt;0x000000062f7ee850&amp;gt; (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
	- &amp;lt;0x00000006b5985ec8&amp;gt; (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


</comment>
                            <comment id="15308776" author="sergey.soldatov" created="Tue, 31 May 2016 22:35:49 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=chenheng&quot; class=&quot;user-hover&quot; rel=&quot;chenheng&quot;&gt;Heng Chen&lt;/a&gt; Which HDFS version you are using? It may be a &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-7005&quot; title=&quot;DFS input streams do not timeout&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-7005&quot;&gt;&lt;del&gt;HDFS-7005&lt;/del&gt;&lt;/a&gt; since hdfs client just stuck during the reading. &lt;/p&gt;</comment>
                            <comment id="15309259" author="chenheng" created="Wed, 1 Jun 2016 04:52:49 +0000"  >&lt;p&gt;Maybe.   HDFS version is 2.5.1.   Let me try to upgrade my hdfs cluster and test again.   Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sergey.soldatov&quot; class=&quot;user-hover&quot; rel=&quot;sergey.soldatov&quot;&gt;Sergey Soldatov&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15309386" author="chenheng" created="Wed, 1 Jun 2016 06:31:42 +0000"  >&lt;p&gt;Yeah , it looks like this bug block the longCompaction thread.&lt;/p&gt;

&lt;p&gt;I notice &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ndimiduk&quot; class=&quot;user-hover&quot; rel=&quot;ndimiduk&quot;&gt;Nick Dimiduk&lt;/a&gt; comments,  any plan to upgrade branch-1.1 hadoop version to 2.6.0 ? &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ndimiduk&quot; class=&quot;user-hover&quot; rel=&quot;ndimiduk&quot;&gt;Nick Dimiduk&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15309393" author="chenheng" created="Wed, 1 Jun 2016 06:37:20 +0000"  >&lt;p&gt;branch-1.3 seems compile with 2.5.x  too.  &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mantonov&quot; class=&quot;user-hover&quot; rel=&quot;mantonov&quot;&gt;Mikhail Antonov&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15309900" author="mantonov" created="Wed, 1 Jun 2016 08:10:41 +0000"  >&lt;p&gt;As in the support / compatibility matrix we say that branch-1.2 supports 2.7.1, I&apos;d propose that branch-1.3 should just build against Hadoop 2.7.2, but that&apos;s probably up for mail list discussion. &lt;/p&gt;

&lt;p&gt;But what version we compile HBase against shouldn&apos;t matter in your case, right? As long as your runtime is covered as &quot;supported&quot; in the compatibility matrix you should be fine?&lt;/p&gt;</comment>
                            <comment id="15309910" author="mantonov" created="Wed, 1 Jun 2016 08:17:08 +0000"  >&lt;p&gt;Hm. I &lt;em&gt;think&lt;/em&gt; I&apos;ve seen stuck memstores on 2.7+ clusters, but I could be wrong here. Has anyone seen issues like that on 2.7?&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=chenheng&quot; class=&quot;user-hover&quot; rel=&quot;chenheng&quot;&gt;Heng Chen&lt;/a&gt; as you perhaps spent the most time on this issue, do you happen to have any steps to reliably (or, well, with high probability &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; ) reproduce it on the vanilla cluster? Since this issue has likely been there for long time, I&apos;m reluctant to mark it as critical/release blocker, but would be really nice to have.&lt;/p&gt;</comment>
                            <comment id="15309926" author="chenheng" created="Wed, 1 Jun 2016 08:30:46 +0000"  >&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt; you happen to have any steps to reliably (or, well, with high probability  ) reproduce it on the vanilla cluster?
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Yeah,  almost every time when i run my MR job to write HBase,  it will be reproduced.   &lt;/p&gt;

&lt;p&gt;I will also try to write a test case to reproduce it.&lt;/p&gt;</comment>
                            <comment id="15309944" author="sergey.soldatov" created="Wed, 1 Jun 2016 08:41:18 +0000"  >&lt;p&gt;Well, there are a lot of possibilities to block MemStoreFlusher. each case need to be investigated separately. It would be nice if you next time make a jstack dump. &lt;/p&gt;</comment>
                            <comment id="15315121" author="ndimiduk" created="Fri, 3 Jun 2016 23:52:49 +0000"  >&lt;blockquote&gt;&lt;p&gt;any plan to upgrade branch-1.1 hadoop version to 2.6.0&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The upgrade ship has sailed. Once we ship a minor release we&apos;re bound to the dependency versions versions for the remainder of the line. Thus, no, the compile-time dependency will not change. This does not prevent you from running this build of hbase on any supported hdfs version. Just be sure to replace the hadoop-*.jars in hbase&apos;s lib dir.&lt;/p&gt;</comment>
                            <comment id="15319790" author="apurtell" created="Wed, 8 Jun 2016 00:41:28 +0000"  >&lt;p&gt;We also saw long stuck flushes or compactions, and blocked region actions, until we patched our Hadoop for 7005. There have been other fixes for missing timeouts committed since. We probably should be recommending 2.7.2 or 2.6.4 for production.&lt;/p&gt;</comment>
                            <comment id="15319795" author="mantonov" created="Wed, 8 Jun 2016 00:44:06 +0000"  >&lt;p&gt;For 1.3 I think 2.7.2 should be the default version, for earlier release lines we may not be able to change the defaults but may recommend newer versions.&lt;/p&gt;</comment>
                            <comment id="15319797" author="mantonov" created="Wed, 8 Jun 2016 00:45:41 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=chenheng&quot; class=&quot;user-hover&quot; rel=&quot;chenheng&quot;&gt;Heng Chen&lt;/a&gt; curious if you had a chance to test your cluster on 2.7.2?&lt;/p&gt;</comment>
                            <comment id="15319877" author="chenheng" created="Wed, 8 Jun 2016 02:35:07 +0000"  >&lt;p&gt;I have upgraded my hbase cluster hadoop version to be 2.6.0,  it looks good now on my production cluster (It has run three days.).   &lt;/p&gt;

&lt;p&gt;Let me run 2.6.0 at least one week before change hadoop version to be 2.7.2,  next week i will test it on my production cluster.  &lt;/p&gt;

&lt;p&gt;Wait for my good news!  &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;
</comment>
                            <comment id="15345872" author="chenheng" created="Thu, 23 Jun 2016 06:22:13 +0000"  >&lt;p&gt;After test 2.6.0 with one week,  and 2.7.2 with one more week.  The issue never appear after upgrade dfs client version.&lt;/p&gt;


&lt;p&gt;Let me resolve the issue.  I think it is duplicate with &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-7005&quot; title=&quot;DFS input streams do not timeout&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-7005&quot;&gt;&lt;del&gt;HDFS-7005&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15346220" author="carp84" created="Thu, 23 Jun 2016 10:15:36 +0000"  >&lt;p&gt;We were running 0.98.12 with hadoop 2.6.2 for a long time and now running 1.1 with hadoop 2.6.5 on production, never saw such long stuck flushes/compactions, JFYI.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12739582">HDFS-7005</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12807077" name="0d32a6bab354e6cc170cd59a2d485797.jstack.txt" size="230219" author="chenheng" created="Tue, 31 May 2016 08:34:48 +0000"/>
                            <attachment id="12807076" name="0d32a6bab354e6cc170cd59a2d485797.rs.log" size="50998" author="chenheng" created="Tue, 31 May 2016 08:31:51 +0000"/>
                            <attachment id="12806911" name="9fe15a52_9fe15a52_save" size="326141" author="chenheng" created="Mon, 30 May 2016 03:24:31 +0000"/>
                            <attachment id="12806596" name="c91324eb_81194e359707acadee2906ffe36ab130.log" size="384339" author="chenheng" created="Fri, 27 May 2016 08:45:30 +0000"/>
                            <attachment id="12806589" name="dump.txt" size="645354" author="chenheng" created="Fri, 27 May 2016 07:38:03 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>5.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Fri, 27 May 2016 08:31:24 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            23 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2yl33:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>