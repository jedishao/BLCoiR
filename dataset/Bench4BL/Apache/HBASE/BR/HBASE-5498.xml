<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sat Dec 03 18:11:10 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-5498/HBASE-5498.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-5498] Secure Bulk Load</title>
                <link>https://issues.apache.org/jira/browse/HBASE-5498</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Design doc: &lt;a href=&quot;https://cwiki.apache.org/confluence/display/HCATALOG/HBase+Secure+Bulk+Load&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://cwiki.apache.org/confluence/display/HCATALOG/HBase+Secure+Bulk+Load&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Short summary:&lt;/p&gt;

&lt;p&gt;Security as it stands does not cover the bulkLoadHFiles() feature. Users calling this method will bypass ACLs. Also loading is made more cumbersome in a secure setting because of hdfs privileges. bulkLoadHFiles() moves the data from user&apos;s directory to the hbase directory, which would require certain write access privileges set.&lt;/p&gt;

&lt;p&gt;Our solution is to create a coprocessor which makes use of AuthManager to verify if a user has write access to the table. If so, launches a MR job as the hbase user to do the importing (ie rewrite from text to hfiles). One tricky part this job will have to do is impersonate the calling user when reading the input files. We can do this by expecting the user to pass an hdfs delegation token as part of the secureBulkLoad() coprocessor call and extend an inputformat to make use of that token. The output is written to a temporary directory accessible only by hbase and then bulkloadHFiles() is called.&lt;/p&gt;
</description>
                <environment></environment>
        <key id="12544698">HBASE-5498</key>
            <summary>Secure Bulk Load</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="toffer">Francis Liu</assignee>
                                    <reporter username="toffer">Francis Liu</reporter>
                        <labels>
                    </labels>
                <created>Thu, 1 Mar 2012 00:20:46 +0000</created>
                <updated>Fri, 7 Feb 2014 23:11:45 +0000</updated>
                            <resolved>Tue, 15 Jan 2013 23:54:10 +0000</resolved>
                                                    <fixVersion>0.94.5</fixVersion>
                    <fixVersion>0.95.0</fixVersion>
                                    <component>security</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>28</watches>
                                                                                                            <comments>
                            <comment id="13219705" author="toffer" created="Thu, 1 Mar 2012 01:27:08 +0000"  >&lt;p&gt;sorry todd, submitted the jira before completing it.&lt;/p&gt;</comment>
                            <comment id="13219708" author="tlipcon" created="Thu, 1 Mar 2012 01:33:01 +0000"  >&lt;p&gt;This seems backwards - not all bulk loads are text import jobs, so you don&apos;t want HBase to be running the MR job. Instead, it seems you should let the user run his MR job to generate HFiles however he sees fit, and then work on making the &lt;tt&gt;completeBulkLoad&lt;/tt&gt; portion able to transfer ownership.&lt;/p&gt;

&lt;p&gt;It seems like a useful HDFS API to allow a &lt;tt&gt;chown&lt;/tt&gt; call to work so long as you have credentials for one user and a delegation token for another. We don&apos;t have such a facility today, but I&apos;m much more in support of adding that than having hbase run user code under its own credentials.&lt;/p&gt;</comment>
                            <comment id="13219713" author="toffer" created="Thu, 1 Mar 2012 01:49:46 +0000"  >&lt;p&gt;The MR code will be part of hbase code just like importTSV. We can support other file formats just not all of them.&lt;/p&gt;</comment>
                            <comment id="13219715" author="toffer" created="Thu, 1 Mar 2012 01:53:56 +0000"  >&lt;p&gt;The hdfs api sounds like a more efficient solution. My main concern is the time it would take to get that feature into production? As a stopgap we can have this api available and then just change the implementation to include support for hfiles later on. We will need a coprocessor anyway not unless we instrument bulkLoadHFiles() with coprocessor hooks as well? &lt;/p&gt;</comment>
                            <comment id="13219838" author="apurtell" created="Thu, 1 Mar 2012 06:33:11 +0000"  >&lt;p&gt;The API gap was discussed on the mailing list but it didn&apos;t make it into a JIRA.&lt;/p&gt;

&lt;p&gt;See &lt;a href=&quot;http://search-hadoop.com/m/eEUHK1s4fo81/bulk+loading+and+RegionObservers&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://search-hadoop.com/m/eEUHK1s4fo81/bulk+loading+and+RegionObservers&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The salient detail:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A simple and straightforward course of action is to give the CP the option of rewriting the submitted store file(s) before the regionserver attempts to validate and move them into the store. This is similar to how CPs are hooked into compaction: CPs hook compaction by allowing one to wrap the scanner that is iterating over the store files. So the wrapper gets a chance to examine the KeyValues being processed and also has an opportunity to modify or drop them.&lt;/p&gt;

&lt;p&gt;Similarly for incoming HFiles for bulk load, the CP could be given a scanner iterating over those files, if you had a RegionObserver installed. You would be given the option in effect to rewrite the incoming HFiles before they are handed over to the RegionServer for addition to the region.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think this is a reasonable approach to interface design, because the fact you are given a scanner highlights the bulk nature of the input. However I think there should be two hooks here: one that allows for a simple yes/no answer as to whether the bulk load should proceed; and one that allows for a more expensive filtering or transformation or whatever via scanner-like interface. Bulk loads could be potentially very large so requiring a scan over them always is not a good idea.&lt;/p&gt;

&lt;p&gt;Transferring ownership at the HDFS level can be done as suggested with a &apos;chown&apos; enhancement IMO. &lt;/p&gt;</comment>
                            <comment id="13219848" author="tlipcon" created="Thu, 1 Mar 2012 06:41:12 +0000"  >&lt;p&gt;Adding a coprocessor hook to bulk load makes sense, but that seems like a generic coprocessor enhancement. The specific case of secure bulk load is a separate problem of ownership transfer, and actually is present even without the authorization enhancements, so long as HDFS permissions are enabled. Currently, our suggested workaround is that the user who generates the HFiles should chmod the output directory to be world writable, thus allowing the hbase user to move the files from the output directory into the hbase directories. This seems sufficient as a stop-gap until we can add an &quot;ownership transfer&quot; functionality in HDFS, no?&lt;/p&gt;</comment>
                            <comment id="13219878" author="apurtell@yahoo.com" created="Thu, 1 Mar 2012 07:25:56 +0000"  >&lt;blockquote&gt;&lt;/blockquote&gt;

&lt;p&gt;+1&lt;/p&gt;

&lt;p&gt;That stop-gap is what we do in production.&lt;/p&gt;</comment>
                            <comment id="13219887" author="toffer" created="Thu, 1 Mar 2012 07:54:29 +0000"  >&lt;p&gt;Andrew, thanks for point that discussion out. Can&apos;t those two hooks be combined into one? The user can just ignore the scanner if he doesn&apos;t need it. Or is there a large overhead on even just creating the scanner? If the hdfs level &apos;chown&apos; enhancement is implemented, wouldn&apos;t you need to change the method signature, which would make hbase dependent on security-enabled hadoop deployments?&lt;/p&gt;

&lt;p&gt;The bulk enhancement I am proposing is used for more than just &apos;chown&apos;. Correct me if I&apos;m wrong here but given the partitioning constraint needed to generate the HFiles, very few users will actually call completeBulkUpload after their processing job. A lot of them will have their own import MR jobs which converts processed data from one format into HFiles and then call completeBulkUpload. Users can be smart and create a job which does most of it&apos;s work map-side then be able to do the correct partitioning. But the trend at least at Y! is that the majority of the users are using DSLs and its going to keep growing. In effect we are not introducing any added overhead to the user only making their lives easier. With the &apos;chown&apos; enhancement we can make it so that an MR job doesn&apos;t have to be launched for importing hfiles.&lt;/p&gt;</comment>
                            <comment id="13219892" author="toffer" created="Thu, 1 Mar 2012 08:01:35 +0000"  >&lt;p&gt;Todd, the world readable workaround won&apos;t get past security review at work. Since it is a point of vulnerability. Especially since we will be running hbase in a multi-tenant environment.&lt;/p&gt;</comment>
                            <comment id="13219897" author="tlipcon" created="Thu, 1 Mar 2012 08:19:23 +0000"  >&lt;p&gt;Another workaround is to build a simple service which runs with credentials that are part of the HDFS supergroup. This service would have a single API call which would chgrp a directory to an hbase group, allowing hbase to move the HFiles out of it.&lt;/p&gt;

&lt;p&gt;A third workaround would be to have HBase itself run with superuser privileges, so it could chown the user&apos;s files to hbase ownership before moving.&lt;/p&gt;

&lt;p&gt;I think all of these are vastly superior to the option where HBase itself is submitting MR jobs to perform the import.&lt;/p&gt;</comment>
                            <comment id="13219904" author="toffer" created="Thu, 1 Mar 2012 08:29:34 +0000"  >&lt;p&gt;Just to clarify my concern is securing the entire process of bulk import which includes authorization checks.&lt;/p&gt;

&lt;p&gt;The 3rd is definitely a no go. HBase shouldn&apos;t be a super user.&lt;/p&gt;

&lt;p&gt;I&apos;m curious what makes hbase submitting MR jobs that bad? Is it because of a new system dependency?&lt;/p&gt;</comment>
                            <comment id="13219915" author="tlipcon" created="Thu, 1 Mar 2012 08:37:44 +0000"  >&lt;p&gt;Two things:&lt;br/&gt;
1) HBase currently has no dependencies on MR. One can submit MR jobs that write to/from HBase, but really those are just using the MR client from within a MR context.&lt;br/&gt;
2) A large proportion of bulk load use cases generate HFiles at the end of a pipeline which involves custom user code &amp;#8211; for example the map phase parses and processes a custom format, emitting keys into whatever structure is needed in HBase. Then, the reducer performs the partition/sort to write out the appropriate HFiles. Thus the jobs themselves are running user code, not just a pre-baked example like ImportTSV. The proposed solution doesn&apos;t address this use case &amp;#8211; it&apos;s totally unacceptable to run user code under an HBase security context in a multi-tenant environment.&lt;/p&gt;</comment>
                            <comment id="13219931" author="toffer" created="Thu, 1 Mar 2012 08:54:46 +0000"  >&lt;p&gt;Thanks for clearing that up. I wasn&apos;t aware that most users actually do processing as part of the job that generates HFiles which was an assumption I had stated previously. I completely agree with user code not allowed to run within the hbase context. I&apos;ll try and come up with a solution that addresses your concerns.&lt;/p&gt;</comment>
                            <comment id="13220186" author="tlipcon" created="Thu, 1 Mar 2012 17:51:18 +0000"  >&lt;p&gt;Cool, I&apos;ll keep watching this JIRA. Thanks for working on the problem - the current workarounds are definitely annoying and less than optimal - will be good to have a true solution in place.&lt;/p&gt;</comment>
                            <comment id="13220530" author="toffer" created="Fri, 2 Mar 2012 00:23:53 +0000"  >&lt;p&gt;So here&apos;s another solution. It&apos;s computationally secure just like delegation tokens.&lt;/p&gt;

&lt;p&gt;The idea is to have an hbase owned staging directory which is world traversable (711): /hbase/staging&lt;/p&gt;

&lt;p&gt;A user writes out MR data to his secure output directory: /user/foo/data&lt;/p&gt;

&lt;p&gt;A call is made to hbase to create a secret staging directory which is rwx (777): /user/staging/averylongandrandomdirectoryname&lt;/p&gt;

&lt;p&gt;The user makes the data world readable and writable, then moves it into the secret staging directory, then calls completeBulkLoad.&lt;/p&gt;

&lt;p&gt;Like delegation tokens the strength of the security lies in the length and randomness of the secret directory. If we mimic SHA1 it&apos;d be a 40 character hexstring. Though we might need something longer since delegation tokens include timestamps and a nonce.&lt;/p&gt;

&lt;p&gt;Some issues:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Automated way of cleaning up secret directories in the absence of hbase (ie hbase failure).&lt;/li&gt;
	&lt;li&gt;side channels leaking the secret directory (ie logs), though this may be only on secured nodes?&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13220533" author="toffer" created="Fri, 2 Mar 2012 00:25:48 +0000"  >&lt;p&gt;I forgot to mention cleanup of the secret directory after completeBulkLoad. This could all be encapsulated as part of the completeBulkLoad call.&lt;/p&gt;</comment>
                            <comment id="13220539" author="tlipcon" created="Fri, 2 Mar 2012 00:33:17 +0000"  >&lt;p&gt;Nice idea, seems reasonable to me.&lt;/p&gt;</comment>
                            <comment id="13220573" author="toffer" created="Fri, 2 Mar 2012 01:23:36 +0000"  >&lt;p&gt;Thanks, I&apos;ll come up a patch.&lt;/p&gt;</comment>
                            <comment id="13283724" author="apurtell" created="Fri, 25 May 2012 20:10:06 +0000"  >&lt;p&gt;@Francis, do you have any work, even if in a partially completed state?&lt;/p&gt;</comment>
                            <comment id="13285976" author="toffer" created="Wed, 30 May 2012 20:10:07 +0000"  >&lt;p&gt;@Andy, I have some code. Sorry for the delay, I got caught up in another project. Let me work on it a bit more and come up with a first draft next week. Does that sound ok?&lt;/p&gt;</comment>
                            <comment id="13286488" author="apurtell" created="Thu, 31 May 2012 11:32:17 +0000"  >&lt;p&gt;@Francis great to hear! Just pinging because we collectively are working in this area again now. Would be great if you have something to put up even if unfinished. &lt;/p&gt;</comment>
                            <comment id="13290834" author="toffer" created="Thu, 7 Jun 2012 06:34:03 +0000"  >&lt;p&gt;Partially finished patch to get feedback. Missing things:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;client api for secureBulkLoad, where should this go? Should we have completeBulkLoad API for the client and have that use secure when security is enabled?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;doesn&apos;t handle failure scenarios.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;HFile names have to be numeric so we can create storeFiles. Thinking about renaming each file while permission are being changed.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Tests&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13291104" author="jmhsieh" created="Thu, 7 Jun 2012 16:16:53 +0000"  >&lt;p&gt;This is a little bit outside my wheelhouse so expect I&apos;d expect more feedback from Andrew or Todd (or me once I ingest more of this).  Here are some quick comments / questions from a skim of the patch:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Is the design doc on HCatalog wiki relevant to the new implementation?  (Why in HCatalog?)&lt;/li&gt;
	&lt;li&gt;We should also probably add a postBulkLoadHFile to be symmetric.  If this drags out, consider adding a &quot;add postBulkLoadHFile and preBulkLoadHFile to RegionObserver/BaseRegionObserver&quot; subissue &amp;#8211; it would likely committed quickly alone.&lt;/li&gt;
	&lt;li&gt;This needs null check guard on getCoprocessHost()
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
       }
+      region.getCoprocessorHost()
+          .preBulkLoadHFile(familyPaths);
       &lt;span class=&quot;code-object&quot;&gt;boolean&lt;/span&gt; loaded = region.bulkLoadHFiles(familyPaths);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/li&gt;
	&lt;li&gt;SecureBulkLoadEndPoint/SecureBulkLoadProtocol will need an apache license header&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13291182" author="zhihyu@ebaysf.com" created="Thu, 7 Jun 2012 18:22:23 +0000"  >&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+   * @param familyPaths list of family names to store files adding
+   * or removing from &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; list will add or remove HFiles to be bulk loaded.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Add a period between files and adding. Capitalize &apos;a&apos; of adding.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+    &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt;(Pair&amp;lt;&lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt;[], &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;&amp;gt; el: familyPaths)
+        families.add(el.getFirst());
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Space between for and (, el and colon. families.add() should be put on the same line as for.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
-class StoreFileScanner &lt;span class=&quot;code-keyword&quot;&gt;implements&lt;/span&gt; KeyValueScanner {
+&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; class StoreFileScanner &lt;span class=&quot;code-keyword&quot;&gt;implements&lt;/span&gt; KeyValueScanner {
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;I don&apos;t see StoreFileScanner accessed in AccessController. So the above change is not needed.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+  &lt;span class=&quot;code-comment&quot;&gt;//TODO make &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; configurable
&lt;/span&gt;+  &lt;span class=&quot;code-comment&quot;&gt;//two levels so it doesn&apos;t get deleted accidentally
&lt;/span&gt;+  &lt;span class=&quot;code-comment&quot;&gt;//no sticky bit in Hadoop 1.0
&lt;/span&gt;+  &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; Path stagingDir = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Path(&lt;span class=&quot;code-quote&quot;&gt;&quot;/tmp/hbase-staging&quot;&lt;/span&gt;);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;I think the path should be configurable.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+  &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; User getActiveUser() &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
+    User user = RequestContext.getRequestUser();
+    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (!RequestContext.isInRequestContext()) {
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;if statement can be lifted above assignment.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;interface&lt;/span&gt; SecureBulkLoadProtocol &lt;span class=&quot;code-keyword&quot;&gt;extends&lt;/span&gt; CoprocessorProtocol {
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Add javadoc for the protocol.&lt;/p&gt;</comment>
                            <comment id="13291273" author="zhihyu@ebaysf.com" created="Thu, 7 Jun 2012 20:12:10 +0000"  >&lt;p&gt;What happens if user continues using LoadIncrementalHFiles directly ?&lt;/p&gt;</comment>
                            <comment id="13291468" author="toffer" created="Fri, 8 Jun 2012 00:52:23 +0000"  >&lt;p&gt;@Jonathan thanks for the comments will incorporate them in the next patch.&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;The HCatalog wiki is no longer relevant. The original implementation was being done in HCatalog, then we realized it made more sense to contribute it to HBase.&lt;/li&gt;
	&lt;li&gt;Yes, probably having the same signature as pre (but with updated family paths) though I haven&apos;t thought about how to go about doing that.&lt;/li&gt;
&lt;/ul&gt;

</comment>
                            <comment id="13291471" author="toffer" created="Fri, 8 Jun 2012 00:58:07 +0000"  >&lt;p&gt;@Ted, the access controller may not need to get a scanner but other coprocessor implementations may. See Andy&apos;s first comment. If the user calls LoadIncrementalHFiles directly then the user has to take care of doing chmod, for it to load successfully. As well as doing the procedure described here to make it secure. Maybe we should make LoadIncrementalHFiles use the secure api when security is enabled?&lt;/p&gt;
</comment>
                            <comment id="13291474" author="zhihyu@ebaysf.com" created="Fri, 8 Jun 2012 01:04:26 +0000"  >&lt;blockquote&gt;&lt;p&gt;we should make LoadIncrementalHFiles use the secure api when security is enabled?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Sure.&lt;/p&gt;</comment>
                            <comment id="13393646" author="toffer" created="Mon, 18 Jun 2012 01:59:16 +0000"  >&lt;p&gt;I&apos;ve split this work into two sub-tasks. Task #1 should be good to go pending any more comments. &lt;/p&gt;</comment>
                            <comment id="13410151" author="lakshman" created="Tue, 10 Jul 2012 09:08:51 +0000"  >&lt;p&gt;@Francis, do you have updated patch for Task #1.&lt;/p&gt;
</comment>
                            <comment id="13410152" author="lakshman" created="Tue, 10 Jul 2012 09:09:57 +0000"  >&lt;blockquote&gt;&lt;p&gt;@Francis, do you have updated patch for Task #1.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Sorry. Task #2 actually. &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6225&quot; title=&quot;create secure bulk load endpoint&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6225&quot;&gt;&lt;del&gt;HBASE-6225&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13410591" author="toffer" created="Tue, 10 Jul 2012 17:39:11 +0000"  >&lt;p&gt;Hi Laxman, not yet. Will look into it this week, if that&apos;s ok?&lt;/p&gt;</comment>
                            <comment id="13411237" author="lakshman" created="Wed, 11 Jul 2012 04:57:42 +0000"  >&lt;p&gt;That&apos;s fine Francis. Currently I&apos;m trying out your initial patch.&lt;br/&gt;
Will post my findings soon.&lt;/p&gt;

&lt;p&gt;BTW, can you please tell me how to invoke this secure bulkload (complete bulkload step)?&lt;br/&gt;
Should this be a plugged into Driver like earlier bulkload?&lt;/p&gt;</comment>
                            <comment id="13411543" author="lakshman" created="Wed, 11 Jul 2012 14:12:25 +0000"  >&lt;p&gt;I tried this draft patch. &lt;/p&gt;

&lt;p&gt;&lt;b&gt;Scenario&lt;/b&gt;&lt;br/&gt;
1) As &quot;hbase&quot;: Create a table &quot;employee&quot;. Grant permissions to &quot;test&quot; user.&lt;br/&gt;
2) As &quot;hdfs&quot;: Create dir &quot;/test&quot; and make &quot;test&quot; as owner.&lt;br/&gt;
3) As &quot;test&quot;: Place the tsv input in &quot;/test/input&quot; and run importtsv from command line as earlier.&lt;br/&gt;
4) As &quot;test&quot;: Run the following snippet to do &quot;completebulkload&quot;. I&apos;m not sure whether this is correct usage.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    HTable table = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; HTable(HBaseConfiguration.create(), &lt;span class=&quot;code-quote&quot;&gt;&quot;employee&quot;&lt;/span&gt;);
    AuthenticationProtocol auth = table.coprocessorProxy(AuthenticationProtocol.class,
        HConstants.EMPTY_START_ROW);
    Token&amp;lt;AuthenticationTokenIdentifier&amp;gt; token = auth.getAuthenticationToken();
    SecureBulkLoadProtocol proxy = table.coprocessorProxy(SecureBulkLoadProtocol.class,
        HConstants.EMPTY_START_ROW);
    proxy.secureCompleteBulkLoad(&lt;span class=&quot;code-quote&quot;&gt;&quot;employee&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;hdfs:&lt;span class=&quot;code-comment&quot;&gt;//10.18.40.92:9000/test/bulkload/output&quot;&lt;/span&gt;, token);&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;First phase (i.e. importtsv) worked fine.&lt;br/&gt;
For the second phase (i.e. completebulkload) I used the following snippet as my client code.&lt;/p&gt;

&lt;p&gt;But I&apos;m hitting the following exception on region server side.&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;2012-07-11 19:23:38,529 ERROR org.apache.hadoop.hbase.security.access.SecureBulkLoadEndPoint: Failed to secure bulk load
org.apache.hadoop.security.AccessControlException: Permission denied
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkOwner(FSPermissionChecker.java:155)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:147)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:4271)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOwner(FSNamesystem.java:4227)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setPermission(FSNamesystem.java:1010)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13412542" author="toffer" created="Thu, 12 Jul 2012 06:09:47 +0000"  >&lt;p&gt;Hi Laxman,&lt;/p&gt;

&lt;p&gt;Looks right to me apart from the delegation token. You need to pass an hdfs delegation token because we&apos;d like to impersonate the user when changing permissions on hdfs. Also the path doesn&apos;t need to be the full URI.&lt;/p&gt;

&lt;p&gt;Getting the token should be something like this:&lt;/p&gt;

&lt;p&gt;FileSystem fs = FileSystem.get(conf);&lt;br/&gt;
Token&amp;lt;?&amp;gt; token = fs.getDelegationToken(&quot;renewer&quot;);&lt;/p&gt;

&lt;p&gt;Let me know how things go.&lt;/p&gt;

&lt;p&gt;-Francis&lt;/p&gt;

</comment>
                            <comment id="13412719" author="lakshman" created="Thu, 12 Jul 2012 11:45:05 +0000"  >&lt;p&gt;Thanks for correction Francis.&lt;/p&gt;

&lt;p&gt;I tried as per your suggestion. Still no luck. Now we are hitting the same &quot;Permission denied&quot; problem. On further investigation, I found that the DFS requests(rename) is still going with &quot;hbase&quot; user. FS is initialized as &quot;hbase&quot; user during coprocessor initialization on startup. I moved FS initialization like below. &lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    &lt;span class=&quot;code-object&quot;&gt;boolean&lt;/span&gt; success = ugi.doAs(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; PrivilegedAction&amp;lt;&lt;span class=&quot;code-object&quot;&gt;Boolean&lt;/span&gt;&amp;gt;() {
      @Override
      &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;Boolean&lt;/span&gt; run() {
        FileSystem fs=&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;;
        &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
          fs=FileSystem.get(conf);
          setPermission(fs, srcPath, &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; FsPermission((&lt;span class=&quot;code-object&quot;&gt;short&lt;/span&gt;) 0777));
          fs.rename(srcPath, contentPath);
          LOG.info(&lt;span class=&quot;code-quote&quot;&gt;&quot;moving &quot;&lt;/span&gt; + srcPath + &lt;span class=&quot;code-quote&quot;&gt;&quot; to &quot;&lt;/span&gt; + contentPath);
          completeBulkLoad(tableName, contentPath);
          ...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With this, I&apos;m hitting a brand new problem.&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;2012-07-12 16:23:43,839 ERROR org.apache.hadoop.hbase.security.access.SecureBulkLoadEndPoint: Failed to complete bulk load
java.lang.NullPointerException
	at javax.security.auth.Subject$ClassSet.populateSet(Subject.java:1351)
	at javax.security.auth.Subject$ClassSet.&amp;lt;init&amp;gt;(Subject.java:1317)
	at javax.security.auth.Subject.getPrivateCredentials(Subject.java:731)
	at org.apache.hadoop.security.UserGroupInformation.&amp;lt;init&amp;gt;(UserGroupInformation.java:488)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:514)
	at org.apache.hadoop.hdfs.DFSClient.&amp;lt;init&amp;gt;(DFSClient.java:346)
	at org.apache.hadoop.hdfs.DFSClient.&amp;lt;init&amp;gt;(DFSClient.java:327)
	at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:121)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Based on this stacktrace analysis, this occurred as we continue without logging in &quot;ugi.login()&quot;.&lt;br/&gt;
But this ugi.login supports keytab/ticket cache which I don&apos;t have for &quot;test&quot; on region-server side.&lt;/p&gt;

&lt;p&gt;I think I&apos;m missing some fundamental point in token delegation.&lt;/p&gt;

&lt;p&gt;Apologies for too much of text.&lt;/p&gt;</comment>
                            <comment id="13415834" author="toffer" created="Tue, 17 Jul 2012 01:29:38 +0000"  >&lt;p&gt;Laxman, what are version are you running it on? I deployed the patch on 0.94 and didn&apos;t hit that problem (wasn&apos;t sure if trunk was deployable). I hit other problems tho, give me a day or two to iron things out and I&apos;ll post a patch. &lt;/p&gt;</comment>
                            <comment id="13415903" author="lakshman" created="Tue, 17 Jul 2012 05:18:32 +0000"  >&lt;p&gt;That&apos;s fine Francis.&lt;br/&gt;
I&apos;m also trying out on HBase 0.94.x + Hadoop 2.0.x.&lt;/p&gt;

&lt;p&gt;When you give a try, ensure the bulkload is done from a non-super user (not HBase and not HDFS).&lt;/p&gt;
</comment>
                            <comment id="13418928" author="toffer" created="Fri, 20 Jul 2012 05:32:15 +0000"  >&lt;p&gt;Laxman, here&apos;s a working patch. It incorporates &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6432&quot; title=&quot;HRegionServer doesn&amp;#39;t properly set clusterId in conf&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6432&quot;&gt;&lt;del&gt;HBASE-6432&lt;/del&gt;&lt;/a&gt; which took some time debugging. I still have to address the other comments, some cleanup and TODOs. Let me know if this works for you.&lt;/p&gt;</comment>
                            <comment id="13433064" author="ram_krish" created="Mon, 13 Aug 2012 11:08:39 +0000"  >&lt;p&gt;@Francis&lt;br/&gt;
Thanks for the latest patch.  With the latest patch bulk load seems to work fine. Thanks..&lt;br/&gt;
But one problem we are facing here is when we have permissions given on a specific qualifier.  As far i see in the code, in HRS.bulkLoadHFiles() we call the preBulkLoadHFile().&lt;br/&gt;
But the familyPaths that is created here is from the HFilePaths.  So we don&apos;t have any info on the qualifiers.  So inside the AccessController we are able to validate only the permission available on the Col families and not on qualifiers.  Pls do correct me if am missing something here.&lt;/p&gt;

</comment>
                            <comment id="13433239" author="apurtell" created="Mon, 13 Aug 2012 16:09:20 +0000"  >&lt;p&gt;@Ram Unless we have a scanner type interface for validating bulk loaded files, we can&apos;t exclude KVs on a qualifier basis. See above comment on that. Francis&apos; patch is providing the first facility, a decision point for moving files in place. Since it operates on HFiles, the best we can do as you note is check for table and column family permissions. We should document this clearly. IMHO, later we can add (if necessary) a scanner like interface for filtering by security policy the HFiles after they have been moved in place.&lt;/p&gt;</comment>
                            <comment id="13433893" author="ram_krish" created="Tue, 14 Aug 2012 05:01:13 +0000"  >&lt;p&gt;@Andy&lt;br/&gt;
Thanks for your reply.  &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;We should document this clearly&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;+1 on clear documentation atleast for now.&lt;/p&gt;</comment>
                            <comment id="13449468" author="toffer" created="Thu, 6 Sep 2012 06:47:09 +0000"  >&lt;p&gt;Here&apos;s close to the final patch. It&apos;s just missing unit tests for secure bulk load. Putting it up for initial comments.&lt;/p&gt;

&lt;p&gt;After thinking more about dealing with failure scenarios. I changed the design a bit. The secure bulk load RPC now mimics the existing bulkLoadHFiles() api. Failure became easier to deal with if all the necessary checks are done prior to staging an HFile for the actual bulk load.&lt;/p&gt;

&lt;p&gt;Given the similarity between the secure and non-secure apis. We should probably consider integrating the secure bulkload RPC into the non-security classes (ie HRegionServer, HRegion, etc.) in 0.96. Which will streamline the implementation.&lt;/p&gt;

&lt;p&gt;Usage of secure mode is now done under the covers. LoadIncrementalHFiles will automatically switch to using the secure mode if hbase security is enabled. &lt;/p&gt;



</comment>
                            <comment id="13458407" author="toffer" created="Wed, 19 Sep 2012 04:53:05 +0000"  >&lt;p&gt;patches including unit tests and some changes. Notice that the trunk patch is a rebase of 0.94 for simplicity.&lt;/p&gt;</comment>
                            <comment id="13458462" author="hadoopqa" created="Wed, 19 Sep 2012 06:52:51 +0000"  >&lt;p&gt;-1 overall.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12545667/HBASE-5498_trunk.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12545667/HBASE-5498_trunk.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    +1 @author.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    +1 tests included.  The patch appears to include 16 new or modified tests.&lt;/p&gt;

&lt;p&gt;    +1 hadoop2.0.  The patch compiles against the hadoop 2.0 profile.&lt;/p&gt;

&lt;p&gt;    -1 javadoc.  The javadoc tool appears to have generated 140 warning messages.&lt;/p&gt;

&lt;p&gt;    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    -1 findbugs.  The patch appears to introduce 14 new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    +1 release audit.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;     -1 core tests.  The patch failed these unit tests:&lt;br/&gt;
                       org.apache.hadoop.hbase.io.hfile.TestForceCacheImportantBlocks&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2896//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2896//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2896//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2896//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2896//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2896//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2896//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2896//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2896//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2896//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2896//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2896//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2896//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2896//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13465042" author="stack" created="Thu, 27 Sep 2012 20:21:11 +0000"  >&lt;p&gt;Can we have review of Francis&apos;s last patch set?  &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ram_krish&quot; class=&quot;user-hover&quot; rel=&quot;ram_krish&quot;&gt;ramkrishna.s.vasudevan&lt;/a&gt;, or &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lakshman&quot; class=&quot;user-hover&quot; rel=&quot;lakshman&quot;&gt;Laxman&lt;/a&gt;, or &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=andrew.purtell%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;andrew.purtell@gmail.com&quot;&gt;Andrew Purtell&lt;/a&gt;  Thanks.&lt;/p&gt;</comment>
                            <comment id="13465327" author="ram_krish" created="Fri, 28 Sep 2012 04:04:47 +0000"  >&lt;p&gt;@Stack&lt;br/&gt;
May be am not very much familiar with this code.  So Laxman or Andy can check this.&lt;/p&gt;</comment>
                            <comment id="13465352" author="apurtell" created="Fri, 28 Sep 2012 04:55:07 +0000"  >&lt;p&gt;I made a pass over the trunk patch. New tests look good. Seems if they pass the basics are there. The design of the new coprocessor and its protocol are ok.&lt;/p&gt;

&lt;p&gt;A couple of minor issues.&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Spelling: grep for &quot;faildBulkLoad&quot;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&quot;InternalBulkLoadListener&quot; isn&apos;t necessary because there is no &quot;BulkLoadListener&quot; &amp;#8211; just call it BulkLoadListener?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;The new &apos;// TODO deal with viewFS&apos; in HStore gives me concern. I think this should be implemented, but don&apos;t have a strong opinion. There are other places where this is going to be an issue I suspect.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;In BaseRegionObserver we have &quot;//TODO this should end up as a coprocessor hook&quot; &amp;#8211; Those proposed hooks should be added as part of this change IMO. I don&apos;t like the idea of BaseRegionObserver exporting something not part of the RegionObserver interface. It is supposed to be a default implementation of that interface not a superset.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;SecureBulkLoadEndPoint should be SecureBulkLoadEndpoint &amp;#8211; endpoint is one word.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;In SecureBulkLoadEndpoint we have &quot;//TODO make this configurable&quot; &amp;#8211; This should either be done or not?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;In SecureTestUtil, should we be loading the SecureBulkLoad support unconditionally? How about just for the relevant tests?&lt;/li&gt;
&lt;/ul&gt;


</comment>
                            <comment id="13465358" author="apurtell" created="Fri, 28 Sep 2012 05:03:25 +0000"  >&lt;p&gt;And maybe SecureBulkLoadProxy could be moved out of LoadIncrementalHFiles to a util class? Perhaps others will want to programatically import HFiles securely. &lt;/p&gt;</comment>
                            <comment id="13465376" author="yuzhihong@gmail.com" created="Fri, 28 Sep 2012 05:35:28 +0000"  >&lt;p&gt;Haven&apos;t gone through the whole trunk patch.&lt;br/&gt;
One important aspect is that SecureBulkLoadEndpoint should be implementing CoprocessorService which was introduced in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5448&quot; title=&quot;Support for dynamic coprocessor endpoints with PB-based RPC&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5448&quot;&gt;&lt;del&gt;HBASE-5448&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;License header is missing in a few new classes.&lt;/p&gt;</comment>
                            <comment id="13466933" author="toffer" created="Mon, 1 Oct 2012 16:52:19 +0000"  >&lt;p&gt;Thanks for the feedback guys. I&apos;ll post an updated patch sometime this week. &lt;/p&gt;</comment>
                            <comment id="13466938" author="toffer" created="Mon, 1 Oct 2012 16:56:46 +0000"  >&lt;p&gt;@Ted, Is coprocessorService an 0.96 enhancement? I&apos;m hoping we can address 0.96 streamlining in a separate patch since there&apos;s a bunch of work to do there already (removing some workarounds to prevent security bleeding into the core code). &lt;/p&gt;</comment>
                            <comment id="13466947" author="yuzhihong@gmail.com" created="Mon, 1 Oct 2012 17:05:57 +0000"  >&lt;blockquote&gt;&lt;p&gt;Is coprocessorService an 0.96 enhancement&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yes.&lt;br/&gt;
According to recent discussion on mailing list, support for the existing endpoint would be dropped in 0.96&lt;br/&gt;
If this patch adds another endpoint in the old way, 0.96 would be blocked further.&lt;/p&gt;

&lt;p&gt;@Stack, @Gary:&lt;br/&gt;
What do you think ?&lt;/p&gt;</comment>
                            <comment id="13466993" author="ghelmling" created="Mon, 1 Oct 2012 17:40:17 +0000"  >&lt;p&gt;Re: SecureBulkLoadEndpoint and CoprocessorService&lt;/p&gt;

&lt;p&gt;Yes the plan is to remove CoprocessorProtocol support (the old Writable implementations) for 0.96 &amp;#8211; see &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6895&quot; title=&quot;Remove CoprocessorProtocol support and implementations&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6895&quot;&gt;&lt;del&gt;HBASE-6895&lt;/del&gt;&lt;/a&gt;.  If this patch is otherwise ready (haven&apos;t reviewed it myself), I&apos;d suggest opening a new issue to deal with converting SecureBulkLoadEndpoint to a PB/CoprocessorService implementation, and link that to &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6895&quot; title=&quot;Remove CoprocessorProtocol support and implementations&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6895&quot;&gt;&lt;del&gt;HBASE-6895&lt;/del&gt;&lt;/a&gt; as a blocker for 0.96.  That way the 0.94 and 0.96 versions of this patch don&apos;t diverge and the PB conversion only needs to be addressed for 0.96.&lt;/p&gt;</comment>
                            <comment id="13467011" author="toffer" created="Mon, 1 Oct 2012 17:57:00 +0000"  >&lt;p&gt;Re: SecureBulkLoadEndpoint and CoprocessorService&lt;/p&gt;

&lt;p&gt;That sounds like a viable proposal to me given that the comments to this patch so far has been minor. Thoughts?&lt;/p&gt;</comment>
                            <comment id="13467033" author="toffer" created="Mon, 1 Oct 2012 18:13:32 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=andrew.purtell%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;andrew.purtell@gmail.com&quot;&gt;Andrew Purtell&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&quot;InternalBulkLoadListener&quot; isn&apos;t necessary because there is no &quot;BulkLoadListener&quot; &#8211; just call it BulkLoadListener?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I added internal to disambiguate as a listener to only the actual moving of the file and not a listener to the entire bulkload process which is what the coprocessor hook does. I&apos;m fine either way was worried it&apos;ll be misunderstood.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The new &apos;// TODO deal with viewFS&apos; in HStore gives me concern. I think this should be implemented, but don&apos;t have a strong opinion. There are other places where this is going to be an issue I suspect.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;My assumption was that HBase wasn&apos;t federation compatible yet. If that is true I think it&apos;s safe to push this to that future effort.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;In BaseRegionObserver we have &quot;//TODO this should end up as a coprocessor hook&quot; &#8211; Those proposed hooks should be added as part of this change IMO. I don&apos;t like the idea of BaseRegionObserver exporting something not part of the RegionObserver interface. It is supposed to be a default implementation of that interface not a superset.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I didn&apos;t add this as a coprocessor hook as these methods are security only methods which we don&apos;t want to bleed into the core code in 0.94. I added it as a TODO so we can address this in 0.96 as part of streamlining things since we don&apos;t need have the artificial security separation in that codebase?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;In SecureBulkLoadEndpoint we have &quot;//TODO make this configurable&quot; &#8211; This should either be done or not?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;It is already configurable, I seem to have forgotten to remove the todo.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;In SecureTestUtil, should we be loading the SecureBulkLoad support unconditionally? How about just for the relevant tests?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Not sure what the downside would be? Since it is expected to always be enabled in a secure deployment should it be always be available in the tests?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;And maybe SecureBulkLoadProxy could be moved out of LoadIncrementalHFiles to a util class? Perhaps others will want to programatically import HFiles securely. &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I added the proxy to prevent the security code bleeding into the core code. I extract this as a helper class if you think it&apos;s useful? It seemed to me that LoadIncrementalHFiles was the entry point for users that wanted to do bulk load as it does a lot of things that I believe users wouldn&apos;t want to re-roll again.&lt;/p&gt;</comment>
                            <comment id="13472195" author="toffer" created="Tue, 9 Oct 2012 07:27:15 +0000"  >&lt;p&gt;Updated trunk patches incorporating review comments. Except for the following:&lt;/p&gt;

&lt;p&gt;-added coprocessor hooks for the new SecureBulkLoadCoprocessors (I&apos;d like to address this as part of the coprocessor refactor for 0.96)&lt;br/&gt;
-SecureTestUtil continues to load SecureBulkLoad by default, this is better for consistency and overall testing&lt;br/&gt;
-Support for federation&lt;/p&gt;</comment>
                            <comment id="13472206" author="toffer" created="Tue, 9 Oct 2012 07:47:49 +0000"  >&lt;p&gt;attached 0.94 addressing the same issues.&lt;/p&gt;</comment>
                            <comment id="13472208" author="hadoopqa" created="Tue, 9 Oct 2012 07:53:02 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12548376/HBASE-5498_94_2.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12548376/HBASE-5498_94_2.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 tests included&lt;/font&gt;.  The patch appears to include 19 new or modified tests.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 patch&lt;/font&gt;.  The patch command could not apply the patch.&lt;/p&gt;

&lt;p&gt;Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3021//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3021//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13472217" author="hadoopqa" created="Tue, 9 Oct 2012 08:33:01 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12548374/HBASE-5498_trunk_2.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12548374/HBASE-5498_trunk_2.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 tests included&lt;/font&gt;.  The patch appears to include 16 new or modified tests.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 hadoop2.0&lt;/font&gt;.  The patch compiles against the hadoop 2.0 profile.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 javadoc&lt;/font&gt;.  The javadoc tool appears to have generated 82 warning messages.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 findbugs&lt;/font&gt;.  The patch appears to introduce 5 new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;     &lt;font color=&quot;red&quot;&gt;-1 core tests&lt;/font&gt;.  The patch failed these unit tests:&lt;br/&gt;
                       org.apache.hadoop.hbase.replication.TestReplication&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3020//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3020//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3020//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3020//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3020//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3020//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3020//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3020//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3020//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3020//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3020//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3020//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3020//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3020//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13478533" author="toffer" created="Thu, 18 Oct 2012 00:40:45 +0000"  >&lt;p&gt;fixed a filename typo.&lt;/p&gt;</comment>
                            <comment id="13478628" author="toffer" created="Thu, 18 Oct 2012 03:36:58 +0000"  >&lt;p&gt;fixed a filename typo.&lt;/p&gt;</comment>
                            <comment id="13478630" author="toffer" created="Thu, 18 Oct 2012 03:38:27 +0000"  >&lt;p&gt;Hi Guys, can we get a resolution on wether I have to convert the trunk patch to the new implementation?&lt;/p&gt;</comment>
                            <comment id="13478646" author="yuzhihong@gmail.com" created="Thu, 18 Oct 2012 04:19:26 +0000"  >&lt;p&gt;To convert SecureBulkLoadProtocol to PB, you can use &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6785&quot; title=&quot;Convert AggregateProtocol to protobuf defined coprocessor service&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6785&quot;&gt;&lt;del&gt;HBASE-6785&lt;/del&gt;&lt;/a&gt; &apos;Convert AggregateProtocol to protobuf defined coprocessor service&apos; as an example.&lt;br/&gt;
prepareBulkLoad() and cleanupBulkLoad() are relatively straightforward.&lt;br/&gt;
For bulkLoadHFiles(), please refer to message BulkLoadHFileRequest in Client.proto&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;</comment>
                            <comment id="13478694" author="hadoopqa" created="Thu, 18 Oct 2012 05:41:28 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12549633/HBASE-5498_trunk_3.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12549633/HBASE-5498_trunk_3.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 tests included&lt;/font&gt;.  The patch appears to include 16 new or modified tests.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 hadoop2.0&lt;/font&gt;.  The patch compiles against the hadoop 2.0 profile.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 javadoc&lt;/font&gt;.  The javadoc tool appears to have generated 83 warning messages.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 findbugs&lt;/font&gt;.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 core tests&lt;/font&gt;.  The patch passed unit tests in .&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3073//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3073//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3073//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3073//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3073//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3073//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3073//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3073//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3073//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3073//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3073//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3073//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3073//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3073//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13478736" author="hadoopqa" created="Thu, 18 Oct 2012 06:51:27 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12549633/HBASE-5498_trunk_3.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12549633/HBASE-5498_trunk_3.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 tests included&lt;/font&gt;.  The patch appears to include 16 new or modified tests.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 hadoop2.0&lt;/font&gt;.  The patch compiles against the hadoop 2.0 profile.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 javadoc&lt;/font&gt;.  The javadoc tool appears to have generated 83 warning messages.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 findbugs&lt;/font&gt;.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;     &lt;font color=&quot;red&quot;&gt;-1 core tests&lt;/font&gt;.  The patch failed these unit tests:&lt;br/&gt;
                       org.apache.hadoop.hbase.client.TestMultiParallel&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3074//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3074//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3074//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3074//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3074//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3074//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3074//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3074//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3074//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3074//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3074//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3074//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3074//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3074//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13486300" author="apurtell" created="Mon, 29 Oct 2012 20:02:24 +0000"  >&lt;p&gt;Yes, for trunk we need coprocessors to use PB.&lt;/p&gt;</comment>
                            <comment id="13488090" author="lhofhansl" created="Wed, 31 Oct 2012 18:56:18 +0000"  >&lt;p&gt;Moving to 0.94.4&lt;/p&gt;</comment>
                            <comment id="13489303" author="toffer" created="Fri, 2 Nov 2012 08:02:00 +0000"  >&lt;p&gt;protobuff version for trunk&lt;/p&gt;</comment>
                            <comment id="13489309" author="toffer" created="Fri, 2 Nov 2012 08:27:13 +0000"  >&lt;p&gt;Though I&apos;m wondering why it&apos;s necessary to add &quot;throws Throwable&quot; to HTable.coprocessorService?&lt;/p&gt;</comment>
                            <comment id="13489316" author="hadoopqa" created="Fri, 2 Nov 2012 09:05:02 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12551833/HBASE-5498_trunk_4.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12551833/HBASE-5498_trunk_4.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 tests included&lt;/font&gt;.  The patch appears to include 16 new or modified tests.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 hadoop2.0&lt;/font&gt;.  The patch compiles against the hadoop 2.0 profile.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 javadoc&lt;/font&gt;.  The javadoc tool appears to have generated 85 warning messages.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 findbugs&lt;/font&gt;.  The patch appears to introduce 3 new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;     &lt;font color=&quot;red&quot;&gt;-1 core tests&lt;/font&gt;.  The patch failed these unit tests:&lt;br/&gt;
                       org.apache.hadoop.hbase.client.TestFromClientSide&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3214//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3214//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3214//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3214//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3214//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3214//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3214//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3214//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3214//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3214//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3214//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3214//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3214//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3214//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13489860" author="zhihyu@ebaysf.com" created="Sat, 3 Nov 2012 00:08:32 +0000"  >&lt;p&gt;Comment for trunk patch:&lt;br/&gt;
I agree that HTable.coprocessorService doesn&apos;t need to throw Throwable.&lt;br/&gt;
If you don&apos;t want to use the try/catch block, you can follow the example of AggregationClient:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &amp;lt;R, S&amp;gt; R max(&lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt;[] tableName, &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; ColumnInterpreter&amp;lt;R, S&amp;gt; ci,
      &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; Scan scan) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; Throwable {
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+ */
+&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; class SecureBulkLoadClient {
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Please add annotation for new public classes.&lt;br/&gt;
For LoadIncrementalHFiles:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+  &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;boolean&lt;/span&gt; useSecure;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Should the above field be named useSecurity or secureLoad ?&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt;(useSecure) {
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;nit: insert space between if and (.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+          } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (InterruptedException e) {
+            LOG.warn(&lt;span class=&quot;code-quote&quot;&gt;&quot;Failed to cancel HDFS delegation token.&quot;&lt;/span&gt;, e);
+          }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Please restore interrupt status above.&lt;/p&gt;

&lt;p&gt;More reviews to follow.&lt;/p&gt;</comment>
                            <comment id="13489881" author="toffer" created="Sat, 3 Nov 2012 00:41:37 +0000"  >&lt;p&gt;Thanks Ted, I forgot to put up the reviewboard link:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://reviews.apache.org/r/7849/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.apache.org/r/7849/&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13535736" author="lhofhansl" created="Wed, 19 Dec 2012 07:17:31 +0000"  >&lt;p&gt;Moving out to 0.94.5. We should finish this, though. Not sure why it got stuck.&lt;/p&gt;</comment>
                            <comment id="13536293" author="apurtell" created="Wed, 19 Dec 2012 18:52:24 +0000"  >&lt;p&gt;My guess is the impact of the PB conversion in trunk became a time sink. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=toffer&quot; class=&quot;user-hover&quot; rel=&quot;toffer&quot;&gt;Francis Liu&lt;/a&gt; how would you like to proceed here? Carry it over the finish line, or transfer to me for same?&lt;/p&gt;</comment>
                            <comment id="13545681" author="toffer" created="Mon, 7 Jan 2013 07:14:37 +0000"  >&lt;p&gt;I&apos;ll finish it, I&apos;ll address Ted&apos;s comments this week. Sorry for the delay, got caught up with internal releases.&lt;/p&gt;</comment>
                            <comment id="13552514" author="toffer" created="Mon, 14 Jan 2013 08:43:18 +0000"  >&lt;p&gt;updated trunk patch. also uploaded 0.94 patch reflecting comments from trunk:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://reviews.apache.org/r/8942/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.apache.org/r/8942/&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13553078" author="hadoopqa" created="Mon, 14 Jan 2013 20:22:30 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12564756/HBASE-5498_trunk_2.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12564756/HBASE-5498_trunk_2.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 tests included&lt;/font&gt;.  The patch appears to include 16 new or modified tests.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 hadoop2.0&lt;/font&gt;.  The patch compiles against the hadoop 2.0 profile.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javadoc&lt;/font&gt;.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 findbugs&lt;/font&gt;.  The patch appears to introduce 2 new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 lineLengths&lt;/font&gt;.  The patch introduces lines longer than 100&lt;/p&gt;

&lt;p&gt;     &lt;font color=&quot;red&quot;&gt;-1 core tests&lt;/font&gt;.  The patch failed these unit tests:&lt;br/&gt;
                       org.apache.hadoop.hbase.TestLocalHBaseCluster&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4006//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4006//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4006//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4006//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4006//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4006//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4006//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4006//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4006//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4006//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4006//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4006//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4006//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4006//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4006//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4006//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4006//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4006//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13553235" author="apurtell" created="Mon, 14 Jan 2013 23:00:58 +0000"  >&lt;p&gt;Thanks for putting up the latest &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=toffer&quot; class=&quot;user-hover&quot; rel=&quot;toffer&quot;&gt;Francis Liu&lt;/a&gt;. Applying this locally to run tests.&lt;/p&gt;</comment>
                            <comment id="13553464" author="apurtell" created="Tue, 15 Jan 2013 03:46:29 +0000"  >&lt;p&gt;Tests pass locally. Minor issue on trunk patch is all proto files (and generated source) are now in the hbase-protocol module, instead of hbase-server. Could be fixed up on commit. +1&lt;/p&gt;</comment>
                            <comment id="13553490" author="toffer" created="Tue, 15 Jan 2013 05:01:06 +0000"  >&lt;p&gt;updated patch moved, SecureBulkLoad.proto and generated file to proto module&lt;/p&gt;</comment>
                            <comment id="13553534" author="hadoopqa" created="Tue, 15 Jan 2013 06:02:52 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12564873/HBASE-5498_trunk_5.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12564873/HBASE-5498_trunk_5.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 tests included&lt;/font&gt;.  The patch appears to include 16 new or modified tests.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 hadoop2.0&lt;/font&gt;.  The patch compiles against the hadoop 2.0 profile.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javadoc&lt;/font&gt;.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 findbugs&lt;/font&gt;.  The patch appears to introduce 2 new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 lineLengths&lt;/font&gt;.  The patch introduces lines longer than 100&lt;/p&gt;

&lt;p&gt;     &lt;font color=&quot;red&quot;&gt;-1 core tests&lt;/font&gt;.  The patch failed these unit tests:&lt;br/&gt;
                       org.apache.hadoop.hbase.client.TestFromClientSide&lt;br/&gt;
                  org.apache.hadoop.hbase.TestLocalHBaseCluster&lt;/p&gt;

&lt;p&gt;     &lt;font color=&quot;red&quot;&gt;-1 core zombie tests&lt;/font&gt;.  There are 1 zombie test(s): &lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4020//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4020//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4020//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4020//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4020//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4020//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4020//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4020//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4020//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4020//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4020//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4020//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4020//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4020//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4020//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4020//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4020//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4020//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13553875" author="yuzhihong@gmail.com" created="Tue, 15 Jan 2013 15:21:05 +0000"  >&lt;p&gt;Integrated to trunk.&lt;br/&gt;
Wrapped long line in SecureBulkLoadEndpoint.java&lt;/p&gt;

&lt;p&gt;Thanks for the patch, Francis.&lt;/p&gt;

&lt;p&gt;Thanks for the review, Andy.&lt;/p&gt;</comment>
                            <comment id="13553957" author="hudson" created="Tue, 15 Jan 2013 16:27:35 +0000"  >&lt;p&gt;Integrated in HBase-TRUNK #3750 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-TRUNK/3750/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-TRUNK/3750/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5498&quot; title=&quot;Secure Bulk Load&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5498&quot;&gt;&lt;del&gt;HBASE-5498&lt;/del&gt;&lt;/a&gt; Secure Bulk Load (Francis Liu) (Revision 1433452)&lt;/p&gt;

&lt;p&gt;     Result = FAILURE&lt;br/&gt;
tedyu : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/trunk/hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/SecureBulkLoadProtos.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-protocol/src/main/protobuf/SecureBulkLoad.proto&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/client/coprocessor/SecureBulkLoadClient.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/LoadIncrementalHFiles.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/SecureBulkLoadEndpoint.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestLoadIncrementalHFiles.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestLoadIncrementalHFilesSplitRecovery.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestSecureLoadIncrementalHFiles.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestSecureLoadIncrementalHFilesSplitRecovery.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/SecureTestUtil.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestAccessController.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13554018" author="yuzhihong@gmail.com" created="Tue, 15 Jan 2013 17:20:28 +0000"  >&lt;p&gt;Integrated to 0.94 as well.&lt;/p&gt;</comment>
                            <comment id="13554191" author="toffer" created="Tue, 15 Jan 2013 19:25:46 +0000"  >&lt;p&gt;Thanks Andy and Ted!&lt;/p&gt;</comment>
                            <comment id="13554192" author="hudson" created="Tue, 15 Jan 2013 19:26:03 +0000"  >&lt;p&gt;Integrated in HBase-0.94 #735 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-0.94/735/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-0.94/735/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5498&quot; title=&quot;Secure Bulk Load&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5498&quot;&gt;&lt;del&gt;HBASE-5498&lt;/del&gt;&lt;/a&gt; Secure Bulk Load (Francis Liu) (Revision 1433532)&lt;/p&gt;

&lt;p&gt;     Result = SUCCESS&lt;br/&gt;
tedyu : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/branches/0.94/security/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/security/src/main/java/org/apache/hadoop/hbase/security/access/SecureBulkLoadEndpoint.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/security/src/main/java/org/apache/hadoop/hbase/security/access/SecureBulkLoadProtocol.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/security/src/test/java/org/apache/hadoop/hbase/mapreduce&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/security/src/test/java/org/apache/hadoop/hbase/mapreduce/TestSecureLoadIncrementalHFiles.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/security/src/test/java/org/apache/hadoop/hbase/mapreduce/TestSecureLoadIncrementalHFilesSplitRecovery.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/security/src/test/java/org/apache/hadoop/hbase/security/access/SecureTestUtil.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/security/src/test/java/org/apache/hadoop/hbase/security/access/TestAccessController.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/src/main/java/org/apache/hadoop/hbase/coprocessor/SecureBulkLoadClient.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/src/main/java/org/apache/hadoop/hbase/mapreduce/LoadIncrementalHFiles.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/src/main/java/org/apache/hadoop/hbase/regionserver/Store.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/src/test/java/org/apache/hadoop/hbase/mapreduce/TestLoadIncrementalHFiles.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/src/test/java/org/apache/hadoop/hbase/mapreduce/TestLoadIncrementalHFilesSplitRecovery.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13554214" author="hudson" created="Tue, 15 Jan 2013 19:40:16 +0000"  >&lt;p&gt;Integrated in HBase-0.94-security #95 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-0.94-security/95/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-0.94-security/95/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5498&quot; title=&quot;Secure Bulk Load&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5498&quot;&gt;&lt;del&gt;HBASE-5498&lt;/del&gt;&lt;/a&gt; Secure Bulk Load (Francis Liu) (Revision 1433532)&lt;/p&gt;

&lt;p&gt;     Result = SUCCESS&lt;br/&gt;
tedyu : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/branches/0.94/security/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/security/src/main/java/org/apache/hadoop/hbase/security/access/SecureBulkLoadEndpoint.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/security/src/main/java/org/apache/hadoop/hbase/security/access/SecureBulkLoadProtocol.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/security/src/test/java/org/apache/hadoop/hbase/mapreduce&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/security/src/test/java/org/apache/hadoop/hbase/mapreduce/TestSecureLoadIncrementalHFiles.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/security/src/test/java/org/apache/hadoop/hbase/mapreduce/TestSecureLoadIncrementalHFilesSplitRecovery.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/security/src/test/java/org/apache/hadoop/hbase/security/access/SecureTestUtil.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/security/src/test/java/org/apache/hadoop/hbase/security/access/TestAccessController.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/src/main/java/org/apache/hadoop/hbase/coprocessor/SecureBulkLoadClient.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/src/main/java/org/apache/hadoop/hbase/mapreduce/LoadIncrementalHFiles.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/src/main/java/org/apache/hadoop/hbase/regionserver/Store.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/src/test/java/org/apache/hadoop/hbase/mapreduce/TestLoadIncrementalHFiles.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/src/test/java/org/apache/hadoop/hbase/mapreduce/TestLoadIncrementalHFilesSplitRecovery.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13554247" author="apurtell" created="Tue, 15 Jan 2013 19:53:00 +0000"  >&lt;p&gt;Thank you for sticking with this Francis!&lt;/p&gt;</comment>
                            <comment id="13554646" author="hudson" created="Wed, 16 Jan 2013 02:04:14 +0000"  >&lt;p&gt;Integrated in HBase-TRUNK-on-Hadoop-2.0.0 #349 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-2.0.0/349/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-2.0.0/349/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5498&quot; title=&quot;Secure Bulk Load&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5498&quot;&gt;&lt;del&gt;HBASE-5498&lt;/del&gt;&lt;/a&gt; Secure Bulk Load (Francis Liu) (Revision 1433452)&lt;/p&gt;

&lt;p&gt;     Result = FAILURE&lt;br/&gt;
tedyu : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/trunk/hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/SecureBulkLoadProtos.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-protocol/src/main/protobuf/SecureBulkLoad.proto&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/client/coprocessor/SecureBulkLoadClient.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/LoadIncrementalHFiles.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/SecureBulkLoadEndpoint.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestLoadIncrementalHFiles.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestLoadIncrementalHFilesSplitRecovery.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestSecureLoadIncrementalHFiles.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestSecureLoadIncrementalHFilesSplitRecovery.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/SecureTestUtil.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestAccessController.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13570974" author="hudson" created="Tue, 5 Feb 2013 03:58:24 +0000"  >&lt;p&gt;Integrated in HBase-0.94-security-on-Hadoop-23 #11 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-0.94-security-on-Hadoop-23/11/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-0.94-security-on-Hadoop-23/11/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5498&quot; title=&quot;Secure Bulk Load&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5498&quot;&gt;&lt;del&gt;HBASE-5498&lt;/del&gt;&lt;/a&gt; Secure Bulk Load (Francis Liu) (Revision 1433532)&lt;/p&gt;

&lt;p&gt;     Result = FAILURE&lt;br/&gt;
tedyu : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/branches/0.94/security/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/security/src/main/java/org/apache/hadoop/hbase/security/access/SecureBulkLoadEndpoint.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/security/src/main/java/org/apache/hadoop/hbase/security/access/SecureBulkLoadProtocol.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/security/src/test/java/org/apache/hadoop/hbase/mapreduce&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/security/src/test/java/org/apache/hadoop/hbase/mapreduce/TestSecureLoadIncrementalHFiles.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/security/src/test/java/org/apache/hadoop/hbase/mapreduce/TestSecureLoadIncrementalHFilesSplitRecovery.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/security/src/test/java/org/apache/hadoop/hbase/security/access/SecureTestUtil.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/security/src/test/java/org/apache/hadoop/hbase/security/access/TestAccessController.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/src/main/java/org/apache/hadoop/hbase/coprocessor/SecureBulkLoadClient.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/src/main/java/org/apache/hadoop/hbase/mapreduce/LoadIncrementalHFiles.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/src/main/java/org/apache/hadoop/hbase/regionserver/Store.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/src/test/java/org/apache/hadoop/hbase/mapreduce/TestLoadIncrementalHFiles.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/src/test/java/org/apache/hadoop/hbase/mapreduce/TestLoadIncrementalHFilesSplitRecovery.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13587816" author="stack" created="Wed, 27 Feb 2013 00:58:07 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=toffer&quot; class=&quot;user-hover&quot; rel=&quot;toffer&quot;&gt;Francis Liu&lt;/a&gt; Frances, what was the intent below?&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+  &lt;span class=&quot;code-comment&quot;&gt;//Disabling &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; test as it does not work in secure mode
&lt;/span&gt;+  @Test
+  @Override
+  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void testBulkLoadPhaseFailure() {
+  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Did you mean to disable the test?&lt;/p&gt;

&lt;p&gt;Thanks.&lt;/p&gt;</comment>
                            <comment id="13587828" author="enis" created="Wed, 27 Feb 2013 01:12:47 +0000"  >&lt;p&gt;I think by overriding the method in the subclass with a nop implementation, he is effectively disabling the test. &lt;/p&gt;</comment>
                            <comment id="13589930" author="toffer" created="Thu, 28 Feb 2013 21:14:38 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=saint.ack%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;saint.ack@gmail.com&quot;&gt;Stack&lt;/a&gt; Yep, the purpose was to disable the test. Overriding the method wasn&apos;t enough. I can add an Ignore annotation to have the test skipped if you&apos;re concern is it popping up in the test results?&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10032">
                    <name>Blocker</name>
                                                                <inwardlinks description="is blocked by">
                                        <issuelink>
            <issuekey id="12599576">HBASE-6432</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="12599324">HBASE-6422</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="12310010">
                    <name>Incorporates</name>
                                                                <inwardlinks description="is part of">
                                        <issuelink>
            <issuekey id="12558155">HBASE-6101</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12558146">HBASE-6096</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12545666" name="HBASE-5498_94.patch" size="48319" author="toffer" created="Wed, 19 Sep 2012 04:51:15 +0000"/>
                            <attachment id="12543998" name="HBASE-5498_94.patch" size="38275" author="toffer" created="Thu, 6 Sep 2012 06:47:09 +0000"/>
                            <attachment id="12548376" name="HBASE-5498_94_2.patch" size="50599" author="toffer" created="Tue, 9 Oct 2012 07:47:49 +0000"/>
                            <attachment id="12564874" name="HBASE-5498_94_3.patch" size="52771" author="toffer" created="Tue, 15 Jan 2013 05:11:44 +0000"/>
                            <attachment id="12549621" name="HBASE-5498_94_3.patch" size="51069" author="toffer" created="Thu, 18 Oct 2012 00:40:45 +0000"/>
                            <attachment id="12531228" name="HBASE-5498_draft.patch" size="22892" author="toffer" created="Thu, 7 Jun 2012 06:34:02 +0000"/>
                            <attachment id="12537297" name="HBASE-5498_draft_94.patch" size="18345" author="toffer" created="Fri, 20 Jul 2012 05:32:15 +0000"/>
                            <attachment id="12545667" name="HBASE-5498_trunk.patch" size="47585" author="toffer" created="Wed, 19 Sep 2012 04:53:05 +0000"/>
                            <attachment id="12564756" name="HBASE-5498_trunk_2.patch" size="228645" author="yuzhihong@gmail.com" created="Mon, 14 Jan 2013 19:21:05 +0000"/>
                            <attachment id="12548374" name="HBASE-5498_trunk_2.patch" size="50822" author="toffer" created="Tue, 9 Oct 2012 07:27:15 +0000"/>
                            <attachment id="12549633" name="HBASE-5498_trunk_3.patch" size="50916" author="toffer" created="Thu, 18 Oct 2012 03:36:58 +0000"/>
                            <attachment id="12551833" name="HBASE-5498_trunk_4.patch" size="227397" author="toffer" created="Fri, 2 Nov 2012 08:02:00 +0000"/>
                            <attachment id="12564873" name="HBASE-5498_trunk_5.patch" size="228657" author="toffer" created="Tue, 15 Jan 2013 05:01:06 +0000"/>
                    </attachments>
                <subtasks>
                            <subtask id="12593986">HBASE-6224</subtask>
                            <subtask id="12593987">HBASE-6225</subtask>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>13.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 1 Mar 2012 01:33:01 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>229884</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            3 years, 40 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i00rcf:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2362</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310192" key="com.atlassian.jira.plugin.system.customfieldtypes:textarea">
                        <customfieldname>Release Note</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>This feature adds a new optional configuration parameter:&lt;br/&gt;
&lt;br/&gt;
hbase.bulkload.staging.dir&lt;br/&gt;
&lt;br/&gt;
This defines the path on DFS that HBase will use to create random/secret directories under.&lt;br/&gt;
The default location is /tmp/hbase-staging</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>