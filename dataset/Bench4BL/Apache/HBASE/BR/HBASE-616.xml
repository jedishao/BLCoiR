<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sat Dec 03 18:09:09 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-616/HBASE-616.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-616] &quot; We slept XXXXXX ms, ten times longer than scheduled: 3000&quot; happens frequently.</title>
                <link>https://issues.apache.org/jira/browse/HBASE-616</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Just saw the below in a log... all in a row on the one server.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
   4493 2008-05-05 18:08:17,512 WARN org.apache.hadoop.hbase.util.Sleeper: We slept 34557ms, ten times longer than scheduled: 3000
   4494 2008-05-05 18:11:08,879 WARN org.apache.hadoop.hbase.util.Sleeper: We slept 30576ms, ten times longer than scheduled: 3000
   4495 2008-05-05 18:30:45,056 WARN org.apache.hadoop.hbase.util.Sleeper: We slept 1091720ms, ten times longer than scheduled: 3000
   4496 2008-05-05 18:30:45,056 WARN org.apache.hadoop.hbase.util.Sleeper: We slept 1094209ms, ten times longer than scheduled: 10000
   4497 2008-05-05 18:30:45,429 FATAL org.apache.hadoop.hbase.HRegionServer: unable to report to master &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 1092093 milliseconds - aborting server
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We&apos;re seeing these kinda outages pretty frequently.  In the case above, it was small cluster that was using TableReduce to insert.  The MR, HDFS and HBase were all running on same nodes.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12395381">HBASE-616</key>
            <summary>&quot; We slept XXXXXX ms, ten times longer than scheduled: 3000&quot; happens frequently.</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="6">Invalid</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="stack">stack</reporter>
                        <labels>
                    </labels>
                <created>Tue, 6 May 2008 04:01:22 +0000</created>
                <updated>Wed, 8 Jul 2009 20:23:52 +0000</updated>
                            <resolved>Wed, 8 Jul 2009 20:23:52 +0000</resolved>
                                                                        <due></due>
                            <votes>1</votes>
                                    <watches>2</watches>
                                                                <comments>
                            <comment id="12595016" author="stack" created="Wed, 7 May 2008 19:50:54 +0000"  >&lt;p&gt;Looking at a log which has a bunch of these in it &amp;#8211; the user has set timeout to 3minutes instead of default one minute &amp;#8211; and they seem to happen when a compaction is running.  It may be possible that compaction on weaker boxes can make it so other threads are starved.&lt;/p&gt;</comment>
                            <comment id="12595023" author="stack" created="Wed, 7 May 2008 20:01:58 +0000"  >&lt;p&gt;Watching logs on this &apos;weaker&apos; machine, I can predict when I&apos;ll see &apos;... ten times longer...&apos; messages; they come in middle of compaction.  Compacting sends the load way up from about 1 up to 4 or so on these machines (they are running datanodes concurrently).&lt;/p&gt;</comment>
                            <comment id="12603736" author="viper799" created="Tue, 10 Jun 2008 00:21:46 +0000"  >&lt;p&gt;I am seeing the same stuff when one of my servers are vary loaded also see this more often when the server is havening to swap for memory.&lt;br/&gt;
This was causing my regions server to lose there leases so I had to extent the time in case one of these timeout happens longer then 30 sec.&lt;/p&gt;

&lt;p&gt;Also not when this happens on a region server that is hosting the meta table the master starts to hang up some too looking to scan the meta table.&lt;br/&gt;
Seams the only time I see these are when the server are under heavy load and is maxed out on resources.&lt;/p&gt;</comment>
                            <comment id="12615023" author="apurtell" created="Sat, 19 Jul 2008 17:07:41 +0000"  >&lt;p&gt;I think this issue is related to &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-15&quot; title=&quot;[hbase] Could not complete hdfs write out to flush file forcing regionserver restart&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-15&quot;&gt;&lt;del&gt;HBASE-15&lt;/del&gt;&lt;/a&gt;. DFS transaction timeouts or excessive sleeps are both indications of excessive system load. &lt;/p&gt;</comment>
                            <comment id="12650074" author="stack" created="Sun, 23 Nov 2008 23:06:25 +0000"  >&lt;p&gt;Seeing this on the streamy test cluster.  2 core machines.  Whats up with our JVM scheduler such that a thread can get no time for periods that are &amp;gt; 10X the sleep time.  Is there something better than Thread.sleep that we should be using instead?&lt;/p&gt;</comment>
                            <comment id="12650076" author="stack" created="Sun, 23 Nov 2008 23:07:39 +0000"  >&lt;p&gt;Here&apos;s log excerpt:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2008-11-22 17:12:42,426 INFO org.apache.hadoop.hbase.regionserver.HLog: New log writer created at /hbase/log_XX.XX.249.103_1227396990675_60020/hlog.dat.1227402762424
2008-11-22 17:47:47,792 WARN org.apache.hadoop.hbase.util.Sleeper: We slept 193824ms, ten times longer than scheduled: 3000
2008-11-22 17:47:47,792 WARN org.apache.hadoop.hbase.util.Sleeper: We slept 193115ms, ten times longer than scheduled: 10000
2008-11-22 17:47:48,113 WARN org.apache.hadoop.hbase.regionserver.HRegionServer: unable to report to master &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 194203 milliseconds - retrying
2008-11-22 17:47:48,395 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: MSG_CALL_SERVER_STARTUP
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When it finally checks in, master tells it restart: MSG_CALL_SERVER_STARTUP.&lt;/p&gt;</comment>
                            <comment id="12650079" author="stack" created="Sun, 23 Nov 2008 23:24:23 +0000"  >&lt;p&gt;Would be interesting to figure if a long-running GC was going on concurrently?  If so, perhaps as suggested by Rong-en a while back, the concurrent low-pause collector might work better when under load: &lt;a href=&quot;http://java.sun.com/javase/technologies/hotspot/gc/gc_tuning_6.html#icms.available_options&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://java.sun.com/javase/technologies/hotspot/gc/gc_tuning_6.html#icms.available_options&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="12651222" author="stack" created="Thu, 27 Nov 2008 02:08:56 +0000"  >&lt;p&gt;jgray ran his uploader. Oddly, all seemed to work fine and the upload seemed to have finished nicely but then the gc log hung like this:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
4702.183: [GC 723285K-&amp;gt;644523K(903424K), 0.1020550 secs]
4741.964: [GC 751595K-&amp;gt;674674K(903488K), 0.1030280 secs]
5386.336: [GC 781746K-&amp;gt;698672K(903744K), 299.1547740 secs]
5685.743: [Full GC
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It was hung after the emission of &apos;Full GC&apos;.&lt;/p&gt;

&lt;p&gt;Eventually it cleared with the GC having been hung for 400 seconds:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
4643.716: [GC 693488K-&amp;gt;616512K(897216K), 0.0775520 secs]
4702.183: [GC 723285K-&amp;gt;644523K(903424K), 0.1020550 secs]
4741.964: [GC 751595K-&amp;gt;674674K(903488K), 0.1030280 secs]
5386.336: [GC 781746K-&amp;gt;698672K(903744K), 299.1547740 secs]
5685.743: [Full GC 698672K-&amp;gt;694148K(903744K), 434.7056360 secs]
6161.052: [Full GC 789962K-&amp;gt;630359K(903744K), 23.9435320 secs]
6197.831: [Full GC 737687K-&amp;gt;478895K(903744K), 28.7396540 secs]
6261.250: [GC 586222K-&amp;gt;481646K(903744K), 0.0385590 secs]
6305.213: [GC 588974K-&amp;gt;481846K(905472K), 0.1706230 secs]
6359.292: [GC 591478K-&amp;gt;482681K(906048K), 0.0147330 secs]
6469.725: [GC 592313K-&amp;gt;482763K(907072K), 0.0059020 secs]
...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Looking in ganglia, the whole cluster had gone into heavy swap.  Jon looking at top, etc., saw that the machines had loads of free memory.  Could not explain why swap, low CPU, and free memory combination.&lt;/p&gt;

&lt;p&gt;Whatever the cause of cluster swap seems to be hanging GC upsetting hbase.&lt;/p&gt;

&lt;p&gt;Looking in logs, nothing really obvious though hdfs and deletes are suspect (pset has reported deletes being problematic in past, so has streamy).  See a bunch of deleted commit files around that time but then looking at block deletes in datanode, they seem to be pretty constantly deleting bunches over time.  Datanodes at the time were handling startup of a new MR job which was opening loads of files.  &lt;/p&gt;

&lt;p&gt;One thing to try is a rerun with delete of commit logs disabled.&lt;/p&gt;</comment>
                            <comment id="12651223" author="stack" created="Thu, 27 Nov 2008 02:10:15 +0000"  >&lt;p&gt;Moved to 0.19 because we&apos;re starting to get a handle on this issue.  Could be root of &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-900&quot; title=&quot;Regionserver memory leak causing OOME during relatively modest bulk importing&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-900&quot;&gt;&lt;del&gt;HBASE-900&lt;/del&gt;&lt;/a&gt; too among other things (Lots of long GC&apos;ing provoking OOME).&lt;/p&gt;</comment>
                            <comment id="12652507" author="stack" created="Tue, 2 Dec 2008 21:04:46 +0000"  >&lt;p&gt;Moving out of 0.19.0.   Can bring back in for 0.19.1 if we figure it meantime.  Otherwise, 0.20.0.&lt;/p&gt;</comment>
                            <comment id="12701646" author="stack" created="Wed, 22 Apr 2009 19:01:31 +0000"  >&lt;p&gt;This is looking to be a symptom of full GC&apos;ing. &lt;/p&gt;</comment>
                            <comment id="12701676" author="viper799" created="Wed, 22 Apr 2009 20:46:53 +0000"  >&lt;p&gt;I use&lt;br/&gt;
-Xloggc:/tmp/gc.log&lt;br/&gt;
on region server&lt;br/&gt;
and when I see these messages there is always a Full GC happening at the same time&lt;/p&gt;
</comment>
                            <comment id="12702595" author="stack" created="Fri, 24 Apr 2009 22:47:47 +0000"  >&lt;p&gt;We likely ain&apos;t going to fix what is seemingly a GC issue for 0.20.0.  Improvements and suggested config. for GC we can stick up on wiki but not a fix I&apos;d say for 0.20.0 timeframe.  Moving out.&lt;/p&gt;</comment>
                            <comment id="12728880" author="stack" created="Wed, 8 Jul 2009 20:23:52 +0000"  >&lt;p&gt;Resolving invalid, as a symptom of a stalled GC.  Wiki has GC tuning steps to help minimize stalls.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="12410625">HBASE-1060</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12383573">HBASE-15</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12387927">HBASE-412</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Tue, 10 Jun 2008 00:21:46 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>25294</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 22 weeks, 2 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0h8ef:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>98623</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>