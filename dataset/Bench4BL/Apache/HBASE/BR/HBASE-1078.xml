<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 02 20:15:25 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-1078/HBASE-1078.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-1078] &quot;java.io.IOException: Could not obtain block&quot;: allthough file is there and accessible through the dfs client</title>
                <link>https://issues.apache.org/jira/browse/HBASE-1078</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Hi,&lt;br/&gt;
after doing some more stress testing, my cluster did just stopped working. The regionserver reponsible for the ROOT region can&apos;t read a block related to the root region, but it is definitely there as I can read the file through the dfs client.&lt;/p&gt;

&lt;p&gt;All new clients fail to start:&lt;/p&gt;

&lt;p&gt;java.io.IOException: java.io.IOException: Could not obtain block: blk_&lt;del&gt;3504243288385983835_18732 file=/hbase/-ROOT&lt;/del&gt;/70236052/info/mapfiles/780254459775584115/data&lt;br/&gt;
        at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.chooseDataNode(DFSClient.java:1708)&lt;br/&gt;
        at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.blockSeekTo(DFSClient.java:1536)&lt;br/&gt;
        at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.read(DFSClient.java:1663)&lt;br/&gt;
        at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.read(DFSClient.java:1593)&lt;br/&gt;
        at java.io.DataInputStream.readInt(DataInputStream.java:370)&lt;br/&gt;
        at org.apache.hadoop.io.SequenceFile$Reader.readRecordLength(SequenceFile.java:1895)&lt;br/&gt;
        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1925)&lt;br/&gt;
        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1830)&lt;br/&gt;
        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1876)&lt;br/&gt;
        at org.apache.hadoop.io.MapFile$Reader.next(MapFile.java:517)&lt;br/&gt;
        at org.apache.hadoop.hbase.regionserver.HStore.rowAtOrBeforeFromMapFile(HStore.java:1709)&lt;br/&gt;
        at org.apache.hadoop.hbase.regionserver.HStore.getRowKeyAtOrBefore(HStore.java:1681)&lt;br/&gt;
        at org.apache.hadoop.hbase.regionserver.HRegion.getClosestRowBefore(HRegion.java:1072)&lt;br/&gt;
        at org.apache.hadoop.hbase.regionserver.HRegionServer.getClosestRowBefore(HRegionServer.java:1466)&lt;br/&gt;
        at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)&lt;br/&gt;
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)&lt;br/&gt;
        at java.lang.reflect.Method.invoke(Method.java:597)&lt;br/&gt;
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:632)&lt;br/&gt;
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:894)&lt;/p&gt;

&lt;p&gt;        at sun.reflect.GeneratedConstructorAccessor13.newInstance(Unknown Source)&lt;br/&gt;
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)&lt;br/&gt;
        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)&lt;br/&gt;
        at org.apache.hadoop.hbase.RemoteExceptionHandler.decodeRemoteException(RemoteExceptionHandler.java:95)&lt;br/&gt;
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegionInMeta(HConnectionManager.java:550)&lt;br/&gt;
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:450)&lt;br/&gt;
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.relocateRegion(HConnectionManager.java:422)&lt;br/&gt;
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegionInMeta(HConnectionManager.java:559)&lt;br/&gt;
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:454)&lt;br/&gt;
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:415)&lt;br/&gt;
        at org.apache.hadoop.hbase.client.HTable.&amp;lt;init&amp;gt;(HTable.java:113)&lt;br/&gt;
        at org.apache.hadoop.hbase.client.HTable.&amp;lt;init&amp;gt;(HTable.java:96)&lt;/p&gt;


&lt;p&gt;Clients that are still connected do still work. (As they have probably cached the ROOT region?)&lt;/p&gt;

&lt;p&gt;This seemed to have happend after one of the region servers (number 3) shut itselfdown due to exceptions (EOFileException, Unable to create new block, etc... see logfile) The ROOT region then probably got moved to region server 2.&lt;/p&gt;

&lt;p&gt;I attached the logs (DEBUG enabled) of the hdfs namenode, the hbase master node and the two log files of regionserver 2 and 3.&lt;/p&gt;



&lt;p&gt;The filesystem is in healthy state. I can also download the file through the hadoop fs command without any problem and without getting an error message about missing blocks.&lt;/p&gt;

&lt;p&gt;Status: HEALTHY&lt;br/&gt;
 Total size:    142881532319 B (Total open files size: 12415139840 B)&lt;br/&gt;
 Total dirs:    4153&lt;br/&gt;
 Total files:   3541 (Files currently being written: 106)&lt;br/&gt;
 Total blocks (validated):      5208 (avg. block size 27435010 B) (Total open file blocks (not validated): 205)&lt;br/&gt;
 Minimally replicated blocks:   5208 (100.0 %)&lt;br/&gt;
 Over-replicated blocks:        0 (0.0 %)&lt;br/&gt;
 Under-replicated blocks:       0 (0.0 %)&lt;br/&gt;
 Mis-replicated blocks:         0 (0.0 %)&lt;br/&gt;
 Default replication factor:    4&lt;br/&gt;
 Average block replication:     4.0&lt;br/&gt;
 Corrupt blocks:                0&lt;br/&gt;
 Missing replicas:              0 (0.0 %)&lt;br/&gt;
 Number of data-nodes:          7&lt;br/&gt;
 Number of racks:               1&lt;br/&gt;
The filesystem under path &apos;/&apos; is HEALTHY&lt;/p&gt;</description>
                <environment>&lt;p&gt;hadoop 0.19.0&lt;br/&gt;
hbase 0.19.0-dev, r728134&lt;/p&gt;</environment>
        <key id="12411148">HBASE-1078</key>
            <summary>&quot;java.io.IOException: Could not obtain block&quot;: allthough file is there and accessible through the dfs client</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="bluelu">Thibaut</reporter>
                        <labels>
                    </labels>
                <created>Mon, 22 Dec 2008 11:19:07 +0000</created>
                <updated>Tue, 10 Mar 2009 01:34:40 +0000</updated>
                            <resolved>Sun, 18 Jan 2009 21:27:40 +0000</resolved>
                                                                        <due></due>
                            <votes>0</votes>
                                    <watches>0</watches>
                                                                <comments>
                            <comment id="12658504" author="bluelu" created="Mon, 22 Dec 2008 11:25:54 +0000"  >&lt;p&gt;Logfiles of hadoop&apos;s Namenode (INFO), the hbase master log, and the log of two failing region servers all in DEBUG mode&lt;/p&gt;</comment>
                            <comment id="12658516" author="bluelu" created="Mon, 22 Dec 2008 11:47:42 +0000"  >&lt;p&gt;At least one DFS client node has lots of DataXceiver exceptions. (I had removed the hadoop config entry for this and I will set dfs.datanode.max.xcievers to 2048 and retry).&lt;/p&gt;

&lt;p&gt;08/12/22 12:33:07 INFO datanode.DataNode: writeBlock blk_2596678729303248869_20816 received exception java.io.EOFException: while trying to read 65557 bytes&lt;br/&gt;
08/12/22 12:33:07 ERROR datanode.DataNode: DatanodeRegistration(192.168.0.7:50010, storageID=DS-1147686120-78.46.82.211-50010-1229624756268, infoPort=50075, ipcPort=50020):DataXceiver&lt;br/&gt;
java.io.EOFException: while trying to read 65557 bytes&lt;br/&gt;
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.readToBuf(BlockReceiver.java:254)&lt;br/&gt;
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.readNextPacket(BlockReceiver.java:298)&lt;br/&gt;
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:362)&lt;br/&gt;
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:514)&lt;br/&gt;
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:356)&lt;br/&gt;
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:102)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:619)&lt;br/&gt;
08/12/22 12:33:07 INFO datanode.DataNode: PacketResponder blk_-3816193612223297244_20739 1 Exception java.net.SocketException: Broken pipe&lt;br/&gt;
        at java.net.SocketOutputStream.socketWrite0(Native Method)&lt;br/&gt;
        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:92)&lt;br/&gt;
        at java.net.SocketOutputStream.write(SocketOutputStream.java:136)&lt;br/&gt;
        at java.io.DataOutputStream.writeLong(DataOutputStream.java:207)&lt;br/&gt;
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.run(BlockReceiver.java:836)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:619)&lt;/p&gt;

&lt;p&gt;08/12/22 12:33:07 INFO datanode.DataNode: PacketResponder blk_-3816193612223297244_20739 1 : Thread is interrupted.&lt;br/&gt;
08/12/22 12:33:07 INFO datanode.DataNode: PacketResponder 1 for block blk_-3816193612223297244_20739 terminating&lt;br/&gt;
08/12/22 12:33:07 INFO datanode.DataNode: writeBlock blk_-3816193612223297244_20739 received exception java.io.EOFException: while trying to read 65557 bytes&lt;br/&gt;
08/12/22 12:33:07 ERROR datanode.DataNode: DatanodeRegistration(192.168.0.7:50010, storageID=DS-1147686120-78.46.82.211-50010-1229624756268, infoPort=50075, ipcPort=50020):DataXceiver&lt;br/&gt;
java.io.EOFException: while trying to read 65557 bytes&lt;br/&gt;
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.readToBuf(BlockReceiver.java:254)&lt;br/&gt;
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.readNextPacket(BlockReceiver.java:298)&lt;br/&gt;
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:362)&lt;br/&gt;
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:514)&lt;br/&gt;
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:356)&lt;br/&gt;
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:102)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:619)&lt;br/&gt;
08/12/22 12:33:10 INFO datanode.DataNode: PacketResponder blk_5759147435898729286_20763 1 Exception java.net.SocketException: Broken pipe&lt;br/&gt;
        at java.net.SocketOutputStream.socketWrite0(Native Method)&lt;br/&gt;
        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:92)&lt;br/&gt;
        at java.net.SocketOutputStream.write(SocketOutputStream.java:136)&lt;br/&gt;
        at java.io.DataOutputStream.writeLong(DataOutputStream.java:207)&lt;br/&gt;
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.run(BlockReceiver.java:836)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:619)&lt;/p&gt;

&lt;p&gt;08/12/22 12:33:10 INFO datanode.DataNode: PacketResponder blk_5759147435898729286_20763 1 : Thread is interrupted.&lt;br/&gt;
08/12/22 12:33:10 INFO datanode.DataNode: PacketResponder 1 for block blk_5759147435898729286_20763 terminating&lt;br/&gt;
08/12/22 12:33:10 INFO datanode.DataNode: writeBlock blk_5759147435898729286_20763 received exception java.io.EOFException: while trying to read 65557 bytes&lt;br/&gt;
08/12/22 12:33:10 ERROR datanode.DataNode: DatanodeRegistration(192.168.0.7:50010, storageID=DS-1147686120-78.46.82.211-50010-1229624756268, infoPort=50075, ipcPort=50020):DataXceiver&lt;br/&gt;
java.io.EOFException: while trying to read 65557 bytes&lt;br/&gt;
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.readToBuf(BlockReceiver.java:254)&lt;br/&gt;
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.readNextPacket(BlockReceiver.java:298)&lt;br/&gt;
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:362)&lt;br/&gt;
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:514)&lt;br/&gt;
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:356)&lt;br/&gt;
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:102)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:619)&lt;/p&gt;

&lt;p&gt;This might be causing the hbase failure then.&lt;/p&gt;


&lt;p&gt;When I tried to shut down the cluster, two region servers hang (the stop-hbase script however finishes).&lt;/p&gt;

&lt;p&gt;Here is the stack trace of one of them, (the other one is identical)&lt;/p&gt;

&lt;p&gt;2008-12-22 12:38:16&lt;br/&gt;
Full thread dump Java HotSpot(TM) 64-Bit Server VM (10.0-b23 mixed mode):&lt;/p&gt;

&lt;p&gt;&quot;Attach Listener&quot; daemon prio=10 tid=0x00002aab354cac00 nid=0x7af7 waiting on condition &lt;span class=&quot;error&quot;&gt;&amp;#91;0x0000000000000000..0x0000000000000000&amp;#93;&lt;/span&gt;&lt;br/&gt;
   java.lang.Thread.State: RUNNABLE&lt;/p&gt;

&lt;p&gt;&quot;pool-1-thread-1&quot; prio=10 tid=0x00002aab35a2b400 nid=0x22b0 waiting on condition &lt;span class=&quot;error&quot;&gt;&amp;#91;0x0000000042a52000..0x0000000042a52aa0&amp;#93;&lt;/span&gt;&lt;br/&gt;
   java.lang.Thread.State: WAITING (parking)&lt;br/&gt;
        at sun.misc.Unsafe.park(Native Method)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;parking to wait for  &amp;lt;0x00002aaab554efc0&amp;gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)&lt;br/&gt;
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)&lt;br/&gt;
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)&lt;br/&gt;
        at java.util.concurrent.DelayQueue.take(DelayQueue.java:160)&lt;br/&gt;
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:582)&lt;br/&gt;
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:575)&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:946)&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:906)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:619)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&quot;DestroyJavaVM&quot; prio=10 tid=0x00002aab381c8000 nid=0x2275 waiting on condition &lt;span class=&quot;error&quot;&gt;&amp;#91;0x0000000000000000..0x000000004022ad10&amp;#93;&lt;/span&gt;&lt;br/&gt;
   java.lang.Thread.State: RUNNABLE&lt;/p&gt;

&lt;p&gt;&quot;Low Memory Detector&quot; daemon prio=10 tid=0x00002aab3483c000 nid=0x227e runnable &lt;span class=&quot;error&quot;&gt;&amp;#91;0x0000000000000000..0x0000000000000000&amp;#93;&lt;/span&gt;&lt;br/&gt;
   java.lang.Thread.State: RUNNABLE&lt;/p&gt;

&lt;p&gt;&quot;CompilerThread1&quot; daemon prio=10 tid=0x00002aab34839800 nid=0x227d waiting on condition &lt;span class=&quot;error&quot;&gt;&amp;#91;0x0000000000000000..0x0000000040a31440&amp;#93;&lt;/span&gt;&lt;br/&gt;
   java.lang.Thread.State: RUNNABLE&lt;/p&gt;

&lt;p&gt;&quot;CompilerThread0&quot; daemon prio=10 tid=0x00002aab34836000 nid=0x227c waiting on condition &lt;span class=&quot;error&quot;&gt;&amp;#91;0x0000000000000000..0x00000000409303c0&amp;#93;&lt;/span&gt;&lt;br/&gt;
   java.lang.Thread.State: RUNNABLE&lt;/p&gt;

&lt;p&gt;&quot;Signal Dispatcher&quot; daemon prio=10 tid=0x00002aab34834c00 nid=0x227b runnable &lt;span class=&quot;error&quot;&gt;&amp;#91;0x0000000000000000..0x0000000040830730&amp;#93;&lt;/span&gt;&lt;br/&gt;
   java.lang.Thread.State: RUNNABLE&lt;/p&gt;

&lt;p&gt;&quot;Finalizer&quot; daemon prio=10 tid=0x00002aab34628400 nid=0x227a in Object.wait() &lt;span class=&quot;error&quot;&gt;&amp;#91;0x000000004072f000..0x000000004072fa20&amp;#93;&lt;/span&gt;&lt;br/&gt;
   java.lang.Thread.State: WAITING (on object monitor)&lt;br/&gt;
        at java.lang.Object.wait(Native Method)&lt;br/&gt;
        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:116)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;locked &amp;lt;0x00002aaab535e3f8&amp;gt; (a java.lang.ref.ReferenceQueue$Lock)&lt;br/&gt;
        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:132)&lt;br/&gt;
        at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:159)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&quot;Reference Handler&quot; daemon prio=10 tid=0x00002aab34627000 nid=0x2279 in Object.wait() &lt;span class=&quot;error&quot;&gt;&amp;#91;0x000000004062e000..0x000000004062eda0&amp;#93;&lt;/span&gt;&lt;br/&gt;
   java.lang.Thread.State: WAITING (on object monitor)&lt;br/&gt;
        at java.lang.Object.wait(Native Method)&lt;br/&gt;
        at java.lang.Object.wait(Object.java:485)&lt;br/&gt;
        at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:116)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;locked &amp;lt;0x00002aaab5290178&amp;gt; (a java.lang.ref.Reference$Lock)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&quot;VM Thread&quot; prio=10 tid=0x00002aab34621c00 nid=0x2278 runnable&lt;/p&gt;

&lt;p&gt;&quot;GC task thread#0 (ParallelGC)&quot; prio=10 tid=0x000000004011e000 nid=0x2276 runnable&lt;/p&gt;

&lt;p&gt;&quot;GC task thread#1 (ParallelGC)&quot; prio=10 tid=0x000000004011f400 nid=0x2277 runnable&lt;/p&gt;

&lt;p&gt;&quot;VM Periodic Task Thread&quot; prio=10 tid=0x00002aab3483dc00 nid=0x227f waiting on condition&lt;/p&gt;

&lt;p&gt;JNI global references: 1001&lt;/p&gt;



&lt;p&gt;There is nothing suspicious in the log file, &lt;br/&gt;
it just ends with:&lt;br/&gt;
2008-12-22 12:26:16,117 INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;regionserver/0:0:0:0:0:0:0:0:60020&amp;#93;&lt;/span&gt; regionserver.HRegionServer(508): telling master that region server is shutting down at: 192.168.0.7:60020&lt;br/&gt;
2008-12-22 12:26:16,122 INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;regionserver/0:0:0:0:0:0:0:0:60020&amp;#93;&lt;/span&gt; regionserver.HRegionServer(515): stopping server at: 192.168.0.7:60020&lt;br/&gt;
2008-12-22 12:26:16,545 INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;regionserver/0:0:0:0:0:0:0:0:60020&amp;#93;&lt;/span&gt; regionserver.HRegionServer(525): regionserver/0:0:0:0:0:0:0:0:60020 exiting&lt;/p&gt;</comment>
                            <comment id="12659005" author="stack" created="Wed, 24 Dec 2008 00:11:45 +0000"  >&lt;p&gt;Thanks for the thread dump.&lt;/p&gt;

&lt;p&gt;This is the culprit I believe: &apos;&quot;pool-1-thread-1&quot; prio=10 tid=0x00002aab35a2b400 nid=0x22b0 waiting on condition &lt;span class=&quot;error&quot;&gt;&amp;#91;0x0000000042a52000..0x0000000042a52aa0&amp;#93;&lt;/span&gt;&apos;.&lt;/p&gt;

&lt;p&gt;Its not a daemon thread.  Update so you get fix for &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-1074&quot; title=&quot;New thread introduced by hbase-900 part 4 is not daemon so can cause JVM to stick around on abort&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-1074&quot;&gt;&lt;del&gt;HBASE-1074&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="12664869" author="stack" created="Sat, 17 Jan 2009 22:19:14 +0000"  >&lt;p&gt;Can I close this Thibaut?  Did hbase-1074 fix this the hang?&lt;/p&gt;</comment>
                            <comment id="12664962" author="bluelu" created="Sun, 18 Jan 2009 13:33:08 +0000"  >&lt;p&gt;Yes, I think so.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12416480">HBASE-1250</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12396589" name="errorlogs.zip" size="2837474" author="bluelu" created="Mon, 22 Dec 2008 11:25:54 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Wed, 24 Dec 2008 00:11:45 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>25559</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 46 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0hb6v:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>99075</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>