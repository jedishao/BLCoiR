<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 02 20:32:06 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-497/HBASE-497.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-497] RegionServer needs to recover if datanode goes down</title>
                <link>https://issues.apache.org/jira/browse/HBASE-497</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;If I take down a datanode, the regionserver will repeatedly return this error:&lt;/p&gt;

&lt;p&gt;java.io.IOException: Stream closed.&lt;br/&gt;
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.isClosed(DFSClient.java:1875)&lt;br/&gt;
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.writeChunk(DFSClient.java:2096)&lt;br/&gt;
        at org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunk(FSOutputSummer.java:141)&lt;br/&gt;
        at org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:124)&lt;br/&gt;
        at org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:112)&lt;br/&gt;
        at org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:86)&lt;br/&gt;
        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:41)&lt;br/&gt;
        at java.io.DataOutputStream.write(Unknown Source)&lt;br/&gt;
        at org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:977)&lt;br/&gt;
        at org.apache.hadoop.hbase.HLog.append(HLog.java:377)&lt;br/&gt;
        at org.apache.hadoop.hbase.HRegion.update(HRegion.java:1455)&lt;br/&gt;
        at org.apache.hadoop.hbase.HRegion.batchUpdate(HRegion.java:1259)&lt;br/&gt;
        at org.apache.hadoop.hbase.HRegionServer.batchUpdate(HRegionServer.java:1433)&lt;br/&gt;
        at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)&lt;br/&gt;
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)&lt;br/&gt;
        at java.lang.reflect.Method.invoke(Unknown Source)&lt;br/&gt;
        at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:413)&lt;br/&gt;
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:910)&lt;/p&gt;

&lt;p&gt;It appears that hbase/dfsclient does not attempt to reopen the stream.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12390392">HBASE-497</key>
            <summary>RegionServer needs to recover if datanode goes down</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.png">Blocker</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="bryanduxbury">Bryan Duxbury</assignee>
                                    <reporter username="bien">Michael Bieniosek</reporter>
                        <labels>
                    </labels>
                <created>Thu, 6 Mar 2008 21:45:18 +0000</created>
                <updated>Fri, 22 Aug 2008 21:17:40 +0000</updated>
                            <resolved>Tue, 18 Mar 2008 19:48:30 +0000</resolved>
                                    <version>0.16.0</version>
                    <version>0.1.0</version>
                    <version>0.2.0</version>
                                    <fixVersion>0.1.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>0</watches>
                                                                <comments>
                            <comment id="12575918" author="bryanduxbury" created="Thu, 6 Mar 2008 22:10:57 +0000"  >&lt;p&gt;This is a big deal. We need to be resilient to datanode failures.&lt;/p&gt;</comment>
                            <comment id="12576469" author="bryanduxbury" created="Sat, 8 Mar 2008 02:02:40 +0000"  >&lt;p&gt;At least in 0.1, where we don&apos;t have appends to logs, when we get an error trying to append to a log, there&apos;s no real way to recover the lost log data. This is because HDFS files don&apos;t exist until they&apos;re closed. (See &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-1700&quot; title=&quot;Append to files in HDFS&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-1700&quot;&gt;&lt;del&gt;HADOOP-1700&lt;/del&gt;&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;Our options are:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Bail the regionserver. There&apos;s been an exception we shouldn&apos;t really ever get, and it&apos;s bad. Let it get worked out by restarting.&lt;/li&gt;
	&lt;li&gt;Bail the regionserver, but also try to flush the caches first. This has the advantage of saving the data already written to caches, if possible. Might end up with a convoluted flow to make it happen.&lt;/li&gt;
	&lt;li&gt;Open a new log like nothing ever happened. We&apos;ll have lost the updates since the last log roll, but who cares, since there&apos;s nothing we can do to recover it, period.&lt;/li&gt;
	&lt;li&gt;Change logging to log to a local file as well as the HDFS file. Then, if there&apos;s an exception at any point writing to the HDFS log, we can copy the local version of the log up to HDFS and keep appending. This gives us some resilience to datanode failures, but doesn&apos;t really make our logs any more useful in the case of dying machines or network partitions. It&apos;s also a lot of new functionality, which doesn&apos;t exactly fit with the goals of 0.1 (bugfixes only).&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Of these options, I think the best one is to just open a new log. This will keep our regionserver online and let us carry on with the minimum of difficulty. Does this seem like enough of a fix to satisfy the 0.1 release block?&lt;/p&gt;</comment>
                            <comment id="12576570" author="bryanduxbury" created="Sat, 8 Mar 2008 17:36:55 +0000"  >&lt;p&gt;Here&apos;s a shot at the &quot;reopen the log&quot; approach for 0.1. It passes unit tests, but no test exercises the dying datanode functionality at the moment, so I don&apos;t put too much stock in that. Will have to try killing a datanode intentionally. &lt;/p&gt;</comment>
                            <comment id="12579584" author="stack" created="Mon, 17 Mar 2008 20:17:05 +0000"  >&lt;p&gt;Make one emission rather than the two that are in your patch:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+          LOG.error(&lt;span class=&quot;code-quote&quot;&gt;&quot;Could not append to log because &quot;&lt;/span&gt; + e.toString());
+          LOG.error(&lt;span class=&quot;code-quote&quot;&gt;&quot;Opening a &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; writer.&quot;&lt;/span&gt;);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Also change it to LOG.error(&quot;Could not append to log... opening a new writer&quot;, e); i.e. pass the actual exception... &lt;/p&gt;

&lt;p&gt;Otherwise, I&apos;m good with just applying this patch and trying this tactic.  I like the way that the new append is done within the IOE block and it in turn catches an IOE.  But on the second throw, shouldn&apos;t we throw something else, something that will break the eternal looping that Michael B reports &amp;#8211; IIUC.  It seems like an IOE won&apos;t do?&lt;/p&gt;</comment>
                            <comment id="12579702" author="bryanduxbury" created="Tue, 18 Mar 2008 01:43:13 +0000"  >&lt;p&gt;If an IOException gets thrown up the stack all the way back to HRegionServer#batchUpdate, then the HRS will call checkFileSystem. If there is actually an FS error (which there would have to be in order for two IOExceptions to occur in a row inside rollWriter - I think), then checkFileSystem sets abortRequested and stopRequested, which should kill the main HRS thread.&lt;/p&gt;

&lt;p&gt;In this issue, the reason we are not seeing the HRS go down is that there actually isn&apos;t an FS problem - it&apos;s totally our fault for not trying to reopen the log writer. I suspect that we will be able to recover from this kind of error with the code previously posted.&lt;/p&gt;

&lt;p&gt;I will put up a new patch with the logging improvements requested.&lt;/p&gt;</comment>
                            <comment id="12579703" author="bryanduxbury" created="Tue, 18 Mar 2008 01:45:20 +0000"  >&lt;p&gt;Passes unit tests locally.&lt;/p&gt;</comment>
                            <comment id="12579704" author="bryanduxbury" created="Tue, 18 Mar 2008 01:45:36 +0000"  >&lt;p&gt;Please review patch.&lt;/p&gt;</comment>
                            <comment id="12579940" author="stack" created="Tue, 18 Mar 2008 17:33:28 +0000"  >&lt;p&gt;+1 on v2 of patch&lt;/p&gt;</comment>
                            <comment id="12580035" author="bryanduxbury" created="Tue, 18 Mar 2008 19:48:30 +0000"  >&lt;p&gt;I just committed this to 0.1 and opened a new issue for porting to trunk.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10020">
                    <name>Cloners</name>
                                                                <inwardlinks description="is cloned by">
                                        <issuelink>
            <issuekey id="12391781">HBASE-529</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12378095" name="497-0.1-v2.patch" size="1849" author="bryanduxbury" created="Tue, 18 Mar 2008 01:45:20 +0000"/>
                            <attachment id="12377449" name="497_0.1.patch" size="1060" author="bryanduxbury" created="Sat, 8 Mar 2008 17:36:55 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 6 Mar 2008 22:10:57 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>25230</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 38 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0h7nr:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>98503</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>