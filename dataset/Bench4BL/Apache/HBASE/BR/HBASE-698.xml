<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 02 19:41:25 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-698/HBASE-698.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-698] HLog recovery is not performed after master failure</title>
                <link>https://issues.apache.org/jira/browse/HBASE-698</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;I have a local cluster running, and its logging to&lt;br/&gt;
&amp;lt;hbase&amp;gt;/log_X.X.X.X_1213228101021_60020/&lt;/p&gt;

&lt;p&gt;Then I kill both master and regionserver, and restart. Looking through&lt;br/&gt;
the logs I don&apos;t see anything about trying to recover from this hlog,&lt;br/&gt;
it just creates a new hlog alongside the existing one (with a new&lt;br/&gt;
startcode).  The older hlog seems to be ignored, and the tables&lt;br/&gt;
created in the inital session are all gone.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12398550">HBASE-698</key>
            <summary>HLog recovery is not performed after master failure</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.png">Blocker</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="jdcryans">Jean-Daniel Cryans</assignee>
                                    <reporter username="clint.morgan">Clint Morgan</reporter>
                        <labels>
                    </labels>
                <created>Thu, 19 Jun 2008 00:36:10 +0000</created>
                <updated>Fri, 21 Jan 2011 05:38:37 +0000</updated>
                            <resolved>Tue, 14 Jul 2009 16:49:11 +0000</resolved>
                                    <version>0.1.2</version>
                    <version>0.2.1</version>
                    <version>0.19.3</version>
                                    <fixVersion>0.20.0</fixVersion>
                                    <component>master</component>
                    <component>regionserver</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>0</watches>
                                                                <comments>
                            <comment id="12606230" author="stack" created="Thu, 19 Jun 2008 03:26:26 +0000"  >&lt;p&gt;Moving into 0.2 for now.  Lets take a look at least at whats going on.&lt;/p&gt;</comment>
                            <comment id="12609428" author="jimk" created="Tue, 1 Jul 2008 02:20:12 +0000"  >&lt;p&gt;Moving to 0.3.0 because it really depends on hbase &quot;safe-mode&quot; and hadoop appends which aren&apos;t supported until hadoop 0.18&lt;/p&gt;</comment>
                            <comment id="12619082" author="jimk" created="Fri, 1 Aug 2008 17:41:19 +0000"  >&lt;p&gt;Currently the master will only recover logs during startup if it finds information in in the ROOT or META that indicate that a region was being served. Without appends in hadoop, the edits to the ROOT or META could be lost and so the master would not look for the log of the server that was killed.&lt;/p&gt;

&lt;p&gt;This is not a good way to do this. Once appends make log files persistent, the master should start up in safe mode, and first recover all the log files it finds. Region servers will not create their logs until the master has finished recovering all the existing logs and tells the region servers it has finished.&lt;/p&gt;</comment>
                            <comment id="12646344" author="jimk" created="Mon, 10 Nov 2008 20:07:13 +0000"  >&lt;p&gt;There is a very simple fix if the master comes back up and knows a region server is dead.&lt;/p&gt;

&lt;p&gt;However, if the master dies, region servers hang around until the master comes back up. Thus the master cannot know which HLogs to recover and which belong to running region servers. (&quot;recovering&quot; a HLog from a running region server would produce unpredictable results, most likely leading to data corruption).&lt;/p&gt;

&lt;p&gt;Relying on hdfs lease timeouts on the log files is also not an option as the lease timeout interval is too long for this purpose.&lt;/p&gt;

&lt;p&gt;The master can therefore not recover any region server logs unless it knows that region server is dead.  This cannot be accomplished without Zookeeper integration, which will monitor the region servers (and the regions they serve) using ephemeral files. At that point, if the master dies and is restarted, it will know which region servers are alive, which ones have died and all the regions that are currently being served. Then it will know which region server logs to recover and which ones can be ignored (because the region server writing it is still alive).&lt;/p&gt;</comment>
                            <comment id="12646376" author="jimk" created="Mon, 10 Nov 2008 21:51:30 +0000"  >&lt;p&gt;Removed Zookeeper integration as a blocker. &lt;/p&gt;

&lt;p&gt;While Zookeeper will make this much easier, we need an interim solution.&lt;/p&gt;</comment>
                            <comment id="12646386" author="jimk" created="Mon, 10 Nov 2008 22:27:15 +0000"  >&lt;p&gt;Making this issue require Zookeeper. We could do something hokey in the mean time, but it would still have holes in it.&lt;/p&gt;</comment>
                            <comment id="12646388" author="jimk" created="Mon, 10 Nov 2008 22:29:13 +0000"  >&lt;p&gt;Moving this issue to 0.20.0 as it requires Zookeeper to close all the holes.&lt;/p&gt;</comment>
                            <comment id="12704821" author="apurtell" created="Thu, 30 Apr 2009 22:28:14 +0000"  >&lt;p&gt;The scope of this problem is narrowed by new developments. With multiple masters on standby and fail over via ZK, the possibility of encountering this situation is reduced. &lt;/p&gt;</comment>
                            <comment id="12705671" author="jimk" created="Mon, 4 May 2009 18:22:35 +0000"  >&lt;p&gt;&amp;gt; Andrew Purtell added a comment - 30/Apr/09 03:28 PM&lt;br/&gt;
&amp;gt; The scope of this problem is narrowed by new developments. With multiple masters on&lt;br/&gt;
&amp;gt; standby and fail over via ZK, the possibility of encountering this situation is reduced.&lt;/p&gt;

&lt;p&gt;Agreed that the window for missing a lease loss narrows the window greatly.&lt;/p&gt;

&lt;p&gt;Do the standby masters watch the region server leases?&lt;/p&gt;

&lt;p&gt;The gist of the patch I was working on was to move the code from BaseScanner.checkAssigned&lt;br/&gt;
(lines 367 - 392) to a new method in HMaster that is called from HMaster.run() &lt;b&gt;before&lt;/b&gt;&lt;br/&gt;
HMaster.startServiceThreads is called.&lt;/p&gt;</comment>
                            <comment id="12711718" author="stack" created="Thu, 21 May 2009 17:38:34 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-1439&quot; title=&quot;race between master and regionserver after missed heartbeat&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-1439&quot;&gt;&lt;del&gt;HBASE-1439&lt;/del&gt;&lt;/a&gt; is talking about ownership of commit logs being orchestrated by zk; perhaps on startup (or on assumption of master role) master can check zk if commit logs to replay.&lt;/p&gt;</comment>
                            <comment id="12717848" author="jdcryans" created="Tue, 9 Jun 2009 22:36:10 +0000"  >&lt;p&gt;When checking the cluster start in a master failover situation, we should check for HLogs to process.&lt;/p&gt;</comment>
                            <comment id="12728112" author="jdcryans" created="Tue, 7 Jul 2009 14:07:11 +0000"  >&lt;p&gt;In &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-1143&quot; title=&quot;region count erratic in master UI (kill server hosting root or meta and see how count goes awry).... make sure you have a bunch of reions in there&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-1143&quot;&gt;&lt;del&gt;HBASE-1143&lt;/del&gt;&lt;/a&gt; we set a very low flush size on .META. to make it flush after every ~10 updates which lessens the impact of a lost hlog. So the original situation can still happen in the way it is described or if a table split was done and that the region server holding META died pretty much right after that (because either a flush will happen or after 1 hour we will roll the commit log ). The hole is there but it&apos;s very small. The only thing missing is appends now. We could punt this to 0.21.0, unless someone votes for a hlog ZK orchestration for 0.20.0. &lt;/p&gt;</comment>
                            <comment id="12728167" author="stack" created="Tue, 7 Jul 2009 16:15:06 +0000"  >&lt;p&gt;Can we not do something basic here?  Even if its master looking into all regions on filesystem for logs to recover on startup.&lt;/p&gt;</comment>
                            <comment id="12728262" author="jimk" created="Tue, 7 Jul 2009 18:03:30 +0000"  >&lt;p&gt;Yes, we need to do something. In particular, the master needs to know what region servers are running and what their start code is, so it does not try to recover a log out from under a running region server.&lt;/p&gt;</comment>
                            <comment id="12728263" author="jdcryans" created="Tue, 7 Jul 2009 18:07:45 +0000"  >&lt;p&gt;Ok, then I&apos;ll simply do a check of the .logs folder. It&apos;s going to be part of the verifyClusterState() method so that I have knowledge of existing RSs.&lt;/p&gt;</comment>
                            <comment id="12728296" author="jdcryans" created="Tue, 7 Jul 2009 18:57:37 +0000"  >&lt;p&gt;This patch fixes the original issue (or as much we can do without appends). This happens on a master failover:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2009-07-07 14:45:28,097 INFO org.apache.hadoop.hbase.master.HMaster: Found log folder : jdcryans.local,60020,1246991630883
2009-07-07 14:45:28,097 INFO org.apache.hadoop.hbase.master.HMaster: Log folder belongs to an existing region server
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Whn know about the alive RS since we just scanned the rs folder in ZK. Then if I kill -9 both Master and RS then I restart the cluster I see:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2009-07-07 14:47:10,034 DEBUG org.apache.hadoop.hbase.master.HMaster: This is a fresh start, proceeding with normal startup
2009-07-07 14:47:10,038 INFO org.apache.hadoop.hbase.master.HMaster: Found log folder : jdcryans.local,60020,1246991630883
2009-07-07 14:47:10,038 INFO org.apache.hadoop.hbase.master.HMaster: Log folder doesn&apos;t belong to a known region server, splitting
2009-07-07 14:47:10,043 INFO org.apache.hadoop.hbase.regionserver.HLog: Splitting 1 hlog(s) in hdfs:&lt;span class=&quot;code-comment&quot;&gt;//localhost:9000/hbase/.logs/jdcryans.local,60020,1246991630883
&lt;/span&gt;...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I also removed the log splitting stuff in BaseScanner.&lt;/p&gt;</comment>
                            <comment id="12728309" author="stack" created="Tue, 7 Jul 2009 19:24:20 +0000"  >&lt;p&gt;I wonder why the code in BaseScanner was insufficient?&lt;/p&gt;</comment>
                            <comment id="12728322" author="jimk" created="Tue, 7 Jul 2009 19:41:22 +0000"  >&lt;p&gt;I don&apos;t think it had any idea of region servers that were running, so it would recover &lt;b&gt;all&lt;/b&gt; logs, including&lt;br/&gt;
those from running region servers, which really messed things up.&lt;/p&gt;</comment>
                            <comment id="12728331" author="stack" created="Tue, 7 Jul 2009 20:05:48 +0000"  >&lt;p&gt;Ok.&lt;/p&gt;

&lt;p&gt;So, j-d, why not mod BaseScanner to check zk rather than do the filesystem check?  Filesystem check is more comprehensive?&lt;/p&gt;</comment>
                            <comment id="12728341" author="jdcryans" created="Tue, 7 Jul 2009 20:16:39 +0000"  >&lt;blockquote&gt;&lt;p&gt;I wonder why the code in BaseScanner was insufficient? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It was reacting only when seeing stale data in META or ROOT so if you lost an edit in META, you won&apos;t see it back in BaseScanner. I think BaseScanner never recovered anything ever because we don&apos;t have appends. Now we will have the issue of recovering regions without a META entry tho...&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;So, j-d, why not mod BaseScanner to check zk rather than do the filesystem check? Filesystem check is more comprehensive?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;ZK won&apos;t tell you about dead region servers, only the live ones.&lt;/p&gt;</comment>
                            <comment id="12728371" author="jimk" created="Tue, 7 Jul 2009 21:28:24 +0000"  >&lt;p&gt;If you scan the file system for region server logs, you can ignore the ones for region servers that ZK says&lt;br/&gt;
are alive.&lt;/p&gt;</comment>
                            <comment id="12728383" author="jdcryans" created="Tue, 7 Jul 2009 21:54:42 +0000"  >&lt;blockquote&gt;&lt;p&gt;If you scan the file system for region server logs, you can ignore the ones for region servers that ZK says are alive. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That&apos;s what my patch does.&lt;/p&gt;</comment>
                            <comment id="12728416" author="stack" created="Tue, 7 Jul 2009 23:32:22 +0000"  >&lt;p&gt;Ok.  Thanks lads.&lt;/p&gt;

&lt;p&gt;+1 on patch.&lt;/p&gt;</comment>
                            <comment id="12728439" author="jdcryans" created="Wed, 8 Jul 2009 00:30:01 +0000"  >&lt;p&gt;Jim, since you worked on this issue, would you like to try it before I commit? (thanks stack)&lt;/p&gt;</comment>
                            <comment id="12729661" author="jdcryans" created="Fri, 10 Jul 2009 15:00:12 +0000"  >&lt;p&gt;I committed this to trunk. Should I make a patch for 0.19?&lt;/p&gt;</comment>
                            <comment id="12729691" author="stack" created="Fri, 10 Jul 2009 15:53:12 +0000"  >&lt;p&gt;I took another look at the patch.  A backport don&apos;t look like it&apos;d be that hard and it&apos;d be a nice fix to have.  Good on you J-D&lt;/p&gt;</comment>
                            <comment id="12729697" author="jimk" created="Fri, 10 Jul 2009 16:08:52 +0000"  >&lt;p&gt;+1 on backport if it is not too complicated.&lt;/p&gt;</comment>
                            <comment id="12730864" author="jdcryans" created="Tue, 14 Jul 2009 14:16:52 +0000"  >&lt;p&gt;Patch against 0.19 branch. It will recover any log directory found at the root HBase folder. I tried it on my machine by creating a table, inserting 500 rows into it and then killing --9 both Master and RS. After restart everything was there. This patch doesn&apos;t cover the situation where the logs weren&apos;t flush to disk (inserting 10 rows didn&apos;t trigger it) and it doesn&apos;t take into account existing RS like the patch for trunk does.&lt;/p&gt;</comment>
                            <comment id="12730921" author="davelatham" created="Tue, 14 Jul 2009 15:43:42 +0000"  >&lt;p&gt;I have a cluster where I can see several old log directories in the root hbase folder.  What would be the affect of deploying this patch?  Would it be best to manually delete those directories first?&lt;/p&gt;</comment>
                            <comment id="12730930" author="jdcryans" created="Tue, 14 Jul 2009 15:55:14 +0000"  >&lt;p&gt;@Dave&lt;/p&gt;

&lt;p&gt;I have the same situation here and I&apos;m doing some tests. I saw that edits going to since deleted regions will still create a directory and put the oldlogfile.log there. This is not so bad but not so good either, it eats up disk space for no reason... Edits going to META are more problematic, you could see old regions coming back and completely mess up your data. I&apos;m currently trying to recreate such a setup to confirm.&lt;/p&gt;

&lt;p&gt;So, if you want to apply this patch, I currently suggest that you delete those folders.&lt;/p&gt;</comment>
                            <comment id="12730934" author="davelatham" created="Tue, 14 Jul 2009 16:04:12 +0000"  >&lt;p&gt;@JD&lt;/p&gt;

&lt;p&gt;Thanks for the helpful info.  It might be worth putting out a warning then with a 0.19.4 release to do that - possibly for migration to 0.20.0 also?&lt;/p&gt;</comment>
                            <comment id="12730935" author="jdcryans" created="Tue, 14 Jul 2009 16:08:33 +0000"  >&lt;p&gt;Ah well I tested it and it&apos;s all OK because of the sequence ID. I used current 0.19, created a table, inserted 500 rows, then killed -9. I redid the same thing in the second run but with a different column family name and shut down clean. I then applied the patch, restarted and I saw:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2009-07-14 12:06:05,783 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Opening region .META.,,1/1028785192
2009-07-14 12:06:05,789 DEBUG org.apache.hadoop.hbase.regionserver.HStore: loaded /tmp/hbase-jdcryans/hbase/.META./1028785192/info/info/7874001023615296020, isReference=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;, sequence id=512, length=559, majorCompaction=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;
2009-07-14 12:06:05,789 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Loaded 1 file(s) in hstore 1028785192/info, max sequence id 512
2009-07-14 12:06:05,791 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Applied 0, skipped 7 because sequence id &amp;lt;= 512
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So basically it&apos;s ok.&lt;/p&gt;</comment>
                            <comment id="12730943" author="stack" created="Tue, 14 Jul 2009 16:24:14 +0000"  >&lt;p&gt;Yes.  Sequenceid should prevent our taking on old edits.  I like the idea of release noting cleanup of dirs going 0.19.3 to 0.19.4 and I&apos;ll add cleanup to migration to 0.20.0.  +1 on patch.&lt;/p&gt;
</comment>
                            <comment id="12730963" author="jdcryans" created="Tue, 14 Jul 2009 16:49:11 +0000"  >&lt;p&gt;Committed to branch 0.19.&lt;/p&gt;</comment>
                            <comment id="12731199" author="stack" created="Tue, 14 Jul 2009 23:22:42 +0000"  >&lt;p&gt;Regards 0.20.x, we don&apos;t need to flag folks.  Logs are now in a subdirectory named .logs.  Old log files in old location will be ignored.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10032">
                    <name>Blocker</name>
                                                                <inwardlinks description="is blocked by">
                                        <issuelink>
            <issuekey id="12399780">HBASE-728</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12392373">HBASE-546</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12421690">HBASE-1302</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12413430" name="hbase-698-0.19.patch" size="4075" author="jdcryans" created="Tue, 14 Jul 2009 14:16:52 +0000"/>
                            <attachment id="12412775" name="hbase-698.patch" size="4966" author="jdcryans" created="Tue, 7 Jul 2009 18:57:37 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 19 Jun 2008 03:26:26 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>25342</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 21 weeks, 2 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0h8wf:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>98704</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310192" key="com.atlassian.jira.plugin.system.customfieldtypes:textarea">
                        <customfieldname>Release Note</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Old setups running 0.19 should first delete everything that&amp;#39;s log_* in HBase root HDFS folder before updating. Make sure HBase is down before doing that. Also this will be done by migration in 0.20.0</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>