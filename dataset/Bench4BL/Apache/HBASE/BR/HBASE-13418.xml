<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sat Dec 03 16:12:02 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-13418/HBASE-13418.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-13418] Regions getting stuck in PENDING_CLOSE state infinitely in high load HA scenarios</title>
                <link>https://issues.apache.org/jira/browse/HBASE-13418</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;In some heavy data load cases when there are multiple RegionServers going up/down (HA) or when we try to shutdown/restart the entire HBase cluster, we are observing that some regions are getting stuck in PENDING_CLOSE state infinitely. &lt;/p&gt;

&lt;p&gt;On going through the logs for a particular region stuck in PENDING_CLOSE state, it looks like for this region two memstore flush got triggered within few milliseconds as given below and after sometime there is Unrecoverable exception while closing region. I am suspecting this could be some kind of race condition but need to check further&lt;/p&gt;

&lt;p&gt;Logs:&lt;br/&gt;
================&lt;br/&gt;
......&lt;br/&gt;
2015-04-06 11:47:33,309 INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;2,queue=0,port=60020&amp;#93;&lt;/span&gt; regionserver.HRegionServer - Close 884fd5819112370d9a9834895b0ec19c, via zk=yes, znode version=0, on blitzhbase01-dnds1-4-crd.eng.sfdc.net,60020,1428318111711&lt;br/&gt;
2015-04-06 11:47:33,309 DEBUG &lt;span class=&quot;error&quot;&gt;&amp;#91;-dnds3-4-crd:60020-0&amp;#93;&lt;/span&gt; handler.CloseRegionHandler - Processing close of RMHA_1,\x04\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00,1428318937003.884fd5819112370d9a9834895b0ec19c.&lt;br/&gt;
2015-04-06 11:47:33,319 DEBUG &lt;span class=&quot;error&quot;&gt;&amp;#91;-dnds3-4-crd:60020-0&amp;#93;&lt;/span&gt; regionserver.HRegion - Closing RMHA_1,\x04\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00,1428318937003.884fd5819112370d9a9834895b0ec19c.: disabling compactions &amp;amp; flushes&lt;br/&gt;
2015-04-06 11:47:33,319 INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;-dnds3-4-crd:60020-0&amp;#93;&lt;/span&gt; regionserver.HRegion - Running close preflush of RMHA_1,\x04\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00,1428318937003.884fd5819112370d9a9834895b0ec19c.&lt;br/&gt;
2015-04-06 11:47:33,319 INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;-dnds3-4-crd:60020-0&amp;#93;&lt;/span&gt; regionserver.HRegion - Started memstore flush for RMHA_1,\x04\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00,1428318937003.884fd5819112370d9a9834895b0ec19c., current region memstore size 70.0 M&lt;br/&gt;
2015-04-06 11:47:33,327 DEBUG &lt;span class=&quot;error&quot;&gt;&amp;#91;-dnds3-4-crd:60020-0&amp;#93;&lt;/span&gt; regionserver.HRegion - Updates disabled for region RMHA_1,\x04\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00,1428318937003.884fd5819112370d9a9834895b0ec19c.&lt;br/&gt;
2015-04-06 11:47:33,328 INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;-dnds3-4-crd:60020-0&amp;#93;&lt;/span&gt; regionserver.HRegion - Started memstore flush for RMHA_1,\x04\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00,1428318937003.884fd5819112370d9a9834895b0ec19c., current region memstore size 70.0 M&lt;br/&gt;
2015-04-06 11:47:33,328 WARN  &lt;span class=&quot;error&quot;&gt;&amp;#91;-dnds3-4-crd:60020-0&amp;#93;&lt;/span&gt; wal.FSHLog - Couldn&apos;t find oldest seqNum for the region we are about to flush: &lt;span class=&quot;error&quot;&gt;&amp;#91;884fd5819112370d9a9834895b0ec19c&amp;#93;&lt;/span&gt;&lt;br/&gt;
2015-04-06 11:47:33,328 WARN  &lt;span class=&quot;error&quot;&gt;&amp;#91;-dnds3-4-crd:60020-0&amp;#93;&lt;/span&gt; regionserver.MemStore - Snapshot called again without clearing previous. Doing nothing. Another ongoing flush or did we fail last attempt?&lt;br/&gt;
2015-04-06 11:47:33,334 FATAL &lt;span class=&quot;error&quot;&gt;&amp;#91;-dnds3-4-crd:60020-0&amp;#93;&lt;/span&gt; regionserver.HRegionServer - ABORTING region server blitzhbase01-dnds3-4-crd.eng.sfdc.net,60020,1428318082860: Unrecoverable exception while closing region RMHA_1,\x04\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00,1428318937003.884fd5819112370d9a9834895b0ec19c., still finishing close&lt;/p&gt;


</description>
                <environment></environment>
        <key id="12818976">HBASE-13418</key>
            <summary>Regions getting stuck in PENDING_CLOSE state infinitely in high load HA scenarios</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="3">Duplicate</resolution>
                                        <assignee username="vik.karma">Vikas Vishwakarma</assignee>
                                    <reporter username="vik.karma">Vikas Vishwakarma</reporter>
                        <labels>
                    </labels>
                <created>Tue, 7 Apr 2015 16:06:34 +0000</created>
                <updated>Fri, 1 May 2015 03:57:44 +0000</updated>
                            <resolved>Fri, 1 May 2015 03:57:43 +0000</resolved>
                                    <version>0.98.10</version>
                                                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                                                <comments>
                            <comment id="14483491" author="esteban" created="Tue, 7 Apr 2015 16:55:15 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vik.karma&quot; class=&quot;user-hover&quot; rel=&quot;vik.karma&quot;&gt;Vikas Vishwakarma&lt;/a&gt; Do you have an stack trace about the unrecoverable exception?&lt;/p&gt;</comment>
                            <comment id="14484607" author="vik.karma" created="Wed, 8 Apr 2015 02:48:27 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=esteban&quot; class=&quot;user-hover&quot; rel=&quot;esteban&quot;&gt;Esteban Gutierrez&lt;/a&gt; I am seeing a socket timeout immediately after this &lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;
2015-04-06 11:47:33,344 INFO  [-dnds3-4-crd:60020-0] regionserver.HRegionServer - STOPPED: Unrecoverable exception while closing region RMHA_1,\x04\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00,1428318937003.884fd5819112370d9a9834895b0ec19c., still finishing close
2015-04-06 11:47:33,344 INFO  [regionserver60020] ipc.RpcServer - Stopping server on 60020
2015-04-06 11:47:33,344 ERROR [-dnds3-4-crd:60020-0] executor.EventHandler - Caught throwable while processing event M_RS_CLOSE_REGION
java.lang.RuntimeException: java.net.SocketTimeoutException: 70000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.230.229.25:42201 remote=/10.230.229.20:50010]
        at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:165)
        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketTimeoutException: 70000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.230.229.25:42201 remote=/10.230.229.20:50010]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Few important log traces from above :&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;2015-04-06 11:47:33,319 INFO ... regionserver.HRegion - Started memstore flush for ... 884fd5819112370d9a9834895b0ec19c ..
..
2015-04-06 11:47:33,328 INFO ... regionserver.HRegion - Started memstore flush for ... 884fd5819112370d9a9834895b0ec19c ..
...
2015-04-06 11:47:33,328 WARN ... regionserver.MemStore - Snapshot called again without clearing previous. Doing nothing. Another ongoing flush or did we fail last attempt?
...
2015-04-06 11:47:33,334 FATAL ... regionserver.HRegionServer - ABORTING region server xyz,60020,1428318082860: Unrecoverable exception while closing region 
...
2015-04-06 11:47:33,344 ERROR ... executor.EventHandler - Caught throwable while processing event M_RS_CLOSE_REGION
java.lang.RuntimeException: java.net.SocketTimeoutException: 70000 millis timeout while waiting for channel to be ready for read. ch 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;What I am suspecting is as follows:&lt;br/&gt;
From above logs it looks like somehow two threads are trying to run close concurrently on the same region. In HRegion class, for region close path, I am seeing one possible concurrency issue in internalFlushcache during memstore snapshoting &amp;amp; flush before close. Here MultiVersionConsistencyControl.WriteEntry is getting initialized with a lock, but mvcc.waitForRead(w); is happening after releasing the lock. In this case second thread can re-initialize the w object (w = mvcc.beginMemstoreInsert()) while the first thread is still using this object ( mvcc.waitForRead(w)). This could lead to a case where waitForRead() continues to wait indefinitely ultimately timing out. &lt;/p&gt;


&lt;p&gt;------------ HRegion.java ---------------&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;protected FlushResult internalFlushcache(final HLog wal, final long myseqid, MonitoredTask status) {
...

MultiVersionConsistencyControl.WriteEntry w = null;  

this.updatesLock.writeLock().lock();

try {

....

w = mvcc.beginMemstoreInsert(); //&amp;lt;-- long nextWriteNumber = ++memstoreWrite; WriteEntry e = new WriteEntry(nextWriteNumber); return e

...

} finally {

      this.updatesLock.writeLock().unlock();

    }

.....

    mvcc.waitForRead(w);  //&amp;lt;-- Concurrency issue if first thread is still using w object and next thread runs mvcc.beginMemstoreInsert() which will re-initialize w leading to indefinite wait and timeout
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So I think moving mvcc.waitForRead(w); within the locking scope of updatesLock.writeLock() should solve this.&lt;/p&gt;

&lt;p&gt;Any thoughts on this ?&lt;/p&gt;
</comment>
                            <comment id="14484660" author="apurtell" created="Wed, 8 Apr 2015 03:41:48 +0000"  >&lt;p&gt;Isn&apos;t &apos;w&apos; in local scope? So two threads both executing internalFlushcache will have distinct private values for &apos;w&apos; in frames on their own stacks. I don&apos;t think the issue is exactly what you describe, but that&apos;s not to say there isn&apos;t a locking protocol problem here somewhere. (FWIW, the &apos;mvcc&apos; object could be shared between multiple threads running on the same region and we do have a synchronized access to the mvcc&apos;s pending queue of writes in beginMemstoreInsert.) Would it be possible to get a complete stack dump from a regionserver that has a region stuck in PENDING_CLOSE? Use jstack or the dump servlet http://&amp;lt;rs&amp;gt;:&amp;lt;http-port&amp;gt;/dump &lt;/p&gt;</comment>
                            <comment id="14484672" author="vik.karma" created="Wed, 8 Apr 2015 03:59:01 +0000"  >&lt;p&gt;Ahh .. right &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=apurtell&quot; class=&quot;user-hover&quot; rel=&quot;apurtell&quot;&gt;Andrew Purtell&lt;/a&gt;, sorry missed that. Unfortunately I don&apos;t have the setup running now we had to schedule another test on it. I will try to reproduce this and post the stack trace once the test env is free again.&lt;/p&gt;</comment>
                            <comment id="14502795" author="anoop.hbase" created="Mon, 20 Apr 2015 13:39:14 +0000"  >&lt;p&gt;Is this any way similar to &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-10499&quot; title=&quot;In write heavy scenario one of the regions does not get flushed causing RegionTooBusyException&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-10499&quot;&gt;&lt;del&gt;HBASE-10499&lt;/del&gt;&lt;/a&gt; Vikas?&lt;br/&gt;
I see that was fixed in 0.98.10 and you use that version already !!!!  &lt;/p&gt;</comment>
                            <comment id="14504833" author="vik.karma" created="Tue, 21 Apr 2015 11:54:08 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=anoop.hbase&quot; class=&quot;user-hover&quot; rel=&quot;anoop.hbase&quot;&gt;Anoop Sam John&lt;/a&gt; let me check and get back on this. I do have a similar reproducible case of regions getting stuck, when all or most of the DataNodes are killed/suspended and restarted while HBase is still up and clients are still writing to it. I am trying out few combinations and to narrow it down. I will post my observations here soon.&lt;/p&gt;</comment>
                            <comment id="14505410" author="esteban" created="Tue, 21 Apr 2015 18:11:57 +0000"  >&lt;p&gt;Are you using the features added in &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-3703&quot; title=&quot;Decrease the datanode failure detection time&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-3703&quot;&gt;&lt;del&gt;HDFS-3703&lt;/del&gt;&lt;/a&gt; to skip stale DNs? If this is happening when killing most of the DNs I think is expected for the RS to enter into a long PENDING_CLOSE (and also during other states) until the HDFS pipeline can be reconstructed. So depending for how long the DNs were down this should or shouldn&apos;t have recovered.&lt;/p&gt;</comment>
                            <comment id="14505873" author="apurtell" created="Tue, 21 Apr 2015 21:59:13 +0000"  >&lt;blockquote&gt;&lt;p&gt;Are you using the features added in &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-3703&quot; title=&quot;Decrease the datanode failure detection time&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-3703&quot;&gt;&lt;del&gt;HDFS-3703&lt;/del&gt;&lt;/a&gt; to skip stale DNs? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes we are, but we not killing most DNs at once. Only one goes down at a time. First we drain the RS, then shut down service on the node, then refresh the software, then start services on the node back up. When I look at release automation logs looks like we are completing work on one node every ~10 minutes.&lt;/p&gt;</comment>
                            <comment id="14506333" author="vik.karma" created="Wed, 22 Apr 2015 03:19:57 +0000"  >&lt;p&gt;yes, as &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=apurtell&quot; class=&quot;user-hover&quot; rel=&quot;apurtell&quot;&gt;Andrew Purtell&lt;/a&gt; mentioned above we are seeing the regions getting stuck in production environment during rolling upgrade (one node at a time) however using the same process on a clean test environment the issue is not very reproducible. On test setup regions getting stuck in different states (PENDING_CLOSE, FAILED_CLOSE, PENDING_OPEN) is fairly reproducible when we do some disruptive scenarios like killing and re-starting many datanodes at the same time. Having said that this may not be the same as production case but I am just working with what is reproducible and seeing if I can get any clues from there. &lt;/p&gt;</comment>
                            <comment id="14506464" author="vik.karma" created="Wed, 22 Apr 2015 05:41:09 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=esteban&quot; class=&quot;user-hover&quot; rel=&quot;esteban&quot;&gt;Esteban Gutierrez&lt;/a&gt;, also in the disruptive case the issue is that the regions continue to be stuck forever even after all the DataNodes are back up and the HDFS layer has recovered completely. I am checking with DFS timeout fix provided by &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=apurtell&quot; class=&quot;user-hover&quot; rel=&quot;apurtell&quot;&gt;Andrew Purtell&lt;/a&gt; which is a clone of &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-7005&quot; title=&quot;DFS input streams do not timeout&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-7005&quot;&gt;&lt;del&gt;HDFS-7005&lt;/del&gt;&lt;/a&gt;, for the reproducible case first, then possibly check if it fixes other similar scenarios also. &lt;/p&gt;</comment>
                            <comment id="14507195" author="esteban" created="Wed, 22 Apr 2015 14:51:25 +0000"  >&lt;p&gt;That is very interesting. I think I saw something like that long time ago if using Hadoop 2.0 and SCRs. Since we need to read back the snapshot to verify it was written to HDFS and then it hangs since the underlying DN is gone. In newer versions of Hadoop I haven&apos;t seen it, perhaps related?&lt;/p&gt;</comment>
                            <comment id="14507791" author="apurtell" created="Wed, 22 Apr 2015 20:08:21 +0000"  >&lt;blockquote&gt;&lt;p&gt;I think I saw something like that long time ago if using Hadoop 2.0 and SCRs.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Maybe. We are using SCR. Our Hadoop is based on 2.3.0 (CDH 5.0.1 - yes, I know, we are upgrading soon - plus the patch on &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-6440&quot; title=&quot;Support more than 2 NameNodes&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-6440&quot;&gt;&lt;del&gt;HDFS-6440&lt;/del&gt;&lt;/a&gt; ported to that version)&lt;/p&gt;</comment>
                            <comment id="14522733" author="vik.karma" created="Fri, 1 May 2015 03:56:29 +0000"  >&lt;p&gt;After going through the logs for this issue again post &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-13592&quot; title=&quot;RegionServer sometimes gets stuck during shutdown in case of cache flush failures&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-13592&quot;&gt;&lt;del&gt;HBASE-13592&lt;/del&gt;&lt;/a&gt;, both look similar. We can close this as a duplicate of &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-13592&quot; title=&quot;RegionServer sometimes gets stuck during shutdown in case of cache flush failures&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-13592&quot;&gt;&lt;del&gt;HBASE-13592&lt;/del&gt;&lt;/a&gt;. &lt;/p&gt;</comment>
                            <comment id="14522734" author="vik.karma" created="Fri, 1 May 2015 03:57:44 +0000"  >&lt;p&gt;Duplicate of &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-13592&quot; title=&quot;RegionServer sometimes gets stuck during shutdown in case of cache flush failures&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-13592&quot;&gt;&lt;del&gt;HBASE-13592&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Tue, 7 Apr 2015 16:55:15 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            1 year, 31 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2cx07:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>