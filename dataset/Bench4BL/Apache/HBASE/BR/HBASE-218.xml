<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 02 19:50:28 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-218/HBASE-218.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-218] [hbase] Hudson hangs AFTER test has finished</title>
                <link>https://issues.apache.org/jira/browse/HBASE-218</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Weird.  Last night TestBloomFilter was hung after junit had printed test had completed without error.  Just now, I noticed a hung TestHStore &amp;#8211; again after junit had printed out test had succeeded (Nigel Daley has reported he&apos;s seen at least two hangs in TestHStoreFile, perhaps in same location).&lt;/p&gt;

&lt;p&gt;Last night and just now I was unable to get a thread dump.&lt;/p&gt;

&lt;p&gt;Here is log from around this evenings hang:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;...
    [junit] 2007-10-12 04:19:28,477 INFO  [main] org.apache.hadoop.hbase.TestHStoreFile.testOutOfRangeMidkeyHalfMapFile(TestHStoreFile.java:366): Last bottom when key &amp;gt; top: zz/zz/1192162768317
    [junit] 2007-10-12 04:19:28,493 WARN  [IPC Server handler 0 on 36620] org.apache.hadoop.dfs.FSDirectory.unprotectedDelete(FSDirectory.java:400): DIR* FSDirectory.unprotectedDelete: failed to remove /testOutOfRangeMidkeyHalfMapFile because it does not exist
    [junit] Shutting down the Mini HDFS Cluster
    [junit] Shutting down DataNode 1
    [junit] Shutting down DataNode 0
    [junit] 2007-10-12 04:19:29,316 WARN  [org.apache.hadoop.dfs.PendingReplicationBlocks$PendingReplicationMonitor@ed9f47] org.apache.hadoop.dfs.PendingReplicationBlocks$PendingReplicationMonitor.run(PendingReplicationBlocks.java:186): PendingReplicationMonitor thread received exception. java.lang.InterruptedException: sleep interrupted
    [junit] Tests run: 4, Failures: 0, Errors: 0, Time elapsed: 16.274 sec
    [junit] Running org.apache.hadoop.hbase.TestHTable
    [junit] Starting DataNode 0 with dfs.data.dir: /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/build/contrib/hbase/test/data/dfs/data/data1,/export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/build/contrib/hbase/test/data/dfs/data/data2
    [junit] Starting DataNode 1 with dfs.data.dir: /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/build/contrib/hbase/test/data/dfs/data/data3,/export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/build/contrib/hbase/test/data/dfs/data/data4
    [junit] 2007-10-12 05:21:48,332 INFO  [main] org.apache.hadoop.hbase.HMaster.&amp;lt;init&amp;gt;(HMaster.java:862): Root region dir: /hbase/hregion_-ROOT-,,0
...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Notice the hour of elapsed (hung) time in above.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12380213">HBASE-218</key>
            <summary>[hbase] Hudson hangs AFTER test has finished</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="stack">stack</reporter>
                        <labels>
                    </labels>
                <created>Fri, 12 Oct 2007 05:27:33 +0000</created>
                <updated>Fri, 22 Aug 2008 21:34:55 +0000</updated>
                            <resolved>Mon, 26 Nov 2007 18:47:23 +0000</resolved>
                                                                        <due></due>
                            <votes>0</votes>
                                    <watches>0</watches>
                                                                <comments>
                            <comment id="12534225" author="stack" created="Fri, 12 Oct 2007 05:46:09 +0000"  >&lt;p&gt;Looks like it hung again in same build &amp;#8211; #931 &amp;#8211; but this time in a test that hasn&apos;t been prone to hanging, TestListTables.  Again I can&apos;t get a thread dump but log is interesting on the way out:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    [junit] Shutting down the Mini HDFS Cluster
    [junit] Shutting down DataNode 1
    [junit] 2007-10-12 05:23:16,082 WARN  [DataNode: [/export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/build/contrib/hbase/test/data/dfs/data/data3,/export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/build/contrib/hbase/test/data/dfs/data/data4]] org.apache.hadoop.dfs.DataNode.offerService(DataNode.java:596): java.io.IOException: java.lang.InterruptedException
    [junit] Shutting down DataNode 0
    [junit] 	at org.apache.hadoop.fs.ShellCommand.runCommand(ShellCommand.java:59)
    [junit] 	at org.apache.hadoop.fs.ShellCommand.run(ShellCommand.java:42)
    [junit] 	at org.apache.hadoop.fs.DU.getUsed(DU.java:52)
    [junit] 	at org.apache.hadoop.dfs.FSDataset$FSVolume.getDfsUsed(FSDataset.java:299)
    [junit] 	at org.apache.hadoop.dfs.FSDataset$FSVolumeSet.getDfsUsed(FSDataset.java:396)
    [junit] 	at org.apache.hadoop.dfs.FSDataset.getDfsUsed(FSDataset.java:495)
    [junit] 	at org.apache.hadoop.dfs.DataNode.offerService(DataNode.java:520)
    [junit] 	at org.apache.hadoop.dfs.DataNode.run(DataNode.java:1494)
    [junit] 	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:595)

    [junit] 2007-10-12 05:23:16,349 WARN  [DataNode: [/export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/build/contrib/hbase/test/data/dfs/data/data1,/export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/build/contrib/hbase/test/data/dfs/data/data2]] org.apache.hadoop.dfs.DataNode.offerService(DataNode.java:596): java.io.InterruptedIOException
    [junit] 	at java.net.SocketOutputStream.socketWrite0(Native Method)
    [junit] 	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:92)
    [junit] 	at java.net.SocketOutputStream.write(SocketOutputStream.java:136)
    [junit] 	at org.apache.hadoop.ipc.Client$Connection$2.write(Client.java:192)
    [junit] 	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
    [junit] 	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)
    [junit] 	at java.io.DataOutputStream.flush(DataOutputStream.java:106)
    [junit] 	at org.apache.hadoop.ipc.Client$Connection.sendParam(Client.java:327)
    [junit] 	at org.apache.hadoop.ipc.Client.call(Client.java:474)
    [junit] 	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:184)
    [junit] 	at org.apache.hadoop.dfs.$Proxy1.sendHeartbeat(Unknown Source)
    [junit] 	at org.apache.hadoop.dfs.DataNode.offerService(DataNode.java:520)
    [junit] 	at org.apache.hadoop.dfs.DataNode.run(DataNode.java:1494)
    [junit] 	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:595)

    [junit] 2007-10-12 05:23:16,351 WARN  [org.apache.hadoop.dfs.PendingReplicationBlocks$PendingReplicationMonitor@157c2bd] org.apache.hadoop.dfs.PendingReplicationBlocks$PendingReplicationMonitor.run(PendingReplicationBlocks.java:186): PendingReplicationMonitor thread received exception. java.lang.InterruptedException: sleep interrupted
    [junit] 2007-10-12 05:23:16,610 INFO  [main] org.apache.hadoop.hbase.MiniHBaseCluster.shutdown(MiniHBaseCluster.java:424): Shutting down FileSystem
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 36.108 sec
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It reports tests succeeded but just before hand its reporting and interrupted flush.  I wonder if interrupt broke the flush.  It would be interesting to know (for &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-103&quot; title=&quot;[hbase] TestDFSAbort failed in nightly #242&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-103&quot;&gt;&lt;del&gt;HADOOP-1924&lt;/del&gt;&lt;/a&gt;).&lt;/p&gt;</comment>
                            <comment id="12534449" author="stack" created="Sat, 13 Oct 2007 00:10:51 +0000"  >&lt;p&gt;Patch to end thread dumping to hbase unit test teardown so can learn more about whats going on in these end-of-test hangs.  Intend to let it run on hudson for a day or two.&lt;/p&gt;</comment>
                            <comment id="12534450" author="stack" created="Sat, 13 Oct 2007 00:11:37 +0000"  >&lt;p&gt;Passing to hudson.&lt;/p&gt;</comment>
                            <comment id="12534464" author="stack" created="Sat, 13 Oct 2007 04:18:14 +0000"  >&lt;p&gt;The hudson patch build #940 is hung in TestHBaseCluster.  Here is the end-of-test thread dump (just before it prints test completed w/o error):&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    [junit] &lt;span class=&quot;code-object&quot;&gt;Process&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt; Dump: Temporary end-of-test thread dump debugging HADOOP-2040: testHBaseCluster
    [junit] 6 active threads
    [junit] &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt; 53 (process reaper):
    [junit]   State: RUNNABLE
    [junit]   Blocked count: 0
    [junit]   Waited count: 0
    [junit]   Stack:
    [junit]     java.lang.UNIXProcess.waitForProcessExit(Native Method)
    [junit]     java.lang.UNIXProcess.access$900(UNIXProcess.java:17)
    [junit]     java.lang.UNIXProcess$2$1.run(UNIXProcess.java:86)
    [junit] &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt; 30 (org.apache.hadoop.io.ObjectWritable Connection Culler):
    [junit]   State: TIMED_WAITING
    [junit]   Blocked count: 0
    [junit]   Waited count: 0
    [junit]   Stack:
    [junit]     java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.sleep(Native Method)
    [junit]     org.apache.hadoop.ipc.Client$ConnectionCuller.run(Client.java:404)
    [junit] &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt; 4 (Signal Dispatcher):
    [junit]   State: RUNNABLE
    [junit]   Blocked count: 0
    [junit]   Waited count: 0
    [junit]   Stack:
    [junit] &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt; 3 (Finalizer):
    [junit]   State: WAITING
    [junit]   Blocked count: 137
    [junit]   Waited count: 21
    [junit]   Waiting on java.lang.ref.ReferenceQueue$Lock@1931579
    [junit]   Stack:
    [junit]     java.lang.&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;.wait(Native Method)
    [junit]     java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:116)
    [junit]     java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:132)
    [junit]     java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:159)
    [junit] &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt; 2 (Reference Handler):
    [junit]   State: WAITING
    [junit]   Blocked count: 209
    [junit]   Waited count: 17
    [junit]   Waiting on java.lang.ref.Reference$Lock@166bfd8
    [junit]   Stack:
    [junit]     java.lang.&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;.wait(Native Method)
    [junit]     java.lang.&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;.wait(&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;.java:474)
    [junit]     java.lang.ref.Reference$ReferenceHandler.run(Reference.java:116)
    [junit] &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt; 1 (main):
    [junit]   State: RUNNABLE
    [junit]   Blocked count: 44
    [junit]   Waited count: 4095
    [junit]   Stack:
    [junit]     sun.management.ThreadImpl.getThreadInfo0(Native Method)
    [junit]     sun.management.ThreadImpl.getThreadInfo(ThreadImpl.java:144)
    [junit]     sun.management.ThreadImpl.getThreadInfo(ThreadImpl.java:120)
    [junit]     org.apache.hadoop.util.ReflectionUtils.printThreadInfo(ReflectionUtils.java:114)
    [junit]     org.apache.hadoop.hbase.HBaseClusterTestCase.tearDown(HBaseClusterTestCase.java:94)
    [junit]     junit.framework.TestCase.runBare(TestCase.java:130)
    [junit]     junit.framework.TestResult$1.protect(TestResult.java:106)
    [junit]     junit.framework.TestResult.runProtected(TestResult.java:124)
    [junit]     junit.framework.TestResult.run(TestResult.java:109)
    [junit]     junit.framework.TestCase.run(TestCase.java:118)
    [junit]     junit.framework.TestSuite.runTest(TestSuite.java:208)
    [junit]     junit.framework.TestSuite.run(TestSuite.java:203)
    [junit]     org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:297)
    [junit]     org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:672)
    [junit]     org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:567)
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 64.096 sec
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Looking at previous unit tests, the odd-man-out is the unix process waiting on process end.  Whats that from?&lt;/p&gt;

&lt;p&gt;I can&apos;t get a thread dump at &lt;span class=&quot;error&quot;&gt;&amp;#91;hudson&amp;#93;&lt;/span&gt; dateSat Oct 13 04:17:30 GMT 2007.  Killing current test... so build can move on.&lt;/p&gt;</comment>
                            <comment id="12534484" author="hadoopqa" created="Sat, 13 Oct 2007 08:31:25 +0000"  >&lt;p&gt;+1 overall.  Here are the results of testing the latest attachment &lt;br/&gt;
&lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12367672/endoftesttd.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12367672/endoftesttd.patch&lt;/a&gt;&lt;br/&gt;
against trunk revision r584336.&lt;/p&gt;

&lt;p&gt;    @author +1.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    javadoc +1.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    javac +1.  The applied patch does not generate any new compiler warnings.&lt;/p&gt;

&lt;p&gt;    findbugs +1.  The patch does not introduce any new Findbugs warnings.&lt;/p&gt;

&lt;p&gt;    core tests +1.  The patch passed core unit tests.&lt;/p&gt;

&lt;p&gt;    contrib tests +1.  The patch passed contrib unit tests.&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/940/testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/940/testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/940/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/940/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html&lt;/a&gt;&lt;br/&gt;
Checkstyle results: &lt;a href=&quot;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/940/artifact/trunk/build/test/checkstyle-errors.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/940/artifact/trunk/build/test/checkstyle-errors.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/940/console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/940/console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="12534924" author="stack" created="Mon, 15 Oct 2007 18:08:00 +0000"  >&lt;p&gt;Thread dump looks like &lt;a href=&quot;http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4811767&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4811767&lt;/a&gt;.  1.5JVMs are not supposed to have the issue.&lt;/p&gt;
</comment>
                            <comment id="12541356" author="stack" created="Fri, 9 Nov 2007 15:49:45 +0000"  >&lt;p&gt;Hudson is hung.  Here is tail of log.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    [junit] 2007-11-09 08:20:49,385 DEBUG [main] org.apache.hadoop.hbase.TestLogRolling.countLogFiles(TestLogRolling.java:174): number of log files: 1
    [junit] 2007-11-09 08:20:49,386 INFO  [main] org.apache.hadoop.hbase.TestLogRolling.testLogRolling(TestLogRolling.java:191): Finished writing. There are 1 log files. Sleeping to let cache flusher and log roller run
    [junit] 2007-11-09 08:20:49,386 DEBUG [main] org.apache.hadoop.hbase.LocalHBaseCluster.shutdown(LocalHBaseCluster.java:202): Shutting down HBase Cluster
    [junit] 2007-11-09 08:20:49,488 DEBUG [RegionServer:0] org.apache.hadoop.hbase.HRegionServer.run(HRegionServer.java:502): Got regionserver stop message
    [junit] 2007-11-09 08:20:49,488 INFO  [RegionServer:0] org.apache.hadoop.hbase.Leases.close(Leases.java:109): RegionServer:0 closing leases
    [junit] 2007-11-09 08:20:49,489 INFO  [RegionServer:0.leaseChecker] org.apache.hadoop.hbase.Chore.run(Chore.java:62): RegionServer:0.leaseChecker exiting
    [junit] 2007-11-09 08:20:49,489 INFO  [RegionServer:0] org.apache.hadoop.hbase.Leases.close(Leases.java:123): RegionServer:0 closed leases
    [junit] 2007-11-09 08:20:49,490 INFO  [RegionServer:0.logRoller] org.apache.hadoop.hbase.Chore.run(Chore.java:62): RegionServer:0.logRoller exiting
    [junit] 2007-11-09 08:20:49,607 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,608 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,608 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,608 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,608 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,608 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,609 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,609 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,609 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,609 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,609 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,610 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,610 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,610 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,610 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,610 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,611 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,611 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,611 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,611 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,612 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,612 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,612 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,612 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,612 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,613 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,613 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,613 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,613 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,613 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,614 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,614 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,614 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,614 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,615 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,615 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,615 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,615 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,814 WARN  [IPC Server handler 5 on 58346] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,830 DEBUG [RegionServer:0.cacheFlusher] org.apache.hadoop.hbase.HStore.flushCacheHelper(HStore.java:504): Added -1547818355/info/8261001142386214874 with sequence id 2208 and size 16.8k
    [junit] 2007-11-09 08:20:49,830 DEBUG [RegionServer:0.cacheFlusher] org.apache.hadoop.hbase.HRegion.internalFlushcache(HRegion.java:919): Finished memcache flush &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region testLogRolling,row1025,1194596368242 in 523ms
    [junit] 2007-11-09 08:20:49,831 DEBUG [RegionServer:0] org.apache.hadoop.hbase.HRegionServer.closeAllRegions(HRegionServer.java:971): closing region -ROOT-,,0
    [junit] 2007-11-09 08:20:49,831 INFO  [RegionServer:0.splitOrCompactChecker] org.apache.hadoop.hbase.Chore.run(Chore.java:62): RegionServer:0.splitOrCompactChecker exiting
    [junit] 2007-11-09 08:20:49,832 DEBUG [RegionServer:0] org.apache.hadoop.hbase.HRegion.internalFlushcache(HRegion.java:847): Started memcache flush &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region -ROOT-,,0. Size 0.0
    [junit] 2007-11-09 08:20:49,832 DEBUG [RegionServer:0] org.apache.hadoop.hbase.HRegion.internalFlushcache(HRegion.java:865): Finished memcache flush; empty snapshot
    [junit] 2007-11-09 08:20:49,833 DEBUG [RegionServer:0] org.apache.hadoop.hbase.HStore.close(HStore.java:419): closed -70236052/info
    [junit] 2007-11-09 08:20:49,833 INFO  [RegionServer:0] org.apache.hadoop.hbase.HRegion.close(HRegion.java:402): closed -ROOT-,,0
    [junit] 2007-11-09 08:20:49,833 DEBUG [RegionServer:0] org.apache.hadoop.hbase.HRegionServer.closeAllRegions(HRegionServer.java:971): closing region .META.,,1
    [junit] 2007-11-09 08:20:49,833 DEBUG [RegionServer:0] org.apache.hadoop.hbase.HRegion.internalFlushcache(HRegion.java:847): Started memcache flush &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region .META.,,1. Size 0.0
    [junit] 2007-11-09 08:20:49,833 DEBUG [RegionServer:0] org.apache.hadoop.hbase.HRegion.internalFlushcache(HRegion.java:865): Finished memcache flush; empty snapshot
    [junit] 2007-11-09 08:20:49,833 DEBUG [RegionServer:0] org.apache.hadoop.hbase.HStore.close(HStore.java:419): closed 1028785192/info
    [junit] 2007-11-09 08:20:49,834 INFO  [RegionServer:0] org.apache.hadoop.hbase.HRegion.close(HRegion.java:402): closed .META.,,1
    [junit] 2007-11-09 08:20:49,834 DEBUG [RegionServer:0] org.apache.hadoop.hbase.HRegionServer.closeAllRegions(HRegionServer.java:971): closing region testLogRolling,,1194596277787
    [junit] 2007-11-09 08:20:49,834 DEBUG [RegionServer:0] org.apache.hadoop.hbase.HRegion.internalFlushcache(HRegion.java:847): Started memcache flush &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region testLogRolling,,1194596277787. Size 0.0
    [junit] 2007-11-09 08:20:49,834 DEBUG [RegionServer:0] org.apache.hadoop.hbase.HRegion.internalFlushcache(HRegion.java:865): Finished memcache flush; empty snapshot
    [junit] 2007-11-09 08:20:49,835 DEBUG [RegionServer:0] org.apache.hadoop.hbase.HStore.close(HStore.java:419): closed 216611736/info
    [junit] 2007-11-09 08:20:49,835 INFO  [RegionServer:0] org.apache.hadoop.hbase.HRegion.close(HRegion.java:402): closed testLogRolling,,1194596277787
    [junit] 2007-11-09 08:20:49,835 DEBUG [RegionServer:0] org.apache.hadoop.hbase.HRegionServer.closeAllRegions(HRegionServer.java:971): closing region testLogRolling,row0513,1194596368241
    [junit] 2007-11-09 08:20:49,835 DEBUG [RegionServer:0] org.apache.hadoop.hbase.HRegion.internalFlushcache(HRegion.java:847): Started memcache flush &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region testLogRolling,row0513,1194596368241. Size 0.0
    [junit] 2007-11-09 08:20:49,835 DEBUG [RegionServer:0] org.apache.hadoop.hbase.HRegion.internalFlushcache(HRegion.java:865): Finished memcache flush; empty snapshot
    [junit] 2007-11-09 08:20:49,836 DEBUG [RegionServer:0] org.apache.hadoop.hbase.HStore.close(HStore.java:419): closed 1463872906/info
    [junit] 2007-11-09 08:20:49,836 INFO  [RegionServer:0] org.apache.hadoop.hbase.HRegion.close(HRegion.java:402): closed testLogRolling,row0513,1194596368241
    [junit] 2007-11-09 08:20:49,836 DEBUG [RegionServer:0] org.apache.hadoop.hbase.HRegionServer.closeAllRegions(HRegionServer.java:971): closing region testLogRolling,row1025,1194596368242
    [junit] 2007-11-09 08:20:49,836 DEBUG [RegionServer:0] org.apache.hadoop.hbase.HRegion.internalFlushcache(HRegion.java:847): Started memcache flush &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region testLogRolling,row1025,1194596368242. Size 0.0
    [junit] 2007-11-09 08:20:49,836 DEBUG [RegionServer:0] org.apache.hadoop.hbase.HRegion.internalFlushcache(HRegion.java:865): Finished memcache flush; empty snapshot
    [junit] 2007-11-09 08:20:49,837 DEBUG [RegionServer:0] org.apache.hadoop.hbase.HStore.close(HStore.java:419): closed -1547818355/info
    [junit] 2007-11-09 08:20:49,837 INFO  [RegionServer:0] org.apache.hadoop.hbase.HRegion.close(HRegion.java:402): closed testLogRolling,row1025,1194596368242
    [junit] 2007-11-09 08:20:49,837 DEBUG [RegionServer:0] org.apache.hadoop.hbase.HLog.close(HLog.java:382): closing log writer in /hbase/log_140.211.11.75_-2039724685788569167_58358
    [junit] 2007-11-09 08:20:49,838 WARN  [IPC Server handler 3 on 58346] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,848 INFO  [RegionServer:0] org.apache.hadoop.hbase.HRegionServer.run(HRegionServer.java:603): telling master that region server is shutting down at: 140.211.11.75:58358
    [junit] 2007-11-09 08:20:49,849 DEBUG [IPC Server handler 4 on 60000] org.apache.hadoop.hbase.HMaster.regionServerReport(HMaster.java:1316): Region server 140.211.11.75:58358: MSG_REPORT_EXITING -- cancelling lease
    [junit] 2007-11-09 08:20:49,849 INFO  [IPC Server handler 4 on 60000] org.apache.hadoop.hbase.HMaster.cancelLease(HMaster.java:1438): Cancelling lease &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 140.211.11.75:58358
    [junit] 2007-11-09 08:20:49,849 INFO  [IPC Server handler 4 on 60000] org.apache.hadoop.hbase.HMaster.regionServerReport(HMaster.java:1323): Region server 140.211.11.75:58358: MSG_REPORT_EXITING -- lease cancelled
    [junit] 2007-11-09 08:20:49,850 INFO  [RegionServer:0] org.apache.hadoop.hbase.HRegionServer.run(HRegionServer.java:610): stopping server at: 140.211.11.75:58358
    [junit] 2007-11-09 08:20:49,977 INFO  [RegionServer:0.worker] org.apache.hadoop.hbase.HRegionServer$Worker.run(HRegionServer.java:920): worker thread exiting
    [junit] 2007-11-09 08:20:49,977 INFO  [RegionServer:0] org.apache.hadoop.hbase.HRegionServer.run(HRegionServer.java:615): RegionServer:0 exiting
    [junit] 2007-11-09 08:20:50,947 INFO  [HMaster.metaScanner] org.apache.hadoop.hbase.Chore.run(Chore.java:62): HMaster.metaScanner exiting
    [junit] 2007-11-09 08:20:50,948 INFO  [HMaster] org.apache.hadoop.hbase.Leases.close(Leases.java:109): HMaster closing leases
    [junit] 2007-11-09 08:20:50,947 INFO  [HMaster.rootScanner] org.apache.hadoop.hbase.Chore.run(Chore.java:62): HMaster.rootScanner exiting
    [junit] 2007-11-09 08:20:50,949 INFO  [HMaster.leaseChecker] org.apache.hadoop.hbase.Chore.run(Chore.java:62): HMaster.leaseChecker exiting
    [junit] 2007-11-09 08:20:50,949 INFO  [HMaster] org.apache.hadoop.hbase.Leases.close(Leases.java:123): HMaster closed leases
    [junit] 2007-11-09 08:20:50,949 INFO  [HMaster] org.apache.hadoop.hbase.HMaster.run(HMaster.java:1163): HMaster main thread exiting
    [junit] 2007-11-09 08:20:50,949 INFO  [main] org.apache.hadoop.hbase.LocalHBaseCluster.shutdown(LocalHBaseCluster.java:226): Shutdown HMaster 1 region server(s)
    [junit] Shutting down the Mini HDFS Cluster
    [junit] Shutting down DataNode 1
    [junit] 2007-11-09 08:20:51,709 WARN  [DataNode: [/export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/build/contrib/hbase/test/data/dfs/data/data3,/export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/build/contrib/hbase/test/data/dfs/data/data4]] org.apache.hadoop.dfs.DataNode.offerService(DataNode.java:617): java.io.IOException: java.lang.InterruptedException
    [junit] 	at org.apache.hadoop.fs.ShellCommand.runCommand(ShellCommand.java:59)
    [junit] 	at org.apache.hadoop.fs.ShellCommand.run(ShellCommand.java:42)
    [junit] 	at org.apache.hadoop.fs.DU.getUsed(DU.java:52)
    [junit] 	at org.apache.hadoop.dfs.FSDataset$FSVolume.getDfsUsed(FSDataset.java:299)
    [junit] 	at org.apache.hadoop.dfs.FSDataset$FSVolumeSet.getDfsUsed(FSDataset.java:396)
    [junit] 	at org.apache.hadoop.dfs.FSDataset.getDfsUsed(FSDataset.java:495)
    [junit] 	at org.apache.hadoop.dfs.DataNode.offerService(DataNode.java:532)
    [junit] 	at org.apache.hadoop.dfs.DataNode.run(DataNode.java:1695)
    [junit] 	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:595)

    [junit] Shutting down DataNode 0
    [junit] 2007-11-09 08:20:52,252 WARN  [org.apache.hadoop.dfs.PendingReplicationBlocks$PendingReplicationMonitor@105b99f] org.apache.hadoop.dfs.PendingReplicationBlocks$PendingReplicationMonitor.run(PendingReplicationBlocks.java:186): PendingReplicationMonitor thread received exception. java.lang.InterruptedException: sleep interrupted
    [junit] 2007-11-09 08:20:52,570 ERROR [org.apache.hadoop.dfs.DataNode$DataXceiver@166c114] org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:839): DataXceiver: java.io.IOException: df: (/export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/build/contrib/hbase/test/data/dfs/data/data3) not a block device, directory or mounted resource
    [junit] 	at org.apache.hadoop.fs.ShellCommand.runCommand(ShellCommand.java:52)
    [junit] 	at org.apache.hadoop.fs.ShellCommand.run(ShellCommand.java:42)
    [junit] 	at org.apache.hadoop.fs.DF.getAvailable(DF.java:72)
    [junit] 	at org.apache.hadoop.dfs.FSDataset$FSVolume.getAvailable(FSDataset.java:308)
    [junit] 	at org.apache.hadoop.dfs.FSDataset$FSVolumeSet.getNextVolume(FSDataset.java:386)
    [junit] 	at org.apache.hadoop.dfs.FSDataset.writeToBlock(FSDataset.java:580)
    [junit] 	at org.apache.hadoop.dfs.DataNode$BlockReceiver.&amp;lt;init&amp;gt;(DataNode.java:1458)
    [junit] 	at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:929)
    [junit] 	at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:824)
    [junit] 	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:595)

    [junit] Exception! java.io.IOException: No such file or directory
    [junit] 2007-11-09 08:20:52,864 ERROR [org.apache.hadoop.dfs.DataNode$DataXceiver@1c0b8a0] org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:839): DataXceiver: java.io.IOException: df: (/export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/build/contrib/hbase/test/data/dfs/data/data4) not a block device, directory or mounted resource
    [junit] 	at org.apache.hadoop.fs.ShellCommand.runCommand(ShellCommand.java:52)
    [junit] 	at org.apache.hadoop.fs.ShellCommand.run(ShellCommand.java:42)
    [junit] 	at org.apache.hadoop.fs.DF.getCapacity(DF.java:62)
    [junit] 	at org.apache.hadoop.dfs.FSDataset$FSVolume.getCapacity(FSDataset.java:303)
    [junit] 	at org.apache.hadoop.dfs.FSDataset$FSVolume.getAvailable(FSDataset.java:307)
    [junit] 	at org.apache.hadoop.dfs.FSDataset$FSVolumeSet.getNextVolume(FSDataset.java:386)
    [junit] 	at org.apache.hadoop.dfs.FSDataset.writeToBlock(FSDataset.java:580)
    [junit] 	at org.apache.hadoop.dfs.DataNode$BlockReceiver.&amp;lt;init&amp;gt;(DataNode.java:1458)
    [junit] 	at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:929)
    [junit] 	at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:824)
    [junit] 	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:595)

    [junit] 2007-11-09 08:20:52,864 WARN  [org.apache.hadoop.dfs.DataNode$DataTransfer@6e3e5e] org.apache.hadoop.dfs.DataNode$DataTransfer.run(DataNode.java:1668): Failed to transfer blk_-379738272651084333 to 127.0.0.1:50011 got java.io.IOException: operation failed at /127.0.0.1
    [junit] 	at org.apache.hadoop.dfs.DataNode.receiveResponse(DataNode.java:725)
    [junit] 	at org.apache.hadoop.dfs.DataNode.access$200(DataNode.java:80)
    [junit] 	at org.apache.hadoop.dfs.DataNode$DataTransfer.run(DataNode.java:1664)
    [junit] 	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:595)

    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 344.356 sec
    [junit] 2007-11-09 08:20:52,864 WARN  [org.apache.hadoop.dfs.DataNode$DataXceiver@b34646] org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:995): Error writing reply back to /127.0.0.1 &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; writing block blk_-3764842785131980349
    [junit] java.net.SocketException: Broken pipe
    [junit] 	at java.net.SocketOutputStream.socketWrite0(Native Method)
    [junit] 	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:92)
    [junit] 	at java.net.SocketOutputStream.write(SocketOutputStream.java:115)
    [junit] 	at java.io.DataOutputStream.writeShort(DataOutputStream.java:151)
    [junit] 	at org.apache.hadoop.dfs.DataNode.sendResponse(DataNode.java:737)
    [junit] 	at org.apache.hadoop.dfs.DataNode.access$300(DataNode.java:80)
    [junit] 	at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:993)
    [junit] 	at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:824)
    [junit] 	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:595)

    [junit] Exception! java.io.IOException: No such file or directory
    [junit] 2007-11-09 08:20:53,748 ERROR [org.apache.hadoop.dfs.DataNode$DataXceiver@b34646] org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:839): DataXceiver: java.io.IOException: No such file or directory
    [junit] 	at java.io.UnixFileSystem.createFileExclusively(Native Method)
    [junit] 	at java.io.File.createNewFile(File.java:850)
    [junit] 	at org.apache.hadoop.dfs.FSDataset$FSVolume.createTmpFile(FSDataset.java:329)
    [junit] 	at org.apache.hadoop.dfs.FSDataset.createTmpFile(FSDataset.java:606)
    [junit] 	at org.apache.hadoop.dfs.FSDataset.writeToBlock(FSDataset.java:582)
    [junit] 	at org.apache.hadoop.dfs.DataNode$BlockReceiver.&amp;lt;init&amp;gt;(DataNode.java:1458)
    [junit] 	at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:929)
    [junit] 	at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:824)
    [junit] 	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:595)

    [junit] 2007-11-09 08:20:53,748 ERROR [org.apache.hadoop.dfs.DataNode$DataXceiver@1de4376] org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:839): DataXceiver: java.io.IOException: du: /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/build/contrib/hbase/test/data/dfs/data: No such file or directory
    [junit] 	at org.apache.hadoop.fs.ShellCommand.runCommand(ShellCommand.java:52)
    [junit] 	at org.apache.hadoop.fs.ShellCommand.run(ShellCommand.java:42)
    [junit] 	at org.apache.hadoop.fs.DU.getUsed(DU.java:52)
    [junit] 	at org.apache.hadoop.dfs.FSDataset$FSVolume.getDfsUsed(FSDataset.java:299)
    [junit] 	at org.apache.hadoop.dfs.FSDataset$FSVolume.getAvailable(FSDataset.java:307)
    [junit] 	at org.apache.hadoop.dfs.FSDataset$FSVolumeSet.getNextVolume(FSDataset.java:386)
    [junit] 	at org.apache.hadoop.dfs.FSDataset.writeToBlock(FSDataset.java:580)
    [junit] 	at org.apache.hadoop.dfs.DataNode$BlockReceiver.&amp;lt;init&amp;gt;(DataNode.java:1458)
    [junit] 	at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:929)
    [junit] 	at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:824)
    [junit] 	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:595)

    [junit] 2007-11-09 08:20:53,749 ERROR [org.apache.hadoop.dfs.DataNode$DataXceiver@71949b] org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:839): DataXceiver: java.io.IOException: No such file or directory
    [junit] 	at java.io.UnixFileSystem.createFileExclusively(Native Method)
    [junit] 	at java.io.File.createNewFile(File.java:850)
    [junit] 	at org.apache.hadoop.dfs.FSDataset$FSVolume.createTmpFile(FSDataset.java:329)
    [junit] 	at org.apache.hadoop.dfs.FSDataset.createTmpFile(FSDataset.java:606)
    [junit] 	at org.apache.hadoop.dfs.FSDataset.writeToBlock(FSDataset.java:582)
    [junit] 	at org.apache.hadoop.dfs.DataNode$BlockReceiver.&amp;lt;init&amp;gt;(DataNode.java:1458)
    [junit] 	at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:929)
    [junit] 	at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:824)
    [junit] 	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:595)

    [junit] Exception! java.io.IOException: No such file or directory
    [junit] 2007-11-09 08:20:53,800 ERROR [org.apache.hadoop.dfs.DataNode$DataXceiver@8f3d27] org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:839): DataXceiver: java.io.IOException: No such file or directory
    [junit] 	at java.io.UnixFileSystem.createFileExclusively(Native Method)
    [junit] 	at java.io.File.createNewFile(File.java:850)
    [junit] 	at org.apache.hadoop.dfs.FSDataset$FSVolume.createTmpFile(FSDataset.java:329)
    [junit] 	at org.apache.hadoop.dfs.FSDataset.createTmpFile(FSDataset.java:606)
    [junit] 	at org.apache.hadoop.dfs.FSDataset.writeToBlock(FSDataset.java:582)
    [junit] 	at org.apache.hadoop.dfs.DataNode$BlockReceiver.&amp;lt;init&amp;gt;(DataNode.java:1458)
    [junit] 	at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:929)
    [junit] 	at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:824)
    [junit] 	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:595)

    [junit] 2007-11-09 08:20:53,801 WARN  [org.apache.hadoop.dfs.DataNode$DataTransfer@1949f78] org.apache.hadoop.dfs.DataNode$DataTransfer.run(DataNode.java:1668): Failed to transfer blk_1352514030345870875 to 127.0.0.1:50011 got java.io.IOException: operation failed at /127.0.0.1
    [junit] 	at org.apache.hadoop.dfs.DataNode.receiveResponse(DataNode.java:725)
    [junit] 	at org.apache.hadoop.dfs.DataNode.access$200(DataNode.java:80)
    [junit] 	at org.apache.hadoop.dfs.DataNode$DataTransfer.run(DataNode.java:1664)
    [junit] 	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:595)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Test had not reported itself done.  Are these du&apos;ings and unix process invocations of interest? (Check).&lt;/p&gt;</comment>
                            <comment id="12541361" author="stack" created="Fri, 9 Nov 2007 16:03:25 +0000"  >&lt;p&gt;Looking more at this hang from last night, fs was sick from near the get-go:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    [junit] Starting DataNode 0 with dfs.data.dir: /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/build/contrib/hbase/test/data/dfs/data/data1,/export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/build/contrib/hbase/test/data/dfs/data/data2
    [junit] Starting DataNode 1 with dfs.data.dir: /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/build/contrib/hbase/test/data/dfs/data/data3,/export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/build/contrib/hbase/test/data/dfs/data/data4
    [junit] 2007-11-09 08:15:10,894 INFO  [main] org.apache.hadoop.hbase.HMaster.&amp;lt;init&amp;gt;(HMaster.java:895): Root region dir: /hbase/hregion_-70236052
    [junit] 2007-11-09 08:15:10,982 INFO  [main] org.apache.hadoop.hbase.HMaster.&amp;lt;init&amp;gt;(HMaster.java:904): bootstrap: creating ROOT and first META regions
    [junit] 2007-11-09 08:15:11,255 WARN  [main] org.apache.hadoop.util.NativeCodeLoader.&amp;lt;clinit&amp;gt;(NativeCodeLoader.java:51): Unable to load &lt;span class=&quot;code-keyword&quot;&gt;native&lt;/span&gt;-hadoop library &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; your platform... using builtin-java classes where applicable
    [junit] 2007-11-09 08:15:11,268 INFO  [main] org.apache.hadoop.hbase.HLog.rollWriter(HLog.java:298): &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; log writer created at /hbase/hregion_-70236052/log/hlog.dat.000
    [junit] 2007-11-09 08:15:11,341 DEBUG [main] org.apache.hadoop.hbase.HStore.&amp;lt;init&amp;gt;(HStore.java:182): starting -70236052/info (no reconstruction log)
    [junit] 2007-11-09 08:15:11,346 DEBUG [main] org.apache.hadoop.hbase.HStore.&amp;lt;init&amp;gt;(HStore.java:218): maximum sequence id &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; hstore -70236052/info is -1
    [junit] 2007-11-09 08:15:11,348 DEBUG [main] org.apache.hadoop.hbase.HRegion.&amp;lt;init&amp;gt;(HRegion.java:289): Next sequence id &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region -ROOT-,,0 is 0
    [junit] 2007-11-09 08:15:11,351 INFO  [main] org.apache.hadoop.hbase.HRegion.&amp;lt;init&amp;gt;(HRegion.java:315): region -ROOT-,,0 available
    [junit] 2007-11-09 08:15:11,368 INFO  [main] org.apache.hadoop.hbase.HLog.rollWriter(HLog.java:298): &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; log writer created at /hbase/hregion_1028785192/log/hlog.dat.000
    [junit] 2007-11-09 08:15:11,379 DEBUG [main] org.apache.hadoop.hbase.HStore.&amp;lt;init&amp;gt;(HStore.java:182): starting 1028785192/info (no reconstruction log)
    [junit] 2007-11-09 08:15:11,382 DEBUG [main] org.apache.hadoop.hbase.HStore.&amp;lt;init&amp;gt;(HStore.java:218): maximum sequence id &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; hstore 1028785192/info is -1
    [junit] 2007-11-09 08:15:11,384 DEBUG [main] org.apache.hadoop.hbase.HRegion.&amp;lt;init&amp;gt;(HRegion.java:289): Next sequence id &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region .META.,,1 is 0
    [junit] 2007-11-09 08:15:11,391 INFO  [main] org.apache.hadoop.hbase.HRegion.&amp;lt;init&amp;gt;(HRegion.java:315): region .META.,,1 available
    [junit] 2007-11-09 08:15:11,426 DEBUG [main] org.apache.hadoop.hbase.HRegion.internalFlushcache(HRegion.java:847): Started memcache flush &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region -ROOT-,,0. Size 86.0
    [junit] 2007-11-09 08:15:11,428 DEBUG [main] org.apache.hadoop.hbase.HRegion.internalFlushcache(HRegion.java:876): Snapshotted memcache &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region -ROOT-,,0 with sequence id 1 and entries 1
    [junit] 2007-11-09 08:15:11,519 WARN  [IPC Server handler 2 on 58346] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:15:11,988 WARN  [IPC Server handler 5 on 58346] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:15:12,036 WARN  [IPC Server handler 0 on 58346] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:15:12,098 DEBUG [main] org.apache.hadoop.hbase.HStore.flushCacheHelper(HStore.java:504): Added -70236052/info/4372126676279784460 with sequence id 1 and size 210.0
....
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Failure looks unrelated though the du&apos;s at end of test might be whats running the hung unix processes.&lt;/p&gt;</comment>
                            <comment id="12541397" author="shv" created="Fri, 9 Nov 2007 18:56:48 +0000"  >&lt;p&gt;Don&apos;t know whether it will help.&lt;br/&gt;
But it looks like you are shutting down MiniDFSCluster in the middle of file creates.&lt;br/&gt;
Some of the data-nodes were transferring data (I see writeBlock()) during the shutdown.&lt;br/&gt;
And they keep doing transfers even after the test is completed.&lt;/p&gt;</comment>
                            <comment id="12541463" author="stack" created="Fri, 9 Nov 2007 22:33:18 +0000"  >&lt;p&gt;Thanks for jumping in Konstantin.&lt;/p&gt;

&lt;p&gt;Code-wise the shutdown looks orderly... we shutdown hbase, not leaving the shutdown method till all hbase servers have exited.  We then call the shutdown on the mini DFS which waits first on datanodes to go down and then the namenode (The logging above aligns; see &apos;Shutting down the Mini HDFS Cluster&apos; happening after the exit of the HMaster thread).&lt;/p&gt;

&lt;p&gt;But something untoward is going on.  My working theory is that an interrupted invocation of a native unix command (&apos;du&apos; by the FileSystem?) is hanging hudson with some frequency (See 12/Oct/07 09:18 PM comment above).  If the FS were quiescent, I&apos;d imagine the hang could be avoided.&lt;/p&gt;

&lt;p&gt;This failure has loads of complaints of &apos;Not able to place enough replicas, still in need of 1&apos;.   They startup soon after the test starts.  What you think of that?  Maybe the dfs is not quiescent because its trying to replicate.  But I see that in MinDFSCluster it sets the replication to the number of data nodes.&lt;/p&gt;</comment>
                            <comment id="12542223" author="stack" created="Tue, 13 Nov 2007 20:25:36 +0000"  >&lt;p&gt;Patch flips closing filesystem with mindfscluster shutdown.  This is how tests for dfs do it and they don&apos;t seem to have our hanging problem (Hudson hung this morning running testinfoservers).&lt;/p&gt;</comment>
                            <comment id="12542224" author="stack" created="Tue, 13 Nov 2007 20:26:18 +0000"  >&lt;p&gt;Get a new +1 for this latest patch.&lt;/p&gt;</comment>
                            <comment id="12542265" author="hadoopqa" created="Tue, 13 Nov 2007 22:58:15 +0000"  >&lt;p&gt;+1 overall.  Here are the results of testing the latest attachment &lt;br/&gt;
&lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12369448/mhbc.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12369448/mhbc.patch&lt;/a&gt;&lt;br/&gt;
against trunk revision r594460.&lt;/p&gt;

&lt;p&gt;    @author +1.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    javadoc +1.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    javac +1.  The applied patch does not generate any new compiler warnings.&lt;/p&gt;

&lt;p&gt;    findbugs +1.  The patch does not introduce any new Findbugs warnings.&lt;/p&gt;

&lt;p&gt;    core tests +1.  The patch passed core unit tests.&lt;/p&gt;

&lt;p&gt;    contrib tests +1.  The patch passed contrib unit tests.&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1094/testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1094/testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1094/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1094/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html&lt;/a&gt;&lt;br/&gt;
Checkstyle results: &lt;a href=&quot;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1094/artifact/trunk/build/test/checkstyle-errors.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1094/artifact/trunk/build/test/checkstyle-errors.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1094/console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1094/console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="12542279" author="stack" created="Tue, 13 Nov 2007 23:38:45 +0000"  >&lt;p&gt;Committed.  Let it bake a while over on hudson.  See if it reduces hangs before closing.&lt;/p&gt;</comment>
                            <comment id="12542673" author="stack" created="Thu, 15 Nov 2007 05:42:12 +0000"  >&lt;p&gt;hudson was hung just now for 7 hours in TestHRegion.  TestHRegion is unorthodox &amp;#8211; intentionally &amp;#8211; in that it doesn&apos;t use inherit from HBaseClusterTestCase so it didn&apos;t pick up the changes made w/ mhbc.patch.  Hopefully its this anomaly that was responsible for the hang.  Attaching a patch that has TestHRegion do like the rest of the hbase (and hadoop dfs) tests shutting down the filesystem first and then the dfs cluster.&lt;/p&gt;</comment>
                            <comment id="12542674" author="stack" created="Thu, 15 Nov 2007 05:42:31 +0000"  >&lt;p&gt;Builds locally.  Trying hudson.&lt;/p&gt;</comment>
                            <comment id="12542696" author="hudson" created="Thu, 15 Nov 2007 07:31:35 +0000"  >&lt;p&gt;Integrated in Hadoop-Nightly #303 (See &lt;a href=&quot;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/303/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/303/&lt;/a&gt;)&lt;/p&gt;</comment>
                            <comment id="12542842" author="hadoopqa" created="Thu, 15 Nov 2007 18:45:40 +0000"  >&lt;p&gt;+1 overall.  Here are the results of testing the latest attachment &lt;br/&gt;
&lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12369568/thr.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12369568/thr.patch&lt;/a&gt;&lt;br/&gt;
against trunk revision r595246.&lt;/p&gt;

&lt;p&gt;    @author +1.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    javadoc +1.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    javac +1.  The applied patch does not generate any new compiler warnings.&lt;/p&gt;

&lt;p&gt;    findbugs +1.  The patch does not introduce any new Findbugs warnings.&lt;/p&gt;

&lt;p&gt;    core tests +1.  The patch passed core unit tests.&lt;/p&gt;

&lt;p&gt;    contrib tests +1.  The patch passed contrib unit tests.&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1101/testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1101/testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1101/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1101/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html&lt;/a&gt;&lt;br/&gt;
Checkstyle results: &lt;a href=&quot;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1101/artifact/trunk/build/test/checkstyle-errors.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1101/artifact/trunk/build/test/checkstyle-errors.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1101/console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1101/console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="12542859" author="stack" created="Thu, 15 Nov 2007 19:39:01 +0000"  >&lt;p&gt;Committed patch to TestHRegion, the second commit on this JIRA.&lt;/p&gt;</comment>
                            <comment id="12543219" author="hudson" created="Fri, 16 Nov 2007 22:02:31 +0000"  >&lt;p&gt;Integrated in Hadoop-Nightly #305 (See &lt;a href=&quot;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/305/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/305/&lt;/a&gt;)&lt;/p&gt;</comment>
                            <comment id="12545546" author="stack" created="Mon, 26 Nov 2007 18:47:23 +0000"  >&lt;p&gt;Hasn&apos;t been a recurrence in a while.  Reopen if start seeing it again.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12367672" name="endoftesttd.patch" size="2353" author="stack" created="Sat, 13 Oct 2007 00:10:51 +0000"/>
                            <attachment id="12369448" name="mhbc.patch" size="1149" author="stack" created="Tue, 13 Nov 2007 20:25:35 +0000"/>
                            <attachment id="12369568" name="thr.patch" size="1444" author="stack" created="Thu, 15 Nov 2007 05:42:12 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Sat, 13 Oct 2007 08:31:25 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>25002</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 2 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0h5jb:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>98159</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>