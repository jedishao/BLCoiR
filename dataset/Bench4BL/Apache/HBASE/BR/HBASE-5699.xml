<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sat Dec 03 18:14:31 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-5699/HBASE-5699.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-5699] Run with &gt; 1 WAL in HRegionServer</title>
                <link>https://issues.apache.org/jira/browse/HBASE-5699</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description></description>
                <environment></environment>
        <key id="12549194">HBASE-5699</key>
            <summary>Run with &gt; 1 WAL in HRegionServer</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.png">Critical</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="busbey">Sean Busbey</assignee>
                                    <reporter username="aoxiang">binlijin</reporter>
                        <labels>
                    </labels>
                <created>Mon, 2 Apr 2012 18:18:19 +0000</created>
                <updated>Mon, 21 Sep 2015 15:15:13 +0000</updated>
                            <resolved>Fri, 19 Dec 2014 16:49:06 +0000</resolved>
                                                    <fixVersion>1.0.0</fixVersion>
                    <fixVersion>2.0.0</fixVersion>
                    <fixVersion>1.1.0</fixVersion>
                                    <component>Performance</component>
                    <component>wal</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>59</watches>
                                                                                                            <comments>
                            <comment id="13244419" author="stack" created="Mon, 2 Apr 2012 18:34:34 +0000"  >&lt;p&gt;Please provide more detail on what this issue is about and correct the subject so it&apos;s properly spelled.  Thanks.&lt;/p&gt;</comment>
                            <comment id="13246144" author="aoxiang" created="Wed, 4 Apr 2012 09:57:57 +0000"  >&lt;p&gt;@stack, &lt;br/&gt;
There is only one HLog and a Writer in a HRegionServer for write-ahead-log, at any time only one writer takes the HLog lock and the other will wait. If there are muti HLog or Writer, the write-ahead-log can be run parallel, the write performance should be improved.  &lt;/p&gt;</comment>
                            <comment id="13246361" author="stack" created="Wed, 4 Apr 2012 15:27:16 +0000"  >&lt;p&gt;Yes.  This topic comes up from time to time.  Would be nice to try it out.  It is possible to stand up the WAL subsystem on its own so you could experiment having HLog output to &amp;gt; 1 WAL.  A bunch of us would be interested in what you learn.&lt;/p&gt;</comment>
                            <comment id="13246971" author="juhanic" created="Thu, 5 Apr 2012 03:02:41 +0000"  >&lt;p&gt;Since we have had some similar experience posting it here:&lt;br/&gt;
We are finding most of our IPC threads in our region servers locked into HWal.append(42 out of 50. Of those 20 are in sync, and one is actually working... As is to be expected).&lt;br/&gt;
We made the presumption that the problem was with the WAL synchronisation mechanisms holding things up and decided to try running multiple RS per node since we had significant amount of free CPU and memory resources as well as many barely active hard disks.&lt;br/&gt;
By running 3 RS per node, we saw our application specific throughput go from 7k events to 18k. Each event is made up of roughly 2 writes and 2 increments, plus some reads/scans which shouldn&apos;t be touching the WAL.&lt;br/&gt;
This situation is partially also just due to a very high spec per node. I don&apos;t think it would be necessary on more &quot;commodity&quot; type servers, but the option to use multiple WAL&apos;s on each region server may well give some significant throughput gains for some hardware setups.&lt;/p&gt;</comment>
                            <comment id="13249664" author="aoxiang" created="Mon, 9 Apr 2012 04:00:56 +0000"  >&lt;p&gt;@stack,&lt;br/&gt;
I run a test with 0.90 version use 10 writer and 3 nodes, some times it has double write performance, may be it not very well. &lt;/p&gt;</comment>
                            <comment id="13249686" author="zjushch" created="Mon, 9 Apr 2012 05:42:13 +0000"  >&lt;p&gt;I think the number of datanodes is a litte few in the test. Using double hlogWrites in RS, write performance should be nearly double except limit by the HDFS.&lt;/p&gt;</comment>
                            <comment id="13249902" author="stack" created="Mon, 9 Apr 2012 16:08:53 +0000"  >&lt;p&gt;@binlijin What Chunhui says.  I&apos;d think that if it were a bigger cluster you&apos;d see a more marked improvement.  What about recovery?  How does log splitting work with all the extra WALs?&lt;/p&gt;</comment>
                            <comment id="13250374" author="aoxiang" created="Tue, 10 Apr 2012 01:27:12 +0000"  >&lt;p&gt;I just run a test and don&apos;t test the recovery and the others.&lt;/p&gt;</comment>
                            <comment id="13250471" author="li" created="Tue, 10 Apr 2012 06:08:02 +0000"  >&lt;p&gt;This seems interesting. I&apos;ll take a look at doing this.&lt;/p&gt;</comment>
                            <comment id="13260005" author="stack" created="Mon, 23 Apr 2012 22:00:45 +0000"  >&lt;p&gt;@Ted Why delete a comment, especially someone elses?&lt;/p&gt;</comment>
                            <comment id="13260008" author="zhihyu@ebaysf.com" created="Mon, 23 Apr 2012 22:03:30 +0000"  >&lt;p&gt;It was a duplicate message.&lt;/p&gt;</comment>
                            <comment id="13260020" author="stack" created="Mon, 23 Apr 2012 22:24:04 +0000"  >&lt;p&gt;@Ted Would suggest you just leave it. When you delete, we all get a message in our mailbox about the delete transaction.  Then we start to wonder...&lt;/p&gt;</comment>
                            <comment id="13265076" author="zhihyu@ebaysf.com" created="Mon, 30 Apr 2012 18:17:24 +0000"  >&lt;p&gt;Playing with a prototype of this feature using ycsb (half insert, half upate) on a 5-node cluster where usertable has 13 regions on each region server.&lt;br/&gt;
Without this feature:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
 10 sec: 99965 operations; 9996.5 current ops/sec; [UPDATE AverageLatency(us)=258.68] [INSERT AverageLatency(us)=610.28]
 20 sec: 99965 operations; 0 current ops/sec;
 25 sec: 99990 operations; 4.3 current ops/sec; [UPDATE AverageLatency(us)=2594303.62] [INSERT AverageLatency(us)=1240495.41]
[OVERALL], RunTime(ms), 25844.0
[OVERALL], Throughput(ops/sec), 3868.9831295465096
[UPDATE], Operations, 49935
[UPDATE], AverageLatency(us), 674.2635626314209
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;with this feature:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
 10 sec: 99952 operations; 9994.2 current ops/sec; [UPDATE AverageLatency(us)=178.7] [INSERT AverageLatency(us)=584.76]
 20 sec: 99990 operations; 3.8 current ops/sec; [UPDATE AverageLatency(us)=10.88] [INSERT AverageLatency(us)=679174.27]
 20 sec: 99990 operations; 0 current ops/sec;
[OVERALL], RunTime(ms), 20867.0
[OVERALL], Throughput(ops/sec), 4791.776489193463
[UPDATE], Operations, 49992
[UPDATE], AverageLatency(us), 178.6439030244839
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13265112" author="eclark" created="Mon, 30 Apr 2012 18:59:40 +0000"  >&lt;p&gt;Intuitively it seems like the number of WAL&apos;s that are used should be related to the number of spindles available to hbase.  So maybe this should be either a configurable number or something that is derived from the number of mount points hdfs is hosted on ?&lt;/p&gt;</comment>
                            <comment id="13265128" author="zhihyu@ebaysf.com" created="Mon, 30 Apr 2012 19:21:01 +0000"  >&lt;p&gt;Currently I use the following knob for the maximum number of WAL&apos;s on an individual region server:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+    &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; totalInstances = conf.getInt(&lt;span class=&quot;code-quote&quot;&gt;&quot;hbase.regionserver.hlog.total&quot;&lt;/span&gt;, DEFAULT_MAX_HLOG_INSTANCES);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13265167" author="jdcryans" created="Mon, 30 Apr 2012 20:05:36 +0000"  >&lt;blockquote&gt;&lt;p&gt;Intuitively it seems like the number of WAL&apos;s that are used should be related to the number of spindles available to hbase.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I disagree, considering that most of the deployments have rep=3 you&apos;re using three spindles not one. The multiplying effect could generate a lot of disk seeks since the WALs are competing like that (plus flushing, compacting, etc).&lt;/p&gt;</comment>
                            <comment id="13265421" author="tlipcon" created="Mon, 30 Apr 2012 21:56:15 +0000"  >&lt;blockquote&gt;&lt;p&gt;I disagree, considering that most of the deployments have rep=3 you&apos;re using three spindles not one&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That said, most of our customers are deploying 6 disks if not 12 &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;IMO the other big gain we can get from multiple WALs is to automatically switch between WALs when one gets &quot;slow&quot;. IMO we should maintain a count of outstanding requests (probably by size) for each WAL, and submit writes to whichever has fewer outstanding requests. That way if one is faster, it will take more of the load. Then simultaneously measure trailing latency stats on each WAL, and if one is significantly slower than the other for some period of time, have it roll (to try to get a new set of disks/nodes)&lt;/p&gt;</comment>
                            <comment id="13265426" author="li" created="Mon, 30 Apr 2012 22:01:38 +0000"  >&lt;p&gt;Agree with todd on the implementation details. The switching of logs should also serve to help balance our log writes. &lt;/p&gt;</comment>
                            <comment id="13265521" author="zhihyu@ebaysf.com" created="Mon, 30 Apr 2012 23:54:08 +0000"  >&lt;p&gt;Trying to understand the implication of Todd&apos;s suggestion above.&lt;br/&gt;
Currently each HRegion has reference to the HLog it uses. If requests can be freely redirected to the HLog instance having fewer outstanding requests, the reference would be to that of the region server.&lt;br/&gt;
This means additional logic on region server for dispatching the write requests.&lt;/p&gt;</comment>
                            <comment id="13265565" author="jmhsieh" created="Tue, 1 May 2012 01:03:40 +0000"  >&lt;p&gt;Part of the motivation for multiple wals can be found in this tech talk: (most relavent to HBase is backup requests, starting slide 39)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en/us/people/jeff/Berkeley-Latency-Mar2012.pdf&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en/us/people/jeff/Berkeley-Latency-Mar2012.pdf&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13265567" author="jmhsieh" created="Tue, 1 May 2012 01:05:27 +0000"  >&lt;p&gt;The argument here is mostly aimed at read latency, but a similar idea could be used for write latency as well.&lt;/p&gt;</comment>
                            <comment id="13265596" author="tlipcon" created="Tue, 1 May 2012 01:44:38 +0000"  >&lt;blockquote&gt;&lt;p&gt;Currently each HRegion has reference to the HLog it uses. If requests can be freely redirected to the HLog instance having fewer outstanding requests, the reference would be to that of the region server.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Sorry, I should be less free-wheeling with my terminology. My thought was that there is still a single &quot;HLog&quot; class, but underneath it would be multiple &quot;SequenceFileLogWriters&quot;, most likely. Though maybe the correct implementation is to make HLog an interface, and then have a MultiHLog which wraps N other HLogs or something. Either way, any region would only have a reference to one &quot;HLog&quot; object, which might have more than one underlying stream.&lt;/p&gt;</comment>
                            <comment id="13265606" author="zhihyu@ebaysf.com" created="Tue, 1 May 2012 02:19:45 +0000"  >&lt;blockquote&gt;&lt;p&gt;to one &quot;HLog&quot; object, which might have more than one underlying stream.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The above can be a (sub-)task by itself.&lt;/p&gt;</comment>
                            <comment id="13265620" author="zhihyu@ebaysf.com" created="Tue, 1 May 2012 03:12:49 +0000"  >&lt;p&gt;Currently we maintain one sequence number per region per HLog. From append():&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
      &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.lastSeqWritten.putIfAbsent(regionInfo.getEncodedNameAsBytes(),
        &lt;span class=&quot;code-object&quot;&gt;Long&lt;/span&gt;.valueOf(seqNum));
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;If WALEdit&apos;s from a particular region can spread across multiple streams, accounting would be more complex.&lt;/p&gt;</comment>
                            <comment id="13265648" author="ram_krish" created="Tue, 1 May 2012 04:28:12 +0000"  >&lt;p&gt;Do we need to gurantee the HLog edits sequencing even with multiple WALs? Just referring to Stack&apos;s comment in &lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5782?focusedCommentId=13255344&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13255344&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HBASE-5782?focusedCommentId=13255344&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13255344&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13268010" author="li" created="Fri, 4 May 2012 01:16:06 +0000"  >&lt;p&gt;I&apos;m assuming we don&apos;t need to guarantee HLog edit sequencing. If we do, this becomes a bit harder.&lt;/p&gt;</comment>
                            <comment id="13268039" author="zhihyu@ebaysf.com" created="Fri, 4 May 2012 02:07:21 +0000"  >&lt;p&gt;Are replication related unit tests passing ?&lt;/p&gt;

&lt;p&gt;Since the review process would at least take a month, I think developing against a branch would be good practice.&lt;/p&gt;</comment>
                            <comment id="13268048" author="li" created="Fri, 4 May 2012 02:34:36 +0000"  >&lt;p&gt;Replication is the failure point. I haven&apos;t really worked on that yet.&lt;/p&gt;

&lt;p&gt;Talked to Jon about the dev process. I&apos;ll create a seperate Jira for refactoring HLog into an interface. I&apos;ll probably continue to work within trunk. &lt;/p&gt;

&lt;p&gt;Separate JIRA should make things easier though.&lt;/p&gt;</comment>
                            <comment id="13268055" author="zhihyu@ebaysf.com" created="Fri, 4 May 2012 02:49:05 +0000"  >&lt;p&gt;Using trunk has the drawback that performance numbers (without this feature) gathered on day N may be obsolete by day N + 5, considering the amount of changes going into trunk.&lt;/p&gt;

&lt;p&gt;I would suggest tackling replication as first priority. Dictionary WAL compression brought unexpected complexities w.r.t. replication. We shouldn&apos;t make replication any harder.&lt;/p&gt;

&lt;p&gt;w.r.t. refactoring HLog into an interface, I tend to think that the interface should make different implementations possible.&lt;br/&gt;
If we only have one implementation, it is not easy to evaluate the effectiveness of the refactoring.&lt;/p&gt;</comment>
                            <comment id="13268093" author="zhihyu@ebaysf.com" created="Fri, 4 May 2012 03:43:07 +0000"  >&lt;p&gt;Here&apos;re the key unit tests that must pass:&lt;/p&gt;

&lt;p&gt;TestDistributedLogSplitting, TestReplication, TestMasterReplication, TestMultiSlaveReplication, TestHLog, TestHLogSplit, TestLogRollAbort, TestLogRolling&lt;/p&gt;</comment>
                            <comment id="13268101" author="li" created="Fri, 4 May 2012 03:57:30 +0000"  >&lt;p&gt;While performance numbers will change, you can simply test with MultipleHLogs on and MultipleHLogsoff. I don&apos;t think we&apos;re going to move everyone over to multiple Hlogs immediately.&lt;/p&gt;

&lt;p&gt;Will take a look at those tests.&lt;/p&gt;</comment>
                            <comment id="13268103" author="zhihyu@ebaysf.com" created="Fri, 4 May 2012 04:05:56 +0000"  >&lt;p&gt;This feature requires validation in real cluster.&lt;/p&gt;

&lt;p&gt;@Jonathan:&lt;br/&gt;
Are you able to help Li in this regard ?&lt;/p&gt;

&lt;p&gt;From my experience in the past three weeks, development involves coding -&amp;gt; running test suite -&amp;gt; discovering defect through failed unit tests -&amp;gt; bug fixing -&amp;gt; validation through ycsb -&amp;gt; ...&lt;/p&gt;</comment>
                            <comment id="13268110" author="stack" created="Fri, 4 May 2012 04:18:21 +0000"  >&lt;p&gt;@Li You should be able to work whereever you like if you make up an harness for running hlog implementations apart from hbase.  This should be first order of business (unless you are a masochist).  Should be easy enough, if its not possible already, especially after you make it pluggable.&lt;/p&gt;

&lt;p&gt;Regards... &quot;I&apos;m assuming we don&apos;t need to guarantee HLog edit sequencing. If we do, this becomes a bit harder.&quot; &amp;#8211; well thats the way it is currently so onus will be on you to come up a reason why it could be otherwise. In-order makes it easier to reason about whether or not all edits up to a particular sequence id have been sync&apos;d or not.&lt;/p&gt;

&lt;p&gt;And don&apos;t forget the other side of the moon, the (distributed) log splitting story.  That needs to work too after you are done.&lt;/p&gt;</comment>
                            <comment id="13268196" author="jmhsieh" created="Fri, 4 May 2012 08:04:13 +0000"  >&lt;p&gt;Li, if you want to undertake this I&apos;ll help.  Let&apos;s chat and then write a one-two page summary of what our goals are, what our assumptions are, and what our intended mechanisms are, how we are going to test this and then loopback here with a design/plan to get feedback..  &lt;/p&gt;

&lt;p&gt;Another &quot;feature&quot; that may come into play also is the HLog compression.&lt;/p&gt;</comment>
                            <comment id="13268216" author="li" created="Fri, 4 May 2012 09:17:16 +0000"  >&lt;p&gt;I thought about the compression bit already. I was going to compress each separate log individually.&lt;/p&gt;

&lt;p&gt;Yeah, I should have probably wrote up what I was going to do before hacking stuff up. Will switch gears and work on that a bit instead.&lt;/p&gt;</comment>
                            <comment id="13268697" author="zhihyu@ebaysf.com" created="Fri, 4 May 2012 20:50:34 +0000"  >&lt;blockquote&gt;&lt;p&gt;there is still a single &quot;HLog&quot; class, but underneath it would be multiple &quot;SequenceFileLogWriters&quot;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;My approach is different from the above.&lt;br/&gt;
The new interface should be general enough that multi-HLog can be implemented without the requirement that HLog have multiple writers.&lt;/p&gt;</comment>
                            <comment id="13268786" author="jmhsieh" created="Fri, 4 May 2012 22:56:37 +0000"  >&lt;p&gt;Zhihong, I&apos;m curious to learn about the approach you have taken in the prototype that you have.  Is it on github somewhere perhaps?&lt;/p&gt;

&lt;p&gt;If you have multiple hlogs do you use a different hlog in different regions?  &lt;br/&gt;
Do you have a shim that looks like an hlog but has two hlogs inside it (as opposed to hdfs file handles)?&lt;/p&gt;</comment>
                            <comment id="13268789" author="zhihyu@ebaysf.com" created="Fri, 4 May 2012 23:00:25 +0000"  >&lt;blockquote&gt;&lt;p&gt;If you have multiple hlogs do you use a different hlog in different regions?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Correct.&lt;/p&gt;

&lt;p&gt;I have to go through legal procedure at my employer before disclosing my patch.&lt;/p&gt;</comment>
                            <comment id="13278615" author="ram_krish" created="Fri, 18 May 2012 06:43:42 +0000"  >&lt;p&gt;We are also interested in this.&lt;br/&gt;
Worked on a prototype with having one HLog instance but underlying there will be multiple writer instances.  The regions will be allocated with any one of the writer instance and each region will be writing to hlog using the instance associated with it.&lt;/p&gt;

&lt;p&gt;Even on logrolling the instances against each region will be updated and the region will continue to use its mapping.&lt;br/&gt;
Without patch &lt;br/&gt;
~53K puts/sec.&lt;/p&gt;

&lt;p&gt;With patch&lt;br/&gt;
~78-80k puts/sec&lt;/p&gt;

&lt;p&gt;It is a 3 node cluster and the size of each record was 1k. No of regions : 2800&lt;br/&gt;
By default used 3 writer instances.  I was able to pass the testcases related to TestHlog and TestDistributedLogSplitting. But Testmasterreplication was not passing.&lt;br/&gt;
Replication needs some change based on this which i did not work on much.&lt;/p&gt;

&lt;p&gt;The pendingWrites list that we use is now converted into a map having the writer with the list of pending writes.&lt;/p&gt;

&lt;p&gt;Pls provide your suggestions on this.  &lt;br/&gt;
BTW, Li Pi, any progress on this? I would love to help you in this.  &lt;br/&gt;
May be i can prepare a more forma patch and upload over here.&lt;/p&gt;</comment>
                            <comment id="13278927" author="zhihyu@ebaysf.com" created="Fri, 18 May 2012 16:26:24 +0000"  >&lt;p&gt;@Ramkrishna:&lt;br/&gt;
Your numbers look better than mine though the mix in my case was 50% updates and 50% puts.&lt;/p&gt;

&lt;p&gt;Can you publish latency numbers as well ?&lt;/p&gt;</comment>
                            <comment id="13279078" author="lhofhansl" created="Fri, 18 May 2012 18:55:11 +0000"  >&lt;p&gt;Should we explore a WAL per Region? Would be a lot of open files, but if it&apos;d work, we won&apos;t need log spitting anymore.&lt;/p&gt;</comment>
                            <comment id="13279125" author="zhihyu@ebaysf.com" created="Fri, 18 May 2012 19:52:04 +0000"  >&lt;p&gt;There would be many regions in a cluster. They may not receive even write load.&lt;/p&gt;

&lt;p&gt;We should set configuration parameter which governs the maximum number of concurrent WALs on each region server.&lt;/p&gt;</comment>
                            <comment id="13279289" author="li" created="Fri, 18 May 2012 23:01:04 +0000"  >&lt;p&gt;My design is a bit different. Ill upload a patch soon. I&apos;m doing any region&lt;br/&gt;
to any blog. Currently distributed log splitting and replication do not&lt;br/&gt;
work yet.&lt;/p&gt;
</comment>
                            <comment id="13279327" author="a.minor.internet@gmail.com" created="Fri, 18 May 2012 23:19:07 +0000"  >&lt;p&gt;Btw. I have finals and other stuff coming up. So it might be a while before&lt;br/&gt;
I finish my implementation. If anyone else wants to take a go at it. This&lt;br/&gt;
is cool.&lt;/p&gt;
</comment>
                            <comment id="13279661" author="lhofhansl" created="Sun, 20 May 2012 04:47:03 +0000"  >&lt;p&gt;I suspect this will become more important when people eventually turn on &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5954&quot; title=&quot;Allow proper fsync support for HBase&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5954&quot;&gt;&lt;del&gt;HBASE-5954&lt;/del&gt;&lt;/a&gt; (durable sync, if they don&apos;t run in data centers with backup power supplies).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;There would be many regions in a cluster. They may not receive even write load.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Is that necessarily a problem? Just saying that while we are exploring this, might as well explore this option as well. I for one be happy if a region&apos;s edits are tied to that region and log splitting could just go away (well almost, would still need to split if the region is split).&lt;/p&gt;</comment>
                            <comment id="13279667" author="tlipcon" created="Sun, 20 May 2012 05:24:26 +0000"  >&lt;p&gt;I think with durable sync, having a WAL-per-region would be even less feasible than it is today &amp;#8211; we currently depend on batching in order to get good throughput. If a server has 50 regions, then you&apos;d get 50x less batching opportunity and write throughput would grind to a halt. Imagine a fan-out write to all of the regions &amp;#8211; it would generate 50 disk seeks instead of just 1.&lt;/p&gt;</comment>
                            <comment id="13279912" author="lhofhansl" created="Mon, 21 May 2012 01:12:10 +0000"  >&lt;p&gt;Good point.&lt;/p&gt;

&lt;p&gt;Was referring to the general feature, not necessarily WAL/Region.&lt;br/&gt;
It&apos;s a trade off: Batching vs. parallel writes (just to state the obvious)&lt;/p&gt;

&lt;p&gt;Do we batch beyond a region normally, though? Maybe during cache flush.&lt;/p&gt;

&lt;p&gt;Yeah, WAL/Region with sync is probably not a good idea, there just won&apos;t be enough spindles in the HDFS cluster to absorb that.&lt;/p&gt;

&lt;p&gt;So what&apos;s a good heuristic for the number of WALs? Maybe (assuming good block distribution and that HBase is the only user of the cluster) it should be around #spindles/#replicas...?&lt;/p&gt;</comment>
                            <comment id="13288570" author="ram_krish" created="Mon, 4 Jun 2012 13:57:17 +0000"  >&lt;p&gt;Perf results. &lt;br/&gt;
@Ted&lt;br/&gt;
The file attached also has the latency results. Run using LoadTestTool.  Sorry for being little late. Patch will upload later.&lt;/p&gt;</comment>
                            <comment id="13288592" author="zhihyu@ebaysf.com" created="Mon, 4 Jun 2012 14:39:43 +0000"  >&lt;p&gt;Can you run ycsb with 50% insert and 50% update load ?&lt;br/&gt;
Performance numbers in attachment match what I got based on my implementation.&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;</comment>
                            <comment id="13288689" author="lhofhansl" created="Mon, 4 Jun 2012 17:08:27 +0000"  >&lt;p&gt;This is related &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6116&quot; title=&quot;Allow parallel HDFS writes for HLogs.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6116&quot;&gt;&lt;del&gt;HBASE-6116&lt;/del&gt;&lt;/a&gt;.&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6116&quot; title=&quot;Allow parallel HDFS writes for HLogs.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6116&quot;&gt;&lt;del&gt;HBASE-6116&lt;/del&gt;&lt;/a&gt; would improve latency, whereas this issues would mostly improve throughput.&lt;/p&gt;</comment>
                            <comment id="13288699" author="lhofhansl" created="Mon, 4 Jun 2012 17:15:21 +0000"  >&lt;p&gt;@Ted or @Ram: If you have any chance to test &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6116&quot; title=&quot;Allow parallel HDFS writes for HLogs.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6116&quot;&gt;&lt;del&gt;HBASE-6116&lt;/del&gt;&lt;/a&gt; as well, that&apos;d be really cool (although it would be more effort, as it only works against Hadoop trunk - and soon Hadoop 2.0-alpha).&lt;br/&gt;
Andy said he might test against EC2.&lt;/p&gt;</comment>
                            <comment id="13289161" author="stack" created="Tue, 5 Jun 2012 05:27:40 +0000"  >&lt;p&gt;Whats the high level on the perf numbers?  Does more WALs help?  How much?  Thanks.&lt;/p&gt;</comment>
                            <comment id="13289370" author="ram_krish" created="Tue, 5 Jun 2012 12:38:40 +0000"  >&lt;p&gt;@Ted&lt;br/&gt;
The ycsb report we will get it tomorrow.  Today environemnt is busy.&lt;br/&gt;
@Lars&lt;br/&gt;
We will try to check &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6116&quot; title=&quot;Allow parallel HDFS writes for HLogs.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6116&quot;&gt;&lt;del&gt;HBASE-6116&lt;/del&gt;&lt;/a&gt; also but not very sure if in the next couple of days. Anyway will try.&lt;/p&gt;</comment>
                            <comment id="13293471" author="lhofhansl" created="Tue, 12 Jun 2012 09:27:55 +0000"  >&lt;p&gt;I think we should wait for test result with &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6116&quot; title=&quot;Allow parallel HDFS writes for HLogs.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6116&quot;&gt;&lt;del&gt;HBASE-6116&lt;/del&gt;&lt;/a&gt; before we invest more time in this.&lt;br/&gt;
My gut feeling tells me, that is something that is better handled at the HDFS level.&lt;/p&gt;</comment>
                            <comment id="13293517" author="zhihyu@ebaysf.com" created="Tue, 12 Jun 2012 11:23:12 +0000"  >&lt;p&gt;As I mentioned in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6055&quot; title=&quot;Offline Snapshots in HBase 0.96&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6055&quot;&gt;&lt;del&gt;HBASE-6055&lt;/del&gt;&lt;/a&gt; @ 04/Jun/12 17:47, one of the benefits of this feature is for each HLog file to receive edits for one single table.&lt;/p&gt;</comment>
                            <comment id="13293853" author="tlipcon" created="Tue, 12 Jun 2012 19:08:50 +0000"  >&lt;blockquote&gt;&lt;p&gt;I think we should wait for test result with &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6116&quot; title=&quot;Allow parallel HDFS writes for HLogs.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6116&quot;&gt;&lt;del&gt;HBASE-6116&lt;/del&gt;&lt;/a&gt; before we invest more time in this.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6116&quot; title=&quot;Allow parallel HDFS writes for HLogs.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6116&quot;&gt;&lt;del&gt;HBASE-6116&lt;/del&gt;&lt;/a&gt; seems like it would improve latency but hurt throughput &amp;#8211; on a typical gbit link, the parallel writes would limit us to 50M/sec for 3 replicas, whereas pipelined writes could give us 100M+.&lt;/p&gt;

&lt;p&gt;The other main advantage of this JIRA is that the speed of the WAL is currently limited to the minimum speed of the 3 disks chosen in the pipeline. Given that disks can be heavily loaded, the probability of getting even a full disk&apos;s worth of throughput is low &amp;#8211; the likelihood is that at least one of those disks is also being written to or read from at least another client. So typically any single HDFS stream is limited to 35-40MB/sec in my experience.&lt;/p&gt;

&lt;p&gt;Given that gbit is much faster than this, we can get better throughput by adding parallel WALs, so as to stripe across disks and dynamically push writes to less-loaded disks.&lt;/p&gt;</comment>
                            <comment id="13400559" author="lhofhansl" created="Mon, 25 Jun 2012 16:00:10 +0000"  >&lt;p&gt;Assuming Datanodes and RegionServers are colocated no more bits will have to cross the (aggregate) &quot;wires&quot;. Further assuming good load balancing within HBase the net bandwidth is still spread over the cluster (but with lower latency at each RegionServer).&lt;br/&gt;
So I do not believe that &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6116&quot; title=&quot;Allow parallel HDFS writes for HLogs.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6116&quot;&gt;&lt;del&gt;HBASE-6116&lt;/del&gt;&lt;/a&gt; will actually hurt performance.&lt;/p&gt;

&lt;p&gt;The key question is whether WAL writing is mostly bound by latency or bandwidth (And I do not know.)&lt;br/&gt;
Do we get 35-40mb throughput from writing the WAL? If not, it is likely bound by latency.&lt;/p&gt;</comment>
                            <comment id="13468087" author="hudson" created="Tue, 2 Oct 2012 21:25:32 +0000"  >&lt;p&gt;Integrated in HBase-TRUNK #3408 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-TRUNK/3408/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-TRUNK/3408/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5699&quot; title=&quot;Run with &amp;gt; 1 WAL in HRegionServer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5699&quot;&gt;&lt;del&gt;HBASE-5699&lt;/del&gt;&lt;/a&gt; Refactor HLog into an interface (Revision 1393126)&lt;/p&gt;

&lt;p&gt;     Result = FAILURE&lt;br/&gt;
stack : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/backup/example/LongTermArchivingHFileCleaner.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/fs/HFileSystem.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/HLogInputFormat.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/LogCleaner.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/metrics/RegionServerMetrics.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/Compressor.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSHLog.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogFactory.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogMetrics.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogPrettyPrinter.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogUtil.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogReader.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogWriter.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALActionsListener.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALCoprocessorHost.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/util/HFileArchiveUtil.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/util/HMerge.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/util/MetaUtils.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/backup/example/TestZooKeeperTableArchiveClient.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRowProcessorEndpoint.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestWALObserver.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/fs/TestBlockReorder.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHLogRecordReader.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestDistributedLogSplitting.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCacheOnWriteInSchema.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompactSelection.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransaction.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestStore.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/FaultySequenceFileLogReader.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/HLogPerformanceEvaluation.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/HLogUtilsForTests.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLog.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogMethods.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplit.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRollAbort.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRolling.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRollingNoCluster.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALActionsListener.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALReplay.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSource.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/regionserver/TestReplicationSourceManager.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestMergeTool.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13631596" author="nkeywal" created="Mon, 15 Apr 2013 08:53:55 +0000"  >&lt;p&gt;If we implement this, we should test the impact on MTTR as well. My fear is that we will have much more lease to recover, and the way it&apos;s written today (one after this other), it could make failure recovery much slower on a small cluster.&lt;/p&gt;</comment>
                            <comment id="13631631" author="anoop.hbase" created="Mon, 15 Apr 2013 10:11:09 +0000"  >&lt;p&gt;May be we need to combine efforts here with &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-7835&quot; title=&quot;Implementation of the log splitting without creating intermediate files &quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-7835&quot;&gt;&lt;del&gt;HBASE-7835&lt;/del&gt;&lt;/a&gt; &lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jeffreyz&quot; class=&quot;user-hover&quot; rel=&quot;jeffreyz&quot;&gt;Jeffrey Zhong&lt;/a&gt; Working with &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-7835&quot; title=&quot;Implementation of the log splitting without creating intermediate files &quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-7835&quot;&gt;&lt;del&gt;HBASE-7835&lt;/del&gt;&lt;/a&gt; where he try to do WAL replay with HTable#put()&lt;/p&gt;

&lt;p&gt;I had added below comment to &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-7835&quot; title=&quot;Implementation of the log splitting without creating intermediate files &quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-7835&quot;&gt;&lt;del&gt;HBASE-7835&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I was thinking on this area. We have different JIRAs now related to HLOG and its split and replay.&lt;br/&gt;
This one + &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6772&quot; title=&quot;Make the Distributed Split HDFS Location aware&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6772&quot;&gt;HBASE-6772&lt;/a&gt; + multi WAL...&lt;br/&gt;
Can we think on all together. &lt;br/&gt;
For multi WAL if we have a fixed set of regions for one WAL approach, during one RS down Master can assign those regions(Try max) to another one RS &lt;span class=&quot;error&quot;&gt;&amp;#91;Region groups in RS&amp;#93;&lt;/span&gt;. If the corresponding HLog file also assigned to that RS, then for the replay it can directly do puts on the region rather than IPC.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If we can do all these I think MTTR also can be improved.&lt;br/&gt;
I will start working with JIRA (along with Ram) from next week.&lt;/p&gt;</comment>
                            <comment id="13821649" author="yuzhihong@gmail.com" created="Wed, 13 Nov 2013 18:33:43 +0000"  >&lt;blockquote&gt;&lt;p&gt;we will have much more lease to recover&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;At the beginning of recovery, master can send lease recovery requests for outstanding WAL files using thread pool.&lt;br/&gt;
Each split worker would first check whether the WAL file it processes is closed.&lt;/p&gt;</comment>
                            <comment id="13912374" author="stack" created="Wed, 26 Feb 2014 01:27:12 +0000"  >&lt;p&gt;This issue adds switching between WALs&lt;/p&gt;</comment>
                            <comment id="14089418" author="anoop.hbase" created="Thu, 7 Aug 2014 16:35:14 +0000"  >&lt;p&gt;My idea is like to make a multi WAL impl which helps write throughput as well as MTTR.  The MTTR when we have the distributed log replay mode.  If we can make sure to have region grouping policy in selecting the regions for a WAL in multi WAL area,  we can try max to allocate all those regions to same RS on crash.  So this RS can read this WAL and replay locally.  The distributed log replay  batch calls has not to go over RPC ..   Lots of Qs and corner cases there.  But we can discuss more on and try to make it better.&lt;/p&gt;</comment>
                            <comment id="14089439" author="busbey" created="Thu, 7 Aug 2014 16:57:45 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=anoop.hbase&quot; class=&quot;user-hover&quot; rel=&quot;anoop.hbase&quot;&gt;Anoop Sam John&lt;/a&gt;, that sounds like a combination of this and the ideas in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8610&quot; title=&quot;Introduce interfaces to support MultiWAL&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8610&quot;&gt;&lt;del&gt;HBASE-8610&lt;/del&gt;&lt;/a&gt;?&lt;/p&gt;</comment>
                            <comment id="14089464" author="anoop.hbase" created="Thu, 7 Aug 2014 17:08:25 +0000"  >&lt;p&gt;Ya collective ideas around MultiWAL &lt;/p&gt;</comment>
                            <comment id="14089487" author="ram_krish" created="Thu, 7 Aug 2014 17:23:05 +0000"  >&lt;p&gt;Grouping based on region should in itself be a pluggable module because a simple thing could be just based on a specific factor (like group every 5 regions) or could be based on names.  To start with we could do simple grouping.  &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;we can try max to allocate all those regions to same RS on crash. So this RS can read this WAL and replay locally. &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;To replay locally we should avoid the RPC itself totally? Is it possible in the new distributed log replay? It tries to create do table.batchmutate() right.  Need to see the code to confirm this once.  &lt;/p&gt;</comment>
                            <comment id="14089495" author="anoop.hbase" created="Thu, 7 Aug 2014 17:32:41 +0000"  >&lt;p&gt;Yes, the grouping logic should be a pluggable module. The grouping can be per table regions wise or on all regions. It should be inline with balancing strategy. (per table or not)&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;To replay locally we should avoid the RPC itself totally? Is it possible in the new distributed log replay?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Have not checked deeply with code. These are like high level thoughts only. We can check more. If we can avoid RPCs in the replay that would be great IMO.&lt;/p&gt;</comment>
                            <comment id="14181895" author="busbey" created="Thu, 23 Oct 2014 20:24:37 +0000"  >&lt;p&gt;I have a patch implementing this on top of the refactoring in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-10378&quot; title=&quot;Divide HLog interface into User and Implementor specific interfaces&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-10378&quot;&gt;&lt;del&gt;HBASE-10378&lt;/del&gt;&lt;/a&gt;. Any objections to me taking over the issue and posting it?&lt;/p&gt;</comment>
                            <comment id="14213865" author="busbey" created="Sun, 16 Nov 2014 06:24:36 +0000"  >&lt;p&gt;since the set of links is getting shortened here&apos;s an extra link to the &lt;a href=&quot;https://reviews.apache.org/r/28055/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;reviewboard (depends on the changes from HBASE-10378)&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14213867" author="hadoopqa" created="Sun, 16 Nov 2014 06:32:21 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12530779/PerfHbase.txt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12530779/PerfHbase.txt&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;br/&gt;
  ATTACHMENT ID: 12530779&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 tests included&lt;/font&gt;.  The patch doesn&apos;t appear to include any new or modified tests.&lt;br/&gt;
                        Please justify why no new tests are needed for this patch.&lt;br/&gt;
                        Also please list what manual steps were performed to verify this patch.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 patch&lt;/font&gt;.  The patch command could not apply the patch.&lt;/p&gt;

&lt;p&gt;Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/11693//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/11693//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="14241505" author="busbey" created="Wed, 10 Dec 2014 18:33:05 +0000"  >&lt;p&gt;adding current RB version for QA run.&lt;/p&gt;</comment>
                            <comment id="14241558" author="busbey" created="Wed, 10 Dec 2014 19:18:24 +0000"  >&lt;p&gt;Attaching results from some initial testing using WALPerformanceEvaluation with a sync-heavy workload.&lt;/p&gt;

&lt;p&gt;If anyone has other measurements they&apos;d like to see, or a different workload expressed (perhaps to look at limits for pushing bytes given larger edits with fewer syncs), please let me know.&lt;/p&gt;

&lt;h5&gt;&lt;a name=&quot;Overview&quot;&gt;&lt;/a&gt;Overview&lt;/h5&gt;
&lt;p&gt;command used was&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
bin/hbase org.apache.hadoop.hbase.wal.WALPerformanceEvaluation -threads ${threads} -regions $(((threads+1)/2)) -roll 100000 -iterations 1000000 -verify 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;with # threads varied and # regions at ceil(threads/2). default is sync-per-write.&lt;/p&gt;

&lt;p&gt;Test rig is a physical cluster with 4 data nodes and a total of 20 data disks (5 per node). Test client was run on a separate non-loaded host. HDFS 2.5.0-cdh5.2.0&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;&lt;tt&gt;.bz2&lt;/tt&gt; files are the complete logs from the described group of runs.&lt;/li&gt;
	&lt;li&gt;&quot;upstream&quot; and &quot;wals-1&quot; data is from prior to &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-12655&quot; title=&quot;WALPerformanceEvaluation miscalculates append/sync statistics for multiple regions&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-12655&quot;&gt;&lt;del&gt;HBASE-12655&lt;/del&gt;&lt;/a&gt;, so run metrics other than the final benchmark results aren&apos;t comparable to later.&lt;/li&gt;
	&lt;li&gt;There&apos;s an image showing total write iops across the cluster for each of the sets of test runs.&lt;/li&gt;
	&lt;li&gt;hbase-5699_total_throughput_sync_heavy.txt has the final benchmark log from each of the runs, so you can quickly look across successive runs.&lt;/li&gt;
	&lt;li&gt;hbase-5699_multiwal_400-threads_stats_sync_heavy.txt has the run metrics from just the final test of the multiwal options&lt;/li&gt;
&lt;/ul&gt;


&lt;h5&gt;&lt;a name=&quot;upstreamvsmultiwal1&quot;&gt;&lt;/a&gt;upstream vs multiwal-1&lt;/h5&gt;

&lt;p&gt;If you look at the two charts &quot;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5699&quot; title=&quot;Run with &amp;gt; 1 WAL in HRegionServer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5699&quot;&gt;&lt;del&gt;HBASE-5699&lt;/del&gt;&lt;/a&gt;_write_iops_upstream_1_to_200_threads.tiff&quot; and &quot;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5699&quot; title=&quot;Run with &amp;gt; 1 WAL in HRegionServer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5699&quot;&gt;&lt;del&gt;HBASE-5699&lt;/del&gt;&lt;/a&gt;_write_iops_multiwal-1_1_to_200_threads.tiff&quot;, they behave roughly the same modulo noise. (the only difference in the two happens during region set up, which shouldn&apos;t be reflected here.)&lt;/p&gt;

&lt;p&gt;They also level off on ability to push more through the pipeline at near the limit for iops given the 3 disks in the single pipeline.&lt;/p&gt;

&lt;h5&gt;&lt;a name=&quot;increasingnumberofpipelines&quot;&gt;&lt;/a&gt;increasing number of pipelines&lt;/h5&gt;
&lt;p&gt;If you look at each of the &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5699&quot; title=&quot;Run with &amp;gt; 1 WAL in HRegionServer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5699&quot;&gt;&lt;del&gt;HBASE-5699&lt;/del&gt;&lt;/a&gt;_write_iops_multiwal-X_10,50,120,190,260,330,400_threads.tiff charts, as we ramp up the number of writers we manage to push more overall activity through the cluster.&lt;/p&gt;

&lt;p&gt;It&apos;s not a linear gain because splitting out the pipelines means that we do more overall syncs since fewer of them get obviated by our sync grouping.&lt;/p&gt;

&lt;p&gt;In this test, expanding from 2 to 4 or 6 pipelines didn&apos;t provide much benefit because at up to 400 concurrent sync-heavy writers we just get to maxing out the number of iops that can be done with 2 pipelines.&lt;/p&gt;</comment>
                            <comment id="14242007" author="jmhsieh" created="Thu, 11 Dec 2014 02:05:27 +0000"  >&lt;p&gt;Nice results!&lt;/p&gt;

&lt;p&gt;A few questions thoughts. &lt;/p&gt;

&lt;p&gt;How do iops/s translate to mb/s?  Are the reasonably close to max disk speed/3?&lt;/p&gt;

&lt;p&gt;Can you combine a few graphs so that you can see the main jump from 1 pipeline to 2 pipelines?  the graphs currently don&apos;t quite line up.  Instead of having time in x axis, use the # threads in x and show avg/std iop/s of each of the thread settings and # of pipeline settings?&lt;/p&gt;
</comment>
                            <comment id="14243394" author="busbey" created="Thu, 11 Dec 2014 23:56:34 +0000"  >&lt;p&gt;I&apos;m running some more tests. I redid the single wal test up to 400 concurrent sync heavy writers so I could make the comparison charts you asked for. In doing so I got more though-put in that case; it looks like we&apos;re pushing enough data then to require 1 - 1.5 new blocks each second, so we&apos;re effectively spreading across more disks.&lt;/p&gt;</comment>
                            <comment id="14246776" author="hadoopqa" created="Mon, 15 Dec 2014 16:02:02 +0000"  >&lt;p&gt;&lt;font color=&quot;green&quot;&gt;+1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12687232/HBASE-5699.4.patch.txt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12687232/HBASE-5699.4.patch.txt&lt;/a&gt;&lt;br/&gt;
  against master branch at commit db873f0886ec43e2e5b3bdcb56399b3bceb4dcaa.&lt;br/&gt;
  ATTACHMENT ID: 12687232&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 tests included&lt;/font&gt;.  The patch appears to include 4 new or modified tests.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javadoc&lt;/font&gt;.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 checkstyle&lt;/font&gt;.  The applied patch does not increase the total number of checkstyle errors&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 findbugs&lt;/font&gt;.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 lineLengths&lt;/font&gt;.  The patch does not introduce lines longer than 100&lt;/p&gt;

&lt;p&gt;  &lt;font color=&quot;green&quot;&gt;+1 site&lt;/font&gt;.  The mvn site goal succeeds with this patch.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 core tests&lt;/font&gt;.  The patch passed unit tests in .&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12084//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12084//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12084//artifact/patchprocess/newPatchFindbugsWarningshbase-rest.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12084//artifact/patchprocess/newPatchFindbugsWarningshbase-rest.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12084//artifact/patchprocess/newPatchFindbugsWarningshbase-common.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12084//artifact/patchprocess/newPatchFindbugsWarningshbase-common.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12084//artifact/patchprocess/newPatchFindbugsWarningshbase-client.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12084//artifact/patchprocess/newPatchFindbugsWarningshbase-client.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12084//artifact/patchprocess/newPatchFindbugsWarningshbase-annotations.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12084//artifact/patchprocess/newPatchFindbugsWarningshbase-annotations.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12084//artifact/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12084//artifact/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12084//artifact/patchprocess/newPatchFindbugsWarningshbase-server.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12084//artifact/patchprocess/newPatchFindbugsWarningshbase-server.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12084//artifact/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12084//artifact/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12084//artifact/patchprocess/newPatchFindbugsWarningshbase-protocol.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12084//artifact/patchprocess/newPatchFindbugsWarningshbase-protocol.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12084//artifact/patchprocess/newPatchFindbugsWarningshbase-thrift.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12084//artifact/patchprocess/newPatchFindbugsWarningshbase-thrift.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12084//artifact/patchprocess/newPatchFindbugsWarningshbase-examples.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12084//artifact/patchprocess/newPatchFindbugsWarningshbase-examples.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12084//artifact/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12084//artifact/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&lt;/a&gt;&lt;br/&gt;
Checkstyle Errors: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12084//artifact/patchprocess/checkstyle-aggregate.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12084//artifact/patchprocess/checkstyle-aggregate.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;  Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12084//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12084//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="14246902" author="busbey" created="Mon, 15 Dec 2014 17:50:29 +0000"  >&lt;p&gt;Here&apos;s a chart with # workers on the x-axis and MiB/s on the y, named &quot;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5699&quot; title=&quot;Run with &amp;gt; 1 WAL in HRegionServer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5699&quot;&gt;&lt;del&gt;HBASE-5699&lt;/del&gt;&lt;/a&gt;_#workers_vs_MiB_per_s_1x1col_512Bval_wal_count_1,2,4.tiff&quot;&lt;/p&gt;

&lt;p&gt;The MiB/s is the rate for a 30 second sample. The dark blue, green, and red lines are for single wal, 2 wals, and 4 wals respectively. Each one has a lighter shaded line above and below marking 1 std dev in each direction.&lt;/p&gt;

&lt;p&gt;This is with the default block size of 128MiB for wals and a single cf:cq combo with 512B value in each edit.&lt;/p&gt;

&lt;p&gt;I&apos;ve got numbers for some other block sizes finishing up this morning; should have more charts tomorrow.&lt;/p&gt;</comment>
                            <comment id="14247057" author="jmhsieh" created="Mon, 15 Dec 2014 19:09:04 +0000"  >&lt;p&gt;I like the new graph &amp;#8211; summarizes a lot and is still simple to follow.&lt;/p&gt;

&lt;p&gt;I&apos;m trying to make sense of the 20-25% boost (hoped for more!).  These are 5 disk machines &amp;#8211; are these boxes configured so that one disk per machine set aside for the os and 4 disks are &quot;data&quot; drives?  &lt;/p&gt;</comment>
                            <comment id="14247111" author="stack" created="Mon, 15 Dec 2014 19:40:39 +0000"  >&lt;p&gt;What would the charts look like if no disk friction at all, i.e. no a mocked WAL?  Are we using all available i/o or are we blocked internally &amp;#8211; cpu/locks/context switching?  Nice graphs Sean. How&apos;d you make them/run the tests?&lt;/p&gt;
</comment>
                            <comment id="14247132" author="busbey" created="Mon, 15 Dec 2014 19:49:52 +0000"  >&lt;p&gt;These are nodes with 5 data drives. There are 6 physical disks in the machines and one is set aside for OS.&lt;/p&gt;

&lt;p&gt;I was also surprised about the boost, since my initial thinking was that we&apos;d be constrained in the single wal case by writing to one pipeline. However, once I think through things more it makes a little more sense. For one thing we only use hflush and not hsync, so even for network flushes we&apos;re still largely in memory. That also means those datanodes can keep handling the write to disk as we get the next pipeline. On the far end of this chart for 128MB blocks, that should be happening every ~1.5 seconds for the single wal. That allows us to keep more than just 3 disks busy even in the single wal case.&lt;/p&gt;

&lt;p&gt;It&apos;s possible the gain shown by the perf eval will be bigger once I get &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-12339&quot; title=&quot;WAL performance evaluation tool doesn&amp;#39;t roll logs&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-12339&quot;&gt;&lt;del&gt;HBASE-12339&lt;/del&gt;&lt;/a&gt; in place. As is, we&apos;re paying the overhead of new block allocation instead of the overhead of new file allocation. I don&apos;t have enough info to know if that delta matters though.&lt;/p&gt;</comment>
                            <comment id="14247143" author="busbey" created="Mon, 15 Dec 2014 19:56:33 +0000"  >&lt;blockquote&gt;&lt;p&gt;What would the charts look like if no disk friction at all, i.e. no a mocked WAL? Are we using all available i/o or are we blocked internally &#8211; cpu/locks/context switching?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I could run this as a baseline with the DisabledWALProvider. IIRC all it does is increment metric counts.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Nice graphs Sean. How&apos;d you make them/run the tests?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The tests are runs of the WALPerformanceEval tool using the command up above under &quot;Overview&quot;. To alter the WAL count I made multiple conf dirs with that setting switched in hbase-site.xml and then exported an appropriate HBASE_CONF_DIR before each run. The tests I ran over the weekend (but haven&apos;t gotten to chart yet) are the same but with more options configured.&lt;/p&gt;

&lt;p&gt;The chart itself is just the output from the log of the test filtered for the append byte counts that happen every 30 seconds, then put into a google doc to do deltas and avg / stddev. I was considering using the same data to plot the IQR instead of average +- stddev, but I wasn&apos;t sure that would be as consumable.&lt;/p&gt;</comment>
                            <comment id="14247173" author="busbey" created="Mon, 15 Dec 2014 20:25:16 +0000"  >&lt;blockquote&gt;&lt;p&gt;I was also surprised about the boost, since my initial thinking was that we&apos;d be constrained in the single wal case by writing to one pipeline. However, once I think through things more it makes a little more sense. For one thing we only use hflush and not hsync, so even for network flushes we&apos;re still largely in memory. That also means those datanodes can keep handling the write to disk as we get the next pipeline. On the far end of this chart for 128MB blocks, that should be happening every ~1.5 seconds for the single wal. That allows us to keep more than just 3 disks busy even in the single wal case.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;One other note about the rate at which we are rolling pipelines already. This had me thinking about the improvements in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-10278&quot; title=&quot;Provide better write predictability&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-10278&quot;&gt;HBASE-10278&lt;/a&gt;. If we&apos;re rolling that often under load, I wonder if we&apos;d be better off just forcing a roll at whatever the &quot;pipeline sync is slow&quot; threshold is rather than maintain the state to do a switch. A question better exploded on that ticket, I suppose.&lt;/p&gt;</comment>
                            <comment id="14247308" author="stack" created="Mon, 15 Dec 2014 22:09:43 +0000"  >&lt;blockquote&gt;&lt;p&gt;I could run this as a baseline with the DisabledWALProvider.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Could be informative learning upper bound on how many writes/sec we can drive.&lt;/p&gt;</comment>
                            <comment id="14247315" author="eclark" created="Mon, 15 Dec 2014 22:13:06 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=busbey&quot; class=&quot;user-hover&quot; rel=&quot;busbey&quot;&gt;Sean Busbey&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11283&quot; title=&quot;[0.89-fb] Roll HLog if sync time is anomalous &quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11283&quot;&gt;&lt;del&gt;HBASE-11283&lt;/del&gt;&lt;/a&gt; did something like that for 0.89-fb. Basically any sync that took longer than 1 second caused a roll of the HLog. The thought being that we could get a new write pipeline that might avoid a dead/dying disk or datanode.&lt;/p&gt;</comment>
                            <comment id="14248562" author="busbey" created="Tue, 16 Dec 2014 17:54:46 +0000"  >&lt;p&gt;Attaching a plot that includes running the same tests with the delegate wals as DisabledWALProvider, named &quot;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5699&quot; title=&quot;Run with &amp;gt; 1 WAL in HRegionServer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5699&quot;&gt;&lt;del&gt;HBASE-5699&lt;/del&gt;&lt;/a&gt;&lt;em&gt;disabled_and_regular&lt;/em&gt;#workers_vs_MiB_per_s_1x1col_512Bval_wal_count_1,2,4&quot;.&lt;/p&gt;

&lt;p&gt;This should show the limit from context switching and such in the test itself. The DisabledWALProvider doesn&apos;t include any of the overhead from the ringbuffer or sync grouping.&lt;/p&gt;

&lt;p&gt;There are very few data points for the new test cases, so I didn&apos;t include any stddev bars. I just used the average for hte whole run. All of them were so short that they probably didn&apos;t have time to get into steady state.&lt;/p&gt;

&lt;p&gt;(The disabled wals are at the top, the previous runs with datanode writes are at the bottom)&lt;/p&gt;</comment>
                            <comment id="14248660" author="stack" created="Tue, 16 Dec 2014 18:46:12 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=busbey&quot; class=&quot;user-hover&quot; rel=&quot;busbey&quot;&gt;Sean Busbey&lt;/a&gt; Looks like writing datanodes puts a bit of friction on our write path.  I wonder how much the ringbuffer+grouping is costing us? Looking at the graph, you&apos;d think adding extra WALs would make a bigger difference given the large gap between no-friction and one WAL. Good stuff.&lt;/p&gt;</comment>
                            <comment id="14252554" author="stack" created="Thu, 18 Dec 2014 23:39:17 +0000"  >&lt;p&gt;What you thinking here &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=busbey&quot; class=&quot;user-hover&quot; rel=&quot;busbey&quot;&gt;Sean Busbey&lt;/a&gt; ?  So, we&apos;d commit this but default is &apos;identity&apos; as I read it? i.e. a log per region.  Is that so?  Would be cool if multiwal were on by default or does it need to show better numbers to be on by default?&lt;/p&gt;</comment>
                            <comment id="14252622" author="busbey" created="Fri, 19 Dec 2014 00:22:42 +0000"  >&lt;p&gt;I don&apos;t have any ideas for region grouping besides identity; I think it&apos;s a fine starting point. The current configuration makes &quot;multiwal&quot; the bounded version, so there&apos;s only N configurable wals (the current patch says 1, but 2 might make more sense). It doesn&apos;t given an option yet for &quot;wal per region&quot;, so anyone who wants to use that would have to manually specify the fully qualified class name. If we wanted to give them assurances about compatibility we&apos;d need to give them a config name.&lt;/p&gt;

&lt;p&gt;I&apos;m split on wether it makes sense to make it the default given the modest improvement. I&apos;m inclined to leave the default alone for 1.0 and change it for later once we have some more stats on e.g. impact on recovery. My intuition says wal recovery should be roughly the same.&lt;/p&gt;</comment>
                            <comment id="14252635" author="stack" created="Fri, 19 Dec 2014 00:27:56 +0000"  >&lt;blockquote&gt;&lt;p&gt;I&apos;m inclined to leave the default alone for 1.0 and change it for later once we have some more stats on e.g. impact on recovery. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Grand.&lt;/p&gt;

&lt;p&gt;I gave this +1 over on rb if you want to commit.  Fat release note I&apos;d say (but you were probably going to do that anyways)&lt;/p&gt;</comment>
                            <comment id="14252687" author="eclark" created="Fri, 19 Dec 2014 00:53:16 +0000"  >&lt;p&gt;Yeah +1 on getting it into 1.0. For me I think that probably means with only one wal configured by default.&lt;br/&gt;
For what it&apos;s worth, we&apos;ve seen larger improvements from running multi wals but we also have more disks per machine. So could be cluster dependent.&lt;/p&gt;</comment>
                            <comment id="14252858" author="busbey" created="Fri, 19 Dec 2014 03:24:46 +0000"  >&lt;p&gt;sounds great. I&apos;ll leave the default wal provider as is (a single wal to the filesystem) and make it so that if you configure multiwal you start at 2.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=enis&quot; class=&quot;user-hover&quot; rel=&quot;enis&quot;&gt;Enis Soztutar&lt;/a&gt;, you fine with me pushing this to branch-1.0 (either now or after you cut RC0)? or just stick with branch-1? It changes nothing by default but allows some alternate wal configurations.&lt;/p&gt;</comment>
                            <comment id="14253630" author="busbey" created="Fri, 19 Dec 2014 16:48:18 +0000"  >&lt;p&gt;Here&apos;s a proposed release note. Let me know if anyone thinks there&apos;s a big gap.&lt;/p&gt;</comment>
                            <comment id="14253631" author="busbey" created="Fri, 19 Dec 2014 16:49:06 +0000"  >&lt;p&gt;Pushed to branch-1 and master. If it looks like there&apos;ll be a RC1 for 1.0 we can revisit pushing to branch-1.0.&lt;/p&gt;</comment>
                            <comment id="14253694" author="hudson" created="Fri, 19 Dec 2014 17:42:41 +0000"  >&lt;p&gt;FAILURE: Integrated in HBase-1.1 #9 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-1.1/9/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-1.1/9/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5699&quot; title=&quot;Run with &amp;gt; 1 WAL in HRegionServer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5699&quot;&gt;&lt;del&gt;HBASE-5699&lt;/del&gt;&lt;/a&gt; Adds multiple WALs per Region Server based on groups of regions. (busbey: rev 2b94aa8c8599d66858e6a2b5198bd21ded2334ca)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;hbase-server/src/main/java/org/apache/hadoop/hbase/wal/RegionGroupingProvider.java&lt;/li&gt;
	&lt;li&gt;hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestBoundedRegionGroupingProvider.java&lt;/li&gt;
	&lt;li&gt;hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALFactory.java&lt;/li&gt;
	&lt;li&gt;hbase-server/src/main/java/org/apache/hadoop/hbase/wal/BoundedRegionGroupingProvider.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14253791" author="hudson" created="Fri, 19 Dec 2014 18:54:56 +0000"  >&lt;p&gt;SUCCESS: Integrated in HBase-TRUNK #5949 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-TRUNK/5949/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-TRUNK/5949/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5699&quot; title=&quot;Run with &amp;gt; 1 WAL in HRegionServer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5699&quot;&gt;&lt;del&gt;HBASE-5699&lt;/del&gt;&lt;/a&gt; Adds multiple WALs per Region Server based on groups of regions. (busbey: rev f1c41e307e4e55e7849f35e09c3de2fcd4dbbd2b)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALFactory.java&lt;/li&gt;
	&lt;li&gt;hbase-server/src/main/java/org/apache/hadoop/hbase/wal/RegionGroupingProvider.java&lt;/li&gt;
	&lt;li&gt;hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestBoundedRegionGroupingProvider.java&lt;/li&gt;
	&lt;li&gt;hbase-server/src/main/java/org/apache/hadoop/hbase/wal/BoundedRegionGroupingProvider.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14253918" author="lhofhansl" created="Fri, 19 Dec 2014 19:54:18 +0000"  >&lt;blockquote&gt;&lt;p&gt;Altering the WAL provider used by a particular RegionServer requires restarting that instance.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It requires restarting the cluster, right? Otherwise when an RS dies there might another up still with a different wal provider.&lt;/p&gt;</comment>
                            <comment id="14253936" author="busbey" created="Fri, 19 Dec 2014 20:06:53 +0000"  >&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Altering the WAL provider used by a particular RegionServer requires restarting that instance.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It requires restarting the cluster, right? Otherwise when an RS dies there might another up still with a different wal provider.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;In general, yes. In the case of the providers we currently allow via configuration parameters (as opposed to user-provided custom FQCN), they all are compatible on the recovery side. So it doesn&apos;t matter if a RS dies while there are different providers around. That&apos;s why a rolling restart can be used to change from default to multiwal.&lt;/p&gt;

&lt;p&gt;In the release note I was focused on the specific case of default vs multiwal. Should I note the general case?&lt;/p&gt;</comment>
                            <comment id="14257413" author="stack" created="Tue, 23 Dec 2014 19:37:07 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=enis&quot; class=&quot;user-hover&quot; rel=&quot;enis&quot;&gt;Enis Soztutar&lt;/a&gt; this for 1.0?&lt;/p&gt;</comment>
                            <comment id="14263355" author="enis" created="Sat, 3 Jan 2015 01:06:31 +0000"  >&lt;p&gt;Pushed to 1.0.0 as well. All new code, should not affect default stability. &lt;/p&gt;</comment>
                            <comment id="14263398" author="hudson" created="Sat, 3 Jan 2015 03:00:20 +0000"  >&lt;p&gt;SUCCESS: Integrated in HBase-1.0 #627 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-1.0/627/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-1.0/627/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5699&quot; title=&quot;Run with &amp;gt; 1 WAL in HRegionServer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5699&quot;&gt;&lt;del&gt;HBASE-5699&lt;/del&gt;&lt;/a&gt; Adds multiple WALs per Region Server based on groups of regions. (enis: rev 2ebeddfc4276898e42e9d7cadb8092e9f72ed421)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;hbase-server/src/main/java/org/apache/hadoop/hbase/wal/RegionGroupingProvider.java&lt;/li&gt;
	&lt;li&gt;hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALFactory.java&lt;/li&gt;
	&lt;li&gt;hbase-server/src/main/java/org/apache/hadoop/hbase/wal/BoundedRegionGroupingProvider.java&lt;/li&gt;
	&lt;li&gt;hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestBoundedRegionGroupingProvider.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14331918" author="enis" created="Sat, 21 Feb 2015 23:50:04 +0000"  >&lt;p&gt;Closing this issue after 1.0.0 release.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="12611430">HBASE-6981</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12687287">HBASE-10278</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12633539">ACCUMULO-1083</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12649284">HBASE-8610</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12553865">HBASE-5937</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12558321">HBASE-6116</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12717194">HBASE-11270</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12895145">HBASE-14457</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10001">
                    <name>dependent</name>
                                            <outwardlinks description="depends upon">
                                        <issuelink>
            <issuekey id="12689687">HBASE-10378</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is depended upon by">
                                        <issuelink>
            <issuekey id="12642305">HBASE-8338</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12686318" name="HBASE-5699.3.patch.txt" size="22118" author="busbey" created="Wed, 10 Dec 2014 18:33:05 +0000"/>
                            <attachment id="12687232" name="HBASE-5699.4.patch.txt" size="23803" author="busbey" created="Mon, 15 Dec 2014 14:06:58 +0000"/>
                            <attachment id="12687270" name="HBASE-5699_#workers_vs_MiB_per_s_1x1col_512Bval_wal_count_1,2,4.tiff" size="140958" author="busbey" created="Mon, 15 Dec 2014 17:50:29 +0000"/>
                            <attachment id="12687524" name="HBASE-5699_disabled_and_regular_#workers_vs_MiB_per_s_1x1col_512Bval_wal_count_1,2,4.tiff" size="102400" author="busbey" created="Tue, 16 Dec 2014 17:54:46 +0000"/>
                            <attachment id="12686337" name="HBASE-5699_write_iops_multiwal-1_1_to_200_threads.tiff" size="141388" author="busbey" created="Wed, 10 Dec 2014 19:18:24 +0000"/>
                            <attachment id="12686338" name="HBASE-5699_write_iops_multiwal-2_10,50,120,190,260,330,400_threads.tiff" size="68252" author="busbey" created="Wed, 10 Dec 2014 19:18:24 +0000"/>
                            <attachment id="12686339" name="HBASE-5699_write_iops_multiwal-4_10,50,120,190,260,330,400_threads.tiff" size="67434" author="busbey" created="Wed, 10 Dec 2014 19:18:24 +0000"/>
                            <attachment id="12686340" name="HBASE-5699_write_iops_multiwal-6_10,50,120,190,260,330,400_threads.tiff" size="67162" author="busbey" created="Wed, 10 Dec 2014 19:18:24 +0000"/>
                            <attachment id="12686336" name="HBASE-5699_write_iops_upstream_1_to_200_threads.tiff" size="143370" author="busbey" created="Wed, 10 Dec 2014 19:18:24 +0000"/>
                            <attachment id="12530779" name="PerfHbase.txt" size="41275" author="ram_krish" created="Mon, 4 Jun 2012 13:57:17 +0000"/>
                            <attachment id="12686329" name="hbase-5699_multiwal_400-threads_stats_sync_heavy.txt" size="5314" author="busbey" created="Wed, 10 Dec 2014 19:18:24 +0000"/>
                            <attachment id="12686330" name="hbase-5699_total_throughput_sync_heavy.txt" size="11721" author="busbey" created="Wed, 10 Dec 2014 19:18:24 +0000"/>
                            <attachment id="12686335" name="results-hbase5699-upstream.txt.bz2" size="1248678" author="busbey" created="Wed, 10 Dec 2014 19:18:24 +0000"/>
                            <attachment id="12686334" name="results-hbase5699-wals-1.txt.bz2" size="1258903" author="busbey" created="Wed, 10 Dec 2014 19:18:24 +0000"/>
                            <attachment id="12686333" name="results-updated-hbase5699-wals-2.txt.bz2" size="268263" author="busbey" created="Wed, 10 Dec 2014 19:18:24 +0000"/>
                            <attachment id="12686332" name="results-updated-hbase5699-wals-4.txt.bz2" size="291132" author="busbey" created="Wed, 10 Dec 2014 19:18:24 +0000"/>
                            <attachment id="12686331" name="results-updated-hbase5699-wals-6.txt.bz2" size="322178" author="busbey" created="Wed, 10 Dec 2014 19:18:24 +0000"/>
                    </attachments>
                <subtasks>
                            <subtask id="12553865">HBASE-5937</subtask>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>17.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Mon, 2 Apr 2012 18:34:34 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>234185</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            1 year, 40 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i09v9r:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>55507</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310192" key="com.atlassian.jira.plugin.system.customfieldtypes:textarea">
                        <customfieldname>Release Note</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>HBase&amp;#39;s write-ahead-log (WAL) can now be configured to use multiple HDFS pipelines in parallel to provide better write throughput for clusters by using additional disks. By default, HBase will still use only a single HDFS-based WAL. &lt;br/&gt;
&lt;br/&gt;
To run with multiple WALs, alter the hbase-site.xml property &amp;quot;hbase.wal.provider&amp;quot; to have the value &amp;quot;multiwal&amp;quot;. To return to having HBase determine what kind of WAL implementation to use either remove the property all together or set it to &amp;quot;defaultProvider&amp;quot;.&lt;br/&gt;
&lt;br/&gt;
Altering the WAL provider used by a particular RegionServer requires restarting that instance.  RegionServers using the original WAL implementation and those using the &amp;quot;multiwal&amp;quot; implementation can each handle recovery of either set of WALs, so a zero-downtime configuration update is possible through a rolling restart.</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>