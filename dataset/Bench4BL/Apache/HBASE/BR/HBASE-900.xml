<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 02 20:23:08 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-900/HBASE-900.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-900] Regionserver memory leak causing OOME during relatively modest bulk importing</title>
                <link>https://issues.apache.org/jira/browse/HBASE-900</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;I have recreated this issue several times and it appears to have been introduced in 0.2.&lt;/p&gt;

&lt;p&gt;During an import to a single table, memory usage of individual region servers grows w/o bounds and when set to the default 1GB it will eventually die with OOME.  This has happened to me as well as Daniel Ploeg on the mailing list.  In my case, I have 10 RS nodes and OOME happens w/ 1GB heap at only about 30-35 regions per RS.  In previous versions, I have imported to several hundred regions per RS with default heap size.&lt;/p&gt;

&lt;p&gt;I am able to get past this by increasing the max heap to 2GB.  However, the appearance of this in newer versions leads me to believe there is now some kind of memory leak happening in the region servers during import.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12405107">HBASE-900</key>
            <summary>Regionserver memory leak causing OOME during relatively modest bulk importing</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.png">Blocker</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="stack">stack</assignee>
                                    <reporter username="streamy">Jonathan Gray</reporter>
                        <labels>
                    </labels>
                <created>Wed, 24 Sep 2008 23:53:29 +0000</created>
                <updated>Sun, 13 Sep 2009 22:26:27 +0000</updated>
                            <resolved>Fri, 19 Dec 2008 19:03:46 +0000</resolved>
                                    <version>0.18.1</version>
                    <version>0.19.0</version>
                                    <fixVersion>0.19.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                <comments>
                            <comment id="12639707" author="stack" created="Wed, 15 Oct 2008 03:49:37 +0000"  >&lt;p&gt;I got 185 PE regions into single HRS before HDFS went bad; about 32M rows of 1k values and 10byte keys.  Profiling, only thing that grows in memory is the count of HSKs.  Each store file we open has an index of long to HSK.  As the upload progresses, more index is in memory.&lt;/p&gt;

&lt;p&gt;Was going to move this out of 0.18.1 since its not obviously broken but then talked to Rong-en.  He says that it was reading where he was seeing memory issues.  Will try a read test.&lt;/p&gt;</comment>
                            <comment id="12639709" author="stack" created="Wed, 15 Oct 2008 03:56:04 +0000"  >&lt;p&gt;Looks like it was actually 225 regions before things went bad (need to be more patient).&lt;/p&gt;</comment>
                            <comment id="12639770" author="rafan" created="Wed, 15 Oct 2008 09:09:28 +0000"  >&lt;p&gt;I see some possible memory leaks from regionserver after running it w/ ~200 regions per node for few days (it keeps receive read traffic with very few write). My used swap grows slowly. Region servers occupies around 3G memory (both virtual and rss shown in top). Once I restart regionserver, swap space is also freed.&lt;/p&gt;

&lt;p&gt;This is with hbase 0.2.x on hadoop 0.17.x.&lt;/p&gt;</comment>
                            <comment id="12639909" author="stack" created="Wed, 15 Oct 2008 18:01:30 +0000"  >&lt;p&gt;I ran randomread test over night w/ gc logging enabled.  Here are snippets from the gc log from different times during the night showing full gcs:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
3738.529: [Full GC 107893K-&amp;gt;86326K(220480K), 0.3393940 secs]
3944.907: [Full GC 110079K-&amp;gt;90694K(212160K), 0.3828950 secs]
...
43142.078: [Full GC 105996K-&amp;gt;82458K(139840K), 0.3558530 secs]
43339.019: [Full GC 102767K-&amp;gt;86387K(190656K), 0.3512450 secs]
43490.046: [Full GC 105187K-&amp;gt;87709K(212288K), 0.3523640 secs]
43735.589: [Full GC 107799K-&amp;gt;88233K(174784K), 0.3547080 secs]
...
25003.983: [Full GC 105412K-&amp;gt;87523K(205312K), 0.3559230 secs]
25139.998: [Full GC 106102K-&amp;gt;80911K(131712K), 0.3432420 secs]
..
47924.811: [Full GC 105487K-&amp;gt;80566K(148864K), 0.3392500 secs]
48088.641: [Full GC 98025K-&amp;gt;86603K(212736K), 0.3439750 secs]
48338.127: [Full GC 105214K-&amp;gt;87088K(159872K), 0.3481490 secs]
..
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Its holding pretty steady.&lt;/p&gt;

&lt;p&gt;I also attached memory graph from ganglia over night.  Shows nothing untoward.&lt;/p&gt;</comment>
                            <comment id="12639910" author="stack" created="Wed, 15 Oct 2008 18:03:40 +0000"  >&lt;p&gt;Let me move this issue out of 0.18.1.  There is something going on here given its reported by different people but my attempts at replication using simple schema fail.  We need more info.  We can do a 0.18.2 later after we figure whats leaking.&lt;/p&gt;</comment>
                            <comment id="12649899" author="apurtell" created="Sat, 22 Nov 2008 05:34:31 +0000"  >&lt;p&gt;This is a recurring issue presently causing pain on current trunk. Seems to be worse now than 0.18.1. Heap gets out of control (&amp;gt; 1GB) for regionservers hosting only ~20 regions or so on. Much of the heap is tied up in byte referenced by HSKs referenced by the WritableComparable[] arrays used by MapFile indexes.&lt;/p&gt;

&lt;p&gt;From a jgray server:&lt;/p&gt;

&lt;p&gt;class [B 	3525873 	615313626&lt;br/&gt;
class org.apache.hadoop.hbase.HStoreKey 	1605046 	51361472&lt;br/&gt;
class java.util.TreeMap$Entry 	1178067 	48300747&lt;br/&gt;
class [Lorg.apache.hadoop.io.WritableComparable; 	56 	4216992&lt;/p&gt;

&lt;p&gt;Approximately 56 mapfile indexes were resident. Approximately 15-20 regions were being hosted at the time of the crash. &lt;/p&gt;

&lt;p&gt;On an apurtell server, &amp;gt;900MB of heap was observed to be consumed by mapfile indexes for 48 store files corresponding to 16 regions.&lt;/p&gt;</comment>
                            <comment id="12649903" author="stack" created="Sat, 22 Nov 2008 06:59:35 +0000"  >&lt;p&gt;One thought: I wonder if fixing the indexing interval so its actually 32 rather than default 128 helped make this issue worse?&lt;/p&gt;</comment>
                            <comment id="12649971" author="apurtell" created="Sat, 22 Nov 2008 19:52:22 +0000"  >&lt;p&gt;Last night under Heritrix hbase-writer stress I had a regionserver with 2GB heap go down with an OOME. It was serving 4 regions only. This was with 0.18.1, so the line numbers won&apos;t match up with trunk. &lt;/p&gt;

&lt;p&gt;class [B 27343 2008042041&lt;br/&gt;
class [C 11714 966164&lt;br/&gt;
class org.apache.hadoop.hbase.HStoreKey 9781 312992&lt;br/&gt;
class java.util.TreeMap$Entry 7596 311436&lt;br/&gt;
class [Lorg.apache.hadoop.io.WritableComparable; 17 139536&lt;/p&gt;

&lt;p&gt;Incidentally the RS was also hosting ROOT so the whole cluster went down. I agree with jgray this combined with the ROOT SPOF is deadly.&lt;/p&gt;

&lt;p&gt;Stack trace of the OOME:&lt;/p&gt;

&lt;p&gt;2008-11-22 09:24:40,950 INFO org.apache.hadoop.hbase.regionserver.HRegion: starting compaction on region content,29308276c599f8a0baca15c224c22ad2,1227337259738&lt;br/&gt;
2008-11-22 09:24:56,429 FATAL org.apache.hadoop.hbase.regionserver.HRegionServer: Set stop flag in regionserver/0:0:0:0:0:0:0:0:60020.compactor&lt;br/&gt;
java.lang.OutOfMemoryError: Java heap space&lt;br/&gt;
        at java.util.Arrays.copyOf(Arrays.java:2786)&lt;br/&gt;
        at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:94)&lt;br/&gt;
        at java.io.DataOutputStream.write(DataOutputStream.java:90)&lt;br/&gt;
        at org.apache.hadoop.io.compress.CompressorStream.compress(CompressorStream.java:78)&lt;br/&gt;
        at org.apache.hadoop.io.compress.CompressorStream.write(CompressorStream.java:71)&lt;br/&gt;
        at java.io.BufferedOutputStream.write(BufferedOutputStream.java:105)&lt;br/&gt;
        at java.io.DataOutputStream.write(DataOutputStream.java:90)&lt;br/&gt;
        at org.apache.hadoop.hbase.io.ImmutableBytesWritable.write(ImmutableBytesWritable.java:116)&lt;br/&gt;
        at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:90)&lt;br/&gt;
        at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:77)&lt;br/&gt;
        at org.apache.hadoop.io.SequenceFile$RecordCompressWriter.append(SequenceFile.java:1131)&lt;br/&gt;
        at org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:980&lt;br/&gt;
)&lt;br/&gt;
        at org.apache.hadoop.io.MapFile$Writer.append(MapFile.java:198)&lt;br/&gt;
        at org.apache.hadoop.hbase.regionserver.HStoreFile$BloomFilterMapFile$Writer.append(HStoreFile.java:846)&lt;br/&gt;
        at org.apache.hadoop.hbase.regionserver.HStore.compact(HStore.java:988)&lt;br/&gt;
        at org.apache.hadoop.hbase.regionserver.HStore.compact(HStore.java:893)&lt;br/&gt;
        at org.apache.hadoop.hbase.regionserver.HRegion.compactStores(HRegion.java:902)&lt;br/&gt;
        at org.apache.hadoop.hbase.regionserver.HRegion.compactStores(HRegion.java:860)&lt;br/&gt;
        at org.apache.hadoop.hbase.regionserver.CompactSplitThread.run(CompactSplitThread.java:83)&lt;/p&gt;</comment>
                            <comment id="12650048" author="apurtell" created="Sun, 23 Nov 2008 20:09:08 +0000"  >&lt;p&gt;Another RS went down this morning. This time somehow I ended up with 1,790,757,470 bytes in 9760 instances of byte[] on the heap. Scrolling through the list of these objects, most are &amp;lt; 256 bytes, some are &amp;lt;= 5K. Only 2727 HSKs. I also see 2426 instances of Hashtable$Entry, only 148 instances of TreeMap$Entry. 12 HStores with 22 HStoreFiles. &lt;/p&gt;

&lt;p&gt;Found a number of 100MB instances of byte[], e.g. referenced from a ByteArrayOutputStream referenced from an o.a.h.i.compress.CompressionOutputStream. Another referenced from both o.a.h.h.io.DataOutputBuffer and o.a.h.h.io.DataInputBuffer referenced from a SequenceFile$Reader. Looks like a Cell has a reference to a copy of this. Found more with local/weak references from Server$Handler. Did a couple of big files (~100MB) and copies thereof take down the RS? &lt;/p&gt;

</comment>
                            <comment id="12650062" author="stack" created="Sun, 23 Nov 2008 22:17:15 +0000"  >&lt;p&gt;You using compression in your instance Andrew?&lt;br/&gt;
St.Ack&lt;/p&gt;</comment>
                            <comment id="12650069" author="apurtell" created="Sun, 23 Nov 2008 22:51:51 +0000"  >&lt;p&gt;Yes, RECORD compression on &apos;content&apos; family, which will have up to two cells per row: &apos;content:raw&apos; will contain the response body written by a custom Heritrix hbase writer, and if the mimetype is text/*, another cell &apos;content:document&apos; containing a serialized Document object produced by MozillaHtmlParser (&lt;a href=&quot;http://sourceforge.net/projects/mozillaparser/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://sourceforge.net/projects/mozillaparser/&lt;/a&gt;). Some binary content can be very large, e.g. 100MB zip, tgz, etc. Row index is SHA1 hash of content object. There is also an &apos;info&apos; family, not compressed, that stores attributes. Finally there is a &apos;urls&apos; family, not compressed, that will have a cell for each unique URL corresponding to the content object. &lt;/p&gt;</comment>
                            <comment id="12650410" author="stack" created="Tue, 25 Nov 2008 00:29:57 +0000"  >&lt;p&gt;Andrew, what if you disabled compression?  See if you still have issue.  How many heritrix instances?  If one, how many Writers?  5 is default IIRC?  A byte array of 100MB is kinda crazy.  Was there a big page crawled by heritrix?  YOu can check its log.  It outputs sizes.  Maybe you need upper bound on page sizes in heritrix if not there already?&lt;/p&gt;</comment>
                            <comment id="12650421" author="apurtell" created="Tue, 25 Nov 2008 00:44:52 +0000"  >&lt;p&gt;The file involved was a 105MB Win32 executable. Using compression compouded the heap charge already taken by several copies from RPC to Cell to ByteArrayOutputStream, etc. I will use a file size limit of 20MB going forward. Also I filed &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-1024&quot; title=&quot;Rearchitect regionserver I/O&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-1024&quot;&gt;&lt;del&gt;HBASE-1024&lt;/del&gt;&lt;/a&gt;. &lt;/p&gt;</comment>
                            <comment id="12653167" author="apurtell" created="Thu, 4 Dec 2008 04:57:50 +0000"  >&lt;p&gt;Here is a scenario that guarantees a flurry of regionserver OOMEs on my cluster, which is now running latest trunk on top of Hadoop 0.18.2-dev + Ganglia 3.1 patch:&lt;/p&gt;

&lt;p&gt;1) Start up heritrix with hbase-writer. 25 TOEs should do it. Start a long running job.&lt;/p&gt;

&lt;p&gt;2) Build up content until there are ~20 regions per regionserver.&lt;/p&gt;

&lt;p&gt;3) Run a mapreduce job that walks a metadata column of the content table &amp;#8211; not all columns, not the family storing the content itself, just some small auxiliary metadata.&lt;/p&gt;

&lt;p&gt;4) Simultaneously to the scanning read (#3), perform what amounts to a bulk import with 5 concurrent writers. (Typical for my load is 4-8GB in maybe a few 10K updates.) Specifically I am using MozillaHtmlParser to build Document objects from text content and am then storing back serialized representations of those Document objects.&lt;/p&gt;

&lt;p&gt;After an invocation of #4, heap usage has balooned across the cluster and it is only a matter of time. Memcache is within limits and for my configuration represents 25% of heap max (I run with 2G heap), so the remaining data is something else. Heap histograms from jhat show a very large number of allocations of [B which can be as much as 1.5GB in total. Soon the regionservers will start to compact or do other heap intensive activities and will fall over. &lt;/p&gt;

&lt;p&gt;A flurry of OOMEs can confuse the master. It will reject region opens thinking they are closing and the regions will remain offline until a manual restart of the cluster. Disable/enable of the table only makes that particular wrinkle worse. &lt;/p&gt;

&lt;p&gt;After restart, invariably a number of regions want to (and do) split. &lt;/p&gt;</comment>
                            <comment id="12653455" author="stack" created="Thu, 4 Dec 2008 20:14:21 +0000"  >&lt;p&gt;Datapoint: tim sell is having similar OOME&apos;ing issues doing an import.  He reports that he disabled blockcaching to no apparent change in behavior.&lt;/p&gt;

&lt;p&gt;Trying to run a simplified replica of Andrew&apos;s receipe above, the global memflusher &amp;#8211; with new hbase-1027 in place &amp;#8211; got stuck here around an OOME down in DFSClient build up response:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Exception in thread &lt;span class=&quot;code-quote&quot;&gt;&quot;ResponseProcessor &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; block blk_5165224789834035674_1602&quot;&lt;/span&gt; java.lang.OutOfMemoryError: GC overhead limit exceeded
        at java.util.Arrays.copyOf(Unknown Source)
        at java.lang.AbstractStringBuilder.expandCapacity(Unknown Source)
        at java.lang.AbstractStringBuilder.append(Unknown Source)
        at java.lang.StringBuilder.append(Unknown Source)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$ResponseProcessor.run(DFSClient.java:2319)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-quote&quot;&gt;&quot;IPC Server handler 0 on 60020&quot;&lt;/span&gt; daemon prio=10 tid=0x00007f7f501a4400 nid=0x2b31 in &lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;.wait() [0x0000000042b00000..0x0000000042b01b00]
   java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.State: WAITING (on object monitor)
        at java.lang.&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;.wait(Native Method)
        at java.lang.&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;.wait(&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;.java:485)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.flushInternal(DFSClient.java:3026)
        - locked &amp;lt;0x00007f7f8615cd50&amp;gt; (a java.util.LinkedList)
        - locked &amp;lt;0x00007f7f8615c9c0&amp;gt; (a org.apache.hadoop.hdfs.DFSClient$DFSOutputStream)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.closeInternal(DFSClient.java:3104)
        - locked &amp;lt;0x00007f7f8615c9c0&amp;gt; (a org.apache.hadoop.hdfs.DFSClient$DFSOutputStream)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.close(DFSClient.java:3053)
        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:59)
        at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:79)
        at org.apache.hadoop.io.SequenceFile$Writer.close(SequenceFile.java:959)
        - locked &amp;lt;0x00007f7f8615c858&amp;gt; (a org.apache.hadoop.io.SequenceFile$Writer)
        at org.apache.hadoop.io.MapFile$Writer.close(MapFile.java:183)
        - locked &amp;lt;0x00007f7f86157370&amp;gt; (a org.apache.hadoop.hbase.io.BloomFilterMapFile$Writer)
        at org.apache.hadoop.hbase.io.BloomFilterMapFile$Writer.close(BloomFilterMapFile.java:212)
        - locked &amp;lt;0x00007f7f86157370&amp;gt; (a org.apache.hadoop.hbase.io.BloomFilterMapFile$Writer)
        at org.apache.hadoop.hbase.regionserver.HStore.internalFlushCache(HStore.java:680)
        - locked &amp;lt;0x00007f7f5de99c88&amp;gt; (a java.lang.&lt;span class=&quot;code-object&quot;&gt;Integer&lt;/span&gt;)
        at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:627)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:863)
        at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:772) 
        at org.apache.hadoop.hbase.regionserver.MemcacheFlusher.flushRegion(MemcacheFlusher.java:220)
        at org.apache.hadoop.hbase.regionserver.MemcacheFlusher.flushSomeRegions(MemcacheFlusher.java:284)
        - locked &amp;lt;0x00007f7f5dc0f828&amp;gt; (a org.apache.hadoop.hbase.regionserver.MemcacheFlusher)
        at org.apache.hadoop.hbase.regionserver.MemcacheFlusher.reclaimMemcacheMemory(MemcacheFlusher.java:254)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.batchUpdates(HRegionServer.java:1455)
        at sun.reflect.GeneratedMethodAccessor22.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:634)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:892)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="12653623" author="stack" created="Fri, 5 Dec 2008 04:38:35 +0000"  >&lt;p&gt;Here is one theory.  Looking at a heap that OOME&apos;d here on test cluster using jprofiler, there were a bunch of instances of SoftValue (30 or 40k).   I was able to sort them by deep size and most encountered held byte arrays of 16k in size.  This would seem to indicate elements of the blockcache.  Odd thing is that you&apos;d think the SoftValues shouldn&apos;t be in the heap on OOME; they should have been cleared by the GCor.  Looking, each store file instance has a Map of SoftValues.  They are keyed by position into the file.  The GC does the job of moving the blocks that are to be cleared onto a ReferenceQueue but unless the ReferenceQueue gets processed promptly, we&apos;ll hold on to the SoftValue references (JProfiler has a button which says &apos;clean References&apos; and after selecting this, the SoftValues remained).  The ReferenceQueue gets processed when we add a new block to the cache or if we seek to a new location in a block that we got from the cache (only).  Otherwise, blocks to be removed are not processed.  If random-reading or only looking at certain stores in a regionserver, all other storefiles, unless they are accessed, will continue to hold on to blocks via their uncleared ReferenceQueue.&lt;/p&gt;

&lt;p&gt;I tried adding in check of the ReferenceQueue everytime anything was accessed on a file but I still OOME&apos;d using a random read test.&lt;/p&gt;

&lt;p&gt;Next thing to try is a single Map that holds all blockcache entries.  Will be lots of contention on this single Map but better than going to disk any day.  All accesses will check the ReferenceQueue.&lt;/p&gt;

&lt;p&gt;Only downer is that Tim Sell says his last test was run without blockcache enabled and that it made no difference.  Maybe try it Andrew?&lt;/p&gt;

&lt;p&gt;Meantime, I&apos;ll try the above suggestion.  Andrew, any chance of a copy of your heap dump?  Tim the same?&lt;/p&gt;</comment>
                            <comment id="12653992" author="stack" created="Sat, 6 Dec 2008 00:07:22 +0000"  >&lt;p&gt;Looking more at my local heap, I see ReferenceQueues with links to megabytes of unreleased data.  More evidence that we are not processing ReferenceQueues fast enough.  Need to fix.  Might be able to go with bigger blockcache size if one Map of all blockcaches.&lt;/p&gt;

&lt;p&gt;Looking at Andrew Purtell heap dump, it does not have the same character as mine where we are holding on to blockcache items.  His heap has 30 instances of stack-based ByteArrayOutputStreams; together they add up to 690MBs of data.  Trying to figure which BAOS is prob.  Our use in hbase is innocent.  At moment the ipc Server use is suspect.  Digging.&lt;/p&gt;</comment>
                            <comment id="12654009" author="stack" created="Sat, 6 Dec 2008 01:09:36 +0000"  >&lt;p&gt;Yeah, I think its the stack based BAOS in Server.  Rather than allocate a new one each time, its reset.   Reset looks like it keeps the old buffer &amp;#8211; it just resets buffer length.  Saves on allocations.  Means that if we ever return a big Cell in a RowResult, then the server buffer stays that big (I see also that hbase.regionserver.handler.count is set to 30 which jibes with the 30 I saw in the heap dump).  Handlers live the life of the application.&lt;/p&gt;

&lt;p&gt;Working on a patch for Andrew to try.&lt;/p&gt;
</comment>
                            <comment id="12654183" author="stack" created="Sun, 7 Dec 2008 09:21:34 +0000"  >&lt;p&gt;Patch that brings down Server and Client from hadoop ipc.  We now have bulk of hadoop ipc local.  Classes have been renamed to have an HBase prefix to distingush them from their hadoop versions.  Had to bring at least Server local because fix needed meddling in private class (Server.Handler).  Added check on size of stack-based ByteArrayOutputStream size after every use.  It used to always reset.  Now, if BAOS is &amp;gt; initial buffersize, we allocate a new BAOS instance rather than reset.&lt;/p&gt;

&lt;p&gt;Verified in testbed it does the right thing.  Unit tests pass.  Tempted to commit but maybe Andrew you can give it a spin first?&lt;/p&gt;

&lt;p&gt;Next will work on the blockcache leak.&lt;/p&gt;</comment>
                            <comment id="12654197" author="apurtell" created="Sun, 7 Dec 2008 13:56:21 +0000"  >&lt;p&gt;Running with the patch now. There is an improvement. Will have to run for a while to see what the impact on stability is. &lt;/p&gt;
</comment>
                            <comment id="12654240" author="stack" created="Sun, 7 Dec 2008 20:29:03 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-4797&quot; title=&quot;RPC Server can leave a lot of direct buffers &quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-4797&quot;&gt;&lt;del&gt;HADOOP-4797&lt;/del&gt;&lt;/a&gt; is intriguing though not our direct problem (I think &amp;#8211; we can pull it in if we think it can help).  I made &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-4802&quot; title=&quot;RPC Server send buffer retains size of largest response ever sent &quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-4802&quot;&gt;&lt;del&gt;HADOOP-4802&lt;/del&gt;&lt;/a&gt; to fix the apurtell issue up in hadoop.&lt;/p&gt;</comment>
                            <comment id="12654260" author="apurtell" created="Sun, 7 Dec 2008 23:50:50 +0000"  >&lt;p&gt;A scenario where I&apos;m sure I would have seen OOMEs succeeds. However another occurrence of &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-1046&quot; title=&quot;Region assigned to two regionservers after split&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-1046&quot;&gt;&lt;del&gt;HBASE-1046&lt;/del&gt;&lt;/a&gt; might have compromised the testing.&lt;/p&gt;

&lt;p&gt;Definitely +1 on the patch. Heap use on my regionservers is much better. &lt;/p&gt;</comment>
                            <comment id="12654271" author="stack" created="Mon, 8 Dec 2008 01:15:14 +0000"  >&lt;p&gt;It wasn&apos;t an OOME on a regionserver that brought on the hbase-1046?  If you think not, and no OOMEs, great.  I&apos;ll commit.&lt;/p&gt;</comment>
                            <comment id="12654279" author="stack" created="Mon, 8 Dec 2008 01:40:32 +0000"  >&lt;p&gt;nm.  I&apos;ll just go w/ your +1 above.  Will work on the 1046 next.&lt;/p&gt;</comment>
                            <comment id="12654282" author="stack" created="Mon, 8 Dec 2008 01:45:53 +0000"  >&lt;p&gt;I committed above patch as part 1 of this issue.  Thanks for testing Andrew.  Part 2 will be fixing blockcache.  There may be a part 3 (Tim Sell just manufactured an hprof for his OOME&apos;ing cluster &amp;#8211; need to figure whats up w/ his failures) and even a part 4 (jgray&apos;s failure &amp;#8211; though I think this fixed by hbase-1027).&lt;/p&gt;</comment>
                            <comment id="12654729" author="stack" created="Tue, 9 Dec 2008 07:47:55 +0000"  >&lt;p&gt;Have been looking at Tim Sell heaps over last day.   The anomaly is hundreds of thousands of BatchUpdates.  The write rate at OOME is about 15k/20k a second.  Its like we&apos;re retaining arrays of BatchUpdates &amp;#8211; something in the rpc invocation code &amp;#8211; but it looks right when I read it.  I&apos;m missing something obvious.  Will keep at it.&lt;/p&gt;</comment>
                            <comment id="12655100" author="stack" created="Wed, 10 Dec 2008 05:28:00 +0000"  >&lt;p&gt;Ran a MR job using TableOutputFormat &amp;#8211; batches of BatchUpdate &amp;#8211; and got a heap that looked like Tim Sells with 100k BatchUpdate instances.  Its not obvious to me how we&apos;re doing this.  Adding instrumentation to help me narrow in on the issue.  Tim Sell is running a test that avoids TOF.&lt;/p&gt;</comment>
                            <comment id="12655109" author="apurtell" created="Wed, 10 Dec 2008 06:13:49 +0000"  >&lt;p&gt;We use TOF also.&lt;/p&gt;</comment>
                            <comment id="12655297" author="stack" created="Wed, 10 Dec 2008 18:13:13 +0000"  >&lt;p&gt;Studying my replica of Tim Sell job &amp;#8211; i.e. using TOF and seeing 100k+ BatchUpdates in an array held in the HBaseRPC#Invocation#parameters field &amp;#8211; I now conclude that TOF is operating &quot;as-advertised&quot;.  Default is that client marshalls 10MB of data. In PE case, this is 12k edits (We measure the BU to be of size 1039 bytes which is probably low-ball looking at BU up in jhat but near-enough).  If server is running 10 handlers, then a common case is 10x10MB of edits just sitting around while the batch of edits are being processed server-side.  We should set the client-side 10MB down to maybe 2MB as default but this is not the root cause of the Tim Sell OOME (Avoiding TOE, he ran longer but still OOME&apos;d).  In his case, the10MB holds even more edits &amp;#8211; 70k for 10MB seems viable after he described his data format &amp;#8211; and that allowing that our accounting of object sizes is coarse, that the &apos;deep size&apos; reported in the profiler of 318MB is probably about right.&lt;/p&gt;

&lt;p&gt;So, TODO, set the client-side batch of edits flush size down from 10MB to 2MB.&lt;/p&gt;

&lt;p&gt;Now to look at latest Tim Sell heap dump.&lt;/p&gt;</comment>
                            <comment id="12655378" author="stack" created="Wed, 10 Dec 2008 21:53:34 +0000"  >&lt;p&gt;Our calculation of MemCache sizes is way off.  Our math says the aggregate of all Memcaches is 200MB.  In the profiler, the 153 Memcaches present on OOME have 800MBs accumulated.  Working on a better memcache sizer.&lt;/p&gt;

&lt;p&gt;Other items, the presence of compressors/decompressors is &apos;normal&apos;.  A mapfile index is block compressed.  Bad news is that though the index file is closed as soon as possible, allocated buffers for decompressors stick around (MapFile keeps reference to the index SequenceFile so its not GC&apos;d).  OK news is that in scheme of things, accounts for small amount of heap &amp;#8211; about 10MB in tim&apos;s case.&lt;/p&gt;</comment>
                            <comment id="12655453" author="stack" created="Thu, 11 Dec 2008 01:26:31 +0000"  >&lt;p&gt;First cut at new sizing.  Adds new ByteSize interface that things like HSK, BU, and BO implement making estimates of size that is not just count of payload.  Left the flush on client side at 10MB; number of edits should be a good bit smaller now we do things like count the BU row and size of BU+BO when summing to see if we&apos;ve hit the flush boundary.  &lt;/p&gt;

&lt;p&gt;I checked our estimates against files output to the filesystem and they seem close enough.  Doing same comparing memcache size to that given by profiler is a bit tougher but trying.&lt;/p&gt;</comment>
                            <comment id="12655835" author="tim_s" created="Fri, 12 Dec 2008 00:59:20 +0000"  >&lt;p&gt;running test of part2 patch now. I&apos;ll post the results in my morning.&lt;/p&gt;</comment>
                            <comment id="12655842" author="stack" created="Fri, 12 Dec 2008 01:11:59 +0000"  >&lt;p&gt;v4 of part 2 of this issue.  Want to do a bit more testing before I commit.&lt;/p&gt;</comment>
                            <comment id="12655872" author="apurtell" created="Fri, 12 Dec 2008 03:12:12 +0000"  >&lt;p&gt;I&apos;m testing part 2 v4 now also. &lt;/p&gt;</comment>
                            <comment id="12655919" author="tim_s" created="Fri, 12 Dec 2008 08:23:15 +0000"  >&lt;p&gt;Ran with 900 part2. 2 gig heap. using table output format. &lt;br/&gt;
OOME&apos;d. 18 of 88 maps completed.&lt;br/&gt;
Running test again with 900 part2 v4.&lt;br/&gt;
stack I&apos;ll email you a link to the dump / logs.&lt;/p&gt;</comment>
                            <comment id="12656152" author="stack" created="Fri, 12 Dec 2008 20:18:28 +0000"  >&lt;p&gt;v5 just adds a new Memcache to the Memcache#main.  I did some more testing and we are coming in close enough on Memcache sizes.  Would like to commit this part2.&lt;/p&gt;

&lt;p&gt;Tim&apos;s run w/ v4 ran into hdfs issues &amp;#8211; twice.  Didn&apos;t OOME.  What about you Andrew?&lt;/p&gt;

&lt;p&gt;In a 1G heap I OOME&apos;d and it was not blockcache retention nor Memcache size so at least two other fixes coming on this issue.&lt;/p&gt;</comment>
                            <comment id="12656181" author="stack" created="Fri, 12 Dec 2008 22:28:29 +0000"  >&lt;p&gt;v7 adjusts our BatchUpdate sizing (we were a little under).  It also sets down the default for the client write buffer from 10M to 2M.  If 10 handlers, then when it gets to serverside thats 10MB x 10 which is 1/10th of your heap if you are 1G.&lt;/p&gt;</comment>
                            <comment id="12656221" author="stack" created="Sat, 13 Dec 2008 01:14:48 +0000"  >&lt;p&gt;Applying v7.     Can improve on it in later patches as get more info. &lt;/p&gt;

&lt;p&gt;+ Downs the client-side batch write heap default from 10MB to 2MB&lt;br/&gt;
+ Adds ByteSize interface with a heapSize member.  BatchUpdate, HStoreKey, etc., implement it.&lt;br/&gt;
+ The sizes returned out of heapSize favor 64-bit JVMs.  Were obtained from study of heaps made by running HRS and from runs of the new BU and Memcache mains which have little scripts to generate heaps with arrays of BUs and different Memcaches which can then be heap-dumped and studied.&lt;/p&gt;</comment>
                            <comment id="12656257" author="apurtell" created="Sat, 13 Dec 2008 05:29:10 +0000"  >&lt;p&gt;+1 No OOME with part 1 and v7 of part 2, even with heavy write load.&lt;/p&gt;</comment>
                            <comment id="12656292" author="tim_s" created="Sat, 13 Dec 2008 15:23:52 +0000"  >&lt;p&gt;ditto&lt;/p&gt;</comment>
                            <comment id="12656302" author="stack" created="Sat, 13 Dec 2008 16:38:05 +0000"  >&lt;p&gt;Thanks for the +1s.&lt;/p&gt;

&lt;p&gt;I can still make it OOME here locally if I use small BatchOperations &amp;#8211; cells of size 10 bytes &amp;#8211; and if I put up lots of clients.  Investigating.  And I still need to fix the OOME that happens randomreading because the blockcache is not getting processed comprehensively.&lt;/p&gt;</comment>
                            <comment id="12656349" author="stack" created="Sat, 13 Dec 2008 23:04:19 +0000"  >&lt;p&gt;Around flush we make a snapshot.  As soon as the snapshot is made, we zero the memcache size.  Suspicious.  The snapshot hangs out until flush is completed.  Can take anything from millisecond to ten+ seconds.  Its usually 64MB in size.  In 1G heap fielding a withering upload, could be what throws us over.&lt;/p&gt;</comment>
                            <comment id="12656899" author="stack" created="Tue, 16 Dec 2008 06:37:42 +0000"  >&lt;p&gt;part 3 takes size of region memcaches at start of flush and then subtracts the size on flush completion rather than set things to zero as soon as the cache starts up.   Its still not enough for case where cells are 10bytes in size but it goes half-as far again before OOME&apos;ing.  Might be enough for 0.19.0.&lt;/p&gt;</comment>
                            <comment id="12657164" author="stack" created="Tue, 16 Dec 2008 21:14:03 +0000"  >&lt;p&gt;Committed part3.  Its an improvement.  Will look at a few more heaps but this might be good enough for 0.19.0 for writing.  Next up, part 4, making sure blockcache gets cleared promplty; i.e. oome when writing and reading at same time.&lt;/p&gt;</comment>
                            <comment id="12657505" author="stack" created="Wed, 17 Dec 2008 19:37:54 +0000"  >&lt;p&gt;Looking more at why I can make an OOME writing small cells, I see the MapFile indices starting to come to the fore.  I counted 90MB of indices in a heap of 11 regions and 45 storefiles.  A few were up in the 20+MB range.  Accounting for this size, I&apos;ll leave aside for 0.19.0 release (As is, can&apos;t get at the index anyways in current MapFile, not unless we brought MapFile local &amp;#8211; lets not do that for 0.19.0 release).&lt;/p&gt;</comment>
                            <comment id="12657978" author="stack" created="Fri, 19 Dec 2008 02:27:36 +0000"  >&lt;p&gt;Patch that adds scheduled Excecutor to BlockFSInputStream.  Runs on a period to check for any entries in the Soft Values Reference Queue.  Testing it seems to work.  It has hard-coded values though which is kinda ugly but alternative &amp;#8211; passing in Configuration &amp;#8211; is not viable down here low in io classes.&lt;/p&gt;</comment>
                            <comment id="12657982" author="stack" created="Fri, 19 Dec 2008 02:32:29 +0000"  >&lt;p&gt;I&apos;m going to close this issue after p4 goes in.  Enough work has been done on it at least for 0.19.0 time frame even though we will continue to have memory issues until we start counting the size of loaded MapFile indices.  I&apos;ll open a new issue to do this for 0.20.0 timeframe.  To fix, will require our bringing down MapFile into hbase or putting in place the new file format.&lt;/p&gt;</comment>
                            <comment id="12657986" author="stack" created="Fri, 19 Dec 2008 02:37:37 +0000"  >&lt;p&gt;Added comment to hbase-70.  Its about fixing up our memory management story.&lt;/p&gt;</comment>
                            <comment id="12658157" author="stack" created="Fri, 19 Dec 2008 19:03:46 +0000"  >&lt;p&gt;Resolving with commit of part 4.&lt;/p&gt;

&lt;p&gt;Ran more tests with small cells.  We run for longer if hbase.regionserver.globalMemcache.upperLimit is set down from the 0.4 default.  Set it down to 0.3 or even 0.25 to make more room for indices (Means we can carry more regions before OOME).&lt;/p&gt;</comment>
                            <comment id="12658163" author="stack" created="Fri, 19 Dec 2008 19:19:37 +0000"  >&lt;p&gt;To generate OOMEs, change the PE so that cells are 10 bytes in size instead of the default 1000 bytes in size.&lt;/p&gt;</comment>
                            <comment id="12658199" author="stack" created="Fri, 19 Dec 2008 22:04:14 +0000"  >&lt;p&gt;Other thing to do to ameliorate memory usage when small cells is to up the indexing interval from default 32 to 245 or 1024, etc.  Makes a difference for sure, more than changing the upperLimit does.&lt;/p&gt;</comment>
                            <comment id="12658235" author="apurtell" created="Sat, 20 Dec 2008 00:49:01 +0000"  >&lt;p&gt;Should the indexing interval be set higher by default? At least until MapFile is brought down or a custom file format replacement is put in?&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12409054">HBASE-1019</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12409053">HBASE-1018</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12396452" name="900-p4.patch" size="15333" author="stack" created="Fri, 19 Dec 2008 02:27:36 +0000"/>
                            <attachment id="12395894" name="900-part2-v4.patch" size="15043" author="stack" created="Fri, 12 Dec 2008 01:11:59 +0000"/>
                            <attachment id="12395969" name="900-part2-v5.patch" size="15856" author="stack" created="Fri, 12 Dec 2008 20:18:28 +0000"/>
                            <attachment id="12395975" name="900-part2-v7.patch" size="18686" author="stack" created="Fri, 12 Dec 2008 22:28:29 +0000"/>
                            <attachment id="12395784" name="900-part2.patch" size="12729" author="stack" created="Thu, 11 Dec 2008 01:26:31 +0000"/>
                            <attachment id="12395502" name="900.patch" size="132709" author="stack" created="Sun, 7 Dec 2008 09:21:34 +0000"/>
                            <attachment id="12396162" name="hbase-900-part3.patch" size="2813" author="stack" created="Tue, 16 Dec 2008 06:37:42 +0000"/>
                            <attachment id="12392193" name="memoryOn13.png" size="14736" author="stack" created="Wed, 15 Oct 2008 18:01:30 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>8.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Wed, 15 Oct 2008 03:49:37 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>25466</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 50 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0ha4v:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>98904</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>