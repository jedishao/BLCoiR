<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sat Dec 03 16:41:32 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-57/HBASE-57.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-57] [hbase] Master should allocate regions to regionservers based upon data locality and rack awareness</title>
                <link>https://issues.apache.org/jira/browse/HBASE-57</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Currently, regions are assigned regionservers based off a basic loading attribute.  A factor to include in the assignment calcuation is the location of the region in hdfs; i.e. servers hosting region replicas.  If the cluster is such that regionservers are being run on the same nodes as those running hdfs, then ideally the regionserver for a particular region should be running on the same server as hosts a region replica.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12381634">HBASE-57</key>
            <summary>[hbase] Master should allocate regions to regionservers based upon data locality and rack awareness</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="3">Duplicate</resolution>
                                        <assignee username="lichongxin">Li Chongxin</assignee>
                                    <reporter username="stack">stack</reporter>
                        <labels>
                            <label>gsoc</label>
                    </labels>
                <created>Thu, 1 Nov 2007 17:18:04 +0000</created>
                <updated>Fri, 3 May 2013 20:22:51 +0000</updated>
                            <resolved>Fri, 3 May 2013 20:22:51 +0000</resolved>
                                    <version>0.2.0</version>
                                                    <component>master</component>
                        <due></due>
                            <votes>3</votes>
                                    <watches>14</watches>
                                                                                                            <comments>
                            <comment id="12549663" author="bryanduxbury" created="Sat, 8 Dec 2007 06:37:54 +0000"  >&lt;p&gt;This seems like it should be an important issue, since it should significantly improve the performance of the cluster (faster, less network traffic). Elevating to Major.&lt;/p&gt;</comment>
                            <comment id="12549737" author="stack" created="Sat, 8 Dec 2007 18:59:09 +0000"  >&lt;p&gt;This might actually already be happening.  Any time I&apos;ve seen a regionserver reading from hdfs, it seems to be getting its data from the local datanode.  Verify.&lt;/p&gt;</comment>
                            <comment id="12554873" author="viper799" created="Sat, 29 Dec 2007 02:08:00 +0000"  >&lt;p&gt;From what I understand about writing files to data nodes I do not thank is is a major problem. If the regions are not located on the host serving server on the first compaction they would be stored local first and stay local from there on out unless the current servers hard drive is full and it has to store it on a different server. Over all the region servers would self fix this problem by default over time.&lt;/p&gt;

&lt;p&gt;So only time this would be an issue is on a restart or after a failed region server.&lt;/p&gt;</comment>
                            <comment id="12559198" author="jimk" created="Tue, 15 Jan 2008 19:39:32 +0000"  >&lt;p&gt;Downgrading priority because we should leverage Hadoop&apos;s rack awareness where possible, and there is a lot of work left to do (in Hadoop) before we can&lt;/p&gt;</comment>
                            <comment id="12689484" author="hustlmsp" created="Thu, 26 Mar 2009 13:48:23 +0000"  >&lt;p&gt;Hi hbasers,&lt;br/&gt;
I&apos;d like to work on this issue as my GSOC project &quot;Exploit locality when assigning regions in HBase&quot;.&lt;/p&gt;

&lt;p&gt;After talking with Stack in emails, I have got some initial thoughts on this issue. I&apos;d like to share them with you and welcome for your comments.&lt;/p&gt;

&lt;p&gt;Before designing a suitable mechanism to using the region&apos;s locality, we need to know how blocks are allocated in a hbase cluster and the data-blocks distribution of a specified region over its lifetime in hbase. so that we can find out how the region locality effect the performance. It is difficult to capture all these information in a real cluster. An alternative way to study the locality phenomeon may be simulating the data-block placement procedure in HDFS(local node, local rack, and remote rack) and the regions-allocation mechanism of a hbase cluster in a single machine. And a approximate detail report from simulation can be used for analysis and development.&lt;/p&gt;

&lt;p&gt;Although I haven&apos;t got any detail information about the locality phenomeon, I try to give an initial proposal first.  The initial proposal is to schedule the regions to the datanodes(regionservers) that contains most data-blocks of the specified region. The most challenge thing is to know the data-blocks layout(we can query namenode in HDFS to get these information) of a region in master. And an initial method is to record these layout information of regions in .META. table.&lt;br/&gt;
Some background threads may be run on the master scanning the .META. table to pick up the candidate nodes for region-allocation(these nodes may be sorted by the number of blocks they contain). The detail allocation mechanism will be discussed below.&lt;br/&gt;
(1) A blank region created when the table is first created. As we haven&apos;t got any data in it, we can allocate it according to the current loads of the cluster. It is an easy way. And after the region grows up and were flushed back to HDFS, we get the blocks&apos; locations information and records them to .META. table for next-allocation.&lt;br/&gt;
(2) A region is created by splitting its parent region. We can use parent-region&apos;s blocks&apos; location information to make an allocation decision. And after we finish the splitting procedure, we can simply copy the parent-region&apos;s blocks&apos; location information to each sub-region&apos;s .META. table information. &lt;br/&gt;
(3) A region is re-allocated after the regionserver crash. The logfiles&apos; blocks information will be considered into allocation so that we may accelerate the recovery of a failed-region.&lt;/p&gt;</comment>
                            <comment id="12690089" author="jimk" created="Fri, 27 Mar 2009 20:38:21 +0000"  >&lt;p&gt;&amp;gt; Samuel Guo added a comment - 26/Mar/09 06:48 AM&lt;br/&gt;
&amp;gt; Hi hbasers,&lt;br/&gt;
&amp;gt; I&apos;d like to work on this issue as my GSOC project &quot;Exploit locality when assigning regions in HBase&quot;.&lt;br/&gt;
&amp;gt;&lt;br/&gt;
&amp;gt; After talking with Stack in emails, I have got some initial thoughts on this issue. I&apos;d like to share them with you and&lt;br/&gt;
&amp;gt; welcome for your comments.&lt;br/&gt;
&amp;gt; &lt;br/&gt;
&amp;gt; Before designing a suitable mechanism to using the region&apos;s locality, we need to know how blocks are allocated in&lt;br/&gt;
&amp;gt; a hbase cluster and the data-blocks distribution of a specified region over its lifetime in hbase. so that we can find&lt;br/&gt;
&amp;gt; out how the region locality effect the performance. It is difficult to capture all these information in a real cluster. An&lt;br/&gt;
&amp;gt; alternative way to study the locality phenomeon may be simulating the data-block placement procedure in&lt;br/&gt;
&amp;gt; HDFS(local node, local rack, and remote rack) and the regions-allocation mechanism of a hbase cluster in a single&lt;br/&gt;
&amp;gt; machine. And a approximate detail report from simulation can be used for analysis and development.&lt;/p&gt;

&lt;p&gt;Although the JobTracker in Hadoop attempts to assign tasks to machines that are hosting the data, currently&lt;br/&gt;
HDFS clients still must go through the data node to access the blocks. I believe there is a Jira open for Hadoop&lt;br/&gt;
to go directly to local disk to get the blocks (removing communication with the datanode) if the blocks are on&lt;br/&gt;
the local machine.&lt;/p&gt;

&lt;p&gt;I think that direct disk access for local blocks would be the biggest payoff.&lt;/p&gt;

&lt;p&gt;It is unclear if there is any advantage for locality (other than limiting network access) if direct disk access is not&lt;br/&gt;
available. Benchmarking needs to be done to determine the relative latency of accessing a block from another&lt;br/&gt;
server is much faster than accessing a block (via the datanode) if the block resides on the local machine. It may&lt;br/&gt;
not provide much advantage, unless the block is being served from a server on the other side of a switch or&lt;br/&gt;
router (and of course the speed of the connection). &lt;/p&gt;

&lt;p&gt;Solid performance data evaluating the cost of:&lt;br/&gt;
1) network access to a block in a different rack&lt;br/&gt;
2) network access to a block in the same rack but on a different server&lt;br/&gt;
3) network access to a block on the same server&lt;br/&gt;
4) direct disk access to a block on the same server&lt;/p&gt;

&lt;p&gt;would be highly useful. If there is little difference between 1, 2, 3 (access to a block through a datanode) then&lt;br/&gt;
locality may not be useful. On the other hand, if there is a significant difference between 1, 2, 3 then we should&lt;br/&gt;
try to exploit locality if we can.&lt;/p&gt;

&lt;p&gt;I would expect that direct disk access would be much faster than access through a datanode, but there is no&lt;br/&gt;
hard data on that available. If data were available, then that would indicate if direct disk access (bypassing&lt;br/&gt;
the datanode) is useful or not. If so, that argues in favor of implementing direct disk access as has been&lt;br/&gt;
proposed, and if direct disk access were available, that would argue in favor of locality based region assignment.&lt;/p&gt;

&lt;p&gt;As you point out, blocks migrate over time (especially if you are using the HDFS balancer), and that would &lt;br/&gt;
greatly complicate assignment of regions to region servers. &lt;/p&gt;

&lt;p&gt;Suppose there was one &apos;hot&apos; datanode that hosted blocks from many regions. Using locality might end up in&lt;br/&gt;
overloading the region server on that node, resulting in poorer performance.&lt;/p&gt;

&lt;p&gt;There is a lot of performance evaluation that needs to be done before we actually take the step of using&lt;br/&gt;
locality-based region assignment. If doing that performance evaluation sounds interesting to you, I think&lt;br/&gt;
that would be a great GSOC project.&lt;/p&gt;

&lt;p&gt;Before we try locality-based assignment, we need to have this analysis to see if the idea is worth pursuing.&lt;/p&gt;
</comment>
                            <comment id="12693621" author="stack" created="Sun, 29 Mar 2009 21:47:02 +0000"  >&lt;p&gt;The going direct to local blocks reading is &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-347&quot; title=&quot;DFS read performance suboptimal when client co-located on nodes with data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-347&quot;&gt;&lt;del&gt;HADOOP-4801&lt;/del&gt;&lt;/a&gt;.  In summary, the payoff short-circuiting the datanode is small, and yet to be seen &amp;#8211; at least to date &amp;#8211; and it seems doubtful that a second route to the data will be opened because of security concerns, etc.  Thats my take on the issue (It could change of course).&lt;/p&gt;

&lt;p&gt;I think that if we only made savings in network traffic, that&apos;d be reason enough to implement locality algorithms.  JK makes an interesting point above that we could manufacture hot datanodes if we blindly serve regions from a datanode that hosts all the data but this can happen now since we operate blindly and its only smart use of the locality info that will help damp hot spots.&lt;/p&gt;

&lt;p&gt;Samuel, if still interested, have you made petition to become a GSOC student using this issue as your project?  (Add in some of JKs notes on need to research what happens in a running cluster so know best what to implement).&lt;/p&gt;</comment>
                            <comment id="12693770" author="hustlmsp" created="Mon, 30 Mar 2009 13:54:20 +0000"  >&lt;p&gt;Thanks for your comments, Jim.&lt;/p&gt;

&lt;p&gt;&amp;gt; Solid performance data evaluating the cost of:&lt;br/&gt;
&amp;gt; 1) network access to a block in a different rack&lt;br/&gt;
&amp;gt; 2) network access to a block in the same rack but on a different server&lt;br/&gt;
&amp;gt; 3) network access to a block on the same server&lt;br/&gt;
&amp;gt; 4) direct disk access to a block on the same server&lt;br/&gt;
&amp;gt; would be highly useful. If there is little difference between 1, 2, 3 (access to a block through a datanode) then&lt;br/&gt;
&amp;gt; locality may not be useful. On the other hand, if there is a significant difference between 1, 2, 3 then we should&lt;br/&gt;
&amp;gt; try to exploit locality if we can.&lt;/p&gt;

&lt;p&gt;&amp;gt; There is a lot of performance evaluation that needs to be done before we actually take the step of using&lt;br/&gt;
&amp;gt; locality-based region assignment. If doing that performance evaluation sounds interesting to you, I think&lt;br/&gt;
&amp;gt; that would be a great GSOC project.&lt;/p&gt;

&lt;p&gt;Yes, I agree with you. We need to do a detail analysis of most behaviors of HDFS and HBase before we try locality-based assignment. And the analysis work will be the main part of my GSOC project.&lt;/p&gt;

&lt;p&gt;&amp;gt; Suppose there was one &apos;hot&apos; datanode that hosted blocks from many regions. Using locality might end up in&lt;br/&gt;
&amp;gt; overloading the region server on that node, resulting in poorer performance.&lt;/p&gt;

&lt;p&gt;Yes, Locality should be taken carefully not to overload the  region server or the data node.  An ideal region assignment can assign regions close to its data to reduce network traffic while balancing the loads between region servers, datanodes and avoiding disk competition on the same datanode. As what you suggested, we need to know the following things clearly before making it.&lt;br/&gt;
1) what is the difference we access data from different locations(local, local by-pass, remote, remote rack)?&lt;br/&gt;
2) In regions&apos; life time, what is the data-blocks&apos; distribution? And how many bytes that the region reads data from local node? how many from remote? from remote rack? &lt;br/&gt;
3) After a balance operation happened in HDFS, how 2) changes?&lt;br/&gt;
4) After some region servers failed, how 2) changes?&lt;/p&gt;

&lt;p&gt;I am not so clear now about how to analysis it. but I think I can take them one by one to make things clearly. &lt;/p&gt;</comment>
                            <comment id="12693773" author="hustlmsp" created="Mon, 30 Mar 2009 13:56:25 +0000"  >&lt;p&gt;Thanks stack.&lt;/p&gt;

&lt;p&gt;&amp;gt; Samuel, if still interested, have you made petition to become a GSOC student using this issue as your project? (Add in some of JKs notes on need &lt;br/&gt;
&amp;gt; to research what happens in a running cluster so know best what to implement).&lt;/p&gt;

&lt;p&gt;Yes. I will add Jim&apos;s notes on my proposal.&lt;/p&gt;</comment>
                            <comment id="12702601" author="stack" created="Fri, 24 Apr 2009 22:50:03 +0000"  >&lt;p&gt;If this makes it in time for 0.20.0, good, but for now moving it out.&lt;/p&gt;</comment>
                            <comment id="12849063" author="stack" created="Wed, 24 Mar 2010 05:45:17 +0000"  >&lt;p&gt;This thread, &lt;a href=&quot;http://comments.gmane.org/gmane.comp.java.hadoop.hbase.user/9366&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://comments.gmane.org/gmane.comp.java.hadoop.hbase.user/9366&lt;/a&gt;, has another description of what needs fixing here.&lt;/p&gt;

&lt;p&gt;Is there a call you can make to the NN that will give you metadata on a file &amp;#8211; the blocks it comprises of and where they are located?&lt;/p&gt;

&lt;p&gt;Chatting w/ a possible GSOC candidate about doing this one.&lt;/p&gt;</comment>
                            <comment id="12851247" author="kaykay.unique" created="Tue, 30 Mar 2010 04:41:35 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Is there a call you can make to the NN that will give you metadata on a file - the blocks it comprises of and where they are located?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt; BlockLocation[] DistributedFileSystem#getBlockLocations(String src, long start, long length);&lt;/p&gt;

&lt;p&gt;KFS also has an implementation of FileSystem, appropriately overridden. Is that the one being referred here ? &lt;/p&gt;

</comment>
                            <comment id="12851249" author="stack" created="Tue, 30 Mar 2010 04:48:27 +0000"  >&lt;p&gt;That might do.  When it came time to assign a region, you&apos;d get a listing of all the files its comprised of (Fairly cheap interrogation of NN).  You&apos;d then per file, use the above to figure blocks and their hosts per file.  You&apos;d then take the whole mess and figure which RS had the most blocks local to it and assign the region there (with some damping to compensate for case where blocks are unevenly distributed).&lt;/p&gt;</comment>
                            <comment id="12851262" author="kaykay.unique" created="Tue, 30 Mar 2010 05:38:04 +0000"  >&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  figure which RS had the most blocks local to it and assign the region there&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;if dfs.replication  &amp;gt; 1 (say 3 , as advised) with N blocks , that would be finding the RS with the most blocks among the (3 ^ N permutations) + damping to take care of &apos;hot datanodes&apos; as you had pointed out. It should be interesting ! &lt;/p&gt;
</comment>
                            <comment id="12858876" author="je.ik" created="Tue, 20 Apr 2010 13:27:55 +0000"  >&lt;p&gt;Hi,&lt;br/&gt;
I suspect this issue is causing us trouble during Map/Reduce having HBase as data source. TableInputFormat tells JobTracker that regions are data-local to RegionServer, which serves them. This IMO causes serious imbalance of load on small clusters (our has about 10 nodes), because the RegionServer may (and probably will) contact DataNode on different machine. Thus, in extreme case, single DataNode may (in some time) be handling reads from all the Mappers.&lt;/p&gt;

&lt;p&gt;If regions were assigned to RegionServer which holds the most blocks, I suppose this imbalance will be minimized. Stack&apos;s proposed solution seems fairly appropriate to me.&lt;/p&gt;</comment>
                            <comment id="12858975" author="stack" created="Tue, 20 Apr 2010 17:24:37 +0000"  >&lt;p&gt;@Jan Do you see this: &quot;,,,in extreme case, single DataNode may (in some time) be handling reads from all the Mappers&quot;?  On a long-running cluster, blocks will migrate to be local to the regoinserver that is serving them.  Is this not happening in your case?  Are you seeing RSs mostly reaching across the net to remote DNs?&lt;/p&gt;</comment>
                            <comment id="12859244" author="je.ik" created="Wed, 21 Apr 2010 07:32:10 +0000"  >&lt;p&gt;@stack&lt;br/&gt;
Our cluster is not &quot;long running&quot;, we have written approx. 1TB of data, which we are trying to analyze using M/R. After the data was written, we do not modify them. In this case, I&apos;m afraid dislocated blocks will stay put, until major compaction (which might therefore solve this problem for us). The blocks may get dislocated for example after some RS becomes temporarily unavailable. Therefore I suppose assigning regions based on their real location is somewhat &apos;cleaner&apos;, so that after the RS becomes available again it regains its regions.&lt;/p&gt;</comment>
                            <comment id="12861504" author="stack" created="Tue, 27 Apr 2010 19:19:51 +0000"  >&lt;p&gt;Assigning to our successful GSOC applicant, Li Chongxin (Wahoo!!)&lt;/p&gt;</comment>
                            <comment id="12866676" author="streamy" created="Wed, 12 May 2010 18:20:34 +0000"  >&lt;p&gt;I&apos;m going to work on this as part of a load balancer rewrite I&apos;m doing that is part of &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2485&quot; title=&quot;Persist Master in-memory state so on restart or failover, new instance can pick up where the old left off&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2485&quot;&gt;&lt;del&gt;HBASE-2485&lt;/del&gt;&lt;/a&gt;.  Initially my scope is just to add this to cluster startup and not part of normal load balancing but will look at that as well.  I think that doing it as part of normal cluster balancing is less important than introducing read/write load information but at startup we almost guarantee horrible locality &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="12894763" author="whshub" created="Tue, 3 Aug 2010 00:36:04 +0000"  >&lt;p&gt;I&apos;m curious whether there was a discussion around doing this another, less complete way.  Why can&apos;t we persist info about the allocation of regions across cluster shutdown in zk.  We could then use this as a starting point for which region servers serve which regions.  Then assign the remaining regions randomly/however it is being done now.  Wouldn&apos;t this solve 80% of the problem in a simple way?&lt;/p&gt;</comment>
                            <comment id="12894784" author="streamy" created="Tue, 3 Aug 2010 02:04:36 +0000"  >&lt;p&gt;@Jacques, yeah that approach is being considered.  once the master rewrite gets merged into trunk (&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2692&quot; title=&quot;Master rewrite and cleanup for 0.90&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2692&quot;&gt;&lt;del&gt;HBASE-2692&lt;/del&gt;&lt;/a&gt;) it should be possible to retain assignment information across a shutdown.  you wouldn&apos;t even need zk because you could just leave it in META.  it would actually be a fairly simple change once the other stuff is in.&lt;/p&gt;

&lt;p&gt;And yeah, this solves 80% of the problem.  But I&apos;ve also written some partial code to do stuff by locality; it&apos;s not trivial but not too bad.  Perhaps this simple solution first and then we can start looking at block locations when we extend our notion of &quot;load&quot; for the next version of load balancing.&lt;/p&gt;</comment>
                            <comment id="12894787" author="streamy" created="Tue, 3 Aug 2010 02:17:37 +0000"  >&lt;p&gt;Filed &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2896&quot; title=&quot;Retain assignment information between cluster shutdown/startup&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2896&quot;&gt;&lt;del&gt;HBASE-2896&lt;/del&gt;&lt;/a&gt; for the simpler solution&lt;/p&gt;</comment>
                            <comment id="13648762" author="stack" created="Fri, 3 May 2013 20:22:51 +0000"  >&lt;p&gt;Fixed by the stochastic balancer in trunk/0.95&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12470689">HBASE-2896</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10001">
                    <name>dependent</name>
                                            <outwardlinks description="depends upon">
                                        <issuelink>
            <issuekey id="12374577">HADOOP-1652</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12355063">HADOOP-692</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                            <subtask id="12397860">HBASE-675</subtask>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Sat, 8 Dec 2007 06:37:54 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>31655</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            3 years, 31 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i02f73:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>12063</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>