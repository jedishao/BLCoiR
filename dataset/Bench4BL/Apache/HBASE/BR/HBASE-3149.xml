<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sat Dec 03 18:14:46 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-3149/HBASE-3149.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-3149] Make flush decisions per column family</title>
                <link>https://issues.apache.org/jira/browse/HBASE-3149</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt; Today, the flush decision is made using the aggregate size of all column families. When large and small column families co-exist, this causes many small flushes of the smaller CF. We need to make per-CF flush decisions.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12478262">HBASE-3149</key>
            <summary>Make flush decisions per column family</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.png">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="gaurav.menghani">Gaurav Menghani</assignee>
                                    <reporter username="karthik.ranga">Karthik Ranganathan</reporter>
                        <labels>
                    </labels>
                <created>Mon, 25 Oct 2010 19:58:22 +0000</created>
                <updated>Sun, 19 Jul 2015 12:22:01 +0000</updated>
                            <resolved>Wed, 18 Dec 2013 18:50:40 +0000</resolved>
                                                    <fixVersion>0.89-fb</fixVersion>
                                    <component>regionserver</component>
                        <due></due>
                            <votes>5</votes>
                                    <watches>51</watches>
                                                                <comments>
                            <comment id="12924705" author="jdcryans" created="Mon, 25 Oct 2010 20:20:45 +0000"  >&lt;p&gt;I have been thinking about this one for some time... I think it makes sense in loads of ways since a common problem of multi-CF is that during the initial import the user ends up with thousands of small store files because some family grows faster and triggered the flushes, which in turn generates incredible compaction churn. On the other hand, it means that we almost consider a family as a region e.g. one region with 3 CF can have up to 3x64MB in the memstores.&lt;/p&gt;</comment>
                            <comment id="12925090" author="karthik.ranga" created="Tue, 26 Oct 2010 19:03:14 +0000"  >&lt;p&gt;Yes, agreed that the memory implication is different. &lt;/p&gt;

&lt;p&gt;Eventually, is it not better to enforce the memory limit by using a combination of flush sizes and restricting the number of regions we create? Because ideally we should allow different flush sizes for the different CF&apos;s as the KV sizes could be way different...&lt;/p&gt;

&lt;p&gt;Shall I just make this an option in the conf for now with the default the way it is?&lt;/p&gt;</comment>
                            <comment id="12983312" author="nspiegelberg" created="Tue, 18 Jan 2011 18:31:42 +0000"  >&lt;p&gt;Some interesting stats. We did some rough calculations internally to see what effect an uneven distribution of data into column families was having on our network IO. Our data distribution for 3 column families was 1:1:20. When we looked at the flush:minor-compaction ratio for each of the store files, the large column family had a 1:2 ratio but the small CFs both had a 1:20 ratio! We are looking at roughly a 10% network IO decrease if we can bring those other 2 CFs down to a 1:2 ratio as well.&lt;/p&gt;</comment>
                            <comment id="12983484" author="nspiegelberg" created="Wed, 19 Jan 2011 01:22:48 +0000"  >&lt;p&gt;This is a substantial refactoring effort.  My current development strategy is to break this down into 4 parts.  Each one will have a diff + review board so you guys don&apos;t get overwhelmed...&lt;/p&gt;

&lt;p&gt;1. move flushcache() from Region -&amp;gt; Store.  have Region.flushcache loop through Store API&lt;br/&gt;
2. move locks from Region -&amp;gt; Store.  figure out flush/compact/split locking strategy&lt;br/&gt;
3. refactor HLog to store per-CF seqnum info&lt;br/&gt;
4. refactor MemStoreFlusher from regionsInQueue to storesInQueue&lt;/p&gt;</comment>
                            <comment id="12983510" author="ryanobjc" created="Wed, 19 Jan 2011 02:50:45 +0000"  >&lt;p&gt;if you are going to generate a sequence id for every CF, then we will&lt;br/&gt;
need to create and use a new synthetic ID for atomic views.&lt;/p&gt;</comment>
                            <comment id="12983523" author="nspiegelberg" created="Wed, 19 Jan 2011 03:39:00 +0000"  >&lt;p&gt;@ryan: the main work in step #3 isn&apos;t &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2856&quot; title=&quot;TestAcidGuarantee broken on trunk &quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2856&quot;&gt;&lt;del&gt;HBASE-2856&lt;/del&gt;&lt;/a&gt; work.  It&apos;s roughly modifying HLog.lastSeqWritten from Map&amp;lt;region, long&amp;gt; =&amp;gt; Map&amp;lt;store, long&amp;gt; and all the refactoring to the HLog code that it entails. &lt;/p&gt;</comment>
                            <comment id="12983972" author="ryanobjc" created="Thu, 20 Jan 2011 01:09:46 +0000"  >&lt;p&gt;ok got it. as long as we dont generate a seqid per family we are all good.&lt;/p&gt;</comment>
                            <comment id="12984522" author="nspiegelberg" created="Fri, 21 Jan 2011 01:21:59 +0000"  >&lt;p&gt;While our row-level acid guarantees for gets/scans are slightly broken in the trunk (&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2856&quot; title=&quot;TestAcidGuarantee broken on trunk &quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2856&quot;&gt;&lt;del&gt;HBASE-2856&lt;/del&gt;&lt;/a&gt;), per-CF flushes will extremely aggravate this problem because a single user-level Put could span both current memstore and snapshot memstore depending upon what CF the put shard resides in.  Halting progress on this issue until this jira is completed.&lt;/p&gt;</comment>
                            <comment id="12988784" author="schubertzhang" created="Mon, 31 Jan 2011 15:34:50 +0000"  >&lt;p&gt;This jira is very useful in practice. &lt;br/&gt;
In HBase, the horizontal partitions by rowkey-ranges make regions, and the vertical partitions by column-family make stores. These horizontal and vertical partitoning schema make a data tetragonum &amp;#8212; the store in hbase.&lt;/p&gt;

&lt;p&gt;The memstore is base on the store, so the flush and compaction need also be based on store. The memstoreSize in HRegion should be in HStore.&lt;/p&gt;

&lt;p&gt;For flexible configuration, I think we shlould be able to configure memstoresize (i.e. hbase.hregion.memstore.flush.size) in Column-Family level (when create table). And if possaible, I want the maxStoreSize also be configurable for different Column-Family.&lt;/p&gt;</comment>
                            <comment id="13132799" author="yuzhihong@gmail.com" created="Fri, 21 Oct 2011 16:27:37 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-4645&quot; title=&quot;Edits Log recovery losing data across column families&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-4645&quot;&gt;&lt;del&gt;HBASE-4645&lt;/del&gt;&lt;/a&gt; makes edit log recovery conservative to avoid data loss.&lt;br/&gt;
When this feature is implemented, we should revisit edit log recovery by passing an array of maxSeqIds.&lt;/p&gt;</comment>
                            <comment id="13210671" author="mubarakseyed" created="Fri, 17 Feb 2012 23:42:11 +0000"  >&lt;p&gt;@Nicolas,&lt;br/&gt;
Is there any update on this issue? We have a production use-case wherein 80% of data goes to one CF and remaining 20% goes to two other CFs. I can collaborate with you if you are interested to pursue with patch. Thanks.&lt;/p&gt;</comment>
                            <comment id="13210687" author="nspiegelberg" created="Sat, 18 Feb 2012 00:21:35 +0000"  >&lt;p&gt;@Mubarak: I think you probably are more interested in tuning the compaction settings.  The initial reason for this JIRA was higher network IO.  The actual problem was that the min unconditional compact size was too high &amp;amp; caused bad compaction decision.  We fixed this by lowering the min size from the default of the flush size (256MB, for us) to 4MB.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  &amp;lt;property&amp;gt;
   &amp;lt;name&amp;gt;hbase.hstore.compaction.min.size&amp;lt;/name&amp;gt;
   &amp;lt;value&amp;gt;4194304&amp;lt;/value&amp;gt;
   &amp;lt;description&amp;gt;
     The &lt;span class=&quot;code-quote&quot;&gt;&quot;minimum&quot;&lt;/span&gt; compaction size. All files below &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; size are always
     included into a compaction, even &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; outside compaction ratio times
     the total size of all files added to compaction so far.
   &amp;lt;/description&amp;gt;
  &amp;lt;/property&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We identified this a while ago and I thought we were going to change the default for 0.92, but it looks like it&apos;s still in the Store.java code &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;   A better use of your time would be to verify that this reduces your IO and write up a JIRA to change the default.&lt;/p&gt;</comment>
                            <comment id="13210698" author="mubarakseyed" created="Sat, 18 Feb 2012 00:36:11 +0000"  >&lt;p&gt;Thanks Nicolas. Will try with 4 MB and create a JIRA.&lt;/p&gt;</comment>
                            <comment id="13211204" author="stack" created="Sun, 19 Feb 2012 04:39:35 +0000"  >&lt;p&gt;Thanks @Nicolas (and thanks @Mubarak &amp;#8211; sounds like something to indeed get into 0.92).&lt;/p&gt;

&lt;p&gt;At the same time, I&apos;d think this issue still worth some time; if lots of cfs and only one is filling, its silly to flush the others as we do now because one is over the threshold.&lt;/p&gt;</comment>
                            <comment id="13211614" author="lhofhansl" created="Mon, 20 Feb 2012 00:15:08 +0000"  >&lt;p&gt;@Nicolas: Interesting bit about hstore.compaction.min.size. I&apos;m curious, is 4MB something that works specifically for your setup, or would you generally recommend it setting it that low?&lt;br/&gt;
It probably has to do with whether compression is enabled, how many CFs and relative sizes, etc.&lt;/p&gt;

&lt;p&gt;Maybe instead of defaulting it to flushsize, we could default it to flushsize/2 or flushsize/4...?&lt;/p&gt;</comment>
                            <comment id="13212166" author="stack" created="Mon, 20 Feb 2012 22:20:50 +0000"  >&lt;p&gt;@Nicolas I wonder about this... hbase.hstore.compaction.min.size.  When we compact, don&apos;t we have to take adjacent files as part of our ACID guarantees?  This would frustrate that?  (I&apos;ll take a look... tomorrow).  I&apos;m wondering because i want to figure how to make it so we favor reference files... so they are always included in a compaction.&lt;/p&gt;</comment>
                            <comment id="13212167" author="stack" created="Mon, 20 Feb 2012 22:21:22 +0000"  >&lt;p&gt;Making 0.92.1 critical so it gets a bit of loving...&lt;/p&gt;</comment>
                            <comment id="13212938" author="larsgeorge" created="Tue, 21 Feb 2012 20:49:39 +0000"  >&lt;blockquote&gt;&lt;p&gt;At the same time, I&apos;d think this issue still worth some time; if lots of cfs and only one is filling, its silly to flush the others as we do now because one is over the threshold.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I thought so too. Setting the hbase.hstore.compaction.size to 4MB, and having the flush size at 256MB, it means you will never compact flush files larger than 4MB. So, in other words, only if you are flushing small files (say from a small, dependent column family) you are running a minor compaction on them. For the larger family you typically do not run those at all, right?&lt;/p&gt;

&lt;p&gt;This surely seems a specific setting for this use-case, and there are others that need a slightly different setting. If you mix those two on the same cluster, then having only one global setting to adjust this seems restrictive? Should this be a setting per table, like the flush size?&lt;/p&gt;

&lt;p&gt;It still seems to me that decoupling is what we should have available as well. But I thought about it for a while as well as discussed this various people: it seems that decoupling brings its own set of issues, for example, you might end up with too many HLog files because the small family is flushed only rarely. &lt;/p&gt;</comment>
                            <comment id="13213853" author="lhofhansl" created="Wed, 22 Feb 2012 18:57:54 +0000"  >&lt;p&gt;@Lars G: Now I am perfectly confused. The description says that &quot;All files below this size are always included into a compaction&quot;.&lt;br/&gt;
I had assumed this setting is to quickly get rid of the smaller store files. Did I misunderstand?&lt;/p&gt;</comment>
                            <comment id="13213858" author="nspiegelberg" created="Wed, 22 Feb 2012 19:03:26 +0000"  >&lt;p&gt;@Lars/Stack: note that the number of StoreFiles necessary to store N amount of data is order O(log N) with the existing compaction algorithm.  This means that setting the compaction min size to a low value will not result in significantly more files.  Furthermore, what&apos;s hurting performance is not the amount of files but the size of each file.  The extra files will be very small and take up only a minority of the space in the LRU cache.  Every time you unnecessarily compact files, you have to repopulate that StoreFile in the LRU cache and get a lot of disk reads in addition to the obvious write increase.  This is all to say that I would recommend defaulting it to that low because the downsides are very minimal and the benefit can be substantial IO gains.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;At the same time, I&apos;d think this issue still worth some time; if lots of cfs and only one is filling, its silly to flush the others as we do now because one is over the threshold.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Why is this silly?  With cache-on-write, the data is still cached in memory.  It&apos;s just migrated from the MemCache to the BlockCache, which has comparable performance.  Furthermore, BlockCache data is compressed, so it then takes up less space.  Flushing also minimizes the amount of HLogs and decreases recovery time.  Flushing would be bad if it meant we weren&apos;t optimally using the global MemStore size, but we currently are.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;This surely seems a specific setting for this use-case, and there are others that need a slightly different setting. If you mix those two on the same cluster, then having only one global setting to adjust this seems restrictive? Should this be a setting per table, like the flush size?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think this is a better default, not that it&apos;s a one-size setting.  I agree that this should toggleable on a per-CF basis, hence &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5335&quot; title=&quot;Dynamic Schema Configurations&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5335&quot;&gt;&lt;del&gt;HBASE-5335&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="13214372" author="stack" created="Thu, 23 Feb 2012 06:30:45 +0000"  >&lt;p&gt;@Nicolas I think I follow.  I opened &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5461&quot; title=&quot;Set hbase.hstore.compaction.min.size way down to 4MB&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5461&quot;&gt;&lt;del&gt;HBASE-5461&lt;/del&gt;&lt;/a&gt;.  Let me try it.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Why is this silly? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Because I was seeing a plethora of small files a problem but given your explaination above, I think I grok that its not many small files thats the prob; its that w/ the way high min size, our selection was to inclusionary and so we end up doing loads of rewriting.&lt;/p&gt;</comment>
                            <comment id="13214382" author="lhofhansl" created="Thu, 23 Feb 2012 06:40:09 +0000"  >&lt;p&gt;Thanks for explaining Nicolas.&lt;br/&gt;
I wonder if a good default would be some fraction of the flushsize. Maybe 1/4*flushsize, or something.&lt;/p&gt;</comment>
                            <comment id="13577618" author="v.himanshu" created="Wed, 13 Feb 2013 15:02:35 +0000"  >&lt;p&gt;This is a useful feature; I&apos;m working on it.&lt;/p&gt;</comment>
                            <comment id="13680110" author="sershe" created="Tue, 11 Jun 2013 01:00:03 +0000"  >&lt;p&gt;Any update since last comment?&lt;/p&gt;</comment>
                            <comment id="13689511" author="v.himanshu" created="Thu, 20 Jun 2013 18:44:40 +0000"  >&lt;p&gt;I figured that it increases mttr time. I will probably look into it after we fixed mttr issues of late. Un-assigning it for the meanwhile.&lt;/p&gt;</comment>
                            <comment id="13793955" author="gaurav.menghani" created="Mon, 14 Oct 2013 06:41:23 +0000"  >&lt;p&gt;Assigning this to myself, since I have committed this change in the 0.89-fb branch. Will upload the patch shortly.&lt;/p&gt;</comment>
                            <comment id="13794254" author="stack" created="Mon, 14 Oct 2013 16:33:22 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=gaurav.menghani&quot; class=&quot;user-hover&quot; rel=&quot;gaurav.menghani&quot;&gt;Gaurav Menghani&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13804517" author="gaurav.menghani" created="Thu, 24 Oct 2013 18:39:45 +0000"  >&lt;p&gt;Patch for the 0.89-fb version. &lt;/p&gt;</comment>
                            <comment id="13804522" author="gaurav.menghani" created="Thu, 24 Oct 2013 18:41:32 +0000"  >&lt;p&gt;This diff is only for the 0.89-fb branch, I will port this to master soon. &lt;/p&gt;</comment>
                            <comment id="13804528" author="hadoopqa" created="Thu, 24 Oct 2013 18:43:43 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12610129/Per-CF-Memstore-Flush.diff&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12610129/Per-CF-Memstore-Flush.diff&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 tests included&lt;/font&gt;.  The patch appears to include 12 new or modified tests.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 patch&lt;/font&gt;.  The patch command could not apply the patch.&lt;/p&gt;

&lt;p&gt;Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/7618//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/7618//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13804537" author="gaurav.menghani" created="Thu, 24 Oct 2013 18:53:41 +0000"  >&lt;p&gt;The basic idea is to be able to maintain the smallest LSN amongst the edits present in a particular memstore for a column family. When we decide to flush a set of memstores, we find the smallest LSN id amongst the memstores that we are not flushing, say X, and say that we can remove the logs for any edits with LSN less than X. We choose a particular memstore to be flushed, if it occupies more than &apos;t&apos; bytes, when the global memstore size threshold is &apos;T&apos; (and t/T = 1/4 for our configuration). If there is no memstore with &amp;gt;= t bytes but the total size of all the memstores is above T, we flush all the memstores. &lt;/p&gt;</comment>
                            <comment id="13804806" author="yuzhihong@gmail.com" created="Thu, 24 Oct 2013 23:31:18 +0000"  >&lt;p&gt;Unfinished port to trunk.&lt;/p&gt;

&lt;p&gt;There is no log.appendNoSync() call in 89-fb. So that&apos;s difference we need to resolve.&lt;/p&gt;

&lt;p&gt;Talking with Gaurav, I think it would be nice to provide different heuristics (policies) for which column families to flush when there is no single column family whose size exceeds threshold t.&lt;/p&gt;</comment>
                            <comment id="13852025" author="gaurav.menghani" created="Wed, 18 Dec 2013 18:58:00 +0000"  >&lt;p&gt;Ted has volunteered to port this to trunk in a separate JIRA. I will be working on different heuristics to see the benefits that we get.&lt;/p&gt;</comment>
                            <comment id="13852042" author="stack" created="Wed, 18 Dec 2013 19:17:25 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=gaurav.menghani&quot; class=&quot;user-hover&quot; rel=&quot;gaurav.menghani&quot;&gt;Gaurav Menghani&lt;/a&gt; Gaurav, have you deployed this change?  If so, what do you see in operation?  When you talk about different heuristics, what you thinking?  Thanks boss.&lt;/p&gt;</comment>
                            <comment id="13854725" author="gaurav.menghani" created="Sat, 21 Dec 2013 01:15:49 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=saint.ack%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;saint.ack@gmail.com&quot;&gt;Stack&lt;/a&gt; Yes, we have deployed this, with selective flushing disabled for now, since we didn&apos;t see any aggregate benefits yet. The heuristics that I was thinking about were around, which column families to flush when there are no column families above the threshold for flushing families. Eg. if the memstore limit is 128 MB, and the flushing threshold for a CF is 32 MB, there might be a case, where there are like 7-8 CFs, and none of them are above 32 MB. &lt;/p&gt;

&lt;p&gt;In that case, there are a couple of heuristics you can choose. Like: flush the top N column families, flush only as few column families to free up 1/4 th of the memstore, etc. The main benefit I see is the time spent while compacting the smaller CFs will be much lesser, since the number of files created would be much lesser. This is compensated against bigger column families being flushed earlier than before, and having smaller files than without this change, but with the right heuristics we can find a good balance.&lt;/p&gt;</comment>
                            <comment id="14230448" author="bridiver" created="Mon, 1 Dec 2014 20:59:20 +0000"  >&lt;p&gt;What is the status of this in trunk? &lt;/p&gt;</comment>
                            <comment id="14231835" author="jmhsieh" created="Tue, 2 Dec 2014 18:01:39 +0000"  >&lt;p&gt;The jira reflects the latest status &amp;#8211; i believe this issue is waiting for someone to pick up and complete for about a year. &lt;/p&gt;</comment>
                            <comment id="14231843" author="yuzhihong@gmail.com" created="Tue, 2 Dec 2014 18:04:15 +0000"  >&lt;p&gt;Please see &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-10201&quot; title=&quot;Port &amp;#39;Make flush decisions per column family&amp;#39; to trunk&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-10201&quot;&gt;&lt;del&gt;HBASE-10201&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10032">
                    <name>Blocker</name>
                                                                <inwardlinks description="is blocked by">
                                        <issuelink>
            <issuekey id="12469746">HBASE-2856</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="12495832">HBASE-3450</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12788345">HBASE-13408</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12685435">HBASE-10201</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12543806">HBASE-5461</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12610182" name="3149-trunk-v1.txt" size="91031" author="yuzhihong@gmail.com" created="Thu, 24 Oct 2013 23:31:18 +0000"/>
                            <attachment id="12610129" name="Per-CF-Memstore-Flush.diff" size="80577" author="gaurav.menghani" created="Thu, 24 Oct 2013 18:41:32 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Mon, 25 Oct 2010 20:20:45 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>32922</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            2 years, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i02fgn:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>12106</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>