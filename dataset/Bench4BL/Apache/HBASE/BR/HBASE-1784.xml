<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 02 19:11:22 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-1784/HBASE-1784.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-1784] Missing rows after medium intensity insert</title>
                <link>https://issues.apache.org/jira/browse/HBASE-1784</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;This bug was uncovered by Mathias in his mail &quot;Issue on data load with 0.20.0-rc2&quot;. Basically, somehow, after a medium intensity insert a lot of rows goes missing. Easy way to reproduce : PE. Doing a PE scan or randomRead afterwards won&apos;t uncover anything since it doesn&apos;t bother about null rows. Simply do a count in the shell, easy to test (I changed my scanner caching in the shell to do it faster).&lt;/p&gt;

&lt;p&gt;I tested some light insertions with force flush/compact/split in the shell and it doesn&apos;t break.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12433723">HBASE-1784</key>
            <summary>Missing rows after medium intensity insert</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.png">Blocker</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="stack">stack</assignee>
                                    <reporter username="jdcryans">Jean-Daniel Cryans</reporter>
                        <labels>
                    </labels>
                <created>Fri, 21 Aug 2009 19:52:02 +0000</created>
                <updated>Sun, 13 Sep 2009 22:24:55 +0000</updated>
                            <resolved>Tue, 1 Sep 2009 19:56:20 +0000</resolved>
                                    <version>0.20.0</version>
                                    <fixVersion>0.20.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                <comments>
                            <comment id="12746298" author="jdcryans" created="Fri, 21 Aug 2009 22:22:39 +0000"  >&lt;p&gt;Testing this issue is not as easy as I thought it was. I was doing randomWrites and not sequentialWrites... that&apos;s very different, with the random one you don&apos;t write as many rows. So I never really &quot;lost&quot; thoses rows, they were never written.&lt;/p&gt;

&lt;p&gt;But there seems to be something in Mathias&apos;s case, Ryan also got this issue as he told me during the hackathon.&lt;/p&gt;</comment>
                            <comment id="12746901" author="herberts" created="Mon, 24 Aug 2009 14:52:07 +0000"  >&lt;p&gt;This is a stripped down version of the MR job I use to import data into HBase.&lt;/p&gt;

&lt;p&gt;The input consists of SequenceFiles with byte[] keys and byte[] values, respectively 19 and 320 bytes long.&lt;/p&gt;
</comment>
                            <comment id="12746983" author="streamy" created="Mon, 24 Aug 2009 18:06:12 +0000"  >&lt;p&gt;Current thinking is that this is caused by failed compaction(s).  Mathias is turning on DEBUG and running smaller jobs, increasing load each run, until it triggers missing rows.&lt;/p&gt;</comment>
                            <comment id="12747806" author="herberts" created="Wed, 26 Aug 2009 07:11:13 +0000"  >&lt;p&gt;I reran my test to try to corner the problem.&lt;/p&gt;

&lt;p&gt;My last run &lt;b&gt;only&lt;/b&gt; lost around 2 million rows out of 866 millions. Interestingly the logs (attached) only show one compaction failure.&lt;/p&gt;

&lt;p&gt;A side effect observed was that inserting the rows this the WB disabled was almost twice as fast as with the WB set to 1Mb.&lt;/p&gt;</comment>
                            <comment id="12747820" author="herberts" created="Wed, 26 Aug 2009 08:02:27 +0000"  >&lt;p&gt;A scan of the table shows that rows from the region 0TE181121109t\x90_\x7F\xFF\xFF\xFE,1251257023139 whose compaction failed are indeed gone, so this clearly shows that missing rows are indeed linked to the failed compaction.&lt;/p&gt;</comment>
                            <comment id="12748004" author="streamy" created="Wed, 26 Aug 2009 16:11:04 +0000"  >&lt;p&gt;@Mathias, one thing JD found during some import testing we did over the hackathon was to use large write buffers.  Experiment with 10-20MB write buffers rather than 1MB.  And disable the WAL.&lt;/p&gt;

&lt;p&gt;Are you running with WAL disabled on these Puts already?&lt;/p&gt;</comment>
                            <comment id="12748008" author="streamy" created="Wed, 26 Aug 2009 16:15:06 +0000"  >&lt;p&gt;Looking at your code, seems that you are disabling WAL.&lt;/p&gt;

&lt;p&gt;Given that in your testing you don&apos;t generally see full-on cluster breakage, just occasional compactions failing, that if the failed compactions weren&apos;t erradicating data you would probably survive the import to the end.&lt;/p&gt;

&lt;p&gt;That&apos;s a good argument for fixing this in 0.20.0, but it might not be as easy as we&apos;d hope.&lt;/p&gt;

&lt;p&gt;One other note... At least in the Mathias logs I&apos;ve seen, the compactions that are running, and failing, seem to have at least some half readers / referenced storefiles.&lt;/p&gt;</comment>
                            <comment id="12748108" author="stack" created="Wed, 26 Aug 2009 19:47:56 +0000"  >&lt;p&gt;@Mathias If you grep /hdfs/hadoop/hbase/domirama/377086364/batchid/5233728315379016023 in your namenode log or 7352946847213562224 (from blk_7352946847213562224_926536) do you see anything interesting?  The file looks like its been deleted by another process (doubly assigned region?  But this is RC2, right?).&lt;/p&gt;</comment>
                            <comment id="12748167" author="herberts" created="Wed, 26 Aug 2009 21:42:53 +0000"  >&lt;p&gt;The masterlog. show a scenario which seems to have led to a double assignment.&lt;/p&gt;

&lt;p&gt;At 05:20:16,074 RS0 opens region domirama,0TE181121109t\x90_\x7F\xFF\xFF\xFE,125125681462&lt;/p&gt;

&lt;p&gt;At 05:23:43,138 RS0 initiates a split of this region, the split will complete at 05:24:01,315 after 18s.&lt;/p&gt;

&lt;p&gt;At 05:23:50,516 MASTER discovers one of the daughter regions and decides its current assignment is not valid&lt;/p&gt;

&lt;p&gt;At 05:23:50,905 MASTER assigns this daughter region to RS1&lt;/p&gt;

&lt;p&gt;At 05:23:53,957 MASTER receives PROCESS_OPEN / OPEN messages from RS1&lt;/p&gt;

&lt;p&gt;At 05:24:02,389 MASTER receives REPORT_SPLIT message from RS0&lt;/p&gt;

&lt;p&gt;At 05:24:02,390 MASTER assigns the daughter region to RS0&lt;/p&gt;

&lt;p&gt;At 05:24:05,466 MASTER receives PROCESS_OPEN / OPEN messages from RS0&lt;/p&gt;

&lt;p&gt;the daughter region is then doubly assigned.&lt;/p&gt;

&lt;p&gt;This is happening because the split starts by writing info about the daughter regions in META, then if the split takes quite some time (it took 18s in this case), the master might discover the daughter regions prior to being notified of the split. It then acts as if the regions were not assigned and assigns it, but later when it receives the split report, it assigns again the region (to the RS that did the split).&lt;/p&gt;

&lt;p&gt;Shouldn&apos;t a splitting RS assume it is serving the daughters and record this in historian or in META?&lt;br/&gt;
Shouldn&apos;t the master check that the RS that is doing the split is already serving the region?&lt;br/&gt;
Shouldn&apos;t the master check when it receives the split report that it has not already assigned the daughters?&lt;/p&gt;</comment>
                            <comment id="12748190" author="herberts" created="Wed, 26 Aug 2009 22:27:43 +0000"  >&lt;p&gt;The attached patch checks that the daughter regions are not in state &apos;isOpening&apos; prior to setting them unassigned.&lt;/p&gt;</comment>
                            <comment id="12748195" author="ryanobjc" created="Wed, 26 Aug 2009 23:02:09 +0000"  >&lt;p&gt;so a slow split opens us up a race condition with the META scanner, good find.&lt;/p&gt;

&lt;p&gt;any thoughts on why the split is so slow? it should be fast?&lt;/p&gt;

&lt;p&gt;I call this another vote for cutting the master out of the split loop. &lt;/p&gt;</comment>
                            <comment id="12748199" author="herberts" created="Wed, 26 Aug 2009 23:14:26 +0000"  >&lt;p&gt;The split was slow because META and another region were being compacted, as soon as META was scheduled for compaction, IPC were blocked, and they remained blocked during the compaction of ther other region which took 15s and happened BEFORE compaction of META.&lt;/p&gt;
</comment>
                            <comment id="12748276" author="stack" created="Thu, 27 Aug 2009 06:41:08 +0000"  >&lt;p&gt;I think the patch won&apos;t fix what I see in the logs.  What I am seeing is that, as you describe Mathias, the new daughter region has been added to the .META. table, noticed by the scanner and then assigned AHEAD of the master getting the split message that includes said daughter.  The daughter has been successfully opened out on the regionserver.  On successful open, the master no longer carries internal state on a region since its no longer &apos;transitioning&apos;.  The split comes in and the region is reassigned IF !s.isPendingOpen() &amp;amp;&amp;amp; !s.isOpen().  The daughter region will be in neither state.  It won&apos;t be pending since its already opened.  isOpen is a bit of a misnomer in that its a temporary state a region is in after we get the open message from regionserver but before its open details &amp;#8211; the server it was opened on &amp;#8211; has been added to the .META. table.  After the .META. has been updated, the region is no longer transistioning and is removed from the master inner state tables.  So, the daughter won&apos;t be in the isOpen state either.&lt;/p&gt;

&lt;p&gt;Lets chat in morning.  Maybe a reordering of the order in which we send events would make things work well enough in the 0.20 timeframe till we get a chance to redo this state-transition stuff properly in 0.21.&lt;/p&gt;


</comment>
                            <comment id="12748430" author="stack" created="Thu, 27 Aug 2009 16:35:55 +0000"  >&lt;p&gt;Bringing into 0.20.0&lt;/p&gt;</comment>
                            <comment id="12748521" author="stack" created="Thu, 27 Aug 2009 19:28:35 +0000"  >&lt;p&gt;Here is a patch that before it adds new daughter region to unassigned, it checks that region has not already got a server and startcode in the .META. table: i.e that it has not already been assigned.  Its messy and expensive &amp;#8211; e.g. more .META. queries &amp;#8211; but in the scheme of things splits are relatively infrequent.&lt;/p&gt;

&lt;p&gt;I do not see an alternative given the current toolset used keeping region state.  Even in new regime, will need to check for this condition but perhaps in the new context we can make the check more lightweight.&lt;/p&gt;</comment>
                            <comment id="12748587" author="stack" created="Thu, 27 Aug 2009 21:55:25 +0000"  >&lt;p&gt;hbs is running patch overnight on his cluster.  Will have news for us in morning.&lt;/p&gt;</comment>
                            <comment id="12748855" author="herberts" created="Fri, 28 Aug 2009 15:14:38 +0000"  >&lt;p&gt;I ran my import job against the head of the 0.20 branch + patch for 1784 and unfortunately I am still missing some rows.&lt;/p&gt;

&lt;p&gt;The logs don&apos;t show similar messages as the one that lead to discover the double assignment problem. But they show a few&lt;/p&gt;

&lt;p&gt;java.lang.RuntimeException: ScanWildcardColumnTracker.checkColumn ran into a column actually smaller than the previous column&lt;/p&gt;

&lt;p&gt;4 of the 5 reducers were restarted due to timeout being reached when attempting to contact region servers, the batch therefore ran for more than 15 hours.&lt;/p&gt;

&lt;p&gt;Will rerun it once more on an empty table to have a double test, but for now it seems dataloss still occur.&lt;/p&gt;</comment>
                            <comment id="12748880" author="apurtell" created="Fri, 28 Aug 2009 16:25:36 +0000"  >&lt;blockquote&gt;&lt;p&gt;java.lang.RuntimeException: ScanWildcardColumnTracker.checkColumn ran into a column actually smaller than the previous column&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That error means that edits for one family are still ending up in store files for another. I remember how to hack the code to let the store scanner continue around this condition. Otherwise it is in effect data loss because the store scanner cannot continue, though data is there. Let me put up a patch for this.&lt;/p&gt;

&lt;p&gt;There is a separate hack for healing the problem as it is discovered during compaction by moving the edits from the inappropriate store to the appropriate one. Should we do this too as a workaround until the root cause is found and fixed? &lt;/p&gt;</comment>
                            <comment id="12748885" author="stack" created="Fri, 28 Aug 2009 16:29:10 +0000"  >&lt;p&gt;@Mathias Any regionserver crashes during upload?&lt;/p&gt;</comment>
                            <comment id="12749637" author="herberts" created="Mon, 31 Aug 2009 21:45:02 +0000"  >&lt;p&gt;This is with a head of 0.20 with St^Ack&apos;s fix and Andrew&apos;s hack.&lt;/p&gt;

&lt;p&gt;lines of interest have been prefixed with &apos;@@&apos; in file &apos;dbl-assignment-20090831&apos;&lt;/p&gt;

&lt;p&gt;This double assignment is happening after a split, as in the previous case.&lt;/p&gt;

&lt;p&gt;The double assignement takes place after&lt;/p&gt;

&lt;p&gt;2009-08-31 17:03:08,664&lt;/p&gt;

&lt;p&gt;The attachement has logs from Master and 2 RS (the ones involved in the double assignment).&lt;/p&gt;</comment>
                            <comment id="12749646" author="stack" created="Mon, 31 Aug 2009 21:58:01 +0000"  >&lt;p&gt;This is like the last hole we plugged only it&apos;s in basescanner this time.   It&apos;s an interesting one.  Patch co&lt;br/&gt;
ing&lt;/p&gt;</comment>
                            <comment id="12749673" author="stack" created="Mon, 31 Aug 2009 23:29:21 +0000"  >&lt;p&gt;From the log posted by Mathias, this is the pertinent bit:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2009-08-31 17:03:11,689 INFO org.apache.hadoop.hbase.master.RegionServerOperation: domirama,00AZRPXURYW7\x13\x7B_\x7F\xFF\xFF\xFE,1251730985136 open on 10.154.99.183:60020
2009-08-31 17:03:11,690 INFO org.apache.hadoop.hbase.master.RegionServerOperation: Updated row domirama,00AZRPXURYW7\x13\x7B_\x7F\xFF\xFF\xFE,1251730985136 in region .META.,,1 with startcode=1251704407620, server=10.154.99.183:60020
2009-08-31 17:03:11,749 INFO org.apache.hadoop.hbase.master.ServerManager: 5 region servers, 0 dead, average load 50.0
@@2009-08-31 17:03:11,851 DEBUG org.apache.hadoop.hbase.master.BaseScanner: Current assignment of domirama,00AZRPXUPUF7\xFC\xD4\xDF\x7F\xFF\xFF\xFC,1251730985136 is not valid;  serverAddress=, startCode=0 unknown.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here, the split has been opened successfully out on the regionserver, so its state in the master has been cleared... but BaseScanner which has been running during the update sees old state of this row, not the state had been updated 150ms earlier.&lt;/p&gt;

&lt;p&gt;Let me go through all places where we set a region unassigned and add in a get so we get current state rather than a stale state of a row.  I did this for the special case of splits in the previous patch applied against this issue but looks like it needs to be done more generally.&lt;/p&gt;</comment>
                            <comment id="12749718" author="stack" created="Tue, 1 Sep 2009 00:52:39 +0000"  >&lt;p&gt;@Mathias Have you upped hbase.client.scanner.caching in hbase-site.xml?&lt;/p&gt;</comment>
                            <comment id="12749756" author="stack" created="Tue, 1 Sep 2009 04:54:40 +0000"  >&lt;p&gt;So, this patch adds distrust of the view returned by BaseScanner (Scanners do not respect row locks in 0.20.0).   Inside in checkAssign, if server address is null, we&apos;ll do a new Get to ensure its still null just before we decide to set a region as unassigned.  It also adds an explicit set of the BaseScanner caching to 1 in case caching is changed in hbase-site.xml to avoid client-side configurations effecting the BaseScanner running in the Master.&lt;/p&gt;

&lt;p&gt;Patch includes the earlier patch for handling the split message not assigning a region already assigned.&lt;/p&gt;

&lt;p&gt;It does not include Andrews&apos; change (Looks like that can be applied independently).&lt;/p&gt;

&lt;p&gt;Its tough testing for this scenario.  Patch logs if it comes across the issue where Scan sees null but the Get actually gets value.&lt;/p&gt;

&lt;p&gt;Any chance of a review and if it looks good to you, trying it on  your upload Mathias?&lt;/p&gt;

&lt;p&gt;Thanks boss.&lt;/p&gt;</comment>
                            <comment id="12749761" author="stack" created="Tue, 1 Sep 2009 05:06:46 +0000"  >&lt;p&gt;Tests pass.&lt;/p&gt;</comment>
                            <comment id="12749803" author="herberts" created="Tue, 1 Sep 2009 07:17:18 +0000"  >&lt;p&gt;hbase.client.scanner.caching was set to the default of 1 during all my tests.&lt;/p&gt;

&lt;p&gt;Launching a new run right now with you latest patch + Andrew&apos;s change.&lt;/p&gt;</comment>
                            <comment id="12749922" author="apurtell" created="Tue, 1 Sep 2009 14:21:12 +0000"  >&lt;p&gt;However it is done, I don&apos;t think having ScanWildcardColumnTracker.checkColumn throw an exception which kills the store scanner is the right thing to do.&lt;/p&gt;</comment>
                            <comment id="12749972" author="stack" created="Tue, 1 Sep 2009 16:29:03 +0000"  >&lt;p&gt;In this latest run, Mathias is seeing:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
13:06 &amp;lt;hbs_&amp;gt; hbase-hadoop-master-hadooptux-00.gicm.net.log:2009-09-01 14:34:30,214 DEBUG org.apache.hadoop.hbase.master.BaseScanner: GET got values when meta found none: serverAddress=10.154.99.180:60020, startCode=1251791310025
13:06 &amp;lt;hbs_&amp;gt; hbase-hadoop-master-hadooptux-00.gicm.net.log:2009-09-01 14:34:31,473 DEBUG org.apache.hadoop.hbase.master.BaseScanner: GET got values when meta found none: serverAddress=10.154.99.180:60020, startCode=1251791310025
13:06 &amp;lt;hbs_&amp;gt; hbase-hadoop-master-hadooptux-00.gicm.net.log:2009-09-01 14:36:29,485 DEBUG org.apache.hadoop.hbase.master.BaseScanner: GET got values when meta found none: serverAddress=10.154.99.180:60020, startCode=1251791310025
13:06 &amp;lt;hbs_&amp;gt; hbase-hadoop-master-hadooptux-00.gicm.net.log:2009-09-01 14:36:29,494 DEBUG org.apache.hadoop.hbase.master.BaseScanner: GET got values when meta found none: serverAddress=10.154.99.180:60020, startCode=1251791310025
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;These are cases where the content of the row has changed between the scan of the row and inside in the method where we check for assignment.  Each instance above would have made for a double-assignment, if my understanding is correct.&lt;/p&gt;</comment>
                            <comment id="12750035" author="apurtell" created="Tue, 1 Sep 2009 18:46:03 +0000"  >&lt;p&gt;Reopened &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-1715&quot; title=&quot;compaction failure in ScanWildcardColumnTracker.checkColumn&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-1715&quot;&gt;&lt;del&gt;HBASE-1715&lt;/del&gt;&lt;/a&gt; to address the orthogonal ScanWildcardColumnTracker issue.&lt;/p&gt;</comment>
                            <comment id="12750049" author="herberts" created="Tue, 1 Sep 2009 19:02:04 +0000"  >&lt;p&gt;My latest run encountered only the errors included in the log I attached to the issue.&lt;/p&gt;

&lt;p&gt;23 occurrences of the log message &quot;GET got ....&quot; appear in the file, meaning that the latest patch St^Ack put together is working and avoided 23 possible double assignements.&lt;/p&gt;

&lt;p&gt;The row count I did after my import found N-1 records, I need to check the counting job as the offset of 1 is probably due to it.&lt;/p&gt;

&lt;p&gt;The multiple cases of double assignments seem at least gone, thanks St^Ack ! &lt;/p&gt;</comment>
                            <comment id="12750051" author="streamy" created="Tue, 1 Sep 2009 19:05:53 +0000"  >&lt;p&gt;1 row a bit better than 2M &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="12750059" author="herberts" created="Tue, 1 Sep 2009 19:26:05 +0000"  >&lt;p&gt;The row counter (an MR job using TableInputFormat) got N-1 Map Input Records (as reported by Hadoop), so the missing record is either not in the table or lost by TableInputFormat.&lt;/p&gt;</comment>
                            <comment id="12750061" author="streamy" created="Tue, 1 Sep 2009 19:32:11 +0000"  >&lt;p&gt;Where did you determine N from?  Possible two things ended up in the same row, or you have an off-by-one error on the original N?&lt;/p&gt;</comment>
                            <comment id="12750063" author="herberts" created="Tue, 1 Sep 2009 19:38:04 +0000"  >&lt;p&gt;The import job has the following counters:&lt;/p&gt;

&lt;p&gt;Map Input Records: N&lt;br/&gt;
Reduce Input Records: N&lt;/p&gt;

&lt;p&gt;I know all N records have different keys (by construction).&lt;/p&gt;

&lt;p&gt;The counting job (using TableInputFormat) reports:&lt;/p&gt;

&lt;p&gt;Map Input Records: N-1&lt;/p&gt;</comment>
                            <comment id="12750065" author="herberts" created="Tue, 1 Sep 2009 19:46:41 +0000"  >&lt;p&gt;+1 on 1784-v2, fixes the double assignment issues.&lt;/p&gt;</comment>
                            <comment id="12750067" author="stack" created="Tue, 1 Sep 2009 19:56:20 +0000"  >&lt;p&gt;Committed branch and trunk.  Thanks for testing and review Mathias.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12418218" name="1784-v2.patch" size="8393" author="stack" created="Tue, 1 Sep 2009 04:54:40 +0000"/>
                            <attachment id="12417918" name="1784.patch" size="2716" author="stack" created="Thu, 27 Aug 2009 19:28:35 +0000"/>
                            <attachment id="12417480" name="DataLoad.java" size="3936" author="herberts" created="Mon, 24 Aug 2009 14:52:07 +0000"/>
                            <attachment id="12418005" name="HBASE-1784-StoreFileScanner-hack.patch" size="2012" author="apurtell" created="Fri, 28 Aug 2009 16:44:42 +0000"/>
                            <attachment id="12417710" name="HBASE-1784.log" size="57926" author="herberts" created="Wed, 26 Aug 2009 07:11:13 +0000"/>
                            <attachment id="12417824" name="META.log" size="8019" author="herberts" created="Wed, 26 Aug 2009 23:14:26 +0000"/>
                            <attachment id="12418181" name="dbl-assignment-20090831" size="117783" author="herberts" created="Mon, 31 Aug 2009 21:45:02 +0000"/>
                            <attachment id="12417803" name="double-assignment" size="19590" author="herberts" created="Wed, 26 Aug 2009 21:42:53 +0000"/>
                            <attachment id="12418282" name="post-1784v2.log" size="9990" author="herberts" created="Tue, 1 Sep 2009 19:02:04 +0000"/>
                            <attachment id="12417822" name="processSplitRegion-check-regionIsOpening.patch" size="1491" author="herberts" created="Wed, 26 Aug 2009 22:27:43 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>10.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Mon, 24 Aug 2009 14:52:07 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>25978</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 14 weeks, 2 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0hf87:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>99729</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>