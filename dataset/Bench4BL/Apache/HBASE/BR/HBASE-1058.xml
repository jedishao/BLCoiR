<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 02 18:36:32 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-1058/HBASE-1058.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-1058] Prevent runaway compactions</title>
                <link>https://issues.apache.org/jira/browse/HBASE-1058</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;A rabid upload will easily outrun our compaction ability dropping flushes faster than we can compact them up.  Fix.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12410612">HBASE-1058</key>
            <summary>Prevent runaway compactions</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.png">Blocker</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="apurtell">Andrew Purtell</assignee>
                                    <reporter username="stack">stack</reporter>
                        <labels>
                    </labels>
                <created>Fri, 12 Dec 2008 22:01:53 +0000</created>
                <updated>Sun, 13 Sep 2009 22:24:16 +0000</updated>
                            <resolved>Thu, 16 Jul 2009 21:29:09 +0000</resolved>
                                                    <fixVersion>0.20.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>0</watches>
                                                                <comments>
                            <comment id="12664927" author="stack" created="Sun, 18 Jan 2009 05:49:54 +0000"  >&lt;p&gt;You can overrun a cluster by running a PE upload into a small cluster.  Do &apos;./bin/hadoop ......PerformanceEvaluation sequentialWrite 256&apos; ... i.e. try loading 1/4 billion items.  Trying this I produced a region with 1k flush files to compact.  It won&apos;t recover, not inside 1G of heap.   Just OOMEs.&lt;/p&gt;</comment>
                            <comment id="12666003" author="stack" created="Wed, 21 Jan 2009 23:17:45 +0000"  >&lt;p&gt;Need this if we&apos;re to scale and be able to keep taking on writes fast or even just so we look good in single server install.&lt;/p&gt;</comment>
                            <comment id="12697563" author="jdcryans" created="Thu, 9 Apr 2009 17:01:54 +0000"  >&lt;p&gt;I&apos;m using this patch on a cluster of old machines to get the whole wikipedia articles dump in a table. Not using it, I saw a store with 150+ store files in it while it was trying to compact the first 20 or so (which had the machine to start swapping, hell broke loose, etc).&lt;/p&gt;

&lt;p&gt;Basically it does a check after we get the updatesLock write lock to make sure there is not a single store with more than hbase.hregion.memcache.store.maximum files (default is 6) and if there is, sleep 500ms until it&apos;s resolved.&lt;/p&gt;

&lt;p&gt;This patch was made against 0.19 branch.&lt;/p&gt;</comment>
                            <comment id="12697665" author="jdcryans" created="Thu, 9 Apr 2009 21:49:35 +0000"  >&lt;p&gt;With further testing, I see it is better to use the value defined by hbase.hstore.compactionThreshold.&lt;/p&gt;</comment>
                            <comment id="12698144" author="apurtell" created="Sat, 11 Apr 2009 22:41:06 +0000"  >&lt;p&gt;Works fine for me. Commit?&lt;/p&gt;</comment>
                            <comment id="12698152" author="apurtell" created="Sun, 12 Apr 2009 02:04:04 +0000"  >&lt;p&gt;Spoke too soon. In MemcacheFlusher, flushcache comes before compactionRequested. Came across a situation during testing where the regionserver waited indefinitely to flush because no compaction was scheduled.  Attached as v2 is a patch that does the same thing, but also triggers compaction. Testing this now.&lt;/p&gt;</comment>
                            <comment id="12698228" author="jdcryans" created="Sun, 12 Apr 2009 15:32:04 +0000"  >&lt;p&gt;Thanks for trying and improving my patch Andrew. I think your fix should close that hole. &lt;/p&gt;

&lt;p&gt;Just to satisfy my curiosity, are you trying it out with the PE like Stack did or you are uploading something else?&lt;/p&gt;</comment>
                            <comment id="12698261" author="apurtell" created="Sun, 12 Apr 2009 19:34:36 +0000"  >&lt;p&gt;Attached as v3 is a patch that does not wedge under load. This will fail a flush if there are too many store files, and will also request a compaction. Works for me, tested under high stress. &lt;/p&gt;

&lt;p&gt;I&apos;m using my scan-and-heavy-write crawler/document parser scenario. It&apos;s a mixed intensive upload and scanning workload &amp;#8211; very nasty.&lt;/p&gt;</comment>
                            <comment id="12698510" author="stack" created="Mon, 13 Apr 2009 19:26:08 +0000"  >&lt;p&gt;So, this patch schedules a compaction if &amp;gt; threshold files at flush time.  It will not flush until we are below threshold?  So it holds the flush up in memory until compaction completes?  This makes it more likely the &apos;block client writes&apos; gate comes down?    It makes it so our blocking writes gate now comes down not only if we are &amp;gt; flush size times 2 (default) but also if we are &amp;gt; number of files are &amp;gt; threshold?&lt;/p&gt;

&lt;p&gt;Just trying to understand. &lt;/p&gt;

&lt;p&gt;J-D and Andrew, could it be that this patch will make us more stable at the cost of slowing update rate?&lt;/p&gt;</comment>
                            <comment id="12698686" author="apurtell" created="Tue, 14 Apr 2009 07:31:57 +0000"  >&lt;p&gt;v4 patch goes back to holding up the flush rather than failing it (which has bad effects). Does not try to hold up flush indefinitely, either. &lt;/p&gt;

&lt;p&gt;Stack,&lt;/p&gt;

&lt;p&gt;Yes the intent of the patch is to delay flushing if there are too many store files while simultaneously requesting a compaction on the region. Otherwise a backlog of storefiles may build up under very heavy write load such that any HRS that tries to compact will OOME. It does indeed mean the client is more likely to be blocked. &lt;/p&gt;

&lt;p&gt;Right now we tie the determination of what are too many store files to the value of hbase.hstore.compactionThreshold but it can be a separate tunable. &lt;/p&gt;

&lt;p&gt;I only observe this triggered during very heavy write load.&lt;/p&gt;</comment>
                            <comment id="12698691" author="stack" created="Tue, 14 Apr 2009 07:55:22 +0000"  >&lt;p&gt;On v4, is one minute enough Andrew to let a major compaction of many files complete?&lt;/p&gt;

&lt;p&gt;I&apos;d read the following 120 from Configuration (The setting doesn&apos;t have to make it out to hbase*.xml &amp;#8211; its ok if you have to read the src to figure that the setting can be changed IMO).&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+    &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; (count++ &amp;lt; 120) { &lt;span class=&quot;code-comment&quot;&gt;// wait up to 1 minute, max&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Otherwise, I&apos;m good w/ this patch J-D and A.   Its a coarse leash on our tendency to run away regards number of store files; any change is an improvement.  We can improve on this first cut.&lt;/p&gt;</comment>
                            <comment id="12698788" author="jdcryans" created="Tue, 14 Apr 2009 14:36:31 +0000"  >&lt;blockquote&gt;&lt;p&gt;Its a coarse leash on our tendency to run away regards number of store files; Its a coarse leash on our tendency to run away regards number of store files&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree, but IMO it&apos;s already a nice fix wrt the current situation.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;On v4, is one minute enough Andrew to let a major compaction of many files complete? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;In my setup it can take up to 3 minutes for a compaction of 4 files. Maybe we can keep it at 1 min but reset the CompactionLimitThread to 2?&lt;/p&gt;</comment>
                            <comment id="12698923" author="stack" created="Tue, 14 Apr 2009 20:52:55 +0000"  >&lt;p&gt;.bq Maybe we can keep it at 1 min but reset the CompactionLimitThread to 2?&lt;/p&gt;

&lt;p&gt;Do you mean change &apos;hbase.hstore.compactionThreshold&apos; from 3 to 2? (I&apos;m missing something?)&lt;/p&gt;</comment>
                            <comment id="12698947" author="jdcryans" created="Tue, 14 Apr 2009 21:34:27 +0000"  >&lt;p&gt;Stack, I&apos;m really talking about CompactionLimitThread. This is the functionality that limits the number of files that can be compacted at the same time at startup. Playing with this when a region gets a lot of writes, we could lower the compaction time, but it would happen more often. A way to &quot;soften&quot; things up.&lt;/p&gt;</comment>
                            <comment id="12698964" author="stack" created="Tue, 14 Apr 2009 22:14:44 +0000"  >&lt;p&gt;Thanks J-D.  Now I know where you are at  but still don&apos;t understand what &quot;reset the CompactionLimitThread to 2&quot; means; its the thread that slowly ups number of storefiles we compact at a time (Sorry if I&apos;m being dense).&lt;/p&gt;</comment>
                            <comment id="12698987" author="jdcryans" created="Tue, 14 Apr 2009 23:51:12 +0000"  >&lt;p&gt;No problem.&lt;/p&gt;

&lt;p&gt;What I mean is that we should jump start that thread to a defined value.  For example:&lt;/p&gt;

&lt;p&gt;1. While flushing, we see that we have &amp;gt; than 4 stores files so we ask for a compaction and wait for it. The compaction of 4 files took 2 minutes.&lt;/p&gt;

&lt;p&gt;2. While flushing, we see that we have &amp;gt; than 4 stores files so we start the CompactionLimitThread and set it to start on cell 5 of the limitSteps array. We then ask for a compaction and wait for it. The compaction of 2 files took less than 2 minutes.&lt;/p&gt;

&lt;p&gt;Currently we are at 1, I think of something like 2.&lt;/p&gt;</comment>
                            <comment id="12701803" author="stack" created="Thu, 23 Apr 2009 04:45:22 +0000"  >&lt;p&gt;So, idea is to run the CompactionLimitThread on every compaction?  It&apos;ll make it so we never do too many at a time?  Should we redo the patch?&lt;/p&gt;</comment>
                            <comment id="12701808" author="apurtell" created="Thu, 23 Apr 2009 05:22:29 +0000"  >&lt;p&gt;Now I am confused. &lt;/p&gt;</comment>
                            <comment id="12701901" author="jdcryans" created="Thu, 23 Apr 2009 11:27:47 +0000"  >&lt;p&gt;Regards the compactionlimitthread, we can do it in an other jira. I think it would be good to commit v4 with 180 seconds instead of 120 for trunk and branch 0.19.&lt;/p&gt;</comment>
                            <comment id="12701967" author="stack" created="Thu, 23 Apr 2009 15:43:05 +0000"  >&lt;p&gt;I&apos;m good w/ that except that on commit, can the period be read from Configuration rather than hard-coded (The configuration doesn&apos;t have to make it out to hbase-default.xml).&lt;/p&gt;</comment>
                            <comment id="12701988" author="apurtell" created="Thu, 23 Apr 2009 16:41:53 +0000"  >&lt;p&gt;Committed v4 with default of 180 seconds as suggested by J-D and specifiable duration as suggested by stack.&lt;/p&gt;</comment>
                            <comment id="12704078" author="jdcryans" created="Wed, 29 Apr 2009 11:30:09 +0000"  >&lt;p&gt;Andrew, could you commit this also to 0.19 branch? Until trunk is out, this patch is very important for huge imports.&lt;/p&gt;</comment>
                            <comment id="12704126" author="apurtell" created="Wed, 29 Apr 2009 14:12:46 +0000"  >&lt;p&gt;Committed to branch.&lt;/p&gt;</comment>
                            <comment id="12708234" author="apurtell" created="Mon, 11 May 2009 21:35:43 +0000"  >&lt;p&gt;See &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-1394?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&amp;amp;focusedCommentId=12708220#action_12708220&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HBASE-1394?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&amp;amp;focusedCommentId=12708220#action_12708220&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Bring the gate down hard?&lt;/p&gt;</comment>
                            <comment id="12708235" author="stack" created="Mon, 11 May 2009 21:39:33 +0000"  >&lt;p&gt;Should factor in state of compactions?&lt;/p&gt;</comment>
                            <comment id="12708253" author="apurtell" created="Mon, 11 May 2009 22:59:54 +0000"  >&lt;p&gt;hbase-1058-2-v1: Bring the gate down and wait (indefinitely) until no store has too many store files.&lt;/p&gt;</comment>
                            <comment id="12708255" author="apurtell" created="Mon, 11 May 2009 23:10:06 +0000"  >&lt;p&gt;I don&apos;t like this whole approach because it can tie up all IPC handlers, but I&apos;m not sure how to do better. One option is to fail the insert with a TooManyStoreFilesException or something like that and have the client do the waiting, with backoff, etc. However a badly behaved client can still DoS the HRS. Or maybe it is actually fine to let the store files build up and just be smarter during compaction about trying to do too much and blow out heap?&lt;/p&gt;</comment>
                            <comment id="12708540" author="jdcryans" created="Tue, 12 May 2009 18:08:57 +0000"  >&lt;p&gt;+1 on your patch Andrew, but we sure need a better story on the compaction side.&lt;/p&gt;</comment>
                            <comment id="12708583" author="apurtell" created="Tue, 12 May 2009 19:35:34 +0000"  >&lt;p&gt;Committed hbase-1058-2-v1.patch to trunk and 0.19 branch. We can back this out of trunk if &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-1410&quot; title=&quot;compactions are not memory efficient &quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-1410&quot;&gt;&lt;del&gt;HBASE-1410&lt;/del&gt;&lt;/a&gt; or related provides a better solution. &lt;/p&gt;</comment>
                            <comment id="12711410" author="stack" created="Wed, 20 May 2009 23:21:55 +0000"  >&lt;p&gt;I just backed out 1058-2-v1 from 0.19 branch &amp;#8211; &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-1443&quot; title=&quot;Back out  1058-2-v1 from 0.19 branch&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-1443&quot;&gt;&lt;del&gt;HBASE-1443&lt;/del&gt;&lt;/a&gt; &amp;#8211; because of &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-1415&quot; title=&quot;Stuck on memcache flush&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-1415&quot;&gt;&lt;del&gt;HBASE-1415&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="12711416" author="apurtell" created="Wed, 20 May 2009 23:28:11 +0000"  >&lt;p&gt;Up on IRC it was decided to re-resolve this issue as Later. More investigation is needed as to why compactions cannot keep up under high write load. The gating band aid in memcache flusher hurts upload performance.&lt;/p&gt;</comment>
                            <comment id="12732134" author="stack" created="Thu, 16 Jul 2009 20:31:21 +0000"  >&lt;p&gt;We took this patch out of 0.19 but its still in TRUNK.  Reopening to remove this patch from TRUNK.&lt;/p&gt;

&lt;p&gt;What I just saw was on start up of a &amp;gt;1k region cluster, .META. was getting lots of edits as regions were coming on line so it was flushing, flushing, flushing (because we flush region when it hits 32k), but quickly, we got into a state where meta would not flush because too many file on fileystem and we were waiting for the compaction to complete (90seconds).  During this time .META. was taking on no edits because the .META. memstore was &amp;gt; its 32k limit.  Cluster was not opening up regions.&lt;/p&gt;</comment>
                            <comment id="12732137" author="stack" created="Thu, 16 Jul 2009 20:34:16 +0000"  >&lt;p&gt;This patch reverts this stuff in TRUNK.  Testing first before commit.&lt;/p&gt;</comment>
                            <comment id="12732140" author="apurtell" created="Thu, 16 Jul 2009 20:36:56 +0000"  >&lt;p&gt;Or just disable it for catalog tables?&lt;/p&gt;</comment>
                            <comment id="12732153" author="stack" created="Thu, 16 Jul 2009 20:54:39 +0000"  >&lt;p&gt;@Andrew I was going with your comment above &quot;More investigation is needed as to why compactions cannot keep up under high write load.&quot;&lt;/p&gt;

&lt;p&gt;If you want me to make a patch that works just for catalog tables, say Andrew, else I&apos;ll just commit this revert patch.&lt;/p&gt;

&lt;p&gt;I just tested this and at least for this startup of many regions, all runs smoother.  Previous, edits not being able to get into .META. made for double-assignment.  The BaseScanner each time was seeing that server had not been updated so it would assign the region anew.&lt;/p&gt;</comment>
                            <comment id="12732160" author="stack" created="Thu, 16 Jul 2009 21:04:35 +0000"  >&lt;p&gt;JGray argues that we should keep basic functionality.  Says hbase-1618 improved things.   I&apos;ll just have it so doesn&apos;t go into effect if catalog region.&lt;/p&gt;</comment>
                            <comment id="12732168" author="stack" created="Thu, 16 Jul 2009 21:18:35 +0000"  >&lt;p&gt;OK.  How&apos;s this.  Does the revert for meta tables only.  Otherwise, leaves in 1058+1618 jgray fixup.&lt;/p&gt;</comment>
                            <comment id="12732171" author="streamy" created="Thu, 16 Jul 2009 21:22:05 +0000"  >&lt;p&gt;Have not tested, but patch looks good.  +1 for commit.&lt;/p&gt;

&lt;p&gt;We can still debate about whether to get rid of this behavior, but in my testing on my small cluster on trunk, this prevented my cluster from being hosed under heavy stress by bringing up the update wall when we start to overrun compactions (post &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-1618&quot; title=&quot;Investigate further into the MemStoreFlusher StoreFile limit&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-1618&quot;&gt;&lt;del&gt;HBASE-1618&lt;/del&gt;&lt;/a&gt;).&lt;/p&gt;</comment>
                            <comment id="12732173" author="apurtell" created="Thu, 16 Jul 2009 21:23:25 +0000"  >&lt;p&gt;+1 on latest patch. &lt;/p&gt;

&lt;p&gt;Agree, &quot;More investigation is needed as to why compactions cannot keep up under high write load.&quot; This is only a stop gap.&lt;/p&gt;</comment>
                            <comment id="12732179" author="stack" created="Thu, 16 Jul 2009 21:29:09 +0000"  >&lt;p&gt;Committed the selective1058revert under &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-1664&quot; title=&quot;Disable 1058 on catalog tables&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-1664&quot;&gt;&lt;del&gt;HBASE-1664&lt;/del&gt;&lt;/a&gt;.  Closing this issue again.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12413731" name="1058-revert.patch" size="2517" author="stack" created="Thu, 16 Jul 2009 20:34:16 +0000"/>
                            <attachment id="12407827" name="hbase-1058-2-v1.patch" size="1915" author="apurtell" created="Mon, 11 May 2009 22:59:54 +0000"/>
                            <attachment id="12405247" name="hbase-1058-v2.patch" size="1920" author="apurtell" created="Sun, 12 Apr 2009 02:04:04 +0000"/>
                            <attachment id="12405398" name="hbase-1058-v4.patch" size="3231" author="apurtell" created="Tue, 14 Apr 2009 07:31:57 +0000"/>
                            <attachment id="12405083" name="hbase-1058.patch" size="2268" author="jdcryans" created="Thu, 9 Apr 2009 17:01:54 +0000"/>
                            <attachment id="12413739" name="selective1058revert.patch" size="3855" author="stack" created="Thu, 16 Jul 2009 21:18:35 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>6.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 9 Apr 2009 17:01:54 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>25546</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 21 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0hb2n:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>99056</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>