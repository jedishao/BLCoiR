<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 02 18:57:58 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-226/HBASE-226.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-226] [hbase] AlreadyBeingCreatedException (Was: Stuck replay of failed regionserver edits)</title>
                <link>https://issues.apache.org/jira/browse/HBASE-226</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Looking in master for a cluster of ~90 regionservers, the regionserver carrying the ROOT went down (because it hadn&apos;t talked to the master in 30 seconds).&lt;/p&gt;

&lt;p&gt;Master notices the downed regionserver because its lease timesout. It then goes to run the shutdown server sequence only splitting the regionserver&apos;s edit log, it gets stuck trying to split the second of three log files. Eventually, after ~5minutes, the second log split throws:&lt;/p&gt;

&lt;p&gt;34974 2007-11-26 01:21:23,999 WARN  hbase.HMaster - Processing pending operations: ProcessServerShutdown of XX.XX.XX.XX:60020&lt;br/&gt;
  34975 org.apache.hadoop.dfs.AlreadyBeingCreatedException: org.apache.hadoop.dfs.AlreadyBeingCreatedException: failed to create file /hbase/hregion_-1194436719/oldlogfile.log for DFSClient_610028837 on client XX.XX.XX.XX because curren        t leaseholder is trying to recreate file.&lt;br/&gt;
  34976     at org.apache.hadoop.dfs.FSNamesystem.startFileInternal(FSNamesystem.java:848)&lt;br/&gt;
  34977     at org.apache.hadoop.dfs.FSNamesystem.startFile(FSNamesystem.java:804)&lt;br/&gt;
  34978     at org.apache.hadoop.dfs.NameNode.create(NameNode.java:276)&lt;br/&gt;
  34979     at sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)&lt;br/&gt;
  34980     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)&lt;br/&gt;
  34981     at java.lang.reflect.Method.invoke(Method.java:597)&lt;br/&gt;
  34982     at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:379)&lt;br/&gt;
  34983     at org.apache.hadoop.ipc.Server$Handler.run(Server.java:596)&lt;br/&gt;
  34984 &lt;br/&gt;
  34985     at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)&lt;br/&gt;
  34986     at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)&lt;br/&gt;
  34987     at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)&lt;br/&gt;
  34988     at java.lang.reflect.Constructor.newInstance(Constructor.java:513)&lt;br/&gt;
  34989     at org.apache.hadoop.hbase.RemoteExceptionHandler.decodeRemoteException(RemoteExceptionHandler.java:82)&lt;br/&gt;
  34990     at org.apache.hadoop.hbase.HMaster.run(HMaster.java:1094)&lt;/p&gt;

&lt;p&gt;And so on every 5 minutes.&lt;/p&gt;

&lt;p&gt;Because the regionserver that went down had ROOT region, and because we are stuck in this eternal loop, ROOT never gets reallocated.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12383268">HBASE-226</key>
            <summary>[hbase] AlreadyBeingCreatedException (Was: Stuck replay of failed regionserver edits)</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="stack">stack</reporter>
                        <labels>
                    </labels>
                <created>Mon, 26 Nov 2007 21:43:56 +0000</created>
                <updated>Fri, 22 Aug 2008 21:34:56 +0000</updated>
                            <resolved>Mon, 10 Dec 2007 22:36:52 +0000</resolved>
                                                                        <due></due>
                            <votes>0</votes>
                                    <watches>0</watches>
                                                                <comments>
                            <comment id="12545669" author="stack" created="Tue, 27 Nov 2007 01:26:57 +0000"  >&lt;p&gt;Sounds like &lt;a href=&quot;http://www.nabble.com/hdfs-close-of-a-file-failing-t4314401.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.nabble.com/hdfs-close-of-a-file-failing-t4314401.html&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/MAPREDUCE-54&quot; title=&quot;distcp failed due to problem in creating files&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAPREDUCE-54&quot;&gt;&lt;del&gt;HADOOP-2050&lt;/del&gt;&lt;/a&gt;.   In the former case, the directory above was being removed.  That was causing the issue.   I have a log locally.  Its too big to upload.&lt;/p&gt;</comment>
                            <comment id="12545709" author="stack" created="Tue, 27 Nov 2007 04:42:08 +0000"  >&lt;p&gt;Unfortunately, the namenode log has been cleared.  It might have given some clues as to what was happening (It gave clues solving the first of the two similiar issues cited above).&lt;/p&gt;</comment>
                            <comment id="12545720" author="stack" created="Tue, 27 Nov 2007 06:03:36 +0000"  >&lt;p&gt;Turns out the log was still there.  It wasn&apos;t set to DEBUG.  Here&apos;s is what was in it:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2007-11-26 01:16:23,282 WARN  dfs.StateChange - DIR* NameSystem.startFile: failed to create file /hbase/hregion_-1194436719/oldlogfile.log &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; DFSClient_610028837 on client 38.99.77.80 because current leaseholder is trying to recreate file.
..
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Repeated every minute until client goes away.&lt;/p&gt;</comment>
                            <comment id="12546003" author="stack" created="Tue, 27 Nov 2007 21:07:30 +0000"  >&lt;p&gt;Also seeing this compacting:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2007-11-27 04:11:04,173 DEBUG hbase.HStore - started compaction of 4 files in /hbase/compaction.dir/hregion_-1572125711/cookie
2007-11-27 04:11:04,193 DEBUG fs.DFSClient - Failed to connect to /38.99.76.30:50010:java.io.IOException: Got error in response to OP_READ_BLOCK
	at org.apache.hadoop.dfs.DFSClient$BlockReader.newBlockReader(DFSClient.java:753)
	at org.apache.hadoop.dfs.DFSClient$DFSInputStream.blockSeekTo(DFSClient.java:979)
	at org.apache.hadoop.dfs.DFSClient$DFSInputStream.read(DFSClient.java:1075)
	at org.apache.hadoop.dfs.DFSClient$DFSInputStream.read(DFSClient.java:1027)
	at java.io.FilterInputStream.read(FilterInputStream.java:66)
	at java.io.DataInputStream.readByte(DataInputStream.java:248)
	at org.apache.hadoop.hbase.HStoreFile.loadInfo(HStoreFile.java:590)
	at org.apache.hadoop.hbase.HStore.compact(HStore.java:1004)
	at org.apache.hadoop.hbase.HRegion.compactStores(HRegion.java:745)
	at org.apache.hadoop.hbase.HRegion.compactIfNeeded(HRegion.java:704)
	at org.apache.hadoop.hbase.HRegionServer$Compactor.run(HRegionServer.java:378)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Nothing in namenode log about OP_READ_BLOCK complaint or even errors other than a few of these:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2007-11-27 01:29:00,226 WARN  dfs.FSNamesystem - java.io.IOException: Namenode is not expecting an &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; image UPLOAD_START
2007-11-27 01:29:00,496 WARN  dfs.FSNamesystem - java.io.IOException: Namenode is not expecting an &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; image UPLOAD_START
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="12546879" author="stack" created="Thu, 29 Nov 2007 20:48:07 +0000"  >&lt;p&gt;Here is sample with the client and datanode sides of the OP_READ_BLOCK exception&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2007-11-28 21:38:39,391 DEBUG hbase.HStore - started compaction of 3 files in /hbase/compaction.dir/hregion_-1314266377/cookie
2007-11-28 21:38:39,398 DEBUG fs.DFSClient - Failed to connect to /XX.XX.XX.17:50010:java.io.IOException: Got error in response to OP_READ_BLOCK
	at org.apache.hadoop.dfs.DFSClient$BlockReader.newBlockReader(DFSClient.java:753)
	at org.apache.hadoop.dfs.DFSClient$DFSInputStream.blockSeekTo(DFSClient.java:979)
	at org.apache.hadoop.dfs.DFSClient$DFSInputStream.read(DFSClient.java:1075)
	at org.apache.hadoop.dfs.DFSClient$DFSInputStream.read(DFSClient.java:1027)
	at java.io.FilterInputStream.read(FilterInputStream.java:66)
	at java.io.DataInputStream.readByte(DataInputStream.java:248)
	at org.apache.hadoop.hbase.HStoreFile.loadInfo(HStoreFile.java:590)
	at org.apache.hadoop.hbase.HStore.compact(HStore.java:1007)
	at org.apache.hadoop.hbase.HRegion.compactStores(HRegion.java:745)
	at org.apache.hadoop.hbase.HRegion.compactIfNeeded(HRegion.java:704)
	at org.apache.hadoop.hbase.HRegionServer$Compactor.run(HRegionServer.java:378)

2007-11-28 21:38:39,397 WARN  dfs.DataNode - XX.XX.XX.17:50010:Got exception &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; serving blk_5964910456479636527 to /XX.XX.XX.17:
java.io.IOException: Block blk_5964910456479636527 is not valid.
	at org.apache.hadoop.dfs.FSDataset.getBlockFile(FSDataset.java:549)
	at org.apache.hadoop.dfs.FSDataset.getMetaFile(FSDataset.java:466)
	at org.apache.hadoop.dfs.FSDataset.metaFileExists(FSDataset.java:470)
	at org.apache.hadoop.dfs.DataNode$BlockSender.&amp;lt;init&amp;gt;(DataNode.java:1281)
	at org.apache.hadoop.dfs.DataNode$DataXceiver.readBlock(DataNode.java:897)
	at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:849)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:619)

2007-11-28 21:38:39,397 ERROR dfs.DataNode - XX.XX.XX.17:50010:DataXceiver: java.io.IOException: Block blk_5964910456479636527 is not valid.
	at org.apache.hadoop.dfs.FSDataset.getBlockFile(FSDataset.java:549)
	at org.apache.hadoop.dfs.FSDataset.getMetaFile(FSDataset.java:466)
	at org.apache.hadoop.dfs.FSDataset.metaFileExists(FSDataset.java:470)
	at org.apache.hadoop.dfs.DataNode$BlockSender.&amp;lt;init&amp;gt;(DataNode.java:1281)
	at org.apache.hadoop.dfs.DataNode$DataXceiver.readBlock(DataNode.java:897)
	at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:849)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:619)

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="12548457" author="stack" created="Tue, 4 Dec 2007 23:03:03 +0000"  >&lt;p&gt;Redo how compaction dirs are made and cleared.&lt;/p&gt;

&lt;p&gt;Paul Saab ran extra logging on his cluster.  The OP_READ_BLOCK exceptions corresponding to start of the second column family compaction within a compaction session.  The first will have removed the compaction parent directory.  The second will have checked for a parent directory existence with following:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (!fs.mkdirs(curCompactStore)) {
          LOG.warn(&lt;span class=&quot;code-quote&quot;&gt;&quot;Mkdir on &quot;&lt;/span&gt; + curCompactStore.toString() + &lt;span class=&quot;code-quote&quot;&gt;&quot; failed&quot;&lt;/span&gt;);
        }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;... but mayhaps HDFS is getting confused by the remove.&lt;/p&gt;

&lt;p&gt;Patch makes it so less hdfs churn around compactions. &lt;/p&gt;</comment>
                            <comment id="12548560" author="stack" created="Wed, 5 Dec 2007 07:06:31 +0000"  >&lt;p&gt;OP_READ_BLOCK looks like red herring.  Reading code, the datanode that returns the IOE gets put on a deadnodes list and we go retry elsewhere getting block.&lt;/p&gt;

&lt;p&gt;Attached patch is cleanup of compactions... found a few minor issues where we might not close readers on exception... Mostly the code is just cleaner.&lt;/p&gt;</comment>
                            <comment id="12548561" author="stack" created="Wed, 5 Dec 2007 07:12:21 +0000"  >&lt;p&gt;Passes tests locally.&lt;/p&gt;</comment>
                            <comment id="12548674" author="hadoopqa" created="Wed, 5 Dec 2007 14:02:19 +0000"  >&lt;p&gt;+1 overall.  Here are the results of testing the latest attachment &lt;br/&gt;
&lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12371004/compaction.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12371004/compaction.patch&lt;/a&gt;&lt;br/&gt;
against trunk revision r601232.&lt;/p&gt;

&lt;p&gt;    @author +1.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    javadoc +1.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    javac +1.  The applied patch does not generate any new compiler warnings.&lt;/p&gt;

&lt;p&gt;    findbugs +1.  The patch does not introduce any new Findbugs warnings.&lt;/p&gt;

&lt;p&gt;    core tests +1.  The patch passed core unit tests.&lt;/p&gt;

&lt;p&gt;    contrib tests +1.  The patch passed contrib unit tests.&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1271/testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1271/testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1271/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1271/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html&lt;/a&gt;&lt;br/&gt;
Checkstyle results: &lt;a href=&quot;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1271/artifact/trunk/build/test/checkstyle-errors.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1271/artifact/trunk/build/test/checkstyle-errors.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1271/console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1271/console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="12548707" author="stack" created="Wed, 5 Dec 2007 16:09:39 +0000"  >&lt;p&gt;AlreadyBeingCreatedException was seen last night in a Bryan Duxbury upload (Added ABCE to title)&lt;/p&gt;

&lt;p&gt;Committed the compaction.patch as part of &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-325&quot; title=&quot;[hbase] Compaction cleanup; less deleting + prevent possible file leaks&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-325&quot;&gt;&lt;del&gt;HADOOP-2357&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="12549767" author="stack" created="Sun, 9 Dec 2007 00:10:28 +0000"  >&lt;p&gt;Today over on pauls&apos; machines I see this around a new failure to rerun edits:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2007-12-08 00:03:47,981 DEBUG hbase.HLog - Splitting 0 of 11: hdfs:&lt;span class=&quot;code-comment&quot;&gt;//img671:9000/hbase/log_XX.XX.XX.101_1197080909551_60020/hlog.dat.810
&lt;/span&gt;2007-12-08 00:03:48,243 DEBUG hbase.HLog - Creating &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; log file writer &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; path /hbase/hregion_666176028/oldlogfile.log
2007-12-08 00:03:48,249 DEBUG hbase.HLog - Creating &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; log file writer &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; path /hbase/hregion_1820335982/oldlogfile.log
2007-12-08 00:03:48,255 DEBUG hbase.HLog - Creating &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; log file writer &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; path /hbase/hregion_-1253552729/oldlogfile.log
....
2007-12-08 00:03:49,599 DEBUG hbase.HLog - Applied 5500 edits
2007-12-08 00:03:49,612 DEBUG hbase.HLog - Applied 5600 edits
2007-12-08 00:03:49,627 DEBUG hbase.HLog - Creating &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; log file writer &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; path /hbase/hregion_1117809784/oldlogfile.log
2007-12-08 00:03:49,646 DEBUG hbase.HLog - Applied 5700 edits
2007-12-08 00:03:49,660 DEBUG hbase.HLog - Applied 5800 edits
2007-12-08 00:03:49,673 DEBUG hbase.HLog - Applied 5900 edits
....
2007-12-08 00:03:52,035 DEBUG hbase.HLog - Applied 30000 edits
2007-12-08 00:03:52,036 DEBUG hbase.HLog - Applied 30006 total edits
2007-12-08 00:03:52,036 DEBUG hbase.HLog - Splitting 1 of 11: hdfs:&lt;span class=&quot;code-comment&quot;&gt;//img671:9000/hbase/log_XX.XX.XX.101_1197080909551_60020/hlog.dat.811
&lt;/span&gt;2007-12-08 00:03:52,064 DEBUG hbase.HLog - Creating &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; log file writer &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; path /hbase/hregion_1117809784/oldlogfile.log
2007-12-08 00:04:34,397 INFO  hbase.HMaster - HMaster.rootScanner scanning meta region regionname: -ROOT-,,0, startKey: &amp;lt;&amp;gt;, server: XX.XX.XX.103:60020}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and then eventually....&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2007-12-08 00:04:52,069 DEBUG retry.RetryInvocationHandler - Exception &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; invoking create of class org.apache.hadoop.dfs.$Proxy0. Retrying.org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.dfs.AlreadyBeingCreatedException: failed to create file /hbase/hregion_1117809784/oldlogfile.log &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; DFSClient_863222988 on client XX.XX.XX.248 because current leaseholder is trying to recreate file.    at org.apache.hadoop.dfs.FSNamesystem.startFileInternal(FSNamesystem.java:861)    at org.apache.hadoop.dfs.FSNamesystem.startFile(FSNamesystem.java:817)    at org.apache.hadoop.dfs.NameNode.create(NameNode.java:272)    at sun.reflect.GeneratedMethodAccessor22.invoke(Unknown Source)    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)    at java.lang.reflect.Method.invoke(Method.java:597)    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:389)    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:644)    at org.apache.hadoop.ipc.Client.call(Client.java:507)    at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:186)
    at org.apache.hadoop.dfs.$Proxy0.create(Unknown Source)    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)    at java.lang.reflect.Method.invoke(Method.java:597)    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)    at org.apache.hadoop.dfs.$Proxy0.create(Unknown Source)    at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.&amp;lt;init&amp;gt;(DFSClient.java:1424)
    at org.apache.hadoop.dfs.DFSClient.create(DFSClient.java:354)    at org.apache.hadoop.dfs.DistributedFileSystem.create(DistributedFileSystem.java:122)    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:390)

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


&lt;p&gt;There&apos;s a double attempt at creating the file hregion_1117809784/oldlogfile.log.  Should be easy to fix.&lt;/p&gt;</comment>
                            <comment id="12549793" author="jimk" created="Sun, 9 Dec 2007 03:08:25 +0000"  >&lt;p&gt;The only time that a meta scanner should try to recover a log is when the master is starting. &lt;br/&gt;
Since we now have the notion of not assigning any regions other than meta regions (including&lt;br/&gt;
the root region) until they are all on-line, perhaps we could key of that and disable log recovery&lt;br/&gt;
in the meta scanner once we start to assign user regions.&lt;/p&gt;</comment>
                            <comment id="12549812" author="stack" created="Sun, 9 Dec 2007 06:26:42 +0000"  >&lt;p&gt;Sure, but I don&apos;t think that the issue.  Both splitLog invocations are done inside a split log lock  (When the second one ran, there&apos;d be no work for it to do?) and this exception is during ourprocessing of the 2nd of 11 files.&lt;/p&gt;

&lt;p&gt;I can&apos;t see the bug though.  Why would we go about doing a second create of /hbase/hregion_1117809784/oldlogfile.log?  The get from logWriters shouldn&apos;t return null.  The only thing I could think of was that the encoding of the regionname was generating same hash but this is from a Paul run of last night where he first got TRUNK so should have had the bad hashing fix.&lt;/p&gt;
</comment>
                            <comment id="12550191" author="stack" created="Mon, 10 Dec 2007 21:13:55 +0000"  >&lt;p&gt;I wrote a unit test.  That found what the problem was.&lt;/p&gt;

&lt;p&gt;M  src/contrib/hbase/src/java/org/apache/hadoop/hbase/HLog.java&lt;br/&gt;
    (splitLog): The key we were using for logWriters was being changed&lt;br/&gt;
    on us by the call to HStoreKey.getRegionName; use a copy of the&lt;br/&gt;
    region name instead as logWriters key.&lt;br/&gt;
M  src/contrib/hbase/src/test/org/apache/hadoop/hbase/TestHLog.java&lt;br/&gt;
    Refactor.  Added setup and teardown code from testAppend into junit&lt;br/&gt;
    setup and tearDown methods.&lt;br/&gt;
    (testSplit): Run a split w/ multiple log files.&lt;br/&gt;
M  src/contrib/hbase/src/java/org/apache/hadoop/hbase/HStoreFile.java&lt;br/&gt;
M  src/contrib/hbase/src/java/org/apache/hadoop/hbase/HStore.java&lt;br/&gt;
    Add try/finally around open of a Reader file so we close if problem.&lt;/p&gt;</comment>
                            <comment id="12550193" author="stack" created="Mon, 10 Dec 2007 21:15:54 +0000"  >&lt;p&gt;Passes tests locally.  Giving to hudson.&lt;/p&gt;

&lt;p&gt;Also, opened an issue to address the issue Jim raised above where meta scanner should only do log splitting on startup&lt;/p&gt;</comment>
                            <comment id="12550217" author="hadoopqa" created="Mon, 10 Dec 2007 22:33:00 +0000"  >&lt;p&gt;-1 overall.  Here are the results of testing the latest attachment &lt;br/&gt;
&lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12371376/2283.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12371376/2283.patch&lt;/a&gt;&lt;br/&gt;
against trunk revision r603000.&lt;/p&gt;

&lt;p&gt;    @author +1.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    javadoc +1.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    javac +1.  The applied patch does not generate any new compiler warnings.&lt;/p&gt;

&lt;p&gt;    findbugs +1.  The patch does not introduce any new Findbugs warnings.&lt;/p&gt;

&lt;p&gt;    core tests +1.  The patch passed core unit tests.&lt;/p&gt;

&lt;p&gt;    contrib tests -1.  The patch failed contrib unit tests.&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1308/testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1308/testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1308/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1308/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html&lt;/a&gt;&lt;br/&gt;
Checkstyle results: &lt;a href=&quot;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1308/artifact/trunk/build/test/checkstyle-errors.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1308/artifact/trunk/build/test/checkstyle-errors.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1308/console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1308/console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="12550220" author="stack" created="Mon, 10 Dec 2007 22:36:52 +0000"  >&lt;p&gt;Committed (Failure was in the unrelated TSRE).  Resolving.&lt;/p&gt;</comment>
                            <comment id="12550417" author="hudson" created="Tue, 11 Dec 2007 12:09:54 +0000"  >&lt;p&gt;Integrated in Hadoop-Nightly #329 (See &lt;a href=&quot;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/329/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/329/&lt;/a&gt;)&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12371376" name="2283.patch" size="6349" author="stack" created="Mon, 10 Dec 2007 21:14:55 +0000"/>
                            <attachment id="12371375" name="2283.patch" size="6349" author="stack" created="Mon, 10 Dec 2007 21:13:55 +0000"/>
                            <attachment id="12370979" name="OP_READ.patch" size="7797" author="stack" created="Tue, 4 Dec 2007 23:03:02 +0000"/>
                            <attachment id="12371004" name="compaction.patch" size="19727" author="stack" created="Wed, 5 Dec 2007 07:06:31 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>4.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Wed, 5 Dec 2007 14:02:19 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>25010</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0h5mf:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>98173</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>