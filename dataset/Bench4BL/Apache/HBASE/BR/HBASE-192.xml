<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 02 19:39:41 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-192/HBASE-192.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-192] Memcache flush flushing every 60 secs with out considering the max memcache size</title>
                <link>https://issues.apache.org/jira/browse/HBASE-192</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;looks like hbase is flushing all memcache to disk every 60 secs causing a lot of work for the compactor to keep up because column gets its own mapfile and every region is flushed at one time. This could be a vary large number of mapfiles to write if a region server is hosting 100 regions all with milti columns.&lt;/p&gt;

&lt;p&gt;Idea memcache flush&lt;/p&gt;

&lt;p&gt;keep all data in memory until memcache get larger then the conf size with hbase.hregion.memcache.flush.size.&lt;/p&gt;

&lt;p&gt;When we reach this size we should flush the regions that are the largest first stopping once we drop back below the memcache max size maybe 20% below the max. This will to flush only as needed as each flush takes time to compact when compaction runs on a region. while we are flushing a region we should also be blocking new updates from happening on that region so the region server does not get over ran when a high update load hits a region server. By only blocking on the region we are flushing at that time other regions will still be able to do updates this.&lt;/p&gt;

&lt;p&gt;We we still want to use the hbase.regionserver.optionalcacheflushinterval we should set to to run once an hour so something like that so we can recover memory from the memcache on region that do not have a lot updates in memory. But running at the default set now of 60 secs is not so good for the compactor if it has many regions to handle also not good for a scanner to have to scan many small files vs a few larger ones&lt;/p&gt;

&lt;p&gt;Example a compactor may take 15 mins to compact a region in that time we will flush 15 times causeing all other regions to get a new mapfile to compact when it becomes it turn to get compacted if you had many regions getting compacted the last one on the list of say 10 regions would have 10 regions * 15 mins each = 150 mapfiles for each column in the last region written before the compactor can get to it.&lt;/p&gt;
</description>
                <environment></environment>
        <key id="12386339">HBASE-192</key>
            <summary>Memcache flush flushing every 60 secs with out considering the max memcache size</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="viper799">Billy Pearson</reporter>
                        <labels>
                    </labels>
                <created>Wed, 16 Jan 2008 02:26:00 +0000</created>
                <updated>Fri, 22 Aug 2008 21:34:54 +0000</updated>
                            <resolved>Thu, 17 Jan 2008 23:38:43 +0000</resolved>
                                                                        <due></due>
                            <votes>0</votes>
                                    <watches>0</watches>
                                                                <comments>
                            <comment id="12559530" author="stack" created="Wed, 16 Jan 2008 15:23:59 +0000"  >&lt;p&gt;Looks like hbase.hregion.memcache.flush.size is no longer being read/used.  This probably means that the blocking mechanism &amp;#8211; when memcache &amp;gt; twice the limit, we stop taking on updates &amp;#8211; no longer works.  Was useful for when regionservers were on occasion overwhelmed; it gave them a chance to catch their breath.&lt;/p&gt;

&lt;p&gt;From Billy&apos;s description up on IRC, compaction was being overrun by the number of files produced flushing when cluster was under load.  In the past there was an attempt at striking an equilibrium between flush and compaction rates/sizes.&lt;/p&gt;

&lt;p&gt;I&apos;ll take a look at this.&lt;/p&gt;
</comment>
                            <comment id="12559586" author="jimk" created="Wed, 16 Jan 2008 17:02:38 +0000"  >&lt;p&gt;There is an &quot;optional cache flush period&quot; which will cause the cache to be flushed every N seconds provided the memcache is not empty. If it is forcing too many cache flushes, adjusting this parameter may solve the problem.&lt;/p&gt;

&lt;p&gt;The config parameter&apos;s name is &apos;hbase.regionserver.optionalcacheflushinterval&apos; the default interval is 60 seconds. Units are in milliseconds.&lt;/p&gt;</comment>
                            <comment id="12559658" author="viper799" created="Wed, 16 Jan 2008 20:53:09 +0000"  >&lt;p&gt;With out a way to set a limit on memory usage and a limit where we start blocking updates the region server could reach a point where we run out of memory before it could flush the memcache causing a crash on the region server.  I thank we should move back to using hbase.hregion.memcache.block.multiplier and hbase.hregion.memcache.flush.size. Maybe we can have an option flusher the splitter can call to force a flush on a region so the split can do its work.&lt;/p&gt;</comment>
                            <comment id="12559660" author="jimk" created="Wed, 16 Jan 2008 21:01:44 +0000"  >&lt;p&gt;You can configure the memcache flush size by setting the config parameter &quot;hbase.hregion.memcache.flush.size&quot; the default is 64M.&lt;/p&gt;

&lt;p&gt;When a HRegion reaches this threshold, it will call for a cache flush.&lt;/p&gt;

&lt;p&gt;If the cache is flushed, a request is queued to determine if a compaction is necessary.&lt;/p&gt;

&lt;p&gt;If a compaction is done, then a request is queued to determine if the region needs to be split.&lt;/p&gt;</comment>
                            <comment id="12559828" author="stack" created="Thu, 17 Jan 2008 06:35:59 +0000"  >&lt;p&gt;Change the optionalcacheflushinterval default from one minute to 30.   One minute is broke when under any kind of loading.  Gets in the way of flushes triggered by sizes.&lt;/p&gt;

&lt;p&gt;Make &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-69&quot; title=&quot;[hbase] Make cache flush triggering less simplistic&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-69&quot;&gt;&lt;del&gt;HADOOP-2636&lt;/del&gt;&lt;/a&gt; so flushing gets a review.  Doesn&apos;t seem too smart at the moment.&lt;/p&gt;</comment>
                            <comment id="12559829" author="stack" created="Thu, 17 Jan 2008 06:36:14 +0000"  >&lt;p&gt;Passing to hudson.&lt;/p&gt;</comment>
                            <comment id="12560155" author="hadoopqa" created="Thu, 17 Jan 2008 23:29:40 +0000"  >&lt;p&gt;-1 overall.  Here are the results of testing the latest attachment &lt;br/&gt;
&lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12373361/optionalcacheflushinterval.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12373361/optionalcacheflushinterval.patch&lt;/a&gt;&lt;br/&gt;
against trunk revision r612974.&lt;/p&gt;

&lt;p&gt;    @author +1.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    javadoc +1.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    javac +1.  The applied patch does not generate any new compiler warnings.&lt;/p&gt;

&lt;p&gt;    findbugs +1.  The patch does not introduce any new Findbugs warnings.&lt;/p&gt;

&lt;p&gt;    core tests -1.  The patch failed core unit tests.&lt;/p&gt;

&lt;p&gt;    contrib tests +1.  The patch passed contrib unit tests.&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1628/testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1628/testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1628/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1628/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html&lt;/a&gt;&lt;br/&gt;
Checkstyle results: &lt;a href=&quot;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1628/artifact/trunk/build/test/checkstyle-errors.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1628/artifact/trunk/build/test/checkstyle-errors.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1628/console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1628/console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="12560158" author="stack" created="Thu, 17 Jan 2008 23:38:43 +0000"  >&lt;p&gt;Committed (Ignoring failure in core).&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12386455">HBASE-69</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12373361" name="optionalcacheflushinterval.patch" size="1384" author="stack" created="Thu, 17 Jan 2008 06:35:59 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Wed, 16 Jan 2008 15:23:59 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>24976</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 47 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0h5an:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>98120</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>