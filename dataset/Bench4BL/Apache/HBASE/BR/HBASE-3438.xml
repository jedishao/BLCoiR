<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sat Dec 03 16:29:53 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-3438/HBASE-3438.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-3438] Cluster Wide Pauses</title>
                <link>https://issues.apache.org/jira/browse/HBASE-3438</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Under heavy write load the entire cluster seems to pause with all nodes pausing writes/reads for several seconds at a time. This seems to be worse with larger region sizes. One possible explanation is that a single node gets caught/paused/stuck during a split and that all other nodes are waiting on that one node so it looks like a cluster wide pause.&lt;/p&gt;</description>
                <environment>&lt;p&gt;CentOS 5.5, 10 Nodes, 24GB RAM, 4 1TB Disks, 8GB Heap&lt;/p&gt;</environment>
        <key id="12495389">HBASE-3438</key>
            <summary>Cluster Wide Pauses</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="3">Duplicate</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="wav100">Wayne</reporter>
                        <labels>
                    </labels>
                <created>Wed, 12 Jan 2011 17:01:30 +0000</created>
                <updated>Fri, 17 Jun 2011 16:53:37 +0000</updated>
                            <resolved>Fri, 17 Jun 2011 16:53:37 +0000</resolved>
                                    <version>0.89.20100924</version>
                                                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                <comments>
                            <comment id="12980826" author="streamy" created="Wed, 12 Jan 2011 17:42:02 +0000"  >&lt;p&gt;What do the operations coming in to the cluster look like?  Is each client issuing requests that are randomly distributed across all regions/nodes?  If so, then the unavailability of a single region could end up looking like a pause against all your clients (eventually each client piles up waiting on the offline region).  This could be worse if you have larger regions because unavailability could increase and likelihood that a client hits that region increases.&lt;/p&gt;</comment>
                            <comment id="12980854" author="stack" created="Wed, 12 Jan 2011 18:53:13 +0000"  >&lt;p&gt;And Ted, you have one regionserver only?&lt;/p&gt;</comment>
                            <comment id="12980876" author="wav100" created="Wed, 12 Jan 2011 19:35:56 +0000"  >&lt;p&gt;We have 10 nodes with 4 dedicated writers per node. These writers are basically pushing randomly distributed writes via 40 &quot;threads&quot; constantly 7/24. If a single region was locked I guess it could cause a pause for all of them, but it would then would go into a wait state like domino&apos;s one after the other. It appears to occur all at once and reads are blocked as well which is the biggest concern. There are also 40 read workers. It seems a little crazy to me to think 80 threads are all waiting for a single region when there are thousands of regions. A region split only pauses writes anyway correct? Are reads blocked as well?&lt;/p&gt;</comment>
                            <comment id="12980886" author="stack" created="Wed, 12 Jan 2011 19:56:09 +0000"  >&lt;p&gt;Reads would be blocked as well (if region is not online, they have nothing to read).   Tell us more about size of  cells being inserted and if much variance.   I&apos;m trying to reproduce your pause over here on a ten node cluster.  Thanks.&lt;/p&gt;</comment>
                            <comment id="12980891" author="streamy" created="Wed, 12 Jan 2011 20:04:40 +0000"  >&lt;p&gt;Can you add instrumentation in your clients?  Is there something in common with the operation they get blocked on?  (same row or same region, perhaps)&lt;/p&gt;</comment>
                            <comment id="12980901" author="tlipcon" created="Wed, 12 Jan 2011 20:13:01 +0000"  >&lt;p&gt;Another thing to consider is our backoff policy on client retries. If the client hits a NSRE due to split, it might be backing off too quickly to multi-second sleep times?&lt;/p&gt;</comment>
                            <comment id="12980905" author="wav100" created="Wed, 12 Jan 2011 20:22:13 +0000"  >&lt;p&gt;I did not think a region split would block reads, just writes. There is very little variance in the cell size. We have 8 tables 3 of which are used currently each with 4 CFs  and only 1 CF is ever written to for a given row key. For loads the batch mutate is given 10k values at a time which are grouped into a list of row mutations. The # of row mutations in a batch totally varies from a few to thousands. All 10k values are passed to 3 CFs as there are 2 secondary CFs. Below is a typical row key / col / ts / value.&lt;/p&gt;

&lt;p&gt;P.12345_D.1234567890 / P&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/biggrin.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;.1234567800_M.123 / 12948623468629950 / 52.5&lt;/p&gt;</comment>
                            <comment id="12980907" author="streamy" created="Wed, 12 Jan 2011 20:31:21 +0000"  >&lt;p&gt;Sounds like a large number of small, evenly distributed writes.  With every client hitting every region frequently, you can certainly run into an issue where data unavailability of a single region could cause a &quot;global pause&quot;.&lt;/p&gt;

&lt;p&gt;Lots of work has been done to make this faster, and there&apos;s plenty more work that can be done.  Like Todd said, be sure to check on your client retries and you might turn up client-side debugging to look for more signs.  You are also going to pay the cost of a re-lookup in META.  There&apos;s been some discussion for a while about clients more proactively learning of new region locations via ZK (this could also trigger a retry, negating the need to do frequent retries after an NSRE).&lt;/p&gt;</comment>
                            <comment id="12981744" author="wav100" created="Fri, 14 Jan 2011 12:59:48 +0000"  >&lt;p&gt;After 2 more days of testing I think I can confirm your assumption. When we are writing to multiple tables we never see cluster wide pauses. This is a slow split in conjunction with meta table updates that cause all workers to be stuck on a hot region. Why would a split take 10 seconds and what can be done to minimize this pause?&lt;/p&gt;</comment>
                            <comment id="12987593" author="wav100" created="Thu, 27 Jan 2011 14:55:43 +0000"  >&lt;p&gt;We now assume this is caused by a memstore flush. See &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3483&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HBASE-3483&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="12987684" author="tlipcon" created="Thu, 27 Jan 2011 18:12:23 +0000"  >&lt;p&gt;Linking related issue - let&apos;s leave this one open until we verify that &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3483&quot; title=&quot;No soft flush trigger on global memstore limit&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-3483&quot;&gt;&lt;del&gt;HBASE-3483&lt;/del&gt;&lt;/a&gt; was the cause for others and not just me.&lt;/p&gt;</comment>
                            <comment id="12998550" author="stack" created="Wed, 23 Feb 2011 21:18:13 +0000"  >&lt;p&gt;@Wayne Does hbase-3483 fix this issue?&lt;/p&gt;</comment>
                            <comment id="13051192" author="tlipcon" created="Fri, 17 Jun 2011 16:53:37 +0000"  >&lt;p&gt;Resolving this as dup of &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3483&quot; title=&quot;No soft flush trigger on global memstore limit&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-3483&quot;&gt;&lt;del&gt;HBASE-3483&lt;/del&gt;&lt;/a&gt;, I haven&apos;t seen this to be a problem of late.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12496883">HBASE-3483</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Wed, 12 Jan 2011 17:42:02 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>26860</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            5 years, 25 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0hma7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>100872</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>