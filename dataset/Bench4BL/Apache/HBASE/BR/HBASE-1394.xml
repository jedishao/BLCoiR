<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 02 20:17:04 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-1394/HBASE-1394.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-1394] Uploads sometimes fall to 0 requests/second (Binding up on HLog#append?)</title>
                <link>https://issues.apache.org/jira/browse/HBASE-1394</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Trying to figure why rate sometimes goes to zero.&lt;/p&gt;

&lt;p&gt;Studying the reginoserver, HLog#append looks like a possible culprit.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-quote&quot;&gt;&quot;IPC Server handler 7 on 60021&quot;&lt;/span&gt; daemon prio=10 tid=0x000000004057dc00 nid=0x1bc4 in &lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;.wait() [0x0000000043393000..0x0000000043393b80]
   java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.State: WAITING (on object monitor)
        at java.lang.&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;.wait(Native Method)
        at java.lang.&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;.wait(&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;.java:485)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.writeChunk(DFSClient.java:2964)
        - locked &amp;lt;0x00007f9e3e449ff0&amp;gt; (a java.util.LinkedList)
        - locked &amp;lt;0x00007f9e3e449e18&amp;gt; (a org.apache.hadoop.hdfs.DFSClient$DFSOutputStream)
        at org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunk(FSOutputSummer.java:150)
        at org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:100)
        at org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:86)
        - locked &amp;lt;0x00007f9e3e449e18&amp;gt; (a org.apache.hadoop.hdfs.DFSClient$DFSOutputStream)
        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:49)
        at java.io.DataOutputStream.write(DataOutputStream.java:90)
        - locked &amp;lt;0x00007f9e434e5588&amp;gt; (a org.apache.hadoop.fs.FSDataOutputStream)
        at org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:1020)
        - locked &amp;lt;0x00007f9e434e55c0&amp;gt; (a org.apache.hadoop.io.SequenceFile$Writer)
        at org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:984)
        - locked &amp;lt;0x00007f9e434e55c0&amp;gt; (a org.apache.hadoop.io.SequenceFile$Writer)
        at org.apache.hadoop.hbase.regionserver.HLog.doWrite(HLog.java:565)
        at org.apache.hadoop.hbase.regionserver.HLog.append(HLog.java:521)
        - locked &amp;lt;0x00007f9dfa376f70&amp;gt; (a java.lang.&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;)
        at org.apache.hadoop.hbase.regionserver.HRegion.update(HRegion.java:1777)
        at org.apache.hadoop.hbase.regionserver.HRegion.batchUpdate(HRegion.java:1348)
        at org.apache.hadoop.hbase.regionserver.HRegion.batchUpdate(HRegion.java:1289)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.batchUpdates(HRegionServer.java:1727)
        at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:642)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:911)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment></environment>
        <key id="12424950">HBASE-1394</key>
            <summary>Uploads sometimes fall to 0 requests/second (Binding up on HLog#append?)</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.png">Critical</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="stack">stack</reporter>
                        <labels>
                    </labels>
                <created>Fri, 8 May 2009 17:08:14 +0000</created>
                <updated>Thu, 2 May 2013 02:30:56 +0000</updated>
                            <resolved>Sat, 16 May 2009 06:11:55 +0000</resolved>
                                                    <fixVersion>0.20.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>0</watches>
                                                                <comments>
                            <comment id="12707518" author="stack" created="Fri, 8 May 2009 20:35:07 +0000"  >&lt;p&gt;The line we&apos;re blocked on is below:&lt;/p&gt;

&lt;p&gt;20:21 &amp;lt;St^Ack&amp;gt;         // If queue is full, then wait till we can create  enough space&lt;br/&gt;
20:21 &amp;lt;St^Ack&amp;gt;         while (!closed &amp;amp;&amp;amp; dataQueue.size() + ackQueue.size()  &amp;gt; maxPackets) {&lt;br/&gt;
20:21 &amp;lt;St^Ack&amp;gt;           try &lt;/p&gt;
{
20:21 &amp;lt;St^Ack&amp;gt;             dataQueue.wait();
20:21 &amp;lt;St^Ack&amp;gt;           }
&lt;p&gt; catch (InterruptedException  e) &lt;/p&gt;
{
20:21 &amp;lt;St^Ack&amp;gt;           }
&lt;p&gt;20:21 &amp;lt;St^Ack&amp;gt;         }&lt;/p&gt;


&lt;p&gt;Here is note on why we&apos;re blocked &amp;#8211; looks like outstanding packets to send is at a maximum:&lt;/p&gt;

&lt;p&gt;20:26 &amp;lt;St^Ack&amp;gt;    * The client application writes data that is cached internally by&lt;br/&gt;
20:26 &amp;lt;St^Ack&amp;gt;    * this stream. Data is broken up into packets, each packet is&lt;br/&gt;
20:26 &amp;lt;St^Ack&amp;gt;    * typically 64K in size. A packet comprises of chunks. Each chunk&lt;br/&gt;
20:26 &amp;lt;St^Ack&amp;gt;    * is typically 512 bytes and has an associated checksum with it.&lt;br/&gt;
20:26 &amp;lt;St^Ack&amp;gt;    *&lt;br/&gt;
20:26 &amp;lt;St^Ack&amp;gt;    * When a client application fills up the currentPacket, it is&lt;br/&gt;
20:26 &amp;lt;St^Ack&amp;gt;    * enqueued into dataQueue.  The DataStreamer thread picks up&lt;br/&gt;
20:26 &amp;lt;St^Ack&amp;gt;    * packets from the dataQueue, sends it to the first datanode in&lt;br/&gt;
20:26 &amp;lt;St^Ack&amp;gt;    * the pipeline and moves it from the dataQueue to the ackQueue.&lt;br/&gt;
20:26 &amp;lt;St^Ack&amp;gt;    * The ResponseProcessor receives acks from the datanodes. When an&lt;br/&gt;
20:26 &amp;lt;St^Ack&amp;gt;    * successful ack for a packet is received from all datanodes, the&lt;br/&gt;
20:26 &amp;lt;St^Ack&amp;gt;    * ResponseProcessor removes the corresponding packet from the&lt;br/&gt;
20:26 &amp;lt;St^Ack&amp;gt;    * ackQueue.&lt;br/&gt;
20:26 &amp;lt;St^Ack&amp;gt;    *&lt;br/&gt;
20:26 &amp;lt;St^Ack&amp;gt;    * In case of error, all outstanding packets and moved from&lt;br/&gt;
20:26 &amp;lt;St^Ack&amp;gt;    * ackQueue. A new pipeline is setup by eliminating the bad&lt;br/&gt;
20:26 &amp;lt;St^Ack&amp;gt;    * datanode from the original pipeline. The DataStreamer now&lt;br/&gt;
20:26 &amp;lt;St^Ack&amp;gt;    * starts sending packets from the dataQueue.&lt;/p&gt;

&lt;p&gt;Here is maximum outstanding packets:&lt;/p&gt;

&lt;p&gt;20:27 &amp;lt;St^Ack&amp;gt;     private int maxPackets = 80; // each packet 64K, total 5MB&lt;/p&gt;

&lt;p&gt;I looked at the ResponseProcessor and DataStreamer code &amp;#8211; no obvious big stalls/sleeps.&lt;/p&gt;</comment>
                            <comment id="12707615" author="stack" created="Sat, 9 May 2009 06:01:48 +0000"  >&lt;p&gt;This is our main bottleneck inputting to hdfs.  To see for yourself, add logging like below:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Index: src/java/org/apache/hadoop/hbase/regionserver/HLog.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/HLog.java     (revision 773167)
+++ src/java/org/apache/hadoop/hbase/regionserver/HLog.java     (working copy)
@@ -556,11 +556,19 @@
   
   &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; void doWrite(HLogKey logKey, HLogEdit logEdit, &lt;span class=&quot;code-object&quot;&gt;boolean&lt;/span&gt; sync)
   &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
+    &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; now = &lt;span class=&quot;code-object&quot;&gt;System&lt;/span&gt;.currentTimeMillis();
     &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
       &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.writer.append(logKey, logEdit);
       &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (sync || &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.unflushedEntries.incrementAndGet() &amp;gt;= flushlogentries) {
         sync();
       }
+      /* Enable &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; you want to see how sometimes we&apos;re filling the hdfs
+         queue of packets so we stall.  See HBASE-1394.
+      &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; took = &lt;span class=&quot;code-object&quot;&gt;System&lt;/span&gt;.currentTimeMillis() - now;
+      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (LOG.isDebugEnabled() &amp;amp;&amp;amp; took &amp;gt; 1000) {
+        LOG.debug(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.currentThread().getName() + &lt;span class=&quot;code-quote&quot;&gt;&quot; took &quot;&lt;/span&gt; + took + &lt;span class=&quot;code-quote&quot;&gt;&quot;ms to append.&quot;&lt;/span&gt;);
+      }
+      */
     } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (IOException e) {
       LOG.fatal(&lt;span class=&quot;code-quote&quot;&gt;&quot;Could not append. Requesting close of log&quot;&lt;/span&gt;, e);
       requestLogRoll();
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We should work on the writing of files direct to HDFS for hbase to pick up afterward.&lt;/p&gt;</comment>
                            <comment id="12707682" author="stack" created="Sat, 9 May 2009 15:17:58 +0000"  >&lt;p&gt;Here is another permutation on the &quot;HDFS is overwhelmed&quot;:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-quote&quot;&gt;&quot;regionserver/0:0:0:0:0:0:0:0:60021.logRoller&quot;&lt;/span&gt; daemon prio=10 tid=0x00007f3b90248400 nid=0x5dc8 waiting &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; monitor entry [0x0000000041fcd000..0x0000000041fcda80]
   java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.State: BLOCKED (on object monitor)
    at java.lang.&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;.wait(Native Method)
    at java.lang.&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;.wait(&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;.java:485)
    at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.flushInternal(DFSClient.java:3105)
    - locked &amp;lt;0x00007f3bc7716460&amp;gt; (a java.util.LinkedList)
    - locked &amp;lt;0x00007f3bc7716888&amp;gt; (a org.apache.hadoop.hdfs.DFSClient$DFSOutputStream)
    at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.closeInternal(DFSClient.java:3196)
    - locked &amp;lt;0x00007f3bc7716888&amp;gt; (a org.apache.hadoop.hdfs.DFSClient$DFSOutputStream)
    at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.close(DFSClient.java:3145)
    at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:61)
    at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:86)
    at org.apache.hadoop.io.SequenceFile$Writer.close(SequenceFile.java:966)
    - locked &amp;lt;0x00007f3bc7b118b0&amp;gt; (a org.apache.hadoop.io.SequenceFile$Writer)
    at org.apache.hadoop.hbase.regionserver.HLog.cleanupCurrentWriter(HLog.java:369)
    at org.apache.hadoop.hbase.regionserver.HLog.rollWriter(HLog.java:259)
    - locked &amp;lt;0x00007f3b9c8be490&amp;gt; (a java.lang.&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;)
    at org.apache.hadoop.hbase.regionserver.LogRoller.run(LogRoller.java:65)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We&apos;re waiting for all packets to cleared out of the DFSClient queue before closing.  Can take a while.&lt;/p&gt;</comment>
                            <comment id="12707695" author="stack" created="Sat, 9 May 2009 18:18:08 +0000"  >&lt;p&gt;I&apos;d like to highlight how much of a bottleneck this is but I don&apos;t want to add a tax on every HLog.append to do it.  If someone wants to see how this costs, they can apply attached patch.&lt;/p&gt;</comment>
                            <comment id="12707709" author="stack" created="Sat, 9 May 2009 20:54:48 +0000"  >&lt;p&gt;Here is a sample from a recent upload:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2009-05-09 20:50:05,268 [regionserver/0:0:0:0:0:0:0:0:60021.worker] DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Opening region TestTable,0265339432,1241902192276, encoded=904457967
2009-05-09 20:50:05,281 [regionserver/0:0:0:0:0:0:0:0:60021.worker] DEBUG org.apache.hadoop.hbase.regionserver.Store: loaded /hbasetrunk2/TestTable/904457967/info/1846359708595609270.99884447, isReference=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, sequence id=70531758, length=158510337, majorCompaction=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;
2009-05-09 20:50:05,292 [regionserver/0:0:0:0:0:0:0:0:60021.worker] DEBUG org.apache.hadoop.hbase.regionserver.Store: loaded /hbasetrunk2/TestTable/904457967/info/516935528186138325.99884447, isReference=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, sequence id=127140420, length=157409190, majorCompaction=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;
2009-05-09 20:50:05,292 [regionserver/0:0:0:0:0:0:0:0:60021.worker] DEBUG org.apache.hadoop.hbase.regionserver.Store: Loaded 2 file(s) in Store info, max sequence id 127140420
2009-05-09 20:50:05,293 [regionserver/0:0:0:0:0:0:0:0:60021.worker] DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Next sequence id &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0265339432,1241902192276 is 127140421
2009-05-09 20:50:05,294 [regionserver/0:0:0:0:0:0:0:0:60021.worker] INFO org.apache.hadoop.hbase.regionserver.HRegion: region TestTable,0265339432,1241902192276/904457967 available
2009-05-09 20:50:05,294 [regionserver/0:0:0:0:0:0:0:0:60021.worker] DEBUG org.apache.hadoop.hbase.regionserver.CompactSplitThread: Compaction requested &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0265339432,1241902192276/904457967 because: Region open check
2009-05-09 20:50:05,918 [IPC Server handler 8 on 60021] INFO org.apache.hadoop.hbase.regionserver.HLog: IPC Server handler 8 on 60021 TOOK 1386ms to commit 1
2009-05-09 20:50:06,410 [IPC Server handler 9 on 60021] INFO org.apache.hadoop.hbase.regionserver.HLog: IPC Server handler 9 on 60021 TOOK 2140ms to commit 1
2009-05-09 20:50:06,413 [IPC Server handler 5 on 60021] INFO org.apache.hadoop.hbase.regionserver.HLog: IPC Server handler 5 on 60021 TOOK 1774ms to commit 1
2009-05-09 20:50:06,413 [IPC Server handler 6 on 60021] INFO org.apache.hadoop.hbase.regionserver.HLog: IPC Server handler 6 on 60021 TOOK 1882ms to commit 1
2009-05-09 20:50:06,413 [IPC Server handler 1 on 60021] INFO org.apache.hadoop.hbase.regionserver.HLog: IPC Server handler 1 on 60021 TOOK 1882ms to commit 1
2009-05-09 20:50:06,414 [IPC Server handler 7 on 60021] INFO org.apache.hadoop.hbase.regionserver.HLog: IPC Server handler 7 on 60021 TOOK 1883ms to commit 1
2009-05-09 20:50:07,786 [IPC Server handler 8 on 60021] INFO org.apache.hadoop.hbase.regionserver.HLog: IPC Server handler 8 on 60021 TOOK 1371ms to commit 1
2009-05-09 20:50:07,786 [IPC Server handler 1 on 60021] INFO org.apache.hadoop.hbase.regionserver.HLog: IPC Server handler 1 on 60021 TOOK 1372ms to commit 1
2009-05-09 20:50:09,663 [IPC Server handler 7 on 60021] INFO org.apache.hadoop.hbase.regionserver.HLog: IPC Server handler 7 on 60021 TOOK 1129ms to commit 1
2009-05-09 20:50:11,458 [IPC Server handler 6 on 60021] INFO org.apache.hadoop.hbase.regionserver.HLog: IPC Server handler 6 on 60021 TOOK 1192ms to commit 1
2009-05-09 20:50:11,458 [IPC Server handler 0 on 60021] INFO org.apache.hadoop.hbase.regionserver.HLog: IPC Server handler 0 on 60021 TOOK 1068ms to commit 1
2009-05-09 20:50:11,458 [IPC Server handler 9 on 60021] INFO org.apache.hadoop.hbase.regionserver.HLog: IPC Server handler 9 on 60021 TOOK 1177ms to commit 1
2009-05-09 20:50:11,458 [IPC Server handler 7 on 60021] INFO org.apache.hadoop.hbase.regionserver.HLog: IPC Server handler 7 on 60021 TOOK 1184ms to commit 1
2009-05-09 20:50:11,458 [IPC Server handler 8 on 60021] INFO org.apache.hadoop.hbase.regionserver.HLog: IPC Server handler 8 on 60021 TOOK 1177ms to commit 1
2009-05-09 20:50:12,949 [IPC Server handler 6 on 60021] INFO org.apache.hadoop.hbase.regionserver.HLog: IPC Server handler 6 on 60021 TOOK 1061ms to commit 1
...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Thats a single put/append to SequenceFile taking over a second to complete and it goes on for a while.  I had us log at 1000ms so not sure how many are actually going in at this time totally.&lt;/p&gt;</comment>
                            <comment id="12707845" author="stack" created="Sun, 10 May 2009 21:55:45 +0000"  >&lt;p&gt;Looking at log sizes, they are 110MB regularly in this test.  So, there is at least one block transition inline.  That probably costs.  &lt;/p&gt;

&lt;p&gt;So, here are some tests to try using HLog and HDFS only &amp;#8211; can be done without involving regionserver &amp;#8211; just write a harness:&lt;/p&gt;

&lt;p&gt;1. See if writing logs &amp;lt; hdfs block size improves things&lt;br/&gt;
2. In my measurements, it doesn&apos;t seem to be an issue but close of a log file during roll is inline &amp;#8211; all writes stop while the close is being done.  Measure doing close out of band.  Measure too opening the new log file before we need it so that its read to go rather than hold up incoming edits while new file is opened.&lt;/p&gt;

&lt;p&gt;I don&apos;t think the above will address the long time I&apos;m seeing doing appends at certain times during a bulk upload.  Talking with Ryan, maybe we need a pool of loggers where we scatter edits over the pool &amp;#8211; if one is off waiting on a DataStreamer to catchup then that one write will take some time rather than have all blocked.  This&apos;ll make the recovery harder in that we&apos;ll have to sort regionservers&apos;s log pool but we&apos;re going to have to rewrite that anyways.&lt;/p&gt;</comment>
                            <comment id="12707846" author="stack" created="Sun, 10 May 2009 21:58:14 +0000"  >&lt;p&gt;Patch is wrong.  Reverse this line: +      long took = now - System.currentTimeMillis();&lt;/p&gt;</comment>
                            <comment id="12707849" author="stack" created="Sun, 10 May 2009 22:04:22 +0000"  >&lt;p&gt;Made this critical; its critical regards our write performance.&lt;/p&gt;</comment>
                            <comment id="12707899" author="stack" created="Mon, 11 May 2009 06:11:08 +0000"  >&lt;p&gt;From Raghu, write &amp;lt; 3 replicas and it&apos;ll go faster (2?).  Can I tell hdfs NOT to write local.  Ryan says that I can set replication on file after its closed, up it to 3.&lt;/p&gt;</comment>
                            <comment id="12708097" author="stack" created="Mon, 11 May 2009 16:15:40 +0000"  >&lt;p&gt;Bringing into 0.20.0.  It probably won&apos;t be fixed by 0.20.0 release but should be able to make some improvement log writing.&lt;/p&gt;</comment>
                            <comment id="12708137" author="stack" created="Mon, 11 May 2009 18:01:46 +0000"  >&lt;p&gt;Trying to do bulk PE import into  my little 4 node cluster running 3 regionservers and 4 datanodes I&apos;m seeing about 600 rows a second on average.   Thats way too low.&lt;/p&gt;</comment>
                            <comment id="12708220" author="stack" created="Mon, 11 May 2009 21:04:54 +0000"  >&lt;p&gt;TODO: Add disabling the commit log.  Bulk uploaders might appreciate this (Ryan suggestion).&lt;/p&gt;

&lt;p&gt;I tried running without logging to see what would happen.  The behavior of HRegionServer changes drastically from walk-in-the-park dawdling to sprint/rest/sprint/rest, etc.  The rate when its flowing is way up, maybe 10x but then we quickly run up against the flusher; all is paused because memory is saturated and flushing is taking too long. &lt;/p&gt;

&lt;p&gt;It also seems that compactions are overrun &amp;#8211; possibly because of &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-1058&quot; title=&quot;Prevent runaway compactions&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-1058&quot;&gt;&lt;del&gt;HBASE-1058&lt;/del&gt;&lt;/a&gt;:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
[stack@aa0-000-13 ~]$ grep &apos;Too many s&apos; trunk/logs/hbase-stack-regionserver-aa0-000-13.u.powerset.com.log|grep 1242073708841
2009-05-11 20:29:26,931 [regionserver/0:0:0:0:0:0:0:0:60021.cacheFlusher] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0452156162,1242073708841: 5, waiting
2009-05-11 20:29:36,388 [regionserver/0:0:0:0:0:0:0:0:60021.cacheFlusher] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0651581333,1242073708841: 5, waiting
2009-05-11 20:29:41,234 [regionserver/0:0:0:0:0:0:0:0:60021.cacheFlusher] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0452156162,1242073708841: 6, waiting
2009-05-11 20:29:47,664 [regionserver/0:0:0:0:0:0:0:0:60021.cacheFlusher] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0651581333,1242073708841: 6, waiting
2009-05-11 20:29:52,046 [regionserver/0:0:0:0:0:0:0:0:60021.cacheFlusher] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0452156162,1242073708841: 7, waiting
2009-05-11 20:30:11,866 [regionserver/0:0:0:0:0:0:0:0:60021.cacheFlusher] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0452156162,1242073708841: 8, waiting
2009-05-11 20:30:17,860 [regionserver/0:0:0:0:0:0:0:0:60021.cacheFlusher] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0651581333,1242073708841: 7, waiting
2009-05-11 20:30:36,076 [regionserver/0:0:0:0:0:0:0:0:60021.cacheFlusher] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0452156162,1242073708841: 9, waiting
2009-05-11 20:30:55,493 [regionserver/0:0:0:0:0:0:0:0:60021.cacheFlusher] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0651581333,1242073708841: 8, waiting
2009-05-11 20:30:59,658 [IPC Server handler 9 on 60021] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0651581333,1242073708841: 8, waiting
2009-05-11 20:31:11,759 [regionserver/0:0:0:0:0:0:0:0:60021.cacheFlusher] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0452156162,1242073708841: 10, waiting
2009-05-11 20:32:06,046 [regionserver/0:0:0:0:0:0:0:0:60021.cacheFlusher] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0452156162,1242073708841: 11, waiting
2009-05-11 20:32:31,661 [regionserver/0:0:0:0:0:0:0:0:60021.cacheFlusher] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0452156162,1242073708841: 12, waiting
2009-05-11 20:32:51,110 [regionserver/0:0:0:0:0:0:0:0:60021.cacheFlusher] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0452156162,1242073708841: 13, waiting
2009-05-11 20:33:18,677 [regionserver/0:0:0:0:0:0:0:0:60021.cacheFlusher] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0452156162,1242073708841: 14, waiting
2009-05-11 20:33:47,818 [regionserver/0:0:0:0:0:0:0:0:60021.cacheFlusher] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0452156162,1242073708841: 15, waiting
2009-05-11 20:34:01,475 [regionserver/0:0:0:0:0:0:0:0:60021.cacheFlusher] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0651581333,1242073708841: 9, waiting
2009-05-11 20:34:18,966 [regionserver/0:0:0:0:0:0:0:0:60021.cacheFlusher] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0452156162,1242073708841: 16, waiting
2009-05-11 20:34:23,467 [regionserver/0:0:0:0:0:0:0:0:60021.cacheFlusher] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0651581333,1242073708841: 10, waiting
2009-05-11 20:34:46,749 [regionserver/0:0:0:0:0:0:0:0:60021.cacheFlusher] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0452156162,1242073708841: 17, waiting
2009-05-11 20:35:15,330 [regionserver/0:0:0:0:0:0:0:0:60021.cacheFlusher] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0452156162,1242073708841: 18, waiting
2009-05-11 20:35:41,234 [IPC Server handler 7 on 60021] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0452156162,1242073708841: 19, waiting
2009-05-11 20:35:41,671 [regionserver/0:0:0:0:0:0:0:0:60021.cacheFlusher] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0651581333,1242073708841: 11, waiting
2009-05-11 20:36:11,554 [IPC Server handler 0 on 60021] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0651581333,1242073708841: 12, waiting
2009-05-11 20:36:23,204 [regionserver/0:0:0:0:0:0:0:0:60021.cacheFlusher] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0452156162,1242073708841: 20, waiting
2009-05-11 20:37:00,864 [IPC Server handler 4 on 60021] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0651581333,1242073708841: 13, waiting
2009-05-11 20:37:29,841 [regionserver/0:0:0:0:0:0:0:0:60021.cacheFlusher] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0651581333,1242073708841: 14, waiting
2009-05-11 20:37:42,480 [IPC Server handler 5 on 60021] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0651581333,1242073708841: 14, waiting
2009-05-11 20:37:46,068 [IPC Server handler 5 on 60021] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0452156162,1242073708841: 21, waiting
2009-05-11 20:38:09,668 [IPC Server handler 7 on 60021] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0651581333,1242073708841: 15, waiting
2009-05-11 20:38:26,761 [regionserver/0:0:0:0:0:0:0:0:60021.cacheFlusher] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0651581333,1242073708841: 16, waiting
2009-05-11 20:38:38,506 [regionserver/0:0:0:0:0:0:0:0:60021.cacheFlusher] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0452156162,1242073708841: 22, waiting
2009-05-11 20:38:54,858 [regionserver/0:0:0:0:0:0:0:0:60021.cacheFlusher] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0651581333,1242073708841: 17, waiting
2009-05-11 20:39:36,309 [regionserver/0:0:0:0:0:0:0:0:60021.cacheFlusher] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0452156162,1242073708841: 23, waiting
2009-05-11 20:40:30,278 [regionserver/0:0:0:0:0:0:0:0:60021.cacheFlusher] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0452156162,1242073708841: 24, waiting
2009-05-11 20:40:39,268 [IPC Server handler 4 on 60021] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0452156162,1242073708841: 25, waiting
2009-05-11 20:41:12,208 [IPC Server handler 1 on 60021] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0452156162,1242073708841: 26, waiting
2009-05-11 20:41:32,876 [IPC Server handler 1 on 60021] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0452156162,1242073708841: 27, waiting
2009-05-11 20:42:03,696 [IPC Server handler 2 on 60021] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0452156162,1242073708841: 28, waiting
2009-05-11 20:42:35,004 [IPC Server handler 1 on 60021] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0452156162,1242073708841: 29, waiting
2009-05-11 20:43:21,116 [regionserver/0:0:0:0:0:0:0:0:60021.cacheFlusher] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0452156162,1242073708841: 30, waiting
2009-05-11 20:44:01,246 [regionserver/0:0:0:0:0:0:0:0:60021.cacheFlusher] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0452156162,1242073708841: 31, waiting
2009-05-11 20:44:23,626 [regionserver/0:0:0:0:0:0:0:0:60021.cacheFlusher] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0452156162,1242073708841: 32, waiting
2009-05-11 20:44:47,639 [regionserver/0:0:0:0:0:0:0:0:60021.cacheFlusher] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0452156162,1242073708841: 33, waiting
2009-05-11 20:45:29,544 [regionserver/0:0:0:0:0:0:0:0:60021.cacheFlusher] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0452156162,1242073708841: 34, waiting
2009-05-11 20:45:53,016 [regionserver/0:0:0:0:0:0:0:0:60021.cacheFlusher] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0452156162,1242073708841: 35, waiting
2009-05-11 20:46:00,022 [IPC Server handler 1 on 60021] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0452156162,1242073708841: 35, waiting
2009-05-11 20:46:24,341 [regionserver/0:0:0:0:0:0:0:0:60021.cacheFlusher] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0452156162,1242073708841: 36, waiting
2009-05-11 20:47:11,688 [regionserver/0:0:0:0:0:0:0:0:60021.cacheFlusher] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0452156162,1242073708841: 37, waiting
2009-05-11 20:47:33,468 [IPC Server handler 7 on 60021] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0452156162,1242073708841: 37, waiting
2009-05-11 20:48:07,649 [IPC Server handler 4 on 60021] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0452156162,1242073708841: 38, waiting
2009-05-11 20:48:40,982 [regionserver/0:0:0:0:0:0:0:0:60021.cacheFlusher] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0452156162,1242073708841: 39, waiting
2009-05-11 20:49:22,073 [IPC Server handler 8 on 60021] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0452156162,1242073708841: 40, waiting
2009-05-11 20:49:22,081 [regionserver/0:0:0:0:0:0:0:0:60021.cacheFlusher] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0452156162,1242073708841: 40, waiting
2009-05-11 20:49:49,418 [regionserver/0:0:0:0:0:0:0:0:60021.cacheFlusher] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0452156162,1242073708841: 41, waiting
2009-05-11 20:50:27,648 [regionserver/0:0:0:0:0:0:0:0:60021.cacheFlusher] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0452156162,1242073708841: 42, waiting
2009-05-11 20:50:55,221 [regionserver/0:0:0:0:0:0:0:0:60021.cacheFlusher] INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Too many store files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,0452156162,1242073708841: 43, waiting
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;TODO: Make issues for above after its easy to create because commit log can be disabled.&lt;/p&gt;</comment>
                            <comment id="12708270" author="stack" created="Tue, 12 May 2009 00:27:45 +0000"  >&lt;p&gt;Was thinking the log should be compressed.  Now I think otherwise.  If gzip for instance, after the header, data is compressed in &quot;blocks&quot; of 32k (IIRC).  That means that since the gzip block is not tied off properly, we will lose up to 32k edits regardless of whether we hdfs sync.&lt;/p&gt;</comment>
                            <comment id="12708338" author="stack" created="Tue, 12 May 2009 06:10:27 +0000"  >&lt;p&gt;Thinking more, the log split involves reading in a regionservers commit logs and dividing up the edits by region.   I think we can have an HLog that opens a pool of N commit logs now with the logger distributing edits over the pool of log writers.  What it would mean is that edits while written in order, as we spray them across N files, they could be processed out of order as we read them in; e.g. If 3 logs in the pool, let edit A go into file 1, edit B into file 2 and edit C into file 3 and so one.  Since our current basic split process reads a file at a time, we&apos;d read file 1 and find edits A, then D, and so on.  Processing these files, serially, we&apos;d write the region-scoped edits into new SequenceFiles &amp;#8211; not sorted MapFiles or hfiles &amp;#8211; of one per region.   These files are then picked up on region open and the edits inserted into memcache (We read the log on the open of each individual store &amp;#8211; which seems profligate).  The insertion into memcache will put them back into the &apos;right&apos; order &amp;#8211; newer edits will sort before older, etc.  The only place I see there possibly being a problem is if you insert a KV with one value and then afterward insert a  KV with exact same r/c/ts coordinates but with different value.  Its possible that this ordering will not be honored on crash.  Is that OK?&lt;/p&gt;</comment>
                            <comment id="12708616" author="stack" created="Tue, 12 May 2009 20:46:11 +0000"  >&lt;p&gt;Thinking yet more &amp;#8211; with an idea from Ryan &amp;#8211; then I think we can do pooled commit log writing AND have the split run multithreaded.  Here&apos;s how:&lt;/p&gt;

&lt;p&gt;Put up pools of HLog commit log writers.  Say 3 as default.  Round-robin edits.  ADD to the HLogKey a timestamp.  The timestamp will be the time at which we added the edit to the log.  Logs are rotated off as they fill.&lt;/p&gt;

&lt;p&gt;On crash, master starts up as many threads as there are log files up to some limit.  Each thread splits its log writing a new one per Region (though looks like it should do per Store given how stuff is currently processed).  Being multithreaded, the split should run faster.&lt;/p&gt;

&lt;p&gt;When the region comes on line, it&apos;ll have a bunch of files to process &amp;#8211; as many as there were pool writers.  It opens all of them and then applies them to the memcache respecting adding that with the lowest HLogKey timestamp first (Ryan&apos;s idea).&lt;/p&gt;

&lt;p&gt;This should speed our writing and speed recovery on crash.&lt;/p&gt;</comment>
                            <comment id="12708663" author="stack" created="Tue, 12 May 2009 22:35:33 +0000"  >&lt;p&gt;This seems to be main reason logging is slow:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hbase.regionserver.hlog.blocksize&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;1048576&amp;lt;/value&amp;gt;
    &amp;lt;description&amp;gt;Block size &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; HLog files. To minimize potential data loss,
    the size should be (avg key length) * (avg value length) * flushlogentries.
    Default 1MB.
    &amp;lt;/description&amp;gt;
  &amp;lt;/property&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If I put the block size up and up the sequence file buffer size from default 4k to 16k, I no longer see appends taking &amp;gt; 1second.&lt;/p&gt;
</comment>
                            <comment id="12708717" author="stack" created="Wed, 13 May 2009 01:31:27 +0000"  >&lt;p&gt;hbase-1413 removes the 1MB block size. Smaller blocks don&apos;t matter.  We still lose it all.  Removing this restores logging speed in that its no longer common seeing appends take &amp;gt; 1second.&lt;/p&gt;</comment>
                            <comment id="12708762" author="stack" created="Wed, 13 May 2009 05:33:17 +0000"  >&lt;p&gt;So, after remove the 1MB block size, log writing is probably fast enough again.&lt;/p&gt;

&lt;p&gt;I moved the refactoring of log splitting over to hbase-1008.&lt;/p&gt;

&lt;p&gt;That leaves:&lt;/p&gt;

&lt;p&gt;+ Keeping logs under block size so we don&apos;t have edits have to ride over block transition.&lt;br/&gt;
+ Add ability to disable logging so can see what its like running fast&lt;br/&gt;
+ Being clear that commit logs can&apos;t be compressed&lt;/p&gt;

&lt;p&gt;After doing above, I&apos;ll close this issue.&lt;/p&gt;</comment>
                            <comment id="12709000" author="stack" created="Wed, 13 May 2009 16:46:22 +0000"  >&lt;p&gt;Looking at 1008, its multiple writers, single reader thread.  Numbers posted in issue by jdcyans look good.  I&apos;m thinking we should go with that for 0.20.0.  Rewrite of log splitting where we have multiple readers, multiple writers and writers write multiple logs down into Stores can wait.  Meantime, I&apos;ll make the change here in HKeyLog to add write time.   Will also set upper size bound on logs rather than have number of edits since that&apos;ll make splitting less likely OOMEing.&lt;/p&gt;</comment>
                            <comment id="12709228" author="stack" created="Thu, 14 May 2009 00:27:13 +0000"  >&lt;p&gt;Patch that:&lt;/p&gt;

&lt;p&gt;+ removes being able to set compression on commit log&lt;br/&gt;
+ changes the rotation trigger from being # of edits to instead being size-based; we rotate is we exceed 90% of blocksize&lt;br/&gt;
+ added ability to disable logging&lt;br/&gt;
+ Added write time to HLogKey in prep. for when we want to do threaded splitting of logs&lt;/p&gt;

&lt;p&gt;Needs testing.&lt;/p&gt;</comment>
                            <comment id="12710078" author="stack" created="Sat, 16 May 2009 06:11:55 +0000"  >&lt;p&gt;Commited v2 after testing up on our little cluster; logs are rotated more frequently now.  Fixed up logs too a bit so had more info and referred to hlog instead of just &apos;log&apos; so bit clearer what message is about.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12425278">HBASE-1413</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12617562">HBASE-7216</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12408074" name="1394.patch" size="21971" author="stack" created="Thu, 14 May 2009 00:27:13 +0000"/>
                            <attachment id="12407706" name="loghowlongittookappendinghlog.patch" size="935" author="stack" created="Sat, 9 May 2009 18:18:08 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>25734</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 29 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0hczj:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>99366</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>