<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 02 19:31:17 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-1062/HBASE-1062.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-1062] Compactions at (re)start on a large table can overwhelm DFS</title>
                <link>https://issues.apache.org/jira/browse/HBASE-1062</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Given a large table, &amp;gt; 1000 regions for example, if a cluster restart is necessary, the compactions undertaken by the regionservers when the master makes initial region assignments can overwhelm DFS, leading to file errors and data loss. This condition is exacerbated if write load was heavy before restart and so many regions want to split as soon as they are opened. &lt;/p&gt;</description>
                <environment></environment>
        <key id="12410651">HBASE-1062</key>
            <summary>Compactions at (re)start on a large table can overwhelm DFS</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.png">Critical</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="apurtell">Andrew Purtell</assignee>
                                    <reporter username="apurtell">Andrew Purtell</reporter>
                        <labels>
                    </labels>
                <created>Sun, 14 Dec 2008 12:42:15 +0000</created>
                <updated>Tue, 16 Apr 2013 01:19:24 +0000</updated>
                            <resolved>Fri, 26 Dec 2008 23:27:03 +0000</resolved>
                                                    <fixVersion>0.19.0</fixVersion>
                                    <component>regionserver</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>0</watches>
                                                                <comments>
                            <comment id="12656386" author="apurtell" created="Sun, 14 Dec 2008 12:51:29 +0000"  >&lt;p&gt;One way to handle this is to extend the concept of safe mode to the regionservers. They should hold off on compactions and splits for some configurable interval, and then slowly ramp up compactions/splits with randomized hold intervals to avoid thundering herd behavior. &lt;/p&gt;</comment>
                            <comment id="12656387" author="apurtell" created="Sun, 14 Dec 2008 13:14:23 +0000"  >&lt;p&gt;Edited description. 1000 region table is only large, not very large.&lt;/p&gt;</comment>
                            <comment id="12656388" author="apurtell" created="Sun, 14 Dec 2008 13:24:41 +0000"  >&lt;p&gt;Made this a critical issue for 0.20.0, and took ownership. I think I am done with this issue for now &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;, until the patch anyhow. &lt;/p&gt;

&lt;p&gt;Of course the proposed solution will impact cluster (re)start time by lengthening it, perhaps substantially, but I think that is better than data loss. &lt;/p&gt;

&lt;p&gt;A better solution might be to interrogate DFS for its current load before attempting any action that might load it, but definitely the Hadoop filesystem abstraction layer does not support this and the DFS protocols do not either. What do you think about filing an issue about this upstream?&lt;/p&gt;</comment>
                            <comment id="12656475" author="stack" created="Sun, 14 Dec 2008 21:50:57 +0000"  >&lt;p&gt;+1 on it being critical.  Powerset has a cluster of ~5000 regions.  Start-up is a big-bang but steady-state happens eventually.  I haven&apos;t done much study of it.  I can imagine that indeed if cluster went down bad with a few major compactions in the mix, startup could be messy.  How many regionservers Andrew?  And a HRS beside each datanode?  (Our nodes are relatively lightly-loaded &amp;#8211; 50 or so regions in 2G heaps).&lt;/p&gt;</comment>
                            <comment id="12656476" author="stack" created="Sun, 14 Dec 2008 21:55:45 +0000"  >&lt;p&gt;Just saw the &apos;safe mode&apos; for HRSs suggestion.  I like that.  In general, we need to do some work to make it so cluster boots as fast as possible and is immediately useable &amp;#8211; not sort-of online but soaked running major compactions and splits, etc.&lt;/p&gt;</comment>
                            <comment id="12656540" author="apurtell" created="Mon, 15 Dec 2008 03:57:38 +0000"  >&lt;p&gt;Our 25 node cluster layout is:&lt;/p&gt;

&lt;p&gt;1: namenode, datanode&lt;br/&gt;
2: datanode, hmaster, jobtracker&lt;br/&gt;
3-25: datanode, regionserver, tasktracker&lt;/p&gt;

&lt;p&gt;We run datanodes everywhere because each node has 2.5TB of storage that we&apos;d clearly like to include in the DFS volume. &lt;/p&gt;

&lt;p&gt;Tasktrackers do not run on the semi-dedicated namenode node nor the semi-dedicated hmaster node. There is a HRS running alongside every TT. Each TT is configured to allow only four concurrent tasks &amp;#8211; 2 mappers and/or 2 reducers. Some of our tasks can be heavy, running with 1G heap, etc. Especially the document parser really loads CPU, RAM, and DFS while the mappers crunch away. &lt;/p&gt;

&lt;p&gt;Right now our average load is around 50 also.&lt;/p&gt;</comment>
                            <comment id="12657436" author="apurtell" created="Wed, 17 Dec 2008 16:03:32 +0000"  >&lt;p&gt;Attached patch is in testing on my cluster. Seems to work pretty well. May not pass all testsuite tests yet.&lt;/p&gt;</comment>
                            <comment id="12657440" author="apurtell" created="Wed, 17 Dec 2008 16:37:13 +0000"  >&lt;p&gt;Replaced patch with one that has the closing &amp;lt;/description&amp;gt; for the hbase-defaults.xml hunk.&lt;/p&gt;</comment>
                            <comment id="12657610" author="stack" created="Wed, 17 Dec 2008 23:19:41 +0000"  >&lt;p&gt;A few comments on the patch Andrew:&lt;/p&gt;

&lt;p&gt;+ Is it wise postponing memcache flushes?  Even if its only for the 2 minutes of HRS safe mode?  We can take on updates during this time?  If so, could we OOME if rabid uploading afoot?&lt;br/&gt;
+ We schedule compactions on open and on flush.  This would put off the open scheduling for interval of 2 minutes.  If cluster went down ugly, and some regions had References outstanding, then these regions would not be splittable, not until a memcache flush ran; i.e. it took on a bunch of uploads.   Maybe thats OK?&lt;br/&gt;
+ Do we ever break out of this loop:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; ((limit &amp;gt; 0) &amp;amp;&amp;amp; (++count &amp;gt; limit)) {
+          &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
+            &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.sleep(&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.frequency);
+          } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (InterruptedException ex) {
+            &lt;span class=&quot;code-keyword&quot;&gt;continue&lt;/span&gt;;
+          }
+          count = 0;
+        }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Looks like we increment count then set it to zero after sleep. It never progresses?&lt;/p&gt;</comment>
                            <comment id="12657660" author="apurtell" created="Thu, 18 Dec 2008 03:13:32 +0000"  >&lt;p&gt;&amp;gt; Is it wise postponing memcache flushes? &lt;/p&gt;

&lt;p&gt;I thought safe mode should be essentially &quot;don&apos;t touch DFS&quot;. &lt;/p&gt;

&lt;p&gt;&amp;gt; We schedule compactions on open and on flush. This would put off the open scheduling&lt;br/&gt;
&amp;gt; for interval of 2 minutes. If cluster went down ugly, and some regions had References&lt;br/&gt;
&amp;gt; outstanding, then these regions would not be splittable&lt;/p&gt;

&lt;p&gt;Wouldn&apos;t the references be cleared when the deferred compactions finally are allowed to run? Then the split would happen. This is what I observe while testing. &lt;/p&gt;

&lt;p&gt;&amp;gt; Do we ever break out of this loop &lt;span class=&quot;error&quot;&gt;&amp;#91;...&amp;#93;&lt;/span&gt; Looks like we increment count then set it to zero&lt;br/&gt;
&amp;gt; after sleep. It never progresses?&lt;/p&gt;

&lt;p&gt;The code in question just sleeps (once) during the CompactSplitThread main loop if count becomes greater than limit, then count is reset.&lt;/p&gt;

&lt;p&gt;It looks like I still need to be more aggressive with making the compact/split ramp-up a longer slope, at least given our cluster and circumstances. The current patch helps but we can still overwhelm DFS sometimes after a restart. &lt;/p&gt;</comment>
                            <comment id="12658264" author="stack" created="Sat, 20 Dec 2008 05:55:57 +0000"  >&lt;p&gt;Linked to &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-347&quot; title=&quot;DFS read performance suboptimal when client co-located on nodes with data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-347&quot;&gt;&lt;del&gt;HADOOP-4801&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="12658936" author="apurtell" created="Tue, 23 Dec 2008 19:43:23 +0000"  >&lt;p&gt;Updated patch that ramps up compactions/splits more slowly.&lt;/p&gt;</comment>
                            <comment id="12659133" author="apurtell" created="Wed, 24 Dec 2008 18:15:06 +0000"  >&lt;p&gt;The last patch has stabilized restarts on my cluster. Consider for 0.19?&lt;/p&gt;</comment>
                            <comment id="12659136" author="stack" created="Wed, 24 Dec 2008 18:36:51 +0000"  >&lt;p&gt;Bringing in to 0.19.0 so gets reviewed and considered for commit in 0.19.0.&lt;/p&gt;</comment>
                            <comment id="12659147" author="apurtell" created="Wed, 24 Dec 2008 19:17:10 +0000"  >&lt;p&gt;Patch-3 fixes a potential problem where abnormal exit of safe mode thread would prevent memcache flushes and compactions. &lt;/p&gt;</comment>
                            <comment id="12659262" author="stack" created="Fri, 26 Dec 2008 20:19:06 +0000"  >&lt;p&gt;.bq Wouldn&apos;t the references be cleared when the deferred compactions finally are allowed to run? Then the split would happen. This is what I observe while testing.&lt;/p&gt;

&lt;p&gt;Yes.  Since we only put-off the compact-on-open on startup; thereafter compaction-on-open runs on splits, redeploys, etc.  It&apos;ll be fine.&lt;/p&gt;

&lt;p&gt;On this code where you make a thread....&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+      &lt;span class=&quot;code-comment&quot;&gt;// start thread &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; turning off safemode
&lt;/span&gt;+      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (conf.getInt(&lt;span class=&quot;code-quote&quot;&gt;&quot;hbase.regionserver.safemode.period&quot;&lt;/span&gt;, 0) &amp;lt; 1) {
+        safeMode.set(&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;);
+        compactSplitThread.setLimit(-1);
+        LOG.debug(&lt;span class=&quot;code-quote&quot;&gt;&quot;skipping safe mode&quot;&lt;/span&gt;);
+      } &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; {
+        &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; SafemodeThread().start();
+      }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;FYI, we have a bit of a convention regards thread naming and where we start them.   Can you start it in startServiceThreads and name it like the others (if it makes sense) with hrs name as prefix?  Makes it cleaner reading thread dumps figuring which threads are ours and systems.&lt;/p&gt;

&lt;p&gt;Maybe limit should be volatile so changes are seen promptly.&lt;/p&gt;

&lt;p&gt;Won&apos;t below log happen alot when in DEBUG?&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+        LOG.debug(&lt;span class=&quot;code-quote&quot;&gt;&quot;in safe mode, deferring memcache flushes&quot;&lt;/span&gt;);
+        &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.sleep(threadWakeFrequency);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;if safe mode is two minutes and threadWakeFrequency is 10 seconds...&lt;br/&gt;
Perhaps just print entry and exit with log including how long sleep is for... Same for compactions.&lt;/p&gt;

&lt;p&gt;Otherwise patch looks good.  I can try it here if you make a new version to address above.&lt;/p&gt;


</comment>
                            <comment id="12659269" author="apurtell" created="Fri, 26 Dec 2008 20:51:27 +0000"  >&lt;p&gt;Patch -4 addresses stack&apos;s comments.&lt;/p&gt;</comment>
                            <comment id="12659272" author="stack" created="Fri, 26 Dec 2008 21:30:44 +0000"  >&lt;p&gt;Reviewed patch.  Looks good.  Tried it local.  I see it leaving safe mode.  +1 on patch.&lt;/p&gt;</comment>
                            <comment id="12659285" author="apurtell" created="Fri, 26 Dec 2008 23:27:03 +0000"  >&lt;p&gt;Committed. Thanks for the review stack.&lt;/p&gt;</comment>
                            <comment id="12771282" author="stack" created="Thu, 29 Oct 2009 04:30:28 +0000"  >&lt;p&gt;@Andrew In IRC discussions, did we say we could do away with this bandaid now we don&apos;t compact on every open, only if region has references (which is usually well after startup).  I&apos;ve cut this patch from TRUNK.  Let me know if removing it a misstack &lt;span class=&quot;error&quot;&gt;&amp;#91;sic&amp;#93;&lt;/span&gt;.  I did it as part of my hbase-1816 hackup job.&lt;/p&gt;</comment>
                            <comment id="12771312" author="apurtell" created="Thu, 29 Oct 2009 07:16:29 +0000"  >&lt;p&gt;We&apos;re good. +1 No misstack. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12396318" name="1062-1.patch" size="6732" author="apurtell" created="Wed, 17 Dec 2008 16:37:13 +0000"/>
                            <attachment id="12396691" name="1062-2.patch" size="6835" author="apurtell" created="Tue, 23 Dec 2008 19:43:23 +0000"/>
                            <attachment id="12396747" name="1062-3.patch" size="7532" author="apurtell" created="Wed, 24 Dec 2008 19:19:46 +0000"/>
                            <attachment id="12396785" name="1062-4.patch" size="7199" author="apurtell" created="Fri, 26 Dec 2008 20:51:27 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>4.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Sun, 14 Dec 2008 21:50:57 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>25550</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 6 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0hb3j:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>99060</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>