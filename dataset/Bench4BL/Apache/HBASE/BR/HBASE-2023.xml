<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Sat Dec 03 16:49:15 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-2023/HBASE-2023.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-2023] Client sync block can cause 1 thread of a multi-threaded client to block all others</title>
                <link>https://issues.apache.org/jira/browse/HBASE-2023</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Take a highly multithreaded client, processing a few thousand requests a second.  If a table goes offline, one thread will get stuck in &quot;locateRegionInMeta&quot; which is located inside the following sync block:&lt;/p&gt;

&lt;p&gt;        synchronized(userRegionLock)&lt;/p&gt;
{
          return locateRegionInMeta(META_TABLE_NAME, tableName, row, useCache);
        }

&lt;p&gt;So when other threads need to find a region (EVEN IF ITS CACHED!!!) it will encounter this sync and wait. &lt;/p&gt;

&lt;p&gt;This can become an issue on a busy thrift server (where I first noticed the problem), one region offline can prevent access to all other regions!&lt;/p&gt;

&lt;p&gt;Potential solution: narrow this lock, or perhaps just get rid of it completely.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12442153">HBASE-2023</key>
            <summary>Client sync block can cause 1 thread of a multi-threaded client to block all others</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="karthik.ranga">Karthik Ranganathan</assignee>
                                    <reporter username="ryanobjc">ryan rawson</reporter>
                        <labels>
                    </labels>
                <created>Tue, 1 Dec 2009 22:21:04 +0000</created>
                <updated>Fri, 12 Oct 2012 06:14:58 +0000</updated>
                            <resolved>Fri, 12 Mar 2010 07:24:05 +0000</resolved>
                                    <version>0.20.2</version>
                                    <fixVersion>0.20.4</fixVersion>
                    <fixVersion>0.90.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>2</watches>
                                                                <comments>
                            <comment id="12788157" author="jsensarma" created="Wed, 9 Dec 2009 16:55:52 +0000"  >&lt;p&gt;i am looking at some pauses while loading data in and trying to figure out if this is applicable. we have multiple machines loading data - each multithreaded - each thread writing to a different range. all get paused at the same times once in a while. there&apos;s no cpu/io going on the region servers when this happens. (next time i reproduce - i will get a jstack dump on the regionservers).&lt;/p&gt;

&lt;p&gt;can this happen on region splits? (I sure wasn&apos;t going any table offline/online during the test).&lt;/p&gt;</comment>
                            <comment id="12788165" author="jdcryans" created="Wed, 9 Dec 2009 17:10:22 +0000"  >&lt;p&gt;Yes region splits (takes at least 6 seconds in the pre-0.21 architecture) will generate NotServingRegionException from the RS that was holding the parent of the split so if 1 out of 10 threads (so in the same JVM) goes to write to that location then it will block all threads for that time.&lt;/p&gt;</comment>
                            <comment id="12788190" author="jsensarma" created="Wed, 9 Dec 2009 17:57:26 +0000"  >&lt;p&gt;ok - couple of follow on questions:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;would u advise 0.21/trunk for testing instead?&lt;/li&gt;
	&lt;li&gt;we haven&apos;t run ZK on separate nodes yet. i just wanted to confirm whether that could be exacerbating this problem.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12788194" author="jdcryans" created="Wed, 9 Dec 2009 18:09:51 +0000"  >&lt;p&gt;Well the master rewrite code isn&apos;t in 0.21 yet, currently the main advantage in trunk is &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-265&quot; title=&quot;Revisit append&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-265&quot;&gt;&lt;del&gt;HDFS-265&lt;/del&gt;&lt;/a&gt;. WRT ZK, as long as you make sure that the quorum members aren&apos;t IO starved (eg have their own disk) and there&apos;s no swap then you should be good.&lt;/p&gt;</comment>
                            <comment id="12788448" author="jdcryans" created="Thu, 10 Dec 2009 02:11:58 +0000"  >&lt;p&gt;So for this issue I see some kind of trade-off. &lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;If all threads synchronize before the method, stuff in cache won&apos;t be picked up until another thread is done looking for a another row. On the plus side, that thread waiting in line could be needing the new location that will be put in the cache by the thread holding the lock.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;If the synchronize is more narrow eg after looking up the cache, the threads won&apos;t be blocked but some threads looking for a location in .META. could be looking for the same row and yet will all go through that code.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;If no synchronization, it&apos;s like the previous situation but all threads will query .META. around the same time.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I don&apos;t like putting more load on .META. and I don&apos;t like having clients waiting sometimes for nothing.&lt;/p&gt;</comment>
                            <comment id="12788825" author="stack" created="Thu, 10 Dec 2009 18:35:56 +0000"  >&lt;p&gt;Can you add line numbers from code to your comments above so can follow along with your comments please J-D?  Thanks.&lt;/p&gt;
</comment>
                            <comment id="12788834" author="jdcryans" created="Thu, 10 Dec 2009 18:47:17 +0000"  >&lt;p&gt;Ok so in the same order:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;This is the current situation, synchronization at line 613 of HCM.&lt;/li&gt;
	&lt;li&gt;Narrowing down the sync block we could put it at line 637 and cover the rest of the locateRegionInMeta method.&lt;/li&gt;
	&lt;li&gt;Removing the sync means getting rid of synchronized at line 613.&lt;/li&gt;
&lt;/ul&gt;


</comment>
                            <comment id="12806620" author="jdcryans" created="Sat, 30 Jan 2010 03:08:11 +0000"  >&lt;p&gt;Maybe a low hanging fruit would be to narrow the synchronize on the table level:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;synchronized&lt;/span&gt;(getTableLocations(tableName)){
  &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; locateRegionInMeta(META_TABLE_NAME, tableName, row, useCache);
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This way you can even disable a table without stopping all request from coming in.&lt;/p&gt;</comment>
                            <comment id="12806629" author="stack" created="Sat, 30 Jan 2010 04:40:11 +0000"  >&lt;p&gt;I wonder if it&apos;d be possible to do a mock regionserver implemenation and then do a test that had thousands of clients in the one jvm?  The mock would then on a period do a hold on the lookup to locateRegionInMeta.  See how much it effects other threads?&lt;/p&gt;</comment>
                            <comment id="12838137" author="karthik.ranga" created="Thu, 25 Feb 2010 01:32:27 +0000"  >&lt;p&gt;Kannan and I took a look at this issue and came up with yet another possibility in addition to the 3 JD mentioned:&lt;/p&gt;

&lt;p&gt;Move the synchronized block inside the try catch loop just around the getClosestRowBefore() call. This causes each thread to give up the lock before sleeping to retry. This allows other threads to make a call in case one particular region was offline. In addition, if useCache is true, we can look at the cache and return the region right away without ever entering the synchronized section. So the new workflow in  locateRegionInMeta() will look as follows:&lt;/p&gt;

&lt;p&gt;1. If useCache is true and the region is in the cache, return the region. If not, We have to make a remote call. &lt;br/&gt;
2. for the number of retries&lt;br/&gt;
3.   wait for lock&lt;br/&gt;
4.   check cache again (someone could have filled the cache while we were waiting). Return if found.&lt;br/&gt;
5.   make the remote call&lt;br/&gt;
6.   release lock&lt;br/&gt;
7.   return on success, otherwise usual error handling/sleep, goto 2&lt;/p&gt;

&lt;p&gt;I can work on the fix if this sounds good to you guys.&lt;/p&gt;</comment>
                            <comment id="12838145" author="jdcryans" created="Thu, 25 Feb 2010 01:49:48 +0000"  >&lt;p&gt;@Karthik&lt;/p&gt;

&lt;p&gt;That sounds good, only one client hitting the META/ROOT region at a time while not blocking others for seconds.&lt;/p&gt;</comment>
                            <comment id="12844192" author="karthik.ranga" created="Thu, 11 Mar 2010 19:37:11 +0000"  >&lt;p&gt;I have moved the synchronized block inside the try catch loop just around the getClosestRowBefore() call. This causes each thread to give up the lock before sleeping to retry. This allows other threads to make a call in case one particular region was offline. In addition, if useCache is true, we can look at the cache and return the region right away without ever entering the synchronized section. So the new workflow in locateRegionInMeta() will look as follows:&lt;/p&gt;

&lt;p&gt;1. If useCache is true and the region is in the cache, return the region. If not, We have to make a remote call.&lt;br/&gt;
2. for the number of retries&lt;br/&gt;
3. wait for lock&lt;br/&gt;
4. check cache again (someone could have filled the cache while we were waiting). Return if found.&lt;br/&gt;
5. make the remote call&lt;br/&gt;
6. release lock&lt;br/&gt;
7. return on success, otherwise usual error handling/sleep, goto 2&lt;/p&gt;</comment>
                            <comment id="12844194" author="karthik.ranga" created="Thu, 11 Mar 2010 19:39:19 +0000"  >&lt;p&gt;Hey guys,&lt;/p&gt;

&lt;p&gt;I have uploaded the patch, the tests were passing and I did some manual testing as well. Please let me know if you have any comments/suggestions.&lt;/p&gt;

&lt;p&gt;thanks&lt;br/&gt;
Karthik&lt;/p&gt;</comment>
                            <comment id="12844393" author="stack" created="Fri, 12 Mar 2010 07:15:31 +0000"  >&lt;p&gt;Karthik.  Patch looks clean to me.  I tried it here with some small loadings.  Seems fine for variety of smile tests.  I&apos;m going to commit.  If issues, they&apos;ll only show in bigger tests.  We can catch them then.&lt;/p&gt;
</comment>
                            <comment id="12844396" author="stack" created="Fri, 12 Mar 2010 07:24:05 +0000"  >&lt;p&gt;Committed branch and trunk.  Thanks for the patch Karthik (I added you as contributor if you don&apos;t mind).&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12462248">HBASE-2458</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12438529" name="HBASE-2023_0.20.3.patch" size="3713" author="karthik.ranga" created="Thu, 11 Mar 2010 19:37:11 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Wed, 9 Dec 2009 16:55:52 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>26104</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            6 years, 39 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i08svb:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>49277</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>