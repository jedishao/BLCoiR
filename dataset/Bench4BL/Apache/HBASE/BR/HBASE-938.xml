<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 02 20:19:59 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-938/HBASE-938.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-938] major compaction period is not checked periodically</title>
                <link>https://issues.apache.org/jira/browse/HBASE-938</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;The major compaction period, hbase.hregion.majorcompaction, is not checked periodically. Currently, we only request major compaction when the region is open or split at which point we check whether the major compaction period is due.&lt;/p&gt;</description>
                <environment>&lt;p&gt;HBase 0.18 branch (should be RC1) + Hadoop 0.18 branch&lt;/p&gt;</environment>
        <key id="12406772">HBASE-938</key>
            <summary>major compaction period is not checked periodically</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.png">Critical</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="stack">stack</assignee>
                                    <reporter username="rafan">Rong-En Fan</reporter>
                        <labels>
                    </labels>
                <created>Mon, 20 Oct 2008 02:25:15 +0000</created>
                <updated>Mon, 28 Dec 2009 19:24:54 +0000</updated>
                            <resolved>Sat, 15 Nov 2008 00:36:42 +0000</resolved>
                                    <version>0.18.0</version>
                    <version>0.18.1</version>
                                                    <component>regionserver</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>0</watches>
                                                                <comments>
                            <comment id="12640901" author="rafan" created="Mon, 20 Oct 2008 02:56:42 +0000"  >&lt;p&gt;In a ~400 regions cluster, I see 140 major compaction within 40 mins after I started the cluster, and this is still on-going. &lt;/p&gt;</comment>
                            <comment id="12640928" author="rafan" created="Mon, 20 Oct 2008 06:01:33 +0000"  >&lt;p&gt;After few hours, all regions got a major compaction.&lt;/p&gt;</comment>
                            <comment id="12641064" author="stack" created="Mon, 20 Oct 2008 16:42:43 +0000"  >&lt;p&gt;Marking critical fix for 0.19.  Major compactions are expensive, especially in clusters that tend toward the large.  Seems an easy fix persisting last major compaction time so doesn&apos;t happen on every restart.&lt;/p&gt;</comment>
                            <comment id="12641150" author="viper799" created="Mon, 20 Oct 2008 20:51:24 +0000"  >&lt;p&gt;Major compaction check is done in the compaction check so it should be getting checked when there is a memcache flush,open,or a splt.&lt;br/&gt;
There should not be major compaction triggered for all region on a restart that&apos;s not how the code is written to do the major compactions.&lt;/p&gt;

&lt;p&gt;If a table gets no update to trigger a compaction check then the stale (over ttl and max_versions) data never gets removed form the table.&lt;br/&gt;
I have seen this happen on a idle table with no updates over a long time.&lt;/p&gt;

&lt;p&gt;What we should be doing I thank is on the optional flush is queue up a compaction check where there is something to flush or not that way if a major compaction is needed it will run with in the optional flush time setting&lt;br/&gt;
This would allow us to check the hbase.hregion.majorcompaction more periodically on tables with little updates.&lt;br/&gt;
The way the code is now If there is no minor or major compaction needed then it will do do nothing so costing no extra resources to check if a major compaction is needed.&lt;/p&gt;</comment>
                            <comment id="12644445" author="stack" created="Fri, 31 Oct 2008 22:18:28 +0000"  >&lt;p&gt;I need to look into this but I just noticed how Billy made sure we never compact more than often than the major compaction period, even if restarts in between:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
        &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; lowTimestamp = getLowestTimestamp(fs, mapdir);
        lastMajorCompaction = &lt;span class=&quot;code-object&quot;&gt;System&lt;/span&gt;.currentTimeMillis() - lowTimestamp;
        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (lowTimestamp &amp;lt; (&lt;span class=&quot;code-object&quot;&gt;System&lt;/span&gt;.currentTimeMillis() - majorCompactionTime) &amp;amp;&amp;amp;
            lowTimestamp &amp;gt; 0l) {
          &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (LOG.isDebugEnabled()) {
            LOG.debug(&lt;span class=&quot;code-quote&quot;&gt;&quot;Major compaction triggered on store: &quot;&lt;/span&gt; +
              &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.storeNameStr + &lt;span class=&quot;code-quote&quot;&gt;&quot;. Time since last major compaction: &quot;&lt;/span&gt; +
              ((&lt;span class=&quot;code-object&quot;&gt;System&lt;/span&gt;.currentTimeMillis() - lowTimestamp)/1000) + &lt;span class=&quot;code-quote&quot;&gt;&quot; seconds&quot;&lt;/span&gt;);
          }
...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We look at file timestamps and if oldest file was written &amp;gt; major compaction period ago, then we run major compaction next time we check compactions.&lt;/p&gt;

&lt;p&gt;So, for this issue, I need to confirm we only compact on a period and two, since optional flush was removed when we implemented appends, needs to be a thread or something that runs on a period looking to see if any regions in needs of major compaction.&lt;/p&gt;</comment>
                            <comment id="12644483" author="viper799" created="Sat, 1 Nov 2008 03:11:16 +0000"  >&lt;p&gt;Looking at trunk my idea above will not work because we do not have the optional flush any more.&lt;/p&gt;

&lt;p&gt;But yes the major compaction is based off the oldest timestamps from the mapfile&apos;s on the store level.&lt;br/&gt;
So restarts do not interfere with the Major compactions but will queue up over due Major Compactions because of the open.&lt;/p&gt;</comment>
                            <comment id="12646723" author="stack" created="Tue, 11 Nov 2008 23:17:17 +0000"  >&lt;p&gt;Patch to add a thread that checks for major compactions.   Still in need of startup.&lt;/p&gt;</comment>
                            <comment id="12646765" author="stack" created="Wed, 12 Nov 2008 01:10:45 +0000"  >&lt;p&gt;More complete patch.  Testing now.&lt;/p&gt;</comment>
                            <comment id="12646807" author="stack" created="Wed, 12 Nov 2008 05:27:36 +0000"  >&lt;p&gt;Patch seems to be working but it introduces a new issue in that it ensures that we check for major compaction every 24 hours and that a major compation will run every 24 hours, even if no flushes have come through meantime.   We don&apos;t want all hbase data rewritten every 24 hours.&lt;/p&gt;

&lt;p&gt;Will try and fix as part of this issue.&lt;/p&gt;</comment>
                            <comment id="12646809" author="viper799" created="Wed, 12 Nov 2008 06:14:53 +0000"  >&lt;p&gt;default I set my Major Compaction to once a week but on topic we need major compaction to run sometime even if no updates so we will able to remove expired ttl data in a timely fashion. not sure if timely = daily guess that depends on your setup and data.&lt;/p&gt;

&lt;p&gt;Guess we need to decide whats acceptable ttl of expired ttl data.&lt;/p&gt;</comment>
                            <comment id="12646812" author="stack" created="Wed, 12 Nov 2008 06:36:29 +0000"  >&lt;p&gt;As is, if we did a major compaction at the start of the week and then if no updates during the whole week, at the end of the next week, we&apos;ll rewrite an already major compacted file.  I suppose even if no updates, ttls could have expired. Otherwise, Its a waste of CPU and network bandwidth.&lt;/p&gt;

&lt;p&gt;You think we should up the major compaction time default to be a week?&lt;/p&gt;</comment>
                            <comment id="12646820" author="viper799" created="Wed, 12 Nov 2008 07:32:21 +0000"  >&lt;p&gt;On a small cluster with little data it would not not matter so much but once someone got any real amount of data I would thank once a day it way to much for a default.&lt;br/&gt;
So once a week I thank will be more of the norm as data sets get more data.&lt;/p&gt;</comment>
                            <comment id="12646923" author="stack" created="Wed, 12 Nov 2008 15:05:23 +0000"  >&lt;p&gt;I will up the period, make it two days at least.&lt;/p&gt;

&lt;p&gt;But was thinking too that we should mark files that have been major compacted &amp;#8211; write the fact into the files metadata or into the file name &amp;#8211; and before running another, check the column descriptor to see if TTL is forever; if it is, do not run another major compaction if only one file to compact.&lt;/p&gt;</comment>
                            <comment id="12646940" author="viper799" created="Wed, 12 Nov 2008 16:14:22 +0000"  >&lt;p&gt;I like that idea sounds good to me. &lt;br/&gt;
If we can write it in to the meta data then if only one file there and major compaction do as you said above&lt;br/&gt;
that would make the major compaction much smarter and save on cpu and bandwidth.&lt;/p&gt;</comment>
                            <comment id="12647141" author="stack" created="Thu, 13 Nov 2008 00:32:59 +0000"  >&lt;p&gt;Looking at this, best for now is writing the fact that the HStoreFile is result of major compaction into the HSF info file.   When we change file formats, we&apos;ll clean all this up but this should work for now.  Then, yeah, when compacting, if only one file and if we&apos;re doing a major compaction, and if &amp;lt; column family TTL has passed, don&apos;t do a new major compaction.  Should save a bunch of CPU/nio.&lt;/p&gt;</comment>
                            <comment id="12647440" author="stack" created="Thu, 13 Nov 2008 22:37:24 +0000"  >&lt;p&gt;This patch adds whether or not its a major compaction to the info file.  Then, when compacting, if major, will not do another major compaction if last compaction was one (or if time since last major compaction is &amp;lt; ttl).&lt;/p&gt;

&lt;p&gt;Testing now.&lt;/p&gt;</comment>
                            <comment id="12647764" author="stack" created="Sat, 15 Nov 2008 00:29:01 +0000"  >&lt;p&gt;Now you&apos;ll see messages like this if we try to do major compaction on a file that has already been major compacted:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2008-11-15 00:19:30,441 [regionserver/0:0:0:0:0:0:0:0:60020.compactor] DEBUG org.apache.hadoop.hbase.regionserver.HStore: Skipping major compaction because one major compacted file only and elapsedTime 1250119 is &amp;lt; ttl -1
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="12647770" author="stack" created="Sat, 15 Nov 2008 00:36:41 +0000"  >&lt;p&gt;Committed.&lt;/p&gt;</comment>
                            <comment id="12658174" author="stack" created="Fri, 19 Dec 2008 20:15:56 +0000"  >&lt;p&gt;Adding to 0.18.2 because of discussion up on list.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12393902" name="938-v4.patch" size="21334" author="stack" created="Thu, 13 Nov 2008 22:37:24 +0000"/>
                            <attachment id="12393957" name="938-v6.patch" size="21521" author="stack" created="Fri, 14 Nov 2008 20:27:59 +0000"/>
                            <attachment id="12393972" name="938-v7.patch" size="23942" author="stack" created="Sat, 15 Nov 2008 00:29:01 +0000"/>
                            <attachment id="12393752" name="938.patch" size="8729" author="stack" created="Wed, 12 Nov 2008 01:10:45 +0000"/>
                            <attachment id="12393743" name="major.patch" size="3924" author="stack" created="Tue, 11 Nov 2008 23:17:17 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>5.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Mon, 20 Oct 2008 16:42:43 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>25487</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 51 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0hadb:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>98942</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>