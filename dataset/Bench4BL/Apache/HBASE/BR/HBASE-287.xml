<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 02 19:45:17 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-287/HBASE-287.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-287] Code for HBase</title>
                <link>https://issues.apache.org/jira/browse/HBASE-287</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;I&apos;ve written some code for HBase, a BigTable-like file store.  It&apos;s not perfect, but it&apos;s ready for other people to play with and examine.&lt;/p&gt;

&lt;p&gt;The attached tarball has the source and a README&lt;/p&gt;</description>
                <environment>&lt;p&gt;All environments&lt;/p&gt;</environment>
        <key id="12363717">HBASE-287</key>
            <summary>Code for HBase</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="michael_cafarella">Mike Cafarella</reporter>
                        <labels>
                    </labels>
                <created>Tue, 27 Feb 2007 16:51:17 +0000</created>
                <updated>Mon, 4 Feb 2008 18:41:31 +0000</updated>
                            <resolved>Tue, 3 Apr 2007 20:34:55 +0000</resolved>
                                                                        <due>Fri, 30 Mar 2007 07:00:00 +0000</due>
                            <votes>0</votes>
                                    <watches>2</watches>
                                                                <comments>
                            <comment id="12476345" author="jimk" created="Tue, 27 Feb 2007 19:53:54 +0000"  >&lt;p&gt;Problems:&lt;/p&gt;

&lt;p&gt;1. HRegionServer does not implement HRegionServerInterface.openScanner(Text)&lt;/p&gt;

&lt;p&gt;2. HStore needs to pass Configuration object to MapFile.Writer constructor (2 occurrences)&lt;/p&gt;

&lt;p&gt;3. HStore references undefined method MapFile.getClosest(WritableComparable, Writable) (2 occurrences)&lt;/p&gt;

&lt;p&gt;4. HStoreFile needs Configuration object to pass to MapFile.Writer constructor (3 occurrences) - suggest passing Configuration&lt;br/&gt;
    to HStoreFile constructors. (Requires changes to HRegion, HStore and HStoreFile)&lt;/p&gt;</comment>
                            <comment id="12476346" author="jimk" created="Tue, 27 Feb 2007 19:55:57 +0000"  >&lt;p&gt;Patch for problems 2, 4 file 1.&lt;/p&gt;</comment>
                            <comment id="12476348" author="jimk" created="Tue, 27 Feb 2007 19:56:46 +0000"  >&lt;p&gt;Patch for problems 2, 4, file 2&lt;/p&gt;</comment>
                            <comment id="12476349" author="jimk" created="Tue, 27 Feb 2007 19:57:30 +0000"  >&lt;p&gt;Patch for problems 2, 4 file 3.&lt;/p&gt;</comment>
                            <comment id="12476368" author="cutting" created="Tue, 27 Feb 2007 20:33:36 +0000"  >&lt;p&gt;I think until this is more solid we should keep it in the contrib directory.&lt;/p&gt;

&lt;p&gt;Jim, are you willing to convert this to a single patch file that puts this in the contrib directory, complete with a build.xml?  Information on what&apos;s expected from patches is at:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://wiki.apache.org/lucene-hadoop/HowToContribute&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://wiki.apache.org/lucene-hadoop/HowToContribute&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Mike, do you have any unit tests for this?  It&apos;d be nice to start building up a good set.  If Mike doesn&apos;t have any, then perhaps writing a few would be a good way to get to know what&apos;s there.&lt;/p&gt;
</comment>
                            <comment id="12476378" author="jimk" created="Tue, 27 Feb 2007 20:49:17 +0000"  >&lt;p&gt;Yes, I will work on a single patch to put this in contrib.&lt;/p&gt;

&lt;p&gt;Assigning bug to me for now.&lt;/p&gt;</comment>
                            <comment id="12476545" author="jimk" created="Wed, 28 Feb 2007 07:49:13 +0000"  >&lt;p&gt;I assume that before I submit the patch, you want me to complete the client.&lt;/p&gt;

&lt;p&gt;I have reformatted the code per Hadoop standards and have done a lot of work to change Java 2 syntax to use Java 5 generics.&lt;/p&gt;</comment>
                            <comment id="12476651" author="cutting" created="Wed, 28 Feb 2007 17:33:08 +0000"  >&lt;p&gt;&amp;gt; I assume that before I submit the patch, you want me to complete the client.&lt;/p&gt;

&lt;p&gt;Not necessarily.  Revising the patch is a fine development mode for something with only one or two developers, but if you have more collaborators, it will be easier to use subversion.  Committing it to contrib before it&apos;s complete with a clear notice that it&apos;s a work in progress would be acceptable to me.&lt;/p&gt;

&lt;p&gt;Personally I&apos;d prioritize unit tests for the existing code slightly ahead of a complete client implementation.&lt;/p&gt;

&lt;p&gt;However you proceed, it doesn&apos;t hurt to post updated patches frequently.&lt;/p&gt;</comment>
                            <comment id="12476693" author="jimk" created="Wed, 28 Feb 2007 19:50:54 +0000"  >&lt;p&gt;This is a unified patch that adds all the HBase source to contrib/src/hbase&lt;/p&gt;

&lt;p&gt;Please note that this is a work-in-progress, so while it does compile, it is not complete.&lt;br/&gt;
README.txt and build.xml are included&lt;/p&gt;</comment>
                            <comment id="12476713" author="hadoopqa" created="Wed, 28 Feb 2007 21:06:20 +0000"  >&lt;p&gt;-1, because the patch command could not apply the latest attachment &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12352261/hbase-patch.txt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12352261/hbase-patch.txt&lt;/a&gt; as a patch to trunk revision &lt;a href=&quot;http://svn.apache.org/repos/asf/lucene/hadoop/trunk/512944&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://svn.apache.org/repos/asf/lucene/hadoop/trunk/512944&lt;/a&gt;. Please note that this message is automatically generated and may represent a problem with the automation system and not the patch. Results are at &lt;a href=&quot;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="12476716" author="jimk" created="Wed, 28 Feb 2007 21:27:05 +0000"  >&lt;p&gt;Fix path names in patch file so that they are relative instead of absolute.&lt;/p&gt;</comment>
                            <comment id="12476729" author="hadoopqa" created="Wed, 28 Feb 2007 21:54:28 +0000"  >&lt;p&gt;+1, because &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12352270/hbase.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12352270/hbase.patch&lt;/a&gt; applied and successfully tested against trunk revision &lt;a href=&quot;http://svn.apache.org/repos/asf/lucene/hadoop/trunk/512944&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://svn.apache.org/repos/asf/lucene/hadoop/trunk/512944&lt;/a&gt;. Results are at &lt;a href=&quot;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="12476872" author="otis" created="Thu, 1 Mar 2007 09:37:20 +0000"  >&lt;p&gt;Jim,&lt;br/&gt;
You may want to make that contrib/hbase/...  to follow Lucene&apos;s directory structure.  Here is what Lucene&apos;s contrib dir is like:&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;otis@localhost trunk&amp;#93;&lt;/span&gt;$ ls -al contrib/&lt;br/&gt;
total 184&lt;br/&gt;
drwxrwxr-x  22 otis otis 4096 Nov 28 15:44 .&lt;br/&gt;
drwxrwxr-x  10 otis otis 4096 Feb 16 15:03 ..&lt;br/&gt;
drwxrwxr-x   6 otis otis 4096 Feb 21 09:18 analyzers&lt;br/&gt;
drwxrwxr-x   5 otis otis 4096 Mar 16  2006 ant&lt;br/&gt;
drwxrwxr-x   6 otis otis 4096 Feb 16 12:34 benchmark&lt;br/&gt;
&lt;del&gt;rw-rw-r&lt;/del&gt;-   1 otis otis 1347 May 22  2005 contrib-build.xml&lt;br/&gt;
drwxrwxr-x   6 otis otis 4096 Jan  9  2006 db&lt;br/&gt;
drwxrwxr-x   7 otis otis 4096 Dec 17 00:45 gdata-server&lt;br/&gt;
drwxrwxr-x   6 otis otis 4096 May  1  2005 highlighter&lt;br/&gt;
drwxrwxr-x   6 otis otis 4096 Feb 26  2005 javascript&lt;br/&gt;
drwxrwxr-x   6 otis otis 4096 Nov 24 13:34 lucli&lt;br/&gt;
drwxrwxr-x   4 otis otis 4096 May 22  2005 memory&lt;br/&gt;
drwxrwxr-x   4 otis otis 4096 Feb  9  2006 miscellaneous&lt;br/&gt;
drwxrwxr-x   4 otis otis 4096 Mar 16  2006 queries&lt;br/&gt;
drwxrwxr-x   5 otis otis 4096 Jan  9  2006 regex&lt;br/&gt;
drwxrwxr-x   4 otis otis 4096 Nov 28 15:37 similarity&lt;br/&gt;
drwxrwxr-x   7 otis otis 4096 Nov 20 01:32 snowball&lt;br/&gt;
drwxrwxr-x   4 otis otis 4096 Jun 30  2006 spellchecker&lt;br/&gt;
drwxrwxr-x   4 otis otis 4096 Jul 16  2005 surround&lt;br/&gt;
drwxrwxr-x   7 otis otis 4096 Feb 21 09:04 .svn&lt;br/&gt;
drwxrwxr-x   5 otis otis 4096 May  1  2005 swing&lt;br/&gt;
drwxrwxr-x   4 otis otis 4096 May 22  2005 wordnet&lt;br/&gt;
drwxrwxr-x   4 otis otis 4096 Mar 16  2006 xml-query-parser&lt;/p&gt;</comment>
                            <comment id="12477040" author="jimk" created="Thu, 1 Mar 2007 18:15:47 +0000"  >&lt;p&gt;&amp;gt; Otis Gospodnetic &lt;span class=&quot;error&quot;&gt;&amp;#91;01/Mar/07 01:37 AM&amp;#93;&lt;/span&gt; wrote:&lt;br/&gt;
&amp;gt; Jim,&lt;br/&gt;
&amp;gt; You may want to make that contrib/hbase/... to follow Lucene&apos;s directory structure. Here is what Lucene&apos;s contrib dir is like:&lt;br/&gt;
&amp;gt; &lt;br/&gt;
&amp;gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;otis@localhost trunk&amp;#93;&lt;/span&gt;$ ls -al contrib/&lt;br/&gt;
&amp;gt; total 184&lt;br/&gt;
&amp;gt; drwxrwxr-x 22 otis otis 4096 Nov 28 15:44 .&lt;br/&gt;
&amp;gt; drwxrwxr-x 10 otis otis 4096 Feb 16 15:03 ..&lt;br/&gt;
&amp;gt; drwxrwxr-x 6 otis otis 4096 Feb 21 09:18 analyzers&lt;br/&gt;
&amp;gt; drwxrwxr-x 5 otis otis 4096 Mar 16 2006 ant&lt;br/&gt;
&amp;gt; drwxrwxr-x 6 otis otis 4096 Feb 16 12:34 benchmark&lt;br/&gt;
&amp;gt; &lt;del&gt;rw-rw-r&lt;/del&gt;- 1 otis otis 1347 May 22 2005 contrib-build.xml&lt;br/&gt;
&amp;gt; drwxrwxr-x 6 otis otis 4096 Jan 9 2006 db&lt;br/&gt;
&amp;gt; drwxrwxr-x 7 otis otis 4096 Dec 17 00:45 gdata-server&lt;br/&gt;
&amp;gt; drwxrwxr-x 6 otis otis 4096 May 1 2005 highlighter&lt;br/&gt;
&amp;gt; drwxrwxr-x 6 otis otis 4096 Feb 26 2005 javascript&lt;br/&gt;
&amp;gt; drwxrwxr-x 6 otis otis 4096 Nov 24 13:34 lucli&lt;br/&gt;
&amp;gt; drwxrwxr-x 4 otis otis 4096 May 22 2005 memory&lt;br/&gt;
&amp;gt; drwxrwxr-x 4 otis otis 4096 Feb 9 2006 miscellaneous&lt;br/&gt;
&amp;gt; drwxrwxr-x 4 otis otis 4096 Mar 16 2006 queries&lt;br/&gt;
&amp;gt; drwxrwxr-x 5 otis otis 4096 Jan 9 2006 regex&lt;br/&gt;
&amp;gt; drwxrwxr-x 4 otis otis 4096 Nov 28 15:37 similarity&lt;br/&gt;
&amp;gt; drwxrwxr-x 7 otis otis 4096 Nov 20 01:32 snowball&lt;br/&gt;
&amp;gt; drwxrwxr-x 4 otis otis 4096 Jun 30 2006 spellchecker&lt;br/&gt;
&amp;gt; drwxrwxr-x 4 otis otis 4096 Jul 16 2005 surround&lt;br/&gt;
&amp;gt; drwxrwxr-x 7 otis otis 4096 Feb 21 09:04 .svn&lt;br/&gt;
&amp;gt; drwxrwxr-x 5 otis otis 4096 May 1 2005 swing&lt;br/&gt;
&amp;gt; drwxrwxr-x 4 otis otis 4096 May 22 2005 wordnet&lt;br/&gt;
&amp;gt; drwxrwxr-x 4 otis otis 4096 Mar 16 2006 xml-query-parser &lt;/p&gt;

&lt;p&gt;Otis,&lt;/p&gt;

&lt;p&gt;HBase is strictly a hadoop project. Don&apos;t you think it should go in hadoop&apos;s contrib directory?&lt;/p&gt;

&lt;p&gt;hadoop/trunk/src/contrib$ ls -al&lt;br/&gt;
ls -al&lt;br/&gt;
total 24&lt;br/&gt;
drwxr-xr-x   10 jim  jim   340 Feb 28 11:36 .&lt;br/&gt;
drwxr-xr-x   12 jim  jim   408 Feb 28 11:35 ..&lt;br/&gt;
drwxr-xr-x    9 jim  jim   306 Feb 28 11:38 .svn&lt;br/&gt;
drwxr-xr-x    6 jim  jim   204 Feb 28 11:35 abacus&lt;br/&gt;
&lt;del&gt;rw-r&lt;/del&gt;&lt;del&gt;r&lt;/del&gt;-    1 jim  jim  6616 Dec 12 11:36 build-contrib.xml&lt;br/&gt;
&lt;del&gt;rw-r&lt;/del&gt;&lt;del&gt;r&lt;/del&gt;-    1 jim  jim  1264 Jul 18  2006 build.xml&lt;br/&gt;
drwxr-xr-x    5 jim  jim   170 Feb 28 11:35 ec2&lt;br/&gt;
drwxr-xr-x    5 jim  jim   170 Feb 28 11:38 hbase&lt;br/&gt;
drwxr-xr-x    5 jim  jim   170 Feb 28 11:35 streaming&lt;br/&gt;
drwxr-xr-x    5 jim  jim   170 Feb 28 11:35 test&lt;/p&gt;

&lt;p&gt;that is what I intended. Is the path for the patch relative to the repository root, or hadoop&apos;s trunk?&lt;/p&gt;

&lt;p&gt;Once I know how the patch is applied, I&apos;ll fix the paths in the patch file accordingly.&lt;/p&gt;</comment>
                            <comment id="12477049" author="cutting" created="Thu, 1 Mar 2007 18:51:12 +0000"  >&lt;p&gt;&amp;gt; You may want to make that contrib/hbase/... to follow Lucene&apos;s directory structure.&lt;/p&gt;

&lt;p&gt;Hadoop, unlike Lucene, puts contrib sources in contrib/src, so contrib/src/hbase is appropriate.&lt;/p&gt;

&lt;p&gt;&amp;gt; Once I know how the patch is applied, I&apos;ll fix the paths in the patch file accordingly.&lt;/p&gt;

&lt;p&gt;Patches are applied with &apos;patch &lt;del&gt;p 0 &amp;lt; PATCH&apos; while connected to the root of the project (trunk, typically).  Also, no need to remove obsolete patches&lt;/del&gt;-they&apos;re a fine part of history.&lt;/p&gt;</comment>
                            <comment id="12477057" author="jimk" created="Thu, 1 Mar 2007 19:18:48 +0000"  >&lt;p&gt;&amp;gt; Doug Cutting &lt;span class=&quot;error&quot;&gt;&amp;#91;01/Mar/07 10:51 AM&amp;#93;&lt;/span&gt; wrote:&lt;br/&gt;
&amp;gt;&amp;gt; You may want to make that contrib/hbase/... to follow Lucene&apos;s directory structure.&lt;br/&gt;
&amp;gt;&lt;br/&gt;
&amp;gt; Hadoop, unlike Lucene, puts contrib sources in contrib/src, so contrib/src/hbase is appropriate.&lt;/p&gt;

&lt;p&gt;er, don&apos;t you mean src/contrib ?&lt;/p&gt;</comment>
                            <comment id="12477058" author="jimk" created="Thu, 1 Mar 2007 19:19:32 +0000"  >&lt;p&gt;path is src/contrib/hbase&lt;/p&gt;</comment>
                            <comment id="12477064" author="cutting" created="Thu, 1 Mar 2007 19:39:40 +0000"  >&lt;p&gt;&amp;gt; er, don&apos;t you mean src/contrib ?&lt;/p&gt;

&lt;p&gt;Doh!  Yes, that&apos;s what I meant.&lt;/p&gt;</comment>
                            <comment id="12477225" author="jimk" created="Fri, 2 Mar 2007 08:47:15 +0000"  >&lt;p&gt;I&apos;m either missing something or my environment is messed up.&lt;/p&gt;

&lt;p&gt;On test cases, the wiki instructs: &quot;By default, do not let tests write any temporary files to /tmp. Instead, the tests should write to the location specified by the test.build.data system property.&quot;&lt;/p&gt;

&lt;p&gt;However test.build.data is null in the contrib build in my environment. Suggestions?&lt;/p&gt;</comment>
                            <comment id="12478889" author="jimk" created="Wed, 7 Mar 2007 20:45:10 +0000"  >&lt;p&gt;HRegion: remove unnecessary cast&lt;br/&gt;
HRegionServer: add stub method openScanner&lt;br/&gt;
HStoreFile: change constructor so that it creates new Text objects for regionName and colFamily since doing a Text.set gets NullPointerException if the member was never initialized.&lt;/p&gt;</comment>
                            <comment id="12478892" author="jimk" created="Wed, 7 Mar 2007 21:01:47 +0000"  >&lt;p&gt;Latest patch also includes one unit test.&lt;/p&gt;</comment>
                            <comment id="12479653" author="jimk" created="Fri, 9 Mar 2007 17:13:50 +0000"  >&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Fix bug that caused a null pointer exception in HRegion.closeAndMerge&lt;/li&gt;
	&lt;li&gt;Add detail to exception thrown from HRegion.checkRow&lt;/li&gt;
	&lt;li&gt;Change map.seek(); map.next() to map.getClosest() in Hstore.getFull,&lt;br/&gt;
  HStore.get&lt;/li&gt;
	&lt;li&gt;Add constructor and suite() to TestHRegion&lt;/li&gt;
	&lt;li&gt;Removed some constants that are no longer needed&lt;/li&gt;
	&lt;li&gt;Modify MapFile.Reader:&lt;br/&gt;
  o added private int seekInternal(WritableComparable) which is basically&lt;br/&gt;
      the body of public boolean seek(WritableComparable) but returns the&lt;br/&gt;
      integer value of the comparison. This is now called by boolean seek()&lt;br/&gt;
  o added public Writable getClosest(WritableComparable, Writable). Unlike&lt;br/&gt;
     get which only returns a value on an exact match, getClosest returns the&lt;br/&gt;
     value for the key which is closest to the requested key.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12479655" author="jimk" created="Fri, 9 Mar 2007 17:15:48 +0000"  >&lt;p&gt;Latest patch also includes three unit tests, one of which is currently failing and is the subject of current investigation&lt;/p&gt;</comment>
                            <comment id="12479785" author="jimk" created="Sat, 10 Mar 2007 00:57:37 +0000"  >&lt;p&gt;Fixes a nasty bug around compaction. The bug was that records would get written that zero length keys and values. This created widespread unhappiness.&lt;/p&gt;</comment>
                            <comment id="12479929" author="jimk" created="Sun, 11 Mar 2007 17:11:31 +0000"  >&lt;p&gt;TODO: implement some kind of block caching in HRegion. While the DFS isn&apos;t hitting the disk to fetch blocks, HRegion is making IPC calls to DFS (via MapFile). This causes random reads to be &lt;b&gt;very&lt;/b&gt; slow if you do a lot of them.&lt;/p&gt;</comment>
                            <comment id="12479937" author="jimk" created="Sun, 11 Mar 2007 19:01:46 +0000"  >&lt;p&gt;With respect to block caching, it is HStore that &apos;talks&apos; to MapFile. It has no notion of blocks as it just does a seek and a get. It would appear then that what is needed is a MapFile.CachingReader, which in turn would require SequenceFile.CachingReader, since MapFile.Reader merely tells the underlying Sequence file to seek and return the record at the appropriate location. If a SequenceFile.CachingReader could keep some (configurable) number of blocks in memory, it would not have to keep getting them from DFS. This is safe as SequenceFiles are immutable once they are closed after writing.&lt;/p&gt;</comment>
                            <comment id="12480283" author="jimk" created="Tue, 13 Mar 2007 06:10:03 +0000"  >&lt;p&gt;TODO: the current HBase scanner interface allows you to scan through multiple rows for an explicit set of column family members (e.g., contents:firstcolumn and anchor:secondcolumn), but it doesn&apos;t let you iterate over all the members of a column family for a particular row unless you explicitly enumerate them. This is a problem as you may not know apriori the names of all the family members.&lt;/p&gt;

&lt;p&gt;The Bigtable paper states that &quot;For example, we could restrict the scan above to only produce anchors whose columns match the regular expression anchor:&lt;b&gt;.cnn.com&quot; (ignore for the moment that if &quot;anchor:&lt;/b&gt;.cnn.com&quot; were applied as a regular expression, :* means zero or more &apos;:&apos;s and that the &apos;.&apos; between the &apos;*&apos; and &apos;cnn&apos; and between &apos;cnn&apos; and &apos;com&apos; match any character). You should be able to say &apos;anchor:&apos; which means every member of the anchor family or &apos;anchor:anchornum-&lt;span class=&quot;error&quot;&gt;&amp;#91;0-9&amp;#93;&lt;/span&gt;+&apos; which would match every anchor family member that starts with &apos;anchornum-&apos; and then has one or more digits that follow it.&lt;/p&gt;

&lt;p&gt;This was uncovered in unit testing using the tests that were commented out in HRegion.java&lt;/p&gt;</comment>
                            <comment id="12480294" author="jimk" created="Tue, 13 Mar 2007 07:16:48 +0000"  >&lt;p&gt;HRegion.java: remove test code that is now included in unit test.&lt;br/&gt;
HStore.java: add comment where part of the work to enhance scanners needs to happen.&lt;br/&gt;
HStoreKey.java: add toString() method&lt;/p&gt;

&lt;p&gt;TestHRegion.java: added test for scanners, added test to verify that all that was written in the other tests is still there after splitting and merging.&lt;/p&gt;</comment>
                            <comment id="12481083" author="jimk" created="Thu, 15 Mar 2007 09:03:11 +0000"  >&lt;p&gt;Move HTableDescriptor.extractFamily to HStoreKey&lt;/p&gt;

&lt;p&gt;Refactor HMemcacheScanner and HStoreScanner so that they have an abstract base class (HAbstractScanner)&lt;/p&gt;

&lt;p&gt;Add support for column family regex&apos;s and for scanners that iterate over an entire column family&lt;/p&gt;

&lt;p&gt;Update unit tests for changes above. Also add test for fetching an entire column family&lt;/p&gt;

&lt;p&gt;&amp;#8212;&lt;/p&gt;

&lt;p&gt;Unit tests for base functions are pretty much complete.&lt;br/&gt;
For the base functions, still need to look at performance and memory consumption&lt;/p&gt;</comment>
                            <comment id="12481809" author="jimk" created="Sat, 17 Mar 2007 04:04:53 +0000"  >&lt;p&gt;added logging, added compute midKey&lt;/p&gt;</comment>
                            <comment id="12482871" author="jimk" created="Wed, 21 Mar 2007 18:54:08 +0000"  >&lt;p&gt;Starting on client and servers&lt;/p&gt;</comment>
                            <comment id="12485050" author="jimk" created="Thu, 29 Mar 2007 02:39:07 +0000"  >&lt;p&gt;Scanners can now start from a specified row instead of just the beginning of a region (or the memcache). Added test for same.&lt;/p&gt;

&lt;p&gt;Some additional progress on the client-server code, but still more to do.&lt;/p&gt;</comment>
                            <comment id="12486166" author="jimk" created="Mon, 2 Apr 2007 23:15:35 +0000"  >&lt;p&gt;Changes in latest patch:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Scanners can now be started at a specific row and do not have to&lt;br/&gt;
  scan a whole table. &lt;/li&gt;
	&lt;li&gt;The client-server code is now complete but needs to be debugged and&lt;br/&gt;
  tests need to be written for it.&lt;/li&gt;
	&lt;li&gt;There is A Junit test for the base classes that covers most of&lt;br/&gt;
  non-distributed functionality: writing, reading, flushing,&lt;br/&gt;
  log-rolling, and scanning. If the environment variable&lt;br/&gt;
  DEBUGGING=TRUE is set when running the test, it runs a more&lt;br/&gt;
  extensive test that includes writing and reading 10^6^ rows,&lt;br/&gt;
  compaction, splitting and merging. The extensive test is not enabled&lt;br/&gt;
  by default as it takes over 10 minutes to run.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Changes to MapFile.Reader:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Added public method midKey() which returns a key from approximately&lt;br/&gt;
  the middle of the MapFile.&lt;/li&gt;
	&lt;li&gt;Added private method int seekInternal(WritableComparable) whose body&lt;br/&gt;
  is most of what was in the public seek method. The difference is&lt;br/&gt;
  that seekInternal returns an integer value of the comparison.&lt;/li&gt;
	&lt;li&gt;modified public seek method to call seekInternal and do the boolean&lt;br/&gt;
  comparison for exact match.&lt;/li&gt;
	&lt;li&gt;Added public method getClosest which uses seekInternal to find the&lt;br/&gt;
  record whose key is the closest match to the supplied key. (unlike&lt;br/&gt;
  get which requires an exact key match)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;See the Wiki (&lt;a href=&quot;http://wiki.apache.org/lucene-hadoop/Hbase/HbaseArchitecture#status&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://wiki.apache.org/lucene-hadoop/Hbase/HbaseArchitecture#status&lt;/a&gt;) &lt;br/&gt;
for more information about the current project status and todo list.&lt;/p&gt;</comment>
                            <comment id="12486169" author="jimk" created="Mon, 2 Apr 2007 23:17:44 +0000"  >&lt;p&gt;This is HBase revision 0.0.0 &lt;/p&gt;

&lt;p&gt;It is code complete, but the distributed functionality needs debugging and unit tests. However it is a very good starting point for collaboration.&lt;/p&gt;</comment>
                            <comment id="12486172" author="hadoopqa" created="Mon, 2 Apr 2007 23:36:15 +0000"  >&lt;p&gt;-1, because the javadoc tool appears to have generated warning messages when testing the latest attachment &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12354796/hbase.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12354796/hbase.patch&lt;/a&gt; against trunk revision &lt;a href=&quot;http://svn.apache.org/repos/asf/lucene/hadoop/trunk/524929&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://svn.apache.org/repos/asf/lucene/hadoop/trunk/524929&lt;/a&gt;. Please note that this message is automatically generated and may represent a problem with the automation system and not the patch. Results are at &lt;a href=&quot;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="12486183" author="jimk" created="Tue, 3 Apr 2007 00:24:27 +0000"  >&lt;p&gt;Problem with javadoc, ... fixing&lt;/p&gt;</comment>
                            <comment id="12486184" author="jimk" created="Tue, 3 Apr 2007 00:32:28 +0000"  >&lt;p&gt;Fix javadoc problem&lt;/p&gt;</comment>
                            <comment id="12486185" author="jimk" created="Tue, 3 Apr 2007 00:32:52 +0000"  >&lt;p&gt;Fixed javadoc problem&lt;/p&gt;</comment>
                            <comment id="12486197" author="hadoopqa" created="Tue, 3 Apr 2007 01:36:39 +0000"  >&lt;p&gt;+1, because &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12354799/hbase.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12354799/hbase.patch&lt;/a&gt; applied and successfully tested against trunk revision &lt;a href=&quot;http://svn.apache.org/repos/asf/lucene/hadoop/trunk/524929&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://svn.apache.org/repos/asf/lucene/hadoop/trunk/524929&lt;/a&gt;. Results are at &lt;a href=&quot;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="12486486" author="cutting" created="Tue, 3 Apr 2007 20:34:55 +0000"  >&lt;p&gt;I just committed this.  Thanks, Jim!&lt;/p&gt;</comment>
                            <comment id="12486633" author="hadoopqa" created="Wed, 4 Apr 2007 11:26:08 +0000"  >&lt;p&gt;Integrated in Hadoop-Nightly #47 (See &lt;a href=&quot;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/47/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/47/&lt;/a&gt;)&lt;/p&gt;</comment>
                            <comment id="12492284" author="udanax" created="Fri, 27 Apr 2007 12:54:10 +0000"  >&lt;p&gt;I suggest that you check the yahoo research website.  (&lt;a href=&quot;http://research.yahoo.com/project/pig&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://research.yahoo.com/project/pig&lt;/a&gt;)&lt;br/&gt;
It is very impressive......  &lt;/p&gt;

&lt;p&gt;I&apos;ve recently started a similar project to make reports of aggregate statistics.&lt;br/&gt;
but, It is now under pitiful.  &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="12492536" author="udanax" created="Sun, 29 Apr 2007 02:09:18 +0000"  >&lt;p&gt;and,  please check also my &quot;BigTable Simulation Code Source&quot;.&lt;br/&gt;
you can download here.  &lt;a href=&quot;http://www.hadoop.co.kr/moin.cgi/Maplib?action=AttachFile&amp;amp;do=get&amp;amp;target=maplib-0.0.1-alpha.tgz&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.hadoop.co.kr/moin.cgi/Maplib?action=AttachFile&amp;amp;do=get&amp;amp;target=maplib-0.0.1-alpha.tgz&lt;/a&gt;&lt;br/&gt;
it will be used on mapreduce.&lt;/p&gt;

&lt;p&gt;I expect can recording of multiple &amp;lt;K,V&amp;gt; maps from Once-through process to parsing a tab delimited text file.&lt;/p&gt;


&lt;p&gt;                Table t = new Table();&lt;br/&gt;
                t.Create(&quot;webtable&quot;, new String[]&lt;/p&gt;
{&quot;anchor&quot;,&quot;language&quot;}
&lt;p&gt;);&lt;/p&gt;

&lt;p&gt;                //Write a new anchor, language&lt;br/&gt;
                RowMutation r1 = new RowMutation(t.OpenOrDie(&quot;webtable&quot;), &quot;udanax.org&quot;);&lt;/p&gt;

&lt;p&gt;                        r1.Set(&quot;anchor:hadoop.co.kr&quot;,&quot;hadoop korean user group&quot;);&lt;br/&gt;
                        r1.Set(&quot;anchor:joinc.co.kr&quot;,&quot;joinc&quot;);&lt;br/&gt;
                        r1.Set(&quot;language:kr&quot;, &quot;euc-kr&quot;);&lt;br/&gt;
                        r1.Set(&quot;anchor:hadoop.co.kr&quot;, &quot;hadoop&quot;);&lt;br/&gt;
                        r1.Set(&quot;anchor:naver.com&quot;, &quot;naver&quot;);&lt;/p&gt;

&lt;p&gt;                Operation op = new Operation();&lt;br/&gt;
                op.Apply(r1);&lt;/p&gt;

&lt;p&gt;                //Reading from table&lt;br/&gt;
                Scanner stream = new Scanner(&quot;webtable&quot;);&lt;br/&gt;
                stream.FetchColumnFamily(&quot;anchor&quot;);&lt;br/&gt;
                stream.Lookup(&quot;udanax.org&quot;);&lt;/p&gt;

&lt;p&gt;                while(stream.next())&lt;br/&gt;
               (&lt;br/&gt;
                        System.out.print(&quot;(RowName):&quot;);&lt;br/&gt;
                        System.out.print(stream.RowName());&lt;br/&gt;
                        System.out.print(&quot; (ColumnName):&quot;);&lt;br/&gt;
                        System.out.print(stream.ColumnName());&lt;br/&gt;
                        System.out.print(&quot; (Value):&quot;);&lt;br/&gt;
                        System.out.println(stream.Value());&lt;br/&gt;
               )&lt;/p&gt;

&lt;p&gt;                stream.close();&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310010">
                    <name>Incorporates</name>
                                                                <inwardlinks description="is part of">
                                        <issuelink>
            <issuekey id="12367711">HBASE-267</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12354799" name="hbase.patch" size="305313" author="jimk" created="Tue, 3 Apr 2007 00:32:28 +0000"/>
                            <attachment id="12354796" name="hbase.patch" size="305351" author="jimk" created="Mon, 2 Apr 2007 23:15:35 +0000"/>
                            <attachment id="12354477" name="hbase.patch" size="280780" author="jimk" created="Thu, 29 Mar 2007 02:39:07 +0000"/>
                            <attachment id="12353880" name="hbase.patch" size="270861" author="jimk" created="Wed, 21 Mar 2007 18:54:08 +0000"/>
                            <attachment id="12353558" name="hbase.patch" size="265699" author="jimk" created="Sat, 17 Mar 2007 04:04:53 +0000"/>
                            <attachment id="12353360" name="hbase.patch" size="259499" author="jimk" created="Thu, 15 Mar 2007 09:03:11 +0000"/>
                            <attachment id="12353161" name="hbase.patch" size="258686" author="jimk" created="Tue, 13 Mar 2007 07:16:48 +0000"/>
                            <attachment id="12353030" name="hbase.patch" size="267828" author="jimk" created="Sat, 10 Mar 2007 00:57:37 +0000"/>
                            <attachment id="12353002" name="hbase.patch" size="267473" author="jimk" created="Fri, 9 Mar 2007 17:13:50 +0000"/>
                            <attachment id="12352859" name="hbase.patch" size="259829" author="jimk" created="Wed, 7 Mar 2007 20:45:10 +0000"/>
                            <attachment id="12352348" name="hbase.patch" size="254957" author="jimk" created="Thu, 1 Mar 2007 19:19:31 +0000"/>
                            <attachment id="12352140" name="hbase.tar.gz" size="52282" author="michael_cafarella" created="Tue, 27 Feb 2007 16:52:26 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>12.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Tue, 27 Feb 2007 19:53:54 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>25071</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 32 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0h69b:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>98276</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>