<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Thu Dec 01 21:56:47 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-15908/HBASE-15908.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-15908] Checksum verification is broken due to incorrect passing of ByteBuffers in DataChecksum</title>
                <link>https://issues.apache.org/jira/browse/HBASE-15908</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;It looks like &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11625&quot; title=&quot;Reading datablock throws &amp;quot;Invalid HFile block magic&amp;quot; and can not switch to hdfs checksum &quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11625&quot;&gt;&lt;del&gt;HBASE-11625&lt;/del&gt;&lt;/a&gt; (cc &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=appy&quot; class=&quot;user-hover&quot; rel=&quot;appy&quot;&gt;Appy&lt;/a&gt;) has broken checksum verification? I&apos;m seeing the following on my cluster (1.3.0, Hadoop 2.7).&lt;/p&gt;

&lt;p&gt;Caused by: org.apache.hadoop.hbase.io.hfile.CorruptHFileException: Problem reading HFile Trailer from file &amp;lt;file path&amp;gt;&lt;br/&gt;
	at org.apache.hadoop.hbase.io.hfile.HFile.pickReaderVersion(HFile.java:497)&lt;br/&gt;
	at org.apache.hadoop.hbase.io.hfile.HFile.createReader(HFile.java:525)&lt;br/&gt;
	at org.apache.hadoop.hbase.regionserver.StoreFile$Reader.&amp;lt;init&amp;gt;(StoreFile.java:1135)&lt;br/&gt;
	at org.apache.hadoop.hbase.regionserver.StoreFileInfo.open(StoreFileInfo.java:259)&lt;br/&gt;
	at org.apache.hadoop.hbase.regionserver.StoreFile.open(StoreFile.java:427)&lt;br/&gt;
	at org.apache.hadoop.hbase.regionserver.StoreFile.createReader(StoreFile.java:528)&lt;br/&gt;
	at org.apache.hadoop.hbase.regionserver.StoreFile.createReader(StoreFile.java:518)&lt;br/&gt;
	at org.apache.hadoop.hbase.regionserver.HStore.createStoreFileAndReader(HStore.java:652)&lt;br/&gt;
	at org.apache.hadoop.hbase.regionserver.HStore.access$000(HStore.java:117)&lt;br/&gt;
	at org.apache.hadoop.hbase.regionserver.HStore$1.call(HStore.java:519)&lt;br/&gt;
	at org.apache.hadoop.hbase.regionserver.HStore$1.call(HStore.java:516)&lt;br/&gt;
	... 6 more&lt;br/&gt;
Caused by: java.lang.IllegalArgumentException: input ByteBuffers must be direct buffers&lt;br/&gt;
	at org.apache.hadoop.util.NativeCrc32.nativeComputeChunkedSums(Native Method)&lt;br/&gt;
	at org.apache.hadoop.util.NativeCrc32.verifyChunkedSums(NativeCrc32.java:59)&lt;br/&gt;
	at org.apache.hadoop.util.DataChecksum.verifyChunkedSums(DataChecksum.java:301)&lt;br/&gt;
	at org.apache.hadoop.hbase.io.hfile.ChecksumUtil.validateChecksum(ChecksumUtil.java:120)&lt;br/&gt;
	at org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderImpl.validateChecksum(HFileBlock.java:1785)&lt;br/&gt;
	at org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderImpl.readBlockDataInternal(HFileBlock.java:1728)&lt;br/&gt;
	at org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderImpl.readBlockData(HFileBlock.java:1558)&lt;br/&gt;
	at org.apache.hadoop.hbase.io.hfile.HFileBlock$AbstractFSReader$1.nextBlock(HFileBlock.java:1397)&lt;br/&gt;
	at org.apache.hadoop.hbase.io.hfile.HFileBlock$AbstractFSReader$1.nextBlockWithBlockType(HFileBlock.java:1405)&lt;br/&gt;
	at org.apache.hadoop.hbase.io.hfile.HFileReaderV2.&amp;lt;init&amp;gt;(HFileReaderV2.java:151)&lt;br/&gt;
	at org.apache.hadoop.hbase.io.hfile.HFileReaderV3.&amp;lt;init&amp;gt;(HFileReaderV3.java:78)&lt;br/&gt;
	at org.apache.hadoop.hbase.io.hfile.HFile.pickReaderVersion(HFile.java:487)&lt;br/&gt;
	... 16 more&lt;/p&gt;

&lt;p&gt;Prior this change we won&apos;t use use native crc32 checksum verification as in Hadoop&apos;s DataChecksum#verifyChunkedSums we would go this codepath&lt;/p&gt;

&lt;p&gt;if (data.hasArray() &amp;amp;&amp;amp; checksums.hasArray()) &lt;/p&gt;
{
  &amp;lt;check native checksum, but using byte[] instead of byte buffers&amp;gt;
}

&lt;p&gt;So we were fine. However, now we&apos;re dropping below and try to use the slightly different variant of native crc32 (if one is available)  taking ByteBuffer instead of byte[], which expects DirectByteBuffer, not Heap BB. &lt;/p&gt;

&lt;p&gt;I think easiest fix working on all Hadoops would be to remove asReadonly() conversion here:&lt;/p&gt;

&lt;p&gt;!validateChecksum(offset, onDiskBlockByteBuffer.asReadOnlyBuffer(), hdrSize)) {&lt;/p&gt;

&lt;p&gt;I don&apos;t see why do we need it. Let me test.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12973640">HBASE-15908</key>
            <summary>Checksum verification is broken due to incorrect passing of ByteBuffers in DataChecksum</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.png">Blocker</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="mantonov">Mikhail Antonov</assignee>
                                    <reporter username="mantonov">Mikhail Antonov</reporter>
                        <labels>
                    </labels>
                <created>Sat, 28 May 2016 04:45:26 +0000</created>
                <updated>Thu, 16 Jun 2016 18:55:58 +0000</updated>
                            <resolved>Thu, 16 Jun 2016 18:55:58 +0000</resolved>
                                    <version>1.3.0</version>
                                    <fixVersion>1.3.0</fixVersion>
                                    <component>HFile</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>8</watches>
                                                                <comments>
                            <comment id="15305229" author="appy" created="Sat, 28 May 2016 07:57:45 +0000"  >&lt;p&gt;But a byter buffer can be only one of two, array-backed or direct, right?&lt;br/&gt;
So if it&apos;s not direct, it was to be array backed. Why is it skipping the if condition.&lt;br/&gt;
It can&apos;t be that one is array-backed and another is not, because both are made using duplicate() on an array-backed buffer.&lt;br/&gt;
Investigating...&lt;/p&gt;</comment>
                            <comment id="15305233" author="appy" created="Sat, 28 May 2016 08:04:35 +0000"  >&lt;p&gt;Ooooh, burn!&lt;br/&gt;
I was trying to follow good practice by giving read-only object to a function which doesn&apos;t need to change it. So basically give &lt;a href=&quot;https://github.com/apache/hbase/blob/master/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java#L1715&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;asReadOnlyBuffer&lt;/a&gt; to validateChecksum(). But in ByteBuffer.java, we have....&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;    public final boolean hasArray() {
        return (hb != null) &amp;amp;&amp;amp; !isReadOnly;
    }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;:-/&lt;/p&gt;</comment>
                            <comment id="15305237" author="appy" created="Sat, 28 May 2016 08:13:54 +0000"  >&lt;p&gt;Here&apos;s the patch. For some reason, i am not getting option to attach files.&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;From d1c83054e72ec42f4ec499a15bb3287660600d76 Mon Sep 17 00:00:00 2001
From: Apekshit &amp;lt;apeksharma@gmail.com&amp;gt;
Date: Sat, 28 May 2016 01:09:01 -0700
Subject: [PATCH] HBASE-15908 Pass mutable ByteBuffer to validateChecksum() in
 HFileBlock because in downstream code, Hadoop&apos;s DataChecksum class requires
 either direct byte-buffer of mutable byte-buffer. (Apekshit)

Change-Id: I988b18c40f5aa7a3670eb5bf4ece8ac0eca225e2
---
 .../src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java      | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java
index efc9a30..14a5cd1 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java
@@ -1712,7 +1712,7 @@ public class HFileBlock implements Cacheable {
       ByteBuffer onDiskBlockByteBuffer = ByteBuffer.wrap(onDiskBlock, 0, onDiskSizeWithHeader);
       // Verify checksum of the data before using it for building HFileBlock.
       if (verifyChecksum &amp;amp;&amp;amp;
-          !validateChecksum(offset, onDiskBlockByteBuffer.asReadOnlyBuffer(), hdrSize)) {
+          !validateChecksum(offset, onDiskBlockByteBuffer, hdrSize)) {
         return null;
       }
       // The onDiskBlock will become the headerAndDataBuffer for this block.
--
2.3.2 (Apple Git-55)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15305252" author="mantonov" created="Sat, 28 May 2016 09:20:18 +0000"  >&lt;p&gt;Yeah, this is exactly what i also described in the end of the jira and testing now &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; Thanks for taking another look!&lt;/p&gt;

&lt;p&gt;I want to test it on some real cluster before committing though.&lt;/p&gt;</comment>
                            <comment id="15305271" author="mantonov" created="Sat, 28 May 2016 10:08:31 +0000"  >&lt;p&gt;Yes, as I thought switching back to non-readonly duplicate BB fixed it. Let me commit to all branches where the original jira went in. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=appy&quot; class=&quot;user-hover&quot; rel=&quot;appy&quot;&gt;Appy&lt;/a&gt; - thanks for also taking a look at it and confirming!&lt;/p&gt;</comment>
                            <comment id="15305280" author="mantonov" created="Sat, 28 May 2016 10:25:53 +0000"  >&lt;p&gt;FYI, I tested only on Hadoop 2.7 cluster, and in 2.5 checksumming is a little bit different - namely, the second variance of call to DataChecksum verifyChunkedSums(), the one which uses byte arrays instead of byte buffers does not even attempt to use native checksums. But anyway.&lt;/p&gt;

&lt;p&gt;As this is practically an addendum one-liner fix to already committed jira, I&apos;ve pushed it to master, branch-1 and branch-1.3, will push to 1.2 if original jira is backported there.&lt;/p&gt;</comment>
                            <comment id="15305281" author="mantonov" created="Sat, 28 May 2016 10:27:03 +0000"  >&lt;p&gt;that&apos;s what i&apos;ve committed&lt;/p&gt;</comment>
                            <comment id="15305406" author="hudson" created="Sat, 28 May 2016 13:58:29 +0000"  >&lt;p&gt;FAILURE: Integrated in HBase-1.4 #184 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-1.4/184/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-1.4/184/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-15908&quot; title=&quot;Checksum verification is broken due to incorrect passing of ByteBuffers in DataChecksum&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-15908&quot;&gt;&lt;del&gt;HBASE-15908&lt;/del&gt;&lt;/a&gt; Checksum verification is broken due to incorrect passing of (antonov: rev d03ffb078804cbac9f77a8ed17c64ffd43a89ae3)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="15305408" author="hudson" created="Sat, 28 May 2016 14:14:33 +0000"  >&lt;p&gt;FAILURE: Integrated in HBase-1.3 #720 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-1.3/720/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-1.3/720/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-15908&quot; title=&quot;Checksum verification is broken due to incorrect passing of ByteBuffers in DataChecksum&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-15908&quot;&gt;&lt;del&gt;HBASE-15908&lt;/del&gt;&lt;/a&gt; Checksum verification is broken due to incorrect passing of (antonov: rev 1c21c4970596e419a917d63c32acf44e0c4017fd)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="15305427" author="hudson" created="Sat, 28 May 2016 14:51:37 +0000"  >&lt;p&gt;SUCCESS: Integrated in HBase-1.3-IT #686 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-1.3-IT/686/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-1.3-IT/686/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-15908&quot; title=&quot;Checksum verification is broken due to incorrect passing of ByteBuffers in DataChecksum&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-15908&quot;&gt;&lt;del&gt;HBASE-15908&lt;/del&gt;&lt;/a&gt; Checksum verification is broken due to incorrect passing of (antonov: rev 1c21c4970596e419a917d63c32acf44e0c4017fd)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="15305432" author="stack" created="Sat, 28 May 2016 15:00:51 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mantonov&quot; class=&quot;user-hover&quot; rel=&quot;mantonov&quot;&gt;Mikhail Antonov&lt;/a&gt; (and &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=appy&quot; class=&quot;user-hover&quot; rel=&quot;appy&quot;&gt;Appy&lt;/a&gt;)&lt;/p&gt;</comment>
                            <comment id="15305442" author="hudson" created="Sat, 28 May 2016 15:20:56 +0000"  >&lt;p&gt;FAILURE: Integrated in HBase-Trunk_matrix #955 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-Trunk_matrix/955/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-Trunk_matrix/955/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-15908&quot; title=&quot;Checksum verification is broken due to incorrect passing of ByteBuffers in DataChecksum&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-15908&quot;&gt;&lt;del&gt;HBASE-15908&lt;/del&gt;&lt;/a&gt; Checksum verification is broken due to incorrect passing of (antonov: rev 60c8f76a9d888bc93be7f25eeeb5623143efa794)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="15308978" author="huaxiang" created="Wed, 1 Jun 2016 00:51:11 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11625&quot; title=&quot;Reading datablock throws &amp;quot;Invalid HFile block magic&amp;quot; and can not switch to hdfs checksum &quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11625&quot;&gt;&lt;del&gt;HBASE-11625&lt;/del&gt;&lt;/a&gt;  is in branch-1.2, I do not see &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-15908&quot; title=&quot;Checksum verification is broken due to incorrect passing of ByteBuffers in DataChecksum&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-15908&quot;&gt;&lt;del&gt;HBASE-15908&lt;/del&gt;&lt;/a&gt; is in branch-1.2. Does it need be committed to branch-1.2 as well? Thanks.&lt;/p&gt;</comment>
                            <comment id="15308986" author="huaxiang" created="Wed, 1 Jun 2016 00:55:37 +0000"  >&lt;p&gt;Never mind, it seems that branch-1.2 is ok.&lt;/p&gt;</comment>
                            <comment id="15308987" author="mantonov" created="Wed, 1 Jun 2016 00:55:38 +0000"  >&lt;p&gt;No, it doesn&apos;t. See last  several comments to &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11625&quot; title=&quot;Reading datablock throws &amp;quot;Invalid HFile block magic&amp;quot; and can not switch to hdfs checksum &quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11625&quot;&gt;&lt;del&gt;HBASE-11625&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="15308995" author="huaxiang" created="Wed, 1 Jun 2016 01:00:55 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mantonov&quot; class=&quot;user-hover&quot; rel=&quot;mantonov&quot;&gt;Mikhail Antonov&lt;/a&gt;!&lt;/p&gt;</comment>
                            <comment id="15327781" author="asamir" created="Mon, 13 Jun 2016 17:20:29 +0000"  >&lt;p&gt;I still see this issue on master branch + hadoop-2.5.2. Should  we reopen this ticket or crate new one ?&lt;/p&gt;</comment>
                            <comment id="15327892" author="mantonov" created="Mon, 13 Jun 2016 18:43:07 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=asamir&quot; class=&quot;user-hover&quot; rel=&quot;asamir&quot;&gt;Samir Ahmic&lt;/a&gt; Do you see exact same error? Basically class cast error in there?&lt;/p&gt;</comment>
                            <comment id="15327895" author="mantonov" created="Mon, 13 Jun 2016 18:43:43 +0000"  >&lt;p&gt;Hadoop 2.5.2 shouldn&apos;t attempt to use native checksum even on this codepath, as I recall&lt;/p&gt;</comment>
                            <comment id="15328040" author="asamir" created="Mon, 13 Jun 2016 19:43:46 +0000"  >&lt;p&gt;Here is exception i&apos;m seeing &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Caused by: org.apache.hadoop.hbase.io.hfile.CorruptHFileException: Problem reading HFile Trailer from file hdfs:&lt;span class=&quot;code-comment&quot;&gt;//P3cluster/hbase/data/&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;/cluster_test/37b19126a6455b5efd454b7774e22298/test_cf/390bef6889a042d6a08a1a386f29314d
&lt;/span&gt;        at org.apache.hadoop.hbase.io.hfile.HFile.pickReaderVersion(HFile.java:518)
        at org.apache.hadoop.hbase.io.hfile.HFile.createReader(HFile.java:547)
        at org.apache.hadoop.hbase.regionserver.StoreFileReader.&amp;lt;init&amp;gt;(StoreFileReader.java:94)
        at org.apache.hadoop.hbase.regionserver.StoreFileInfo.open(StoreFileInfo.java:270)
        at org.apache.hadoop.hbase.regionserver.StoreFile.open(StoreFile.java:419)
        at org.apache.hadoop.hbase.regionserver.StoreFile.createReader(StoreFile.java:526)
        at org.apache.hadoop.hbase.regionserver.StoreFile.createReader(StoreFile.java:516)
        at org.apache.hadoop.hbase.regionserver.HStore.createStoreFileAndReader(HStore.java:614)
        at org.apache.hadoop.hbase.regionserver.HStore.access$000(HStore.java:115)
        at org.apache.hadoop.hbase.regionserver.HStore$1.call(HStore.java:481)
        at org.apache.hadoop.hbase.regionserver.HStore$1.call(HStore.java:478)
        ... 6 more
-------&amp;gt; Caused by: java.lang.IllegalArgumentException: input ByteBuffers must be direct buffers 
        at org.apache.hadoop.util.NativeCrc32.nativeVerifyChunkedSums(Native Method)
        at org.apache.hadoop.util.NativeCrc32.verifyChunkedSums(NativeCrc32.java:57)
        at org.apache.hadoop.util.DataChecksum.verifyChunkedSums(DataChecksum.java:299)
        at org.apache.hadoop.hbase.io.hfile.ChecksumUtil.validateChecksum(ChecksumUtil.java:120)
        at org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderImpl.validateChecksum(HFileBlock.java:1775)
        at org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderImpl.readBlockDataInternal(HFileBlock.java:1714)
        at org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderImpl.readBlockData(HFileBlock.java:1547)
        at org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderImpl$2.nextBlock(HFileBlock.java:1447)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I have compiled master branch against hadoop-2.5.2 and deployed in distributed mode.&lt;/p&gt;</comment>
                            <comment id="15329026" author="asamir" created="Tue, 14 Jun 2016 07:41:11 +0000"  >&lt;p&gt;Few more information about my environment:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Cent OS 6.7 (deployed in lxc container)&lt;/li&gt;
	&lt;li&gt;java version 1.7.0_80&lt;/li&gt;
	&lt;li&gt;hadoop-2.5.2 with native libs loaded&lt;/li&gt;
	&lt;li&gt;hbase  master branch 2.0.0-SNAPSHOT, revision=2d0448fa84745991d2447ff78600866873b1fec0&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="15331374" author="mantonov" created="Wed, 15 Jun 2016 08:19:48 +0000"  >&lt;p&gt;Will look more at it tomorrow, did you happen to have any results for other HBase branches / Hadoop version?&lt;/p&gt;</comment>
                            <comment id="15331590" author="asamir" created="Wed, 15 Jun 2016 12:01:28 +0000"  >&lt;p&gt;I did not try other hadoop/hbase versions but i&apos;m pretty sure that this will be issue whenever hadoop native libs are loaded. I have checked hadoop-2.7.1 and hadoop-2.5.2 DataChecksum#verifyChunkedSums() and in both branches there is this condition:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
 &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (NativeCrc32.isAvailable()) {
      NativeCrc32.verifyChunkedSums(bytesPerChecksum, type.id, checksums, data,
          fileName, basePos);
      &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt;;
    }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt; 

&lt;p&gt;I was able to workaround this issue by making direct ByteBuffers and use them if native libs are loaded. Here is diff:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/ChecksumUtil.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/ChecksumUtil.java
index a47cc12..48310c9 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/ChecksumUtil.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/ChecksumUtil.java
@@ -27,6 +27,7 @@ &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.hadoop.fs.ChecksumException;
 &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.hadoop.hbase.classification.InterfaceAudience;
 &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.hadoop.hbase.util.ChecksumType;
 &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.hadoop.util.DataChecksum;
+&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.hadoop.util.NativeCodeLoader;
 
 /**
  * Utility methods to compute and validate checksums.
@@ -117,12 +118,27 @@ &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; class ChecksumUtil {
       ByteBuffer data = (ByteBuffer) buffer.duplicate().position(0).limit(onDiskDataSizeWithHeader);
       ByteBuffer checksums = (ByteBuffer) buffer.duplicate().position(onDiskDataSizeWithHeader)
           .limit(buffer.capacity());
+      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt;(NativeCodeLoader.isNativeCodeLoaded()) {
+        dataChecksum.verifyChunkedSums(directify(data), directify(checksums), pathName, 0);
+      } &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; {
       dataChecksum.verifyChunkedSums(data, checksums, pathName, 0);
+      }
     } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (ChecksumException e) {
       &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;;
     }
     &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;;  &lt;span class=&quot;code-comment&quot;&gt;// checksum is valid
&lt;/span&gt;   }
+  
+  &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; ByteBuffer directify(ByteBuffer dataBuf) {
+    ByteBuffer newBuf = ByteBuffer.allocateDirect(dataBuf.capacity());
+    newBuf.position(dataBuf.position());
+    newBuf.mark();
+    newBuf.put(dataBuf);
+    newBuf.reset();
+    newBuf.limit(dataBuf.limit());
+    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; newBuf;
+  }
+
 
   /**
    * Returns the number of bytes needed to store the checksums &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15332572" author="mantonov" created="Wed, 15 Jun 2016 21:10:36 +0000"  >&lt;p&gt;(re-opening until it&apos;s fully figured out)&lt;/p&gt;</comment>
                            <comment id="15332709" author="mantonov" created="Wed, 15 Jun 2016 22:13:34 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=asamir&quot; class=&quot;user-hover&quot; rel=&quot;asamir&quot;&gt;Samir Ahmic&lt;/a&gt; thanks for digging into that!&lt;/p&gt;

&lt;p&gt;What seems weird here that in both 2.7.1 and 2.5.2 in DataChecksum we have this check before the snippet you posted, which should kick in and call the version of method which passes byte[] as params rather than Buffers. The fix above was to avoid setting the buffers as read-only&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (data.hasArray() &amp;amp;&amp;amp; checksums.hasArray()) {
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15333532" author="mantonov" created="Thu, 16 Jun 2016 10:28:04 +0000"  >&lt;p&gt;Looking at the stacktrace you posted and the code in 2.5.2 I&apos;m a bit puzzled - it looks like if your code in HFileBlock just wasn&apos;t patched (but this git hash definitely has this fix)? And I don&apos;t see where this code is different between 2.7.1.and 2.7.2., and 2.7.2 works for me with 1.3 (and this patch was applied to both). Any chance you could try HBase 1.3?&lt;/p&gt;

&lt;p&gt;Meanwhile I&apos;ll see if I can reproduce it.. Before pushing in addendums I&apos;d like to make sure we know why exactly current patch doesn&apos;t work for you &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=asamir&quot; class=&quot;user-hover&quot; rel=&quot;asamir&quot;&gt;Samir Ahmic&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15333562" author="asamir" created="Thu, 16 Jun 2016 10:50:25 +0000"  >&lt;p&gt;Sure thing. Let me try this with 1.3 and will report back. &lt;/p&gt;</comment>
                            <comment id="15333743" author="asamir" created="Thu, 16 Jun 2016 13:12:57 +0000"  >&lt;p&gt;I have just test 1.3+hadoop-2.5.2+native libs loaded all looks clear issue is fixed. &lt;br/&gt;
Also i  have tested master+hadoop-2.5.2+native libs loaded,  issue is fixed. &lt;/p&gt;

&lt;p&gt;Not sure why fix was not present in master branch  2 days ago when i was building code. &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mantonov&quot; class=&quot;user-hover&quot; rel=&quot;mantonov&quot;&gt;Mikhail Antonov&lt;/a&gt; sorry for inconvenience. &lt;/p&gt;</comment>
                            <comment id="15334436" author="mantonov" created="Thu, 16 Jun 2016 18:53:29 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=asamir&quot; class=&quot;user-hover&quot; rel=&quot;asamir&quot;&gt;Samir Ahmic&lt;/a&gt; no worries! Thanks for additional checks. I&apos;m marking this one as resolved again.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310050">
                    <name>Regression</name>
                                                                <inwardlinks description="is broken by">
                                        <issuelink>
            <issuekey id="12731001">HBASE-11625</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12806809" name="master.v1.patch" size="1352" author="mantonov" created="Sat, 28 May 2016 10:27:03 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Sat, 28 May 2016 07:57:45 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            24 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2yn0f:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>