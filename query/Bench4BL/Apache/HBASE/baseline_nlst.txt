543	A region‘s state is kept in several places in the master opening the possibility for race conditions  A region&amp apos s state exists in multiple maps in the RegionManager  unassignedRegions  pendingRegions  regionsToClose  closingRegions  regionsToDelete  etc  One of these race conditions was found in HBASE 534  For HBase 0 1 x  we should just patch the holes we find  The ultimate solution  which requires a lot of changes in HMaster  should be applied to HBase trunk  Proposed solution  Create a class that encapsulates a region’s state and provide synchronized access to the class that validates state changes  There should be a single structure that holds regions in these transitional states and it should be a synchronized collection of some kind
659	HLog cacheFlushLock not cleared  hangs a region  I have a region that is stuck in a close that was ordained by a split  Here is what I have from the log pertaining to the stuck region   4    6416 2008 05 29 22 29 03 433 INFO org apache hadoop hbase HRegion  checking compaction completed on region enwiki IK9sWdHJe6ffGZgFPsqIvk   1212092907061 in 12sec 5    6417 2008 05 29 22 29 03 439 INFO org apache hadoop hbase HRegion  Splitting enwiki IK9sWdHJe6ffGZgFPsqIvk   1212092907061 because largest aggregate size is 288 3m and desired size is 256 0m 6    6418 2008 05 29 22 29 03 443 DEBUG org apache hadoop hbase HRegion  compactions and cache flushes disabled for region enwiki IK9sWdHJe6ffGZgFPsqIvk   1212092907061 7    6419 2008 05 29 22 29 03 443 DEBUG org apache hadoop hbase HRegion  new updates and scanners for region enwiki IK9sWdHJe6ffGZgFPsqIvk   1212092907061 disabled 8    6420 2008 05 29 22 29 03 443 DEBUG org apache hadoop hbase HRegion  no more active scanners for region enwiki IK9sWdHJe6ffGZgFPsqIvk   1212092907061 9    6421 2008 05 29 22 29 03 443 DEBUG org apache hadoop hbase HRegion  no more row locks outstanding on region enwiki IK9sWdHJe6ffGZgFPsqIvk   1212092907061 10   6422 2008 05 29 22 29 03 443 DEBUG org apache hadoop hbase HRegionServer  enwiki IK9sWdHJe6ffGZgFPsqIvk   1212092907061 closing  Adding to retiringRegions  11   6423 2008 05 29 22 29 03 443 DEBUG org apache hadoop hbase HRegion  Started memcache flush for region enwiki IK9sWdHJe6ffGZgFPsqIvk   1212092907061  Current region memcache size 2 1m 12    6424 2008 05 29 22 29 03 561 INFO org apache hadoop ipc Server  IPC Server handler 0 on 60020  call batchUpdate enwiki IK9sWdHJe6ffGZgFPsqIvk   1212092907061  1171081390000  org apache hadoop hbase io BatchUpdate 2eeb0275  from 208 76 44 139 49358  err        or  org         apache hadoop hbase NotServingRegionException  enwiki IK9sWdHJe6ffGZgFPsqIvk   1212092907061 13   6425 org apache hadoop hbase NotServingRegionException  enwiki IK9sWdHJe6ffGZgFPsqIvk   1212092907061 14   6434 2008 05 29 22 29 03 982 INFO org apache hadoop ipc Server  IPC Server handler 9 on 60020  call batchUpdate enwiki IK9sWdHJe6ffGZgFPsqIvk   1212092907061  1202595259000  org apache hadoop hbase io BatchUpdate 46ee6763  from 208 76 44 139 49358  err        or  org         apache hadoop hbase NotServingRegionException  enwiki IK9sWdHJe6ffGZgFPsqIvk   1212092907061 15   6435 org apache hadoop hbase NotServingRegionException  enwiki IK9sWdHJe6ffGZgFPsqIvk   1212092907061  Then in thread dump  I have two threads blocked on the HLog cacheFlushLock but looking in code  there is no obvious code path that would get a situation where a lock is held and then not released    regionserver 0 0 0 0 0 0 0 0 60020 compactor  daemon prio 1 tid 0x00002aab381e5fd0 nid 0x6195 waiting on condition  0x0000000041c6c000  0x0000000041c6ce00  at sun misc Unsafe park Native Method  at java util concurrent locks LockSupport park Unknown Source  at java util concurrent locks AbstractQueuedSynchronizer parkAndCheckInterrupt Unknown Source  at java util concurrent locks AbstractQueuedSynchronizer acquireQueued Unknown Source  at java util concurrent locks AbstractQueuedSynchronizer acquire Unknown Source  at java util concurrent locks ReentrantLock NonfairSync lock Unknown Source  at java util concurrent locks ReentrantLock lock Unknown Source  at org apache hadoop hbase HLog startCacheFlush HLog java 459  at org apache hadoop hbase HRegion internalFlushcache HRegion java 1089  at org apache hadoop hbase HRegion close HRegion java 594    locked &lt 0x00002aaab70bf3a0&gt   a java lang Integer  at org apache hadoop hbase HRegion splitRegion HRegion java 759    locked &lt 0x00002aaab70bf3a0&gt   a java lang Integer  at org apache hadoop hbase HRegionServer CompactSplitThread split HRegionServer java 248  at org apache hadoop hbase HRegionServer CompactSplitThread run HRegionServer java 204          regionserver 0 0 0 0 0 0 0 0 60020 logRoller  daemon prio 1 tid 0x00002aab38181d70 nid 0x6193 waiting on condition  0x0000000041a6a000  0x0000000041a6ab00  at sun misc Unsafe park Native Method  at java util concurrent locks LockSupport park Unknown Source  at java util concurrent locks AbstractQueuedSynchronizer parkAndCheckInterrupt Unknown Source  at java util concurrent locks AbstractQueuedSynchronizer acquireQueued Unknown Source  at java util concurrent locks AbstractQueuedSynchronizer acquire Unknown Source  at java util concurrent locks ReentrantLock NonfairSync lock Unknown Source  at java util concurrent locks ReentrantLock lock Unknown Source  at org apache hadoop hbase HLog rollWriter HLog java 219  at org apache hadoop hbase HRegionServer LogRoller run HRegionServer java 615    locked &lt 0x00002aaab69ccf00&gt   a java lang Integer
663	Incorrect sequence number for cache flushAn HRegion asks each HStore to flush its cache with a sequence number X  The assumption is that all the updates before X will be flushed  So during the startup reconstruction  the updates before X are skipped  The use of updatesLock should guarantee that all the updates before X will be flushed when HStore flushes with X   snapshots are taken after the write lock on updatesLock is acquired  while all the updates are written to the log and to the cache with the read lock on updatesLock is acquired  However  because the sequence number X is obtained without the write lock on updatesLock  some updates with sequence number X may not have been written to the cache which will be flushed
699	Fix TestMigrate up on HudsonIts hanging on hudson again  Caught a threaddump  Its that old waiting on a vanished unix process    no hbase threads hanging out  I tried adding relocateRegion just before taking out scan in verify  That was good for fixing the first region in the table  We hung when we tried to get second region  It was trying to go to old address
818	Deadlock running flushSomeRegions  Playing with MR uploading no a regionserver with 60  regions  I ran into a deadlock  Found one Java level deadlock                                 IPC Server handler 19 on 60020   waiting to lock monitor 0x084be38c  object 0xb6f69a70  a org apache hadoop hbase regionserver Flusher   which is held by  IPC Server handler 16 on 60020   IPC Server handler 16 on 60020   waiting to lock monitor 0x080f8dec  object 0xb73610c0  a org apache hadoop hbase regionserver HRegion WriteState   which is held by  IPC Server handler 2 on 60020   IPC Server handler 2 on 60020   waiting to lock monitor 0x086e8fe8  object 0xb6f69cf0  a java util HashSet   which is held by  IPC Server handler 16 on 60020   Java stack information for the threads listed above                                                       IPC Server handler 19 on 60020   at org apache hadoop hbase regionserver Flusher flushSomeRegions Flusher java 261    waiting to lock &lt 0xb6f69a70&gt   a org apache hadoop hbase regionserver Flusher  at org apache hadoop hbase regionserver Flusher reclaimMemcacheMemory Flusher java 252  at org apache hadoop hbase regionserver HRegionServer batchUpdate HRegionServer java 1136  at sun reflect GeneratedMethodAccessor3 invoke Unknown Source  at sun reflect DelegatingMethodAccessorImpl invoke DelegatingMethodAccessorImpl java 43  at java lang reflect Method invoke Method java 623  at org apache hadoop hbase ipc HbaseRPC Server call HbaseRPC java 473  at org apache hadoop ipc Server Handler run Server java 896   IPC Server handler 16 on 60020   at org apache hadoop hbase regionserver HRegion flushcache HRegion java 948    waiting to lock &lt 0xb73610c0&gt   a org apache hadoop hbase regionserver HRegion WriteState  at org apache hadoop hbase regionserver Flusher flushRegion Flusher java 173    locked &lt 0xb6f69cf0&gt   a java util HashSet  at org apache hadoop hbase regionserver Flusher flushSomeRegions Flusher java 267    locked &lt 0xb6f69a70&gt   a org apache hadoop hbase regionserver Flusher  at org apache hadoop hbase regionserver Flusher reclaimMemcacheMemory Flusher java 252  at org apache hadoop hbase regionserver HRegionServer batchUpdate HRegionServer java 1136  at sun reflect GeneratedMethodAccessor3 invoke Unknown Source  at sun reflect DelegatingMethodAccessorImpl invoke DelegatingMethodAccessorImpl java 43  at java lang reflect Method invoke Method java 623  at org apache hadoop hbase ipc HbaseRPC Server call HbaseRPC java 473  at org apache hadoop ipc Server Handler run Server java 896   IPC Server handler 2 on 60020   at org apache hadoop hbase regionserver Flusher addRegion Flusher java 237    waiting to lock &lt 0xb6f69cf0&gt   a java util HashSet  at org apache hadoop hbase regionserver Flusher request Flusher java 114  at org apache hadoop hbase regionserver HRegion requestFlush HRegion java 1627    locked &lt 0xb73610c0&gt   a org apache hadoop hbase regionserver HRegion WriteState  at org apache hadoop hbase regionserver HRegion update HRegion java 1614  at org apache hadoop hbase regionserver HRegion batchUpdate HRegion java 1398  at org apache hadoop hbase regionserver HRegionServer batchUpdate HRegionServer java 1137  at sun reflect GeneratedMethodAccessor3 invoke Unknown Source  at sun reflect DelegatingMethodAccessorImpl invoke DelegatingMethodAccessorImpl java 43  at java lang reflect Method invoke Method java 623  at org apache hadoop hbase ipc HbaseRPC Server call HbaseRPC java 473  at org apache hadoop ipc Server Handler run Server java 896   Found 1 deadlock   Regionserver is hosed
950	HTable commit no longer works with existing RowLocks though it s still in APIIntroduced by HBASE 748  the RowLock passed into HTable commit is now ignored  This causes the update the hang until that rowlock expires  and then it proceeds with getting a new row lock
1569	rare race condition can take down a regionserver  this happened after &gt  24 hours of heavy import load on my cluster   Luckily the shutdown seemed to be clean  java lang IllegalAccessError  Call open first at org apache hadoop hbase regionserver StoreFile getReader StoreFile java 356  at org apache hadoop hbase regionserver Store getStorefilesIndexSize Store java 1378  at org apache hadoop hbase regionserver HRegionServer doMetrics HRegionServer java 1075  at org apache hadoop hbase regionserver HRegionServer run HRegionServer java 454  at java lang Thread run Thread java 619
1740	ICV has a subtle race condition only visible under high load  ICV demonstrates a race condition under high load  The result is a duplicate KeyValue with the same timestamp  at first in the memcache  and in hfile  then both in hfile  The get scan code doesnt know which one to read  and picks one arbitrarily  One of the keyvalues is correct  one is incorrect  What happens at a deeper level   we start an ICV  a snapshot happens and moves the memstore to the snapshot  the ICV code puts a key value into memstore that has the same timestamp as a keyvalue in the snapshot   This is a deep race condition and several attempts to fix it failed in production here at SU  This issue is about a more permanent fix
1840	RowLock fails when used with IndexTableThe following exception is thrown when using RowLock to update a row in an IndexedTable   junit  java io IOException  java io IOException  Invalid row lock  junit  at org apache hadoop hbase regionserver HRegion getLock HRegion java 1640   junit  at org apache hadoop hbase regionserver HRegion put HRegion java 1244   junit  at org apache hadoop hbase regionserver tableindexed IndexedRegion put IndexedRegion java 97   junit  at org apache hadoop hbase regionserver HRegion put HRegion java 1216   junit  at org apache hadoop hbase regionserver HRegionServer put HRegionServer java 1818   junit  at sun reflect GeneratedMethodAccessor23 invoke Unknown Source   junit  at sun reflect DelegatingMethodAccessorImpl invoke DelegatingMethodAccessorImpl java 25   junit  at java lang reflect Method invoke Method java 597   junit  at org apache hadoop hbase ipc HBaseRPC Server call HBaseRPC java 650   junit  at org apache hadoop hbase ipc HBaseServer Handler run HBaseServer java 915  NOTE  1  Line numbers in stacktrace may not make sense because I&amp apos ve been hacking in loads of debug info  NOTE  2  I attaching a fix which includes unit tests
1869	IndexedTable delete fails when used in conjunction with RowLock    Created the following test in TestIndexedTable  public void testLockedRowDelete   throws IOException    writeInitalRows        Delete the first row   byte   row   PerformanceEvaluation format 0    RowLock lock   table lockRow row    table delete new Delete row  HConstants LATEST_TIMESTAMP  lock     table unlockRow lock      assertRowDeleted NUM_ROWS   1        which fails and throws the following exception  java io IOException  java io IOException  Invalid row lock at org apache hadoop hbase regionserver HRegion getLock HRegion java 1621  at org apache hadoop hbase regionserver HRegion delete HRegion java 1094  at org apache hadoop hbase regionserver tableindexed IndexedRegion delete IndexedRegion java 269  at org apache hadoop hbase regionserver HRegionServer delete HRegionServer java 2014  at sun reflect NativeMethodAccessorImpl invoke0 Native Method  at sun reflect NativeMethodAccessorImpl invoke NativeMethodAccessorImpl java 39  at sun reflect DelegatingMethodAccessorImpl invoke DelegatingMethodAccessorImpl java 25  at java lang reflect Method invoke Method java 597  at org apache hadoop hbase ipc HBaseRPC Server call HBaseRPC java 648  at org apache hadoop hbase ipc HBaseServer Handler run HBaseServer java 915  Patch coded for the latest version in SVN  looks like 0 21 0    just going through final testing and packaging  Will attach shortly
2023	Client sync block can cause 1 thread of a multi threaded client to block all othersTake a highly multithreaded client  processing a few thousand requests a second  If a table goes offline  one thread will get stuck in  locateRegionInMeta  which is located inside the following sync block  synchronized userRegionLock     return locateRegionInMeta META_TABLE_NAME  tableName  row  useCache       So when other threads need to find a region  EVEN IF ITS CACHED  it will encounter this sync and wait  This can become an issue on a busy thrift server  where I first noticed the problem   one region offline can prevent access to all other regions Potential solution  narrow this lock  or perhaps just get rid of it completely
6565	Coprocessor exec result Map is not thread safe  I develop a coprocessor program  but found some different results in repeated tests for example  normally  the result s size is 10  but sometimes it appears 9  I read the HTable java code  found a TreeMap thread unsafe  be used in multithreading environment  It cause the bug happened
8923	ChaosMonkey test teardown may fail due to race with ChaosMonkey  If chaosmonkey restarts RS at the last moment before test teardown happens  cluster recovery may not see it in cluster status  however  RS process is already running  so when cluster recovery also tries to start it  it gets ExitCodeException  Boom
13606	AssignmentManager assign   is not sync in both pathfrom the comment and the expected behavior AssignmentManager assign   should be sync       Assigns specified regions round robin  if any     This is a synchronous call and will return once every region has been  public void assign List&lt HRegionInfo&gt  regions   but the code has two path  1 sync and the async   if  servers    1     regions &lt  bulkAssignThresholdRegions  &amp &amp  servers &lt  bulkAssignThresholdServers      for  HRegionInfo region  plan getValue            invokeAssign region       &lt    this is async threadPool submit assign             else    BulkAssigner ba   new GeneralBulkAssigner        ba bulkAssign        &lt    this is sync  calls BulkAssign waitUntilDone        https   builds apache org job HBase 1 1 452  TestCreateTableProcedure is flaky because of this async behavior
13662	RSRpcService scan   throws an OutOfOrderScannerNext if the scan has a retriable failurewhile fixing HBASE 13651  I noticed that if we have a failure inside the RSRpcService scan    when the request has a hasNextCallSeq    the nextCallSeq is incremented and not rolledback  which means that the client retry will send a request with a nextCallSeq not up to date  which result in an OutOfOrderScannerNextException   if  rows &gt  0     if  request hasNextCallSeq       if  request getNextCallSeq     rsh nextCallSeq     throw new OutOfOrderScannerNextException             Increment the nextCallSeq value which is the next expected from client   rsh nextCallSeq           try       scan code          after the scanner heartbeat patches HBASE 13090  we seems to be able to recover from that OutOfOrder exception  but the error show up anyway  After a discussion with Stack we ended up saying that decrementing the callSeq on exception seems to be fine  but we had the open question about having that nextCallSeq to be atomic  if that was supposed to prevent concurrent requests with the same id  any thoughts
13959	Region splitting uses a single thread in most common cases  When storefiles need to be split as part of a region split  the current logic uses a threadpool with the size set to the size of the number of stores  Since most common table setup involves only a single column family  this translates to having a single store and so the threadpool is run with a single thread  However  in a write heavy workload  there could be several tens of storefiles in a store at the time of splitting  and with a threadpool size of one  these files end up getting split sequentially  With a bit of tracing  I noticed that it takes on an average of 350ms to create a single reference file  and splitting each storefile involves creating two of these  so with a storefile count of 20  it takes about 14s just to get through this phase alone  2 reference files for each storefile   pushing the total time the region is offline to 18s or more  For environments that are setup to fail fast  this makes the client exhaust all retries and fail with NotServingRegionException  The fix should increase the concurrency of this operation
13988	Add exception handler for lease thread  In a prod cluster  a region server exited for some important threads were not alive  After excluding other threads from the log  we doubted the lease thread was the root  So we need to add an exception handler to the lease thread to debug why it exited in future   2015 06 29 12 46 09 222 INFO org apache hadoop hbase regionserver HRegionServer  STOPPED  One or more threads are no longer alive  stop 2015 06 29 12 46 09 223 INFO org apache hadoop ipc HBaseServer  Stopping server on 21600     2015 06 29 12 46 09 330 INFO org apache hadoop hbase regionserver LogRoller  LogRoller exiting  2015 06 29 12 46 09 330 INFO org apache hadoop hbase regionserver MemStoreFlusher  Thread 37 exiting 2015 06 29 12 46 09 330 INFO org apache hadoop hbase regionserver HRegionServer CompactionChecker  regionserver21600 compactionChecker exiting 2015 06 29 12 46 12 403 INFO org apache hadoop hbase regionserver HRegionServer PeriodicMemstoreFlusher  regionserver21600 periodicFlusher exiting
14178	regionserver blocks because of waiting for offsetLock  My regionserver blocks  and all client rpc timeout  I print the regionserver s jstack   it seems a lot of threads were blocked for waiting offsetLock  detail infomation belows  PS  my table s block cache is off   B DefaultRpcServer handler 2 queue 2 port 60020   82 daemon prio 5 os_prio 0 tid 0x0000000001827000 nid 0x2cdc in Object wait    0x00007f3831b72000   java lang Thread State  WAITING  on object monitor   at java lang Object wait Native Method   at java lang Object wait Object java 502   at org apache hadoop hbase util IdLock getLockEntry IdLock java 79     locked &lt 0x0000000773af7c18&gt   a org apache hadoop hbase util IdLock Entry   at org apache hadoop hbase io hfile HFileReaderV2 readBlock HFileReaderV2 java 352   at org apache hadoop hbase io hfile HFileBlockIndex BlockIndexReader loadDataBlockWithScanInfo HFileBlockIndex java 253   at org apache hadoop hbase io hfile HFileReaderV2 AbstractScannerV2 seekTo HFileReaderV2 java 524   at org apache hadoop hbase io hfile HFileReaderV2 AbstractScannerV2 reseekTo HFileReaderV2 java 572   at org apache hadoop hbase regionserver StoreFileScanner reseekAtOrAfter StoreFileScanner java 257   at org apache hadoop hbase regionserver StoreFileScanner reseek StoreFileScanner java 173   at org apache hadoop hbase regionserver NonLazyKeyValueScanner doRealSeek NonLazyKeyValueScanner java 55   at org apache hadoop hbase regionserver KeyValueHeap generalizedSeek KeyValueHeap java 313   at org apache hadoop hbase regionserver KeyValueHeap requestSeek KeyValueHeap java 269   at org apache hadoop hbase regionserver StoreScanner reseek StoreScanner java 695   at org apache hadoop hbase regionserver StoreScanner seekAsDirection StoreScanner java 683   at org apache hadoop hbase regionserver StoreScanner next StoreScanner java 533   at org apache hadoop hbase regionserver KeyValueHeap next KeyValueHeap java 140   at org apache hadoop hbase regionserver HRegion RegionScannerImpl populateResult HRegion java 3889   at org apache hadoop hbase regionserver HRegion RegionScannerImpl nextInternal HRegion java 3969   at org apache hadoop hbase regionserver HRegion RegionScannerImpl nextRaw HRegion java 3847   at org apache hadoop hbase regionserver HRegion RegionScannerImpl next HRegion java 3820     locked &lt 0x00000005e5c55ad0&gt   a org apache hadoop hbase regionserver HRegion RegionScannerImpl   at org apache hadoop hbase regionserver HRegion RegionScannerImpl next HRegion java 3807   at org apache hadoop hbase regionserver HRegion get HRegion java 4779   at org apache hadoop hbase regionserver HRegion get HRegion java 4753   at org apache hadoop hbase regionserver HRegionServer get HRegionServer java 2916   at org apache hadoop hbase protobuf generated ClientProtos ClientService 2 callBlockingMethod ClientProtos java 29583   at org apache hadoop hbase ipc RpcServer call RpcServer java 2027   at org apache hadoop hbase ipc CallRunner run CallRunner java 108   at org apache hadoop hbase ipc RpcExecutor consumerLoop RpcExecutor java 114   at org apache hadoop hbase ipc RpcExecutor 1 run RpcExecutor java 94   at java lang Thread run Thread java 745     Locked ownable synchronizers     &lt 0x00000005e5c55c08&gt   a java util concurrent locks ReentrantLock NonfairSync
14241	Fix deadlock during cluster shutdown due to concurrent connection closeCaught while testing branch 1 0  shutting down TestMasterMetricsWrapper  Found one Java level deadlock                                 MASTER_META_SERVER_OPERATIONS ip 10 32 130 237 55342 0   waiting to lock monitor 0x00007f2a040051c8  object 0x00000007e36108a8  a org apache hadoop hbase util PoolMap   which is held by  M 0 ip 10 32 130 237 55342   M 0 ip 10 32 130 237 55342   waiting to lock monitor 0x00007f2a04005118  object 0x00000007e3610b00  a org apache hadoop hbase ipc RpcClientImpl Connection   which is held by  MASTER_META_SERVER_OPERATIONS ip 10 32 130 237 55342 0  Full stack dump and deadlock debug output attached  Root cause  In RpcClientImpl close    we obtain lock on connections first     synchronized  connections     for  Connection conn   connections values         Then markClosed   tries to obtain lock on connection object     if  conn isAlive       conn markClosed new InterruptedIOException  RpcClient is closing      conn close       Another thread  MetaServerShutdownHandler  calls RpcClientImpl Connection setupIOstreams   where      markClosed e    close       Lock on connection object is obtained first  then lock on connections is attempted  leading to deadlock     synchronized  connections     connections removeValue remoteId  this
14359	HTable close will hang forever if unchecked error exception thrown in AsyncProcess sendMultiActionCurrently  in AsyncProcess sendMultiAction  we only catch the RejectedExecutionException and let other error exception go  which will cause decTaskCounter not invoked  Meanwhile  the recommendation for using HTable is to close the table in the finally clause  and HTable close will call flushCommits and wait until all task done  The problem is when unchecked error exception like OutOfMemoryError thrown  taskSent will never be equal to taskDone  so AsyncProcess waitUntilDone will never return  Especially  if autoflush is set thus no data to flush during table close  there would be no rpc call so rpcTimeOut will not break the call  and thread will wait there forever  In our product env  the unchecked error we observed is  java lang OutOfMemoryError  unable to create new native thread   and we observed the client thread hang for hours
14463	Severe performance downgrade when parallel reading a single key from BucketCacheWe store feature data of online items in HBase  do machine learning on these features  and supply the outputs to our online search engine  In such scenario we will launch hundreds of yarn workers and each worker will read all features of one item i e  single rowkey in HBase   so there ll be heavy parallel reading on a single rowkey  We were using LruCache but start to try BucketCache recently to resolve gc issue  and just as titled we have observed severe performance downgrade  After some analytics we found the root cause is the lock in BucketCache getBlock  as shown below  try    lockEntry   offsetLock getLockEntry bucketEntry offset              if  bucketEntry equals backingMap get key               int len   bucketEntry getLength     Cacheable cachedBlock   ioEngine read bucketEntry offset    len   bucketEntry deserializerReference this deserialiserMap     Since ioEnging read involves array copy  it s much more time costed than the operation in LruCache  And since we re using synchronized in IdLock getLockEntry  parallel read dropping on the same bucket would be executed in serial  which causes a really bad performance  To resolve the problem  we propose to use ReentranceReadWriteLock in BucketCache  and introduce a new class called IdReadWriteLock to implement it
14555	Deadlock in MVCC branch 1 2 toString  Just saw this in an IT test     Thread 75  PriorityRpcServer handler 3 queue 1 port 16020    State  BLOCKED  Blocked count  691635  Waited count  1557446  Blocked on java util LinkedList 32b53d9e  Blocked by 81  PriorityRpcServer handler 9 queue 1 port 16020   Stack   org apache hadoop hbase regionserver MultiVersionConcurrencyControl toString MultiVersionConcurrencyControl java 234   java lang String valueOf String java 2994   java lang StringBuilder append StringBuilder java 131   org apache hadoop hbase regionserver MultiVersionConcurrencyControl waitForRead MultiVersionConcurrencyControl java 209   org apache hadoop hbase regionserver MultiVersionConcurrencyControl completeAndWait MultiVersionConcurrencyControl java 144   org apache hadoop hbase regionserver HRegion doMiniBatchMutation HRegion java 3191   org apache hadoop hbase regionserver HRegion batchMutate HRegion java 2837   org apache hadoop hbase regionserver HRegion batchMutate HRegion java 2779   org apache hadoop hbase regionserver RSRpcServices doBatchOp RSRpcServices java 697   org apache hadoop hbase regionserver RSRpcServices doNonAtomicRegionMutation RSRpcServices java 659   org apache hadoop hbase regionserver RSRpcServices multi RSRpcServices java 2047   org apache hadoop hbase protobuf generated ClientProtos ClientService 2 callBlockingMethod ClientProtos java 32594   org apache hadoop hbase ipc RpcServer call RpcServer java 2120   org apache hadoop hbase ipc CallRunner run CallRunner java 106   org apache hadoop hbase ipc RpcExecutor consumerLoop RpcExecutor java 130   org apache hadoop hbase ipc RpcExecutor 1 run RpcExecutor java 107   java lang Thread run Thread java 745                   Thread 81  PriorityRpcServer handler 9 queue 1 port 16020    State  BLOCKED  Blocked count  691858  Waited count  1558138  Blocked on java lang Object 2a5e9ae8  Blocked by 75  PriorityRpcServer handler 3 queue 1 port 16020   Stack   org apache hadoop hbase regionserver MultiVersionConcurrencyControl complete MultiVersionConcurrencyControl java 191   org apache hadoop hbase regionserver MultiVersionConcurrencyControl completeAndWait MultiVersionConcurrencyControl java 143   org apache hadoop hbase regionserver HRegion doMiniBatchMutation HRegion java 3191   org apache hadoop hbase regionserver HRegion batchMutate HRegion java 2837   org apache hadoop hbase regionserver HRegion batchMutate HRegion java 2779   org apache hadoop hbase regionserver RSRpcServices doBatchOp RSRpcServices java 697   org apache hadoop hbase regionserver RSRpcServices doNonAtomicRegionMutation RSRpcServices java 659   org apache hadoop hbase regionserver RSRpcServices multi RSRpcServices java 2047   org apache hadoop hbase protobuf generated ClientProtos ClientService 2 callBlockingMethod ClientProtos java 32594   org apache hadoop hbase ipc RpcServer call RpcServer java 2120   org apache hadoop hbase ipc CallRunner run CallRunner java 106   org apache hadoop hbase ipc RpcExecutor consumerLoop RpcExecutor java 130   org apache hadoop hbase ipc RpcExecutor 1 run RpcExecutor java 107   java lang Thread run Thread java 745   Thread 80  PriorityRpcServer handler 8 queue 0 port 16020
14812	Fix ResultBoundedCompletionService deadlock    thrift2 worker 0   31 daemon prio 5 os_prio 0 tid 0x00007f5ad9c45000 nid 0x484 in Object wait    0x00007f5aa3832000   java lang Thread State  WAITING  on object monitor   at java lang Object wait Native Method   at java lang Object wait Object java 502   at org apache hadoop hbase client ResultBoundedCompletionService take ResultBoundedCompletionService java 148     locked &lt 0x00000006b6ae7670&gt   a  Lorg apache hadoop hbase client ResultBoundedCompletionService QueueingFuture    at org apache hadoop hbase client ScannerCallableWithReplicas call ScannerCallableWithReplicas java 188   at org apache hadoop hbase client ScannerCallableWithReplicas call ScannerCallableWithReplicas java 59   at org apache hadoop hbase client RpcRetryingCaller callWithoutRetries RpcRetryingCaller java 200   at org apache hadoop hbase client ClientSmallReversedScanner loadCache ClientSmallReversedScanner java 212   at org apache hadoop hbase client ClientSmallReversedScanner next ClientSmallReversedScanner java 186   at org apache hadoop hbase client ConnectionManager HConnectionImplementation locateRegionInMeta ConnectionManager java 1276   at org apache hadoop hbase client ConnectionManager HConnectionImplementation locateRegion ConnectionManager java 1182   at org apache hadoop hbase client AsyncProcess submit AsyncProcess java 370   at org apache hadoop hbase client AsyncProcess submit AsyncProcess java 321   at org apache hadoop hbase client BufferedMutatorImpl backgroundFlushCommits BufferedMutatorImpl java 194   at org apache hadoop hbase client BufferedMutatorImpl flush BufferedMutatorImpl java 171     locked &lt 0x00000006b6ae79c0&gt   a org apache hadoop hbase client BufferedMutatorImpl   at org apache hadoop hbase client HTable flushCommits HTable java 1430   at org apache hadoop hbase client HTable put HTable java 1033   at org apache hadoop hbase thrift2 ThriftHBaseServiceHandler putMultiple ThriftHBaseServiceHandler java 268   at sun reflect GeneratedMethodAccessor11 invoke Unknown Source   at sun reflect DelegatingMethodAccessorImpl invoke DelegatingMethodAccessorImpl java 43   at java lang reflect Method invoke Method java 497   at org apache hadoop hbase thrift2 ThriftHBaseServiceHandler THBaseServiceMetricsProxy invoke ThriftHBaseServiceHandler java 114   at com sun proxy  Proxy10 putMultiple Unknown Source   at org apache hadoop hbase thrift2 generated THBaseService Processor putMultiple getResult THBaseService java 1637   at org apache hadoop hbase thrift2 generated THBaseService Processor putMultiple getResult THBaseService java 1621   at org apache thrift ProcessFunction process ProcessFunction java 39   at org apache thrift TBaseProcessor process TBaseProcessor java 39   at org apache thrift server AbstractNonblockingServer FrameBuffer invoke AbstractNonblockingServer java 478   at org apache thrift server Invocation run Invocation java 18   at org apache hadoop hbase thrift CallQueue Call run CallQueue java 64   at java util concurrent ThreadPoolExecutor runWorker ThreadPoolExecutor java 1142   at java util concurrent ThreadPoolExecutor Worker run ThreadPoolExecutor java 617   at java lang Thread run Thread java 745
14926	Hung ThriftServer  no timeout on read from client  if client crashes  worker thread gets stuck readingThrift server is hung  All worker threads are doing this      thrift worker 0  daemon prio 10 tid 0x00007f0bb95c2800 nid 0xf6a7 runnable  0x00007f0b956e0000   java lang Thread State  RUNNABLE  at java net SocketInputStream socketRead0 Native Method   at java net SocketInputStream read SocketInputStream java 152   at java net SocketInputStream read SocketInputStream java 122   at java io BufferedInputStream fill BufferedInputStream java 235   at java io BufferedInputStream read1 BufferedInputStream java 275   at java io BufferedInputStream read BufferedInputStream java 334     locked &lt 0x000000066d859490&gt   a java io BufferedInputStream   at org apache thrift transport TIOStreamTransport read TIOStreamTransport java 127   at org apache thrift transport TTransport readAll TTransport java 84   at org apache thrift transport TFramedTransport readFrame TFramedTransport java 129   at org apache thrift transport TFramedTransport read TFramedTransport java 101   at org apache thrift transport TTransport readAll TTransport java 84   at org apache thrift protocol TCompactProtocol readByte TCompactProtocol java 601   at org apache thrift protocol TCompactProtocol readMessageBegin TCompactProtocol java 470   at org apache thrift TBaseProcessor process TBaseProcessor java 27   at org apache hadoop hbase thrift TBoundedThreadPoolServer ClientConnnection run TBoundedThreadPoolServer java 289   at org apache hadoop hbase thrift CallQueue Call run CallQueue java 64   at java util concurrent ThreadPoolExecutor runWorker ThreadPoolExecutor java 1145   at java util concurrent ThreadPoolExecutor Worker run ThreadPoolExecutor java 615   at java lang Thread run Thread java 745    They never recover  I don t have client side logs  We ve been here before  HBASE 4967  connected client thrift sockets should have a server side read timeout  but this patch only got applied to fb branch  and thrift has changed since then
15001	Thread Safety issues in ReplicationSinkManager and HBaseInterClusterReplicationEndpoint ReplicationSinkManager is not thread safe  This can cause problems in HBaseInterClusterReplicationEndpoint   when the walprovider is multiwal  For example  1  When multiple threads report bad sinks  the sink list can be non empty but report a negative size because the ArrayList itself is not thread safe  2  HBaseInterClusterReplicationEndpoint depends on the number of sinks to batch edits for shipping  However  it s quite possible that the following code makes it assume that there are no batches to process  sink size is non zero  but by the time we reach the  batching  part  sink size becomes zero    if  replicationSinkMgr getSinks   size      0     return false           int n   Math min Math min this maxThreads  entries size   100 1    replicationSinkMgr getSinks   size         Update  This leads to ArithmeticException  division by zero at     entryLists get Math abs Bytes hashCode e getKey   getEncodedRegionName    n   add e      which is benign and will just lead to retries by the ReplicationSource  The idea is to make all operations in ReplicationSinkManager thread safe and do a verification on the size of replicated edits before we report success
15030	Deadlock in master TableNamespaceManager while running IntegrationTestDDLMasterFailoverI was running IntegrationTestDDLMasterFailover on distributed cluster when i notice this  Here is relevant part of master s jstack     ProcedureExecutor 1  daemon prio 10 tid 0x00007fd2d407f800 nid 0x3332 waiting for monitor entry  0x00007fd2c2834000   java lang Thread State  BLOCKED  on object monitor   at org apache hadoop hbase master TableNamespaceManager releaseExclusiveLock TableNamespaceManager java 157     waiting to lock &lt 0x0000000725c36a48&gt   a org apache hadoop hbase master TableNamespaceManager   at org apache hadoop hbase master procedure CreateNamespaceProcedure releaseLock CreateNamespaceProcedure java 216   at org apache hadoop hbase master procedure CreateNamespaceProcedure releaseLock CreateNamespaceProcedure java 43   at org apache hadoop hbase procedure2 ProcedureExecutor execLoop ProcedureExecutor java 842   at org apache hadoop hbase procedure2 ProcedureExecutor execLoop ProcedureExecutor java 794   at org apache hadoop hbase procedure2 ProcedureExecutor access 400 ProcedureExecutor java 75   at org apache hadoop hbase procedure2 ProcedureExecutor 2 run ProcedureExecutor java 479     Locked ownable synchronizers     &lt 0x000000072574b330&gt   a java util concurrent locks ReentrantReadWriteLock NonfairSync      ProcedureExecutor 3  daemon prio 10 tid 0x00007fd2d41e5800 nid 0x3334 waiting on condition  0x00007fd2c2632000   java lang Thread State  TIMED_WAITING  parking   at sun misc Unsafe park Native Method     parking to wait for  &lt 0x000000072574b330&gt   a java util concurrent locks ReentrantReadWriteLock NonfairSync   at java util concurrent locks LockSupport parkNanos LockSupport java 226   at java util concurrent locks AbstractQueuedSynchronizer doAcquireNanos AbstractQueuedSynchronizer java 929   at java util concurrent locks AbstractQueuedSynchronizer tryAcquireNanos AbstractQueuedSynchronizer java 1245   at java util concurrent locks ReentrantReadWriteLock WriteLock tryLock ReentrantReadWriteLock java 1115   at org apache hadoop hbase master TableNamespaceManager acquireExclusiveLock TableNamespaceManager java 150     locked &lt 0x0000000725c36a48&gt   a org apache hadoop hbase master TableNamespaceManager   at org apache hadoop hbase master procedure CreateNamespaceProcedure acquireLock CreateNamespaceProcedure java 210   at org apache hadoop hbase master procedure CreateNamespaceProcedure acquireLock CreateNamespaceProcedure java 43   at org apache hadoop hbase procedure2 ProcedureExecutor executeRollback ProcedureExecutor java 941   at org apache hadoop hbase procedure2 ProcedureExecutor execLoop ProcedureExecutor java 821   at org apache hadoop hbase procedure2 ProcedureExecutor execLoop ProcedureExecutor java 794   at org apache hadoop hbase procedure2 ProcedureExecutor access 400 ProcedureExecutor java 75   at org apache hadoop hbase procedure2 ProcedureExecutor 2 run ProcedureExecutor java 479     Locked ownable synchronizers     None    Found one Java level deadlock                                   ProcedureExecutor 3    waiting for ownable synchronizer 0x000000072574b330   a java util concurrent locks ReentrantReadWriteLock NonfairSync    which is held by  ProcedureExecutor 1    ProcedureExecutor 1    waiting to lock monitor 0x00007fd2cc328908  object 0x0000000725c36a48  a org apache hadoop hbase master TableNamespaceManager    which is held by  ProcedureExecutor 3     Java stack information for the threads listed above                                                         ProcedureExecutor 3    at sun misc Unsafe park Native Method     parking to wait for  &lt 0x000000072574b330&gt   a java util concurrent locks ReentrantReadWriteLock NonfairSync   at java util concurrent locks LockSupport parkNanos LockSupport java 226   at java util concurrent locks AbstractQueuedSynchronizer doAcquireNanos AbstractQueuedSynchronizer java 929   at java util concurrent locks AbstractQueuedSynchronizer tryAcquireNanos AbstractQueuedSynchronizer java 1245   at java util concurrent locks ReentrantReadWriteLock WriteLock tryLock ReentrantReadWriteLock java 1115   at org apache hadoop hbase master TableNamespaceManager acquireExclusiveLock TableNamespaceManager java 150     locked &lt 0x0000000725c36a48&gt   a org apache hadoop hbase master TableNamespaceManager   at org apache hadoop hbase master procedure CreateNamespaceProcedure acquireLock CreateNamespaceProcedure java 210   at org apache hadoop hbase master procedure CreateNamespaceProcedure acquireLock CreateNamespaceProcedure java 43   at org apache hadoop hbase procedure2 ProcedureExecutor executeRollback ProcedureExecutor java 941   at org apache hadoop hbase procedure2 ProcedureExecutor execLoop ProcedureExecutor java 821   at org apache hadoop hbase procedure2 ProcedureExecutor execLoop ProcedureExecutor java 794   at org apache hadoop hbase procedure2 ProcedureExecutor access 400 ProcedureExecutor java 75   at org apache hadoop hbase procedure2 ProcedureExecutor 2 run ProcedureExecutor java 479    ProcedureExecutor 1    at org apache hadoop hbase master TableNamespaceManager releaseExclusiveLock TableNamespaceManager java 157     waiting to lock &lt 0x0000000725c36a48&gt   a org apache hadoop hbase master TableNamespaceManager   at org apache hadoop hbase master procedure CreateNamespaceProcedure releaseLock CreateNamespaceProcedure java 216   at org apache hadoop hbase master procedure CreateNamespaceProcedure releaseLock CreateNamespaceProcedure java 43   at org apache hadoop hbase procedure2 ProcedureExecutor execLoop ProcedureExecutor java 842   at org apache hadoop hbase procedure2 ProcedureExecutor execLoop ProcedureExecutor java 794   at org apache hadoop hbase procedure2 ProcedureExecutor access 400 ProcedureExecutor java 75   at org apache hadoop hbase procedure2 ProcedureExecutor 2 run ProcedureExecutor java 479     Found 1 deadlock     I will try to dig more info about why this happened logs not showing much
15093	Replication can report incorrect size of log queue for the global source when multiwal is enabled  Replication can report incorrect size for the size of log queue for the global source when multiwal is enabled  This happens because the method MetricsSource setSizeofLogQueue performs non trivial operations in a multithreaded world  even though it is not synchronized  We can simply divide MetricsSource setSizeofLogQueue into MetricsSource incrSizeofLogQueue and MetricsSource decrSizeofLogQueue  Not sure why we are currently directly setting the size instead of incrementing decrementing it
15146	Don t block on Reader threads queueing to a scheduler queueBlocking on the epoll thread is awful  The new rpc scheduler can have lots of different queues  Those queues have different capacity limits  Currently the dispatch method can block trying to add the the blocking queue in any of the schedulers  This causes readers to block  tcp acks are delayed  and everything slows down
15650	Remove TimeRangeTracker as point of contention when many threads reading a StoreFile  HBASE 12148 is about  Remove TimeRangeTracker as point of contention when many threads writing a Store   It is also a point of contention when reading
15957	RpcClientImpl close never ends in some circumstances  This bug is related to HBASE 14241 and HBASE 13851  Fix for HBASE 13851 introduced the check for non alive connections and if connection is not alive  it close it   if  conn isAlive       if  connsToClose    null     connsToClose   new HashSet&lt Connection&gt         connsToClose add conn             if  connsToClose   null     for  Connection conn   connsToClose     if  conn markClosed new InterruptedIOException  RpcClient is closing        conn close                  That worked fine until fix for HBASE 14241 introduced handling for interrupt in writer thread   try    cts   callsToWrite take       catch  InterruptedException e     markClosed new InterruptedIOException           So  if writer thread is running  but connection thread is not started yet  interrupt will cause calling of markClosed which will set shouldCloseConnection flag for the parent connection  And the next time during the handling of non alive connections markClosed will return false and close will not be called  As the result connection will not be removed from the connections pool and RpcClientImpl close never finish
16081	Replication remove_peer gets stuck and blocks WAL rollingWe use a blocking take from CompletionService in HBaseInterClusterReplicationEndpoint  When we remove a peer  we try to shut down all threads gracefully  But  under certain race condition  the underlying executor gets shutdown and the CompletionService take will block forever  which means the remove_peer call will never gracefully finish  Since ReplicationSourceManager removePeer and ReplicationSourceManager recordLog lock on the same object  we are not able to roll WALs in such a situation and will end up with gigantic WALs
16144	Replication queue s lock will live forever if RS acquiring the lock has died prematurelyIn default  we will use multi operation when we claimQueues from ZK  But if we set hbase zookeeper useMulti false  we will add a lock first  then copy nodes  finally clean old queue and the lock  However  if the RS acquiring the lock crash before claimQueues done  the lock will always be there and other RS can never claim the queue
16304	HRegion RegionScannerImpl handleFileNotFoundException may lead to deadlock when trying to obtain write lock on updatesLockhere is my jvm stack   2016 07 29 16 36 56  Full thread dump Java HotSpot TM  64 Bit Server VM  24 72 b04 mixed mode     Timer for &amp apos HBase&amp apos  metrics system  daemon prio 10 tid 0x00007f205cf38000 nid 0xafa5 in Object wait    0x00007f203b353000   java lang Thread State  TIMED_WAITING  on object monitor   at java lang Object wait Native Method   at java util TimerThread mainLoop Timer java 552     locked &lt 0x000000063503c790&gt   a java util TaskQueue   at java util TimerThread run Timer java 505      Attach Listener  daemon prio 10 tid 0x00007f205d017800 nid 0x1300 waiting on condition  0x0000000000000000   java lang Thread State  RUNNABLE     IPC Parameter Sending Thread  2  daemon prio 10 tid 0x00007f205c7c4000 nid 0x4f1a waiting on condition  0x00007f20362e1000   java lang Thread State  TIMED_WAITING  parking   at sun misc Unsafe park Native Method     parking to wait for  &lt 0x000000066f996718&gt   a java util concurrent SynchronousQueue TransferStack   at java util concurrent locks LockSupport parkNanos LockSupport java 226   at java util concurrent SynchronousQueue TransferStack awaitFulfill SynchronousQueue java 460   at java util concurrent SynchronousQueue TransferStack transfer SynchronousQueue java 359   at java util concurrent SynchronousQueue poll SynchronousQueue java 942   at java util concurrent ThreadPoolExecutor getTask ThreadPoolExecutor java 1068   at java util concurrent ThreadPoolExecutor runWorker ThreadPoolExecutor java 1130   at java util concurrent ThreadPoolExecutor Worker run ThreadPoolExecutor java 615   at java lang Thread run Thread java 745      RS_LOG_REPLAY_OPS hadoop datanode 0042 16020 1  prio 10 tid 0x00007f2054ec8000 nid 0x832d waiting on condition  0x00007f2039a18000   java lang Thread State  WAITING  parking   at sun misc Unsafe park Native Method     parking to wait for  &lt 0x000000066ffb5950&gt   a java util concurrent locks AbstractQueuedSynchronizer ConditionObject   at java util concurrent locks LockSupport park LockSupport java 186   at java util concurrent locks AbstractQueuedSynchronizer ConditionObject await AbstractQueuedSynchronizer java 2043   at java util concurrent LinkedBlockingQueue take LinkedBlockingQueue java 442   at java util concurrent ThreadPoolExecutor getTask ThreadPoolExecutor java 1068   at java util concurrent ThreadPoolExecutor runWorker ThreadPoolExecutor java 1130   at java util concurrent ThreadPoolExecutor Worker run ThreadPoolExecutor java 615   at java lang Thread run Thread java 745      RS_LOG_REPLAY_OPS hadoop datanode 0042 16020 0  prio 10 tid 0x00007f20542ca800 nid 0x5a5d waiting on condition  0x00007f2033bba000   java lang Thread State  WAITING  parking   at sun misc Unsafe park Native Method     parking to wait for  &lt 0x000000066ffb5950&gt   a java util concurrent locks AbstractQueuedSynchronizer ConditionObject   at java util concurrent locks LockSupport park LockSupport java 186   at java util concurrent locks AbstractQueuedSynchronizer ConditionObject await AbstractQueuedSynchronizer java 2043   at java util concurrent LinkedBlockingQueue take LinkedBlockingQueue java 442   at java util concurrent ThreadPoolExecutor getTask ThreadPoolExecutor java 1068   at java util concurrent ThreadPoolExecutor runWorker ThreadPoolExecutor java 1130   at java util concurrent ThreadPoolExecutor Worker run ThreadPoolExecutor java 615   at java lang Thread run Thread java 745      hadoop datanode 0042 corp cootek com 16020 1469690065288_ChoreService_2  daemon prio 10 tid 0x00007f205d0d4000 nid 0x72af waiting on condition  0x00007f203b151000   java lang Thread State  WAITING  parking   at sun misc Unsafe park Native Method     parking to wait for  &lt 0x000000066fd70dd8&gt   a java util concurrent locks AbstractQueuedSynchronizer ConditionObject   at java util concurrent locks LockSupport park LockSupport java 186   at java util concurrent locks AbstractQueuedSynchronizer ConditionObject await AbstractQueuedSynchronizer java 2043   at java util concurrent ScheduledThreadPoolExecutor DelayedWorkQueue take ScheduledThreadPoolExecutor java 1079   at java util concurrent ScheduledThreadPoolExecutor DelayedWorkQueue take ScheduledThreadPoolExecutor java 807   at java util concurrent ThreadPoolExecutor getTask ThreadPoolExecutor java 1068   at java util concurrent ThreadPoolExecutor runWorker ThreadPoolExecutor java 1130   at java util concurrent ThreadPoolExecutor Worker run ThreadPoolExecutor java 615   at java lang Thread run Thread java 745      RS_CLOSE_REGION hadoop datanode 0042 16020 2  prio 10 tid 0x0000000002232000 nid 0xab85 waiting on condition  0x00007f2033ab9000   java lang Thread State  WAITING  parking   at sun misc Unsafe park Native Method     parking to wait for   worker 0  Parallel CMS Threads   prio 10 tid 0x00007f205c25b000 nid 0x8edb runnable     Gang worker 1  Parallel CMS Threads   prio 10 tid 0x00007f205c25d000 nid 0x8edc runnable     Gang worker 2  Parallel CMS Threads   prio 10 tid 0x00007f205c25e800 nid 0x8edd runnable     Gang worker 3  Parallel CMS Threads   prio 10 tid 0x00007f205c260800 nid 0x8ede runnable     Gang worker 4  Parallel CMS Threads   prio 10 tid 0x00007f205c262800 nid 0x8edf runnable     Gang worker 5  Parallel CMS Threads   prio 10 tid 0x00007f205c264000 nid 0x8ee0 runnable     VM Periodic Task Thread  prio 10 tid 0x00007f205c2df800 nid 0x8eea waiting on condition    JNI global references  221
16367	Race between master and region server initialization may lead to premature server abort  I was troubleshooting a case where hbase  1 1 2  master always dies shortly after start   see attached master log snippet  It turned out that master initialization thread was racing with HRegionServer preRegistrationInitialization    initializeZooKeeper  actually  since HMaster extends HRegionServer  Through additional logging in master     this oldLogDir   createInitialFileSystemLayout     HFileSystem addLocationsOrderInterceptor conf    LOG info  creating splitLogManager       I found that execution didn t reach the last log line before region server declared cluster Id being null
16429	FSHLog  deadlock if rollWriter called when ring buffer filled with appendsRecently we experienced an online problem that all handlers are stuck  Checking the jstack we could see all handler threads waiting for RingBuffer next  while the single ring buffer consumer dead waiting for safePointReleasedLatch to count down   Normal handler thread    B defaultRpcServer handler 126 queue 9 port 16020  daemon prio 10 tid 0x00007efd4b44f800 nid 0x15f29 runnable  0x00007efd3db7b000   java lang Thread State  TIMED_WAITING  parking   at sun misc Unsafe park Native Method   at java util concurrent locks LockSupport parkNanos LockSupport java 349   at com lmax disruptor MultiProducerSequencer next MultiProducerSequencer java 136   at com lmax disruptor MultiProducerSequencer next MultiProducerSequencer java 105   at com lmax disruptor RingBuffer next RingBuffer java 246   at org apache hadoop hbase regionserver wal FSHLog append FSHLog java 1222   at org apache hadoop hbase regionserver HRegion doMiniBatchMutation HRegion java 3188   at org apache hadoop hbase regionserver HRegion batchMutate HRegion java 2879   at org apache hadoop hbase regionserver HRegion batchMutate HRegion java 2819   at org apache hadoop hbase regionserver RSRpcServices doBatchOp RSRpcServices java 736   at org apache hadoop hbase regionserver RSRpcServices doNonAtomicRegionMutation RSRpcServices java 698   at org apache hadoop hbase regionserver RSRpcServices multi RSRpcServices java 2095   at org apache hadoop hbase protobuf generated ClientProtos ClientService 2 callBlockingMethod ClientProtos java 32213   at org apache hadoop hbase ipc RpcServer call RpcServer java 774   at org apache hadoop hbase ipc CallRunner run CallRunner java 102   at org apache hadoop hbase ipc RpcExecutor consumerLoop RpcExecutor java 133   at org apache hadoop hbase ipc RpcExecutor 1 run RpcExecutor java 108   at java lang Thread run Thread java 756     RingBufferEventHandler thread waiting for safePointReleasedLatch    regionserver hadoop0369 et2 tbsite net 11 251 152 226 16020 append pool2 t1  prio 10 tid 0x00007efd320d0000 nid 0x1777b waiting on condition  0x00007efd2d2fa000   java lang Thread State  WAITING  parking   at sun misc Unsafe park Native Method     parking to wait for  &lt 0x00007f01b48d9178&gt   a java util concurrent CountDownLatch Sync   at java util concurrent locks LockSupport park LockSupport java 186   at java util concurrent locks AbstractQueuedSynchronizer parkAndCheckInterrupt AbstractQueuedSynchronizer java 834   at java util concurrent locks AbstractQueuedSynchronizer doAcquireSharedInterruptibly AbstractQueuedSynchronizer java 994   at java util concurrent locks AbstractQueuedSynchronizer acquireSharedInterruptibly AbstractQueuedSynchronizer java 1303   at java util concurrent CountDownLatch await CountDownLatch java 236   at org apache hadoop hbase regionserver wal FSHLog SafePointZigZagLatch safePointAttained FSHLog java 1866   at org apache hadoop hbase regionserver wal FSHLog RingBufferEventHandler attainSafePoint FSHLog java 2066   at org apache hadoop hbase regionserver wal FSHLog RingBufferEventHandler onEvent FSHLog java 2029   at org apache hadoop hbase regionserver wal FSHLog RingBufferEventHandler onEvent FSHLog java 1909   at com lmax disruptor BatchEventProcessor run BatchEventProcessor java 128   at java util concurrent ThreadPoolExecutor runWorker ThreadPoolExecutor java 1145   at java util concurrent ThreadPoolExecutor Worker run ThreadPoolExecutor java 615   at java lang Thread run Thread java 756     FSHLog replaceWriter will call SafePointZigZagLatch releaseSafePoint to count down safePointReleasedLatch  but replaceWriter got stuck when trying to publish a sync onto ring buffer    regionserver hadoop0369 et2 tbsite net 11 251 152 226 16020 logRoller  daemon prio 10 tid 0x00007efd320c8800 nid 0x16123 runnable  0x00007efd311f6000   java lang Thread State  TIMED_WAITING  parking   at sun misc Unsafe park Native Method   at java util concurrent locks LockSupport parkNanos LockSupport java 349   at com lmax disruptor MultiProducerSequencer next MultiProducerSequencer java 136   at com lmax disruptor MultiProducerSequencer next MultiProducerSequencer java 105   at com lmax disruptor RingBuffer next RingBuffer java 246   at org apache hadoop hbase regionserver wal FSHLog publishSyncOnRingBuffer FSHLog java 1481   at org apache hadoop hbase regionserver wal FSHLog publishSyncOnRingBuffer FSHLog java 1477   at org apache hadoop hbase regionserver wal FSHLog replaceWriter FSHLog java 957   at org apache hadoop hbase regionserver wal FSHLog rollWriter FSHLog java 726   at org apache hadoop hbase regionserver LogRoller run LogRoller java 148   at java lang Thread run Thread java 756     Thus deadlock happens  A brief process of how deadlock forms   ring buffer filled with appends   &gt  rollWriter happens   &gt  the only consumer of ring buffer waiting for safePointReleasedLatch   &gt  rollWriter cannot publish sync since ring buffer is full   &gt  rollWriter won&amp apos t release safePointReleasedLatch    This JIRA targeting at resolve this issue  and will add a UT to cover the case
16699	Overflows in AverageIntervalRateLimiter&apos s refill   and getWaitInterval    It seems that there are more overflow in other places  and a concurrent issue  I will post a patch within one or 2 days after I figure out adding new unittest cases  Please see the following two lines  Once it overflows  it will cause wrong behavior  For unconfigured RateLimiters  we should have simpler logic to byPass the check  https   github com apache hbase blob master hbase server src main java org apache hadoop hbase quotas AverageIntervalRateLimiter java L37 https   github com apache hbase blob master hbase server src main java org apache hadoop hbase quotas AverageIntervalRateLimiter java L51
16721	Concurrency issue in WAL unflushed seqId tracking  I m inspecting an interesting case where in a production cluster  some regionservers ends up accumulating hundreds of WAL files  even with force flushes going on due to max logs  This happened multiple times on the cluster  but not on other clusters  The cluster has periodic memstore flusher disabled  however  this still does not explain why the force flush of regions due to max limit is not working  I think the periodic memstore flusher just masks the underlying problem  which is why we do not see this in other clusters  The problem starts like this    2016 09 21 17 49 18 272 INFO   regionserver  10 2 0 55 16020 logRoller  wal FSHLog  Too many wals  logs 33  maxlogs 32  forcing flush of 1 regions s   d4cf39dc40ea79f5da4d0cf66d03cb1f  2016 09 21 17 49 18 273 WARN   regionserver  10 2 0 55 16020 logRoller  regionserver LogRoller  Failed to schedule flush of d4cf39dc40ea79f5da4d0cf66d03cb1f  region null  requester null   then  it continues until the RS is restarted    2016 09 23 17 43 49 356 INFO   regionserver  10 2 0 55 16020 logRoller  wal FSHLog  Too many wals  logs 721  maxlogs 32  forcing flush of 1 regions s   d4cf39dc40ea79f5da4d0cf66d03cb1f  2016 09 23 17 43 49 357 WARN   regionserver  10 2 0 55 16020 logRoller  regionserver LogRoller  Failed to schedule flush of d4cf39dc40ea79f5da4d0cf66d03cb1f  region null  requester null   The problem is that region d4cf39dc40ea79f5da4d0cf66d03cb1f is already split some time ago  and was able to flush its data and split without any problems  However  the FSHLog still thinks that there is some unflushed data for this region
16788	Race in compacted file deletion between HStore close   and closeAndArchiveCompactedFiles    HBASE 13082 changed the way that compacted files are archived from being done inline on compaction completion to an async cleanup by the CompactedHFilesDischarger chore  It looks like the changes to HStore to support this introduced a race condition in the compacted HFile archiving  In the following sequence  we can wind up with two separate threads trying to archive the same HFiles  causing a regionserver abort   compaction completes normally and the compacted files are added to compactedfiles in HStore s DefaultStoreFileManager  threadA  CompactedHFilesDischargeHandler runs in a RS executor service  calling closeAndArchiveCompactedFiles     obtains HStore readlock  gets a copy of compactedfiles  releases readlock    threadB  calls HStore close   as part of region close   obtains HStore writelock  calls DefaultStoreFileManager clearCompactedfiles    getting a copy of same compactedfiles   threadA  calls HStore removeCompactedfiles compactedfiles    archives files in  compactedfiles   in HRegionFileSystem removeStoreFiles   call HStore clearCompactedFiles   waits on write lock    threadB  continues with close     calls removeCompactedfiles compactedfiles   calls HRegionFIleSystem removeStoreFiles   and HFileArchiver archiveStoreFiles    receives FileNotFoundException because the files have already been archived by thread A throws IOException   RS aborts   I think the combination of fetching the compactedfiles list and removing the files needs to be covered by locking  Options I see are   Modify HStore closeAndArchiveCompactedFiles    use writelock instead of readlock and move the call to removeCompactedfiles   inside the lock  This means the read operations will be blocked while the files are being archived  which is bad  Synchronize closeAndArchiveCompactedFiles   and modify close   to call it instead of calling removeCompactedfiles   directly  Add a separate lock for compacted files removal and use in closeAndArchiveCompactedFiles   and close