17	Race for Netty between cancel and stream creation AbstractClientStream cancel won t cancel the stream on the wire if it appears the stream has not yet been allocated  as is described by the comment     Only send a cancellation to remote side if we have actually been allocated    a stream id and we are not already closed  i e  the server side is aware of the stream  However  what happens if this is the case  is that the transport is not notified of the stream destruction  and the stream will still eventually be created by the transport and not be cancelled  This issue does not seem a problem with the OkHttp transport  since it allocates the stream id before returning any newly created stream  However  Netty delays id allocation until just before the stream headers are sent  which 1  is always done asynchronously and 2  may be strongly delayed due to MAX_CONCURRENT_STREAMS  It appears that the optimization in AbstractClientStream should be removed outright and sendCancel s doc be updated to specify the expectation to handle such cases  as opposed to directly cause RST_STREAM   Both OkHttp and Netty seem to be handling such cases already  More importantly  the optimization seems highly prone for races given that id allocation is occurring in the transport thread whereas AbstractClientStream cancel is happening on some application thread  using the normal synchronization between application and transport threads seems more than efficient enough and simpler
120	Remove blocking parts from NettyClientTransport NettyClientTransport newStream is currently a blocking operation  It blocks until the HEADERS frame has been written on the wire  This is behavior is not what people who use our asynchronous API would come to expect  The blocking also is the cause for severe performance issues in the QPS Client as it results in more or less in as many threads being created as there are concurrent calls going on  We have seen ~850 Threads for 1000 concurrent calls  resulting in OOM   The blocking may also lead to deadlocking the EventLoop in cases where a DirectExecutor is used  One scenario where a deadlock might happen is when the EventLoop is not able to completely flush the HEADERS frame on the wire because then Netty would internally create a task to flush the remaining bytes and put this task in its task queue  This task can never be completed though as the EventLoop Thread is blocked by our very own newStream method waiting for the task to be completed     This issue depends on  116 and  118 to be resolved first
238	Race in Server handler initialization When initializing an incoming client connection  we call startAsync   on the transport  which registers the handler on a separate thread  This is obviously a race  and it would have probably been fixed if I had finished Service removal in  35  Symptom  DEBUG i n channel DefaultChannelPipeline   Discarded inbound message SimpleLeakAwareByteBuf PooledUnsafeDirectByteBuf ridx  0  widx  259  cap  1024   that reached at the tail of the pipeline  Please check your pipeline configuration   The quickest fix would be to call awaitRunning   from initChannel    That reduces the rate new connections can connect  but is probably the most expedient solution  until  35 is finished   nmittler  thoughts
330	OkHttpClientTransport onGoAway   races with startPendingStreams   onGoAway has two phases  do things necessary under lock and final cleanup  In the first phase it collects the streams to terminate in the second and sets goAway  startPendingStreams   does not observe goAway and also creates new streams that should be failed due to the goAway  From an initial look  it seems it would be best to remove failPendingStreams   and simply integrate its two phases into onGoAway   s two phases  that is  when holding the lock in onGoAway  replace pendingStreams with an empty list  and then when not holding the lock call transportReportStatus
583	OkHttp s cancellation is not properly synchronized OkHttpClientStream sendCancel   calls finishStream   from an application thread  But finishStream   calls transportReportStatus   without any lock held  That is not synchronized correctly  as transportReportStatus   may only be called from the transport thread  i e   while lock is held   It seems that all usages of streams is done while lock is held except for within finishStream   and data    data   can actually race with finishStream   and end up sending DATA frames after the RST_STREAM  It seems it would be best to just have stream protected by lock  because it having its own synchronization isn t providing much benefit and isn t leading to correct code
605	BufferingHttp2ConnectionEncoder does not shutdown properly on channelInactive  nmittler There is a nasty race condition during the handling of channelInactive in NettyClientHandler which goes a bit like this      NettyClientHandler channelInactive    for each active stream report closure to GRPC NettyClientHandler channelInactive    Http2ConnectionHandler channelInactive    Http2ConnectionHandler BaseDecoder channelInactive    for each active stream call close    BufferingHttp2ConnectionEncoder Http2ConnectionAdapter onStreamClose    try creating new stream    adds stream to active list  OOPS! this stream is never closed   This reproduces for NettyClientTransportTest bufferedStreamsShouldBeClosedWhenTransportTerminates with 5 0beta5  Having streams being created as a side effect of channel inactivation is undesirable  Potential fixes include  Reorder teardown in Http2ConnectionHandler BaseDecoder channelInactive so encoders are closed   before streams are closed  Make BufferedHttp2ConnectionEncoder check channel isActive   when trying to create streams
696	In process transport deadlock during shutdown Simultaneously shutting down both server and client sharing the same in process transport can lead to a deadlock  During server shutdown  the transport lock is held while calling transportShutdown on the channel listener  which attempts to lock the channel  At the same time  channel shutdownNow   holds the channel lock while also trying to lock the transport which leads to a deadlock  Found one Java level deadlock                                 AccountServer STOPPING   waiting to lock monitor 0x00007f88221d72a8  object 0x000000076eb28a20  a io grpc ChannelImpl   which is held by  main   main   waiting to lock monitor 0x00007f8824015488  object 0x000000076c2afb38  a io grpc transport inprocess InProcessTransport   which is held by  AccountServer STOPPING   Java stack information for the threads listed above                                                       AccountServer STOPPING     waiting to lock  0x000000076eb28a20   a io grpc ChannelImpl    locked  0x000000076c2afb38   a io grpc transport inprocess InProcessTransport    locked  0x000000076c2afb38   a io grpc transport inprocess InProcessTransport    locked  0x000000076bfe81a8   a io grpc ServerImpl    locked  0x000000076be7fdc0   a io grpc transport inprocess InProcessServer    locked  0x000000076bfe81a8   a io grpc ServerImpl   main     waiting to lock  0x000000076c2afb38   a io grpc transport inprocess InProcessTransport    locked  0x000000076eb28a20   a io grpc ChannelImpl    locked  0x000000076eb28a20   a io grpc ChannelImpl   Found 1 deadlock
887	OkHttp  race between sendCancel and sendFrame  If sendCancel is called  by timeout for example  before the stream is started  a following sendFrame will cause a NPE  java lang NullPointerException
999	Possible race condition ServerImpl between start   and shutdown   I believe it may be possible if start and stop are called concurrently that the shared executor may not get released   I m not sure if this is an actual problem  but it does go against the   ThreadSafe annotation
1253	ClientCallImpl operations race with Context cancellation  With be60086  we don t create the stream when the Context is cancelled  so the following request    sendMessage    halfClose   will encounter an IllegalStateException like  java lang IllegalStateException  Not started   louiscryan  FYI  I ll send you a PR to fix it soon
1343	Deadline can fire before stream started In ClientCallImpl the deadline is scheduled before stream start    However  if the deadline has already elapsed the runnable will be executed immediately and race with the start  I ve only looked into how OkHttp may be impacted  I believe a NullPointerException would be thrown when trying to notify the stream listener due to the cancellation  However  due to  1237 the exception won t be logged  Thus  this will result in a hung stream that never completes with no logging as to what went wrong  This was discovered due to timeout_on_sleeping_server on android being flaky  because it uses a very small timeout  The test would fail at awaitCompletion   carl mastrangelo  FYI
1408	Potential risk of deadlock from calling listeners under locks Methods of ClientTransport Listener and ServerTransportListener are usually called under a lock  The biggest reason for locking is to guarantee the ordering of multiple methods on the same listener  However  these listeners usually call into channel layer code  and may in turn acquire locks from there  which forms a transport lock    channel lock lock order  On the other hand  when channel layer calls into transport layer  it s possible to form a channel lock    transport lock lock order  which makes deadlock possible  It s unlikely an issue today because there is an implicit rule today that channel layer will not hold any lock while calling into transport  However  as the code base grows  it will become harder to keep track of such requirement  A possible solution is to always schedule listener calls on a serialized executor  with the cost of a thread  so that listener order can be guaranteed without the need of locking  There may be better options
1510	DelayedClientTransport and InProcessTransport means deadlock There is a chance of deadlock when DelayedClientTransport is linked with an InProcessTransport  See  pull 1503
1981	Executor usage in ClientCallImpl races with channel shutdown and termination  ManagedChannelImpl clear scheduledExecutor in shutdown    and releases  which potentially closes  executor in maybeTerminateChannel    Neither newCall   nor ClientCallImpl checks the shutdown state of the channel  ClientCallImpl relies on FailingClientTransport for the expected behavior  However  ClientCallImpl uses the passed in executors anyway  for scheduling the deadline timer and invoking the call listener  If ClientCallImpl tries to schedule a deadline timer after the channel is shut down  it will get a NPE  If it runs the call listener after the shared executor has been closed  which is 1 second  SharedResourceHolder DESTROY_DELAY_SECONDS  after all references are gone  e g   the application calls Call start   that late  it will get a RejectedExecutionException  Our current tests are not testing for the two cases  This doesn t seem to be a serious issue  It only affect people who try to use Calls after the channel has been shutdown  I am yet to figure out a solution  Anyway  it seems executor should be cleared after being returned to the shared pool  like scheduledExecutor
2152	Deadlock found in TransportSet When running benchmarks where the client started up faster than the server  The first few calls failed as unavailable   Our internal deadlock detector seems to think there is a deadlock around here  Deadlock s  found   grpc client net 1 32  daemon prio 5 Id 175 BLOCKED on java lang Object 7eeb2e6b owned by  grpc client app 5  Id 119    grpc client app 5  daemon prio 5 Id 119 BLOCKED on java lang Object 17902cf5 owned by  grpc client net 1 32  Id 175
2246	Deadlock with TransportSet Hello  I was testing Grpc with RoundRobinLB and a custom NameResolver when this deadlock happened   Found one Java level deadlock   grpc timer 0   waiting to lock monitor 0x00007fa1b00062c8  object 0x00000007397d7f88  a java lang Object   which is held by  main   main   waiting to lock monitor 0x00007fa1800087f8  object 0x00000007397d7e00  a java lang Object   which is held by  grpc timer 0   grpc timer 0    waiting to lock  0x00000007397d7f88   a java lang Object  locked  0x00000007397d7e00   a java lang Object    main    waiting to lock  0x00000007397d7e00   a java lang Object  locked  0x00000007397d7f88   a java lang Object    I don t know if it may relate to my own code or if the issue is on grpc side
2388	New deadlock in TransportSet and GrpcTimer Hi  I have encountered a new deadlock in TransportSet  I m running under v1 0 with  2258 cherry picked   Found one Java level deadlock   consumer 63   waiting to lock monitor 0x00007f7ea408ca78  object 0x0000000733ee5700  a java lang Object   which is held by  consumer 35   consumer 35   waiting to lock monitor 0x00007f7e9052a3c8  object 0x000000070c6272f0  a java lang Object   which is held by  grpc timer 0   grpc timer 0   waiting to lock monitor 0x00007f7ea408ca78  object 0x0000000733ee5700  a java lang Object   which is held by  consumer 35  Java stack information for the threads listed above   consumer 63    waiting to lock  0x0000000733ee5700   a java lang Object    consumer 35    waiting to lock  0x000000070c6272f0   a java lang Object  locked  0x0000000733ee5700   a java lang Object    grpc timer 0    waiting to lock  0x0000000733ee5700   a java lang Object  locked  0x000000070c6272f0   a java lang Object
2453	Threading of StatsTraceContext StatsTraceContext assumes non thread safety  which is fine as long as the RPC is closed by the application through the ClientCall ServerCall interface  which are also not thread safe  However  if the RPC is not closed by the application  but either cancelled by the other side  or closed by transport due to errors  which will call callEnded   from the transport thread which is not synchronized with  the application thread  As the application may not be notified about the closure in time  it may still trying to send messages  resulting in wireBytesSent   etc being called after callEnded    which would trigger a check failure  There is also a data race on the counter fields as wireBytesSent   etc write them and callEnded   reads them from different threads without synchronization  We will remove the preconditions checks from writeBytesSent   etc  For the data race  some kind of synchronization would be required  maybe atomics   ejona86
2562	Race between pick and transport shutdown Right now they are done in two steps   A transport that is in READY state is selected newStream   is called on the selected transport   If transport is shutdown  by LoadBalancer or channel idle mode  between the two steps  Step 2 will fail spuriously  Currently we work around this by adding a delay between stopping selecting a subchannel  which owns the transport  and shutting it down  As long as the delay is longer than the time between Step 1 and Step 2  the race won t happen  This is not ideal because it relies on timing to work correctly  and will still fail in extreme cases where the time between the two steps are longer than the pre set delay  It would be a better solution to differentiate the racy shutdown and the intended shutdown  Channel is shutdown for good   In response to racy shutdown  transport selection will be retried  The clientTransportProvider in ManagedChannelImpl is in the best position to do this  because it knows whether the Channel has shutdown  clientTransportProvider would have to call newStream   and start the stream  and return the started stream to ClientCallImpl instead of a transport
2865	Rare race condition in Client While more prominent when using compression  this race occurs without it as well   The typical race looks something like   Client starts and RPC The transport to the server is not yet available  so a DelayedClientTransport is used  The server handles the RPC and sends back headers and a compressed message  The client sees there are headers  and begins executing the queued stream callbacks  on the channel executor threads instead of the transport thread The client sees the Data frame  and tries to decompress it on the network thread    This fails since the headers from 4 have not yet been processed  The stream has already failed  but the queued callback for onHeaders   is finally executed on the app thread   This is the root cause of  2157   As mentioned  this isn t just for compression   ClientInterceptors will see headers after data has been received    The solution  temporary   seems to be to move OkHttp to used AbstractClientStream2 in  2821  and then move decompression from ClientCallImpl to the stream   That will fix the decompression  but not interceptors
3084	Potential deadlock due to calling callbacks while holding a lock InProcessClientStream and InProcessServerStream are synchronized on their own  InProcessClientStream serverStreamListener is called under synchronized  InProcessClientStream this   and vice versa  If the application tries to call methods on ClientCall or ServerCall from within the callbacks  assuming that it has already taken care of the thread safety of the method calls on  Call  objects   a deadlock is possible when direct executor is used  For example    Thread1  InProcessClientStream serverRequested  locks InProcessClientStream this  InProcessClientStream serverStreamListener messageRead   Eventually reaches application callback  which calls ServerCall close   InProcessServerStream close    locks InProcessServerStream this     Thread2  InProcessServerStream clientRequested  locks InProcessServerStream this  InProcessServerStream clientStreamListener messageRead   Eventually reaches application callback  which calls ClientCall close   InProcessClientStream close    locks InProcessClientStream this     As locks are acquired in reverse orders from two threads  a deadlock is possible  The fundamental issue is that we should not call into application code while holding a lock  because we don t know what application code can do thus we can t control the order of subsequent locking  OkHttp has the same issue  because OkHttpClientStream transportDataReceived    which will call into application code  is called under lock  We could use ChannelExecutor  maybe renamed  to prevent calling into callbacks while holding a lock
3207	Data race in TestServiceImpl  WARNING  ThreadSanitizer  data race  pid 982210  Read of size 8 at 0x7fd444897628 by thread T36   26  Generated Stub   Previous write of size 8 at 0x7fd444897628 by thread T38  mutexes  write M173247660336208224    19  Generated Stub
5015	Revisit LoadBalancer API s threading model The LoadBalancer main interface is not thread safe  and is guaranteed to be called from the SynchronizationContext  This has relieved implementors from worrying about synchronization  As for the auxiliary interfaces  SubchannelPicker is intentionally thread safe because it on the critical path  Helper and Subchannel are not on the critical path  we made them thread safe because they are implemented by GRPC and we thought making them thread safe would probably provide more convenience to their callers  However  client side health checking   4932  and our  Google internal  request routing work revealed two use cases where a LoadBalancer may wrap or delegate to another  while adding additional logic  Helper and Subchannel may also be wrapped in the process  For example  HealthCheckingLoadBalancerFactory wraps Helper createSubchannel   to initialize health checking on the created Subchannel  and we find it much easier to implement if createSubchannel   were always called from the SynchronizationContext  which is not the case right now since createSubchannel   is thread safe  In fact  probably all LoadBalancers always call createSubchannel   from the SynchronizationContext  otherwise it may race with handleSubchannelState   and it s non trivial to handle  and will cancel out the benefits of the threading guarantee on the main LoadBalancer interface  Because of the apparent issue with createSubchannel    we are going to suggest always calling it from the SynchronizationContext  and will log a warning if it s not the case  We d like to discuss whether it makes sense to make Helper and Subchannel non thread safe  and require them to be called from SynchronizationContext  My argument for non thread safety  we made Helper and Subchannel thread safe based on the mindset that they is only one implementation which is from GRPC  In fact  a 3rd party developer may want to wrap them and add their own logic  and it now becomes their burden to make their added logic thread safe too  Possible argument for thread safety  Subchannel requestConnection   may be called from the critical path  However  since it doesn t guarantee any action for the caller  the caller can easily enqueue it to the SynchronizationContext
5450	Data race in NameResolve Listener onError NameResolve Listener onError can be called concurrently in different threads  so the following code in onError   impl may have data race  if  haveBackends    null    haveBackends    channelLogger log ChannelLogLevel WARNING   Failed to resolve name   0    error   haveBackends   false
6601	Deadlock on start gRPC server What version of gRPC Java are you using  1 26 0 What is your environment  Linux  Alpine openjdk version  1 8 0_171  OpenJDK Runtime Environment  IcedTea 3 8 0   Alpine 8 171 11 r0  OpenJDK 64 Bit Server VM  build 25 171 b11  mixed mode  Netty   4 1 44 Final Vertx & Vertx gRPC   3 8 4 What did you expect to see  Proper start of gRPC Server What did you see instead  Start sometimes hangs with deadlock Steps to reproduce the bug int_threaddump txt I suppose it s a race condition related to synchronization in gRPC  ServerImpl   await in NetServer start and vertx nettty event loops  probably single threaded   Probably it could happened at any time if someone start gRPC server and concurrently open new client connection to that server  In my case I stopped and started the gGPR server but I m not sure if it is somehow related  Analysis What I see in the thread dump is the following 2 threads that stays in that state  seems  forever     vert x eventloop thread 0   39 prio 10 os_prio 0 tid 0x000055711e379000 nid 0x2d waiting for monitor entry  0x00007fb72abc8000  java lang Thread State  BLOCKED  on object monitor    waiting to lock  0x00000000c559f1a0   a java lang Object    locked  0x00000000c559bfd8   a io grpc netty NettyServer   vert x worker thread 12   214 prio 10 os_prio 0 tid 0x000055711f2d1800 nid 0x418 in Object wait    0x00007fb720656000  java lang Thread State  WAITING  on object monitor    locked  0x00000000c589baa0   a io netty util concurrent PromiseTask    locked  0x00000000c559f1a0   a java lang Object    From what I see in these thread dumps and the code I think that this could be the problem  deadlock    Vertx grpc starts server  ServerImpl start  in vertx blocking thread ServerImpl synchronize on lock and then try  keeping lock  to start server  NetServer start  NetServer start opens a channel  binds to it  and since that moment it  I assume  may receive connections from remote clients It seems  at this time a remote client opens connection to this server  already bound  Then in channel s event loop  probably single threaded  is received initChannel which try to get ServerImpl lock in ServerListenerImpl transportCreated  coudln t because got by ServerImpl start  NetServer start then schedules runnable in channel s event loop and blocks with channelzFuture await   Now  channelzFuture await   waits for a runnable to be executed in channel s event loop  probably single threaded  At this point channelzFuture await keeps ServerImpl lock lock  while the ServerListenerImpl transportCreated occupies blocks  this is what I suppose  the single threaded channel s event loop thus making impossible to process further  I m attaching file with thread dumps of the whole JVM
6641	Deadlock in server transport with multiple ports ServerImpl start   calls NettyServer start   while holding ServerImpl lock  NettyServer start   awaits a submitted runnable in eventloop  However  this pending runnable may never be executed because the eventloop might be executing some other task    like ServerListenerImpl transportCreated     that is trying to acquire ServerImpl lock causing a deadlock  This is a deadlock for multiple port server transport usecase with the same deadlock mechanism as  6601
8190	grpc hang due to the ELG thread placement of NameResolver refresh method What version of gRPC are you using  grpc v1 32 1 What operating system  Linux  Windows  â€¦  and version  Both Linux and Windows What did you do  Implement a customized NameResolver which extends NameResolver  let s call it  CustomizedNameResolver    In the override refresh   method  it makes a grpc call to service discovery agent to retrieve a list of service instances and then resolve them  What did you expect to see  Expect the customized namer resolver works whenever being called and not hang in the existing grpc call  What did you see instead  grpc calls hang in the customized name resolver  particularly on the grpc calling inside overridden refresh   method  We did a thread dump analysis  the problem is the grpc call inside overridden refresh   method is placed in gRPC ELG thread instead of worker thread  which in turns blocks all gRPC traffic causing grpc call hang indefinitely  According to comment on refresh   method  the document does not clearly states that you must delegate a grpc call to a worker background thread to not block other grpc calls  First  is the placement of grpc call inside overridden refresh   method on the grpc ELG thread an expected behavior  Why cannot we delegate it to worker thread by default  Second  some guides and explanations could be added to the document on NameResolver to further clarify  Attach a thread dump on ELG for your reference  Thank you   grpc default worker ELG 1 6    Thread t 428 java lang Thread State  WAITING   parking to wait for  41c8f5eb   a java util concurrent locks AbstractQueuedSynchronizer ConditionObject      at com xxx xxx CustomizedNameResolver refresh com xxx xxx CustomizedNameResolver refresh 59
8536	BinderChannel flow control can get stuck under load What version of gRPC Java are you using  head What is your environment  Linux Android Steps to reproduce the bug We re launching a unary  GetTile  gRPC interaction between two Android processes  Response message can be ~100kb and clients request several tiles one after the other  Telemetry from the field shows that after a while some clients start experiencing DEADLINE_EXCEEDED errors even though other calls to the same server process over different Channels continue to succeed  By lowering BinderTransport TRANSACTION_BYTES_WINDOW and requesting tiles in a loop I can reproduce similar symptoms locally  Using the debugger I can see the server s BinderTransport transmitWindowFull becomes stuck true even though all bytes have been acknowledged by the client  The server is generating response messages but isn t able to put them on the wire  I believe the problem is that BinderTransport sendTransaction   updates transmitWindowFull based on an unsynchronized read of acknowledgedOutgoingBytes  which may not include concurrent updates by  handleAcknowledgedBytes   on another thread  What did you expect to see  Binder transactions should pause when flow control kicks in then resume when enough outstanding bytes are acknowledged  What did you see instead  Outstanding bytes are acknowledged but transmitWindowFull remains true in a way that s inconsistent with acknowledgedOutgoingBytes and numOutgoingBytes
8565	Data race in RetriableStream onReady   As shown in the following stack trace  RetriableStream onReady   is calling isReady   on transport thread  whereas isReady   is calling frame   isClosed    but framer closed is not thread safe  WARNING  ThreadSanitizer  data race  pid 6458  Write of size 1 at 0x0000cef27b09 by thread T36   0 io grpc internal MessageFramer close  V MessageFramer java 320  1 io grpc internal AbstractStream endOfMessages  V AbstractStream java 84  2 io grpc internal AbstractClientStream halfClose  V AbstractClientStream java 194  3 io grpc internal ForwardingClientStream halfClose  V ForwardingClientStream java 72  4 io grpc internal RetriableStream 1HalfCloseEntry runWith Lio grpc internal RetriableStream Substream  V RetriableStream java 655  5 io grpc internal RetriableStream delayOrExecute Lio grpc internal RetriableStream BufferEntry  V RetriableStream java 526  6 io grpc internal RetriableStream halfClose  V RetriableStream java 659  7 io grpc internal ClientCallImpl halfCloseInternal  V ClientCallImpl java 497  8 io grpc internal ClientCallImpl halfClose  V ClientCallImpl java 486  9 io grpc internal DelayedClientCall 6 run  V DelayedClientCall java 360  10 io grpc internal DelayedClientCall delayOrExecute Ljava lang Runnable  V DelayedClientCall java 247  11 io grpc internal DelayedClientCall halfClose  V DelayedClientCall java 357  12 io grpc PartialForwardingClientCall halfClose  V PartialForwardingClientCall java 44  13 io grpc ForwardingClientCall halfClose  V ForwardingClientCall java 22  14 io grpc ForwardingClientCall SimpleForwardingClientCall halfClose  V ForwardingClientCall java 44  15 io grpc PartialForwardingClientCall halfClose  V PartialForwardingClientCall java 44  16 io grpc ForwardingClientCall halfClose  V ForwardingClientCall java 22  17 io grpc ForwardingClientCall SimpleForwardingClientCall halfClose  V ForwardingClientCall java 44  18 io grpc stub ClientCalls asyncUnaryRequestCall Lio grpc ClientCall Ljava lang Object Lio grpc stub ClientCalls StartableListener  V ClientCalls java 309  19 io grpc stub ClientCalls asyncUnaryRequestCall Lio grpc ClientCall Ljava lang Object Lio grpc stub StreamObserver Z V ClientCalls java 294  20 io grpc stub ClientCalls asyncUnaryCall Lio grpc ClientCall Ljava lang Object Lio grpc stub StreamObserver  V ClientCalls java 68  21 com google apphosting runtime grpc GrpcClientContext call Lio grpc Channel Lio grpc MethodDescriptor Ljava lang Object Lio grpc stub StreamObserver  V GrpcClientContext java 46  22 com google apphosting runtime anyrpc GrpcClients GrpcEvaluationRuntimeClient handleRequest Lcom google apphosting runtime anyrpc AnyRpcClientContext Lcom google apphosting base protos RuntimePb UPRequest Lcom google apphosting runtime anyrpc AnyRpcCallbac GrpcClients java
8642	CsdsService not properly synchronized with XdsClient Consider this code from CsdsService     grpc java xds src main java io grpc xds CsdsService java   Lines 152 to 155 in a46560e        static ClientConfig getClientConfigForXdsClient XdsClient xdsClient        ListenersConfigDump ldsConfig   dumpLdsConfig      xdsClient getSubscribedResourcesMetadata ResourceType LDS       xdsClient getCurrentVersion ResourceType LDS         The initial issue is that getSubscribedResourcesMetadata   and getCurrentVersion   have no synchronization     grpc java xds src main java io grpc xds ClientXdsClient java   Lines 1842 to 1848 in 14eb3b2        Map String  ResourceMetadata  getSubscribedResourcesMetadata ResourceType type        Map String  ResourceMetadata  metadataMap   new HashMap          for  Map Entry String  ResourceSubscriber  entry   getSubscribedResourcesMap type  entrySet          metadataMap put entry getKey    entry getValue   metadata             return metadataMap                grpc java xds src main java io grpc xds AbstractXdsClient java   Lines 257 to 258 in a46560e           Must be synchronized      String getCurrentVersion ResourceType type         That is bad  However  the xdsClient API itself is insufficient for CSDS because those two method calls need to be atomic  even if each of those methods were thread safe the version needs to match the resources returned
8914	binder  Deadlock due to unexpected re entrancy of transactions on process local Binder BinderTransport locking was written under the assumption that calls to IBinder transact   enqueued the Parcel for delivery to the peer and returned immediately  However  Android guarantees the unique object identity of IBinder instances within a process  And so when a client creates a Channel to a Server Service within its own process  BinderClientTransport outgoingBinder    BinderServerTransport outgoingBinder  android os Binder transact   on that object is implemented not as a system call to the binder driver but as a direct call to its own onTransact   method  This is a problem because BinderTransport handleTransaction   holds its  this  lock while calling outgoingBinder transact   in multiple places  If two peer instances of BinderClientTransport and BinderServerTransport are running handleTransaction   on different threads at the same time  they can each end up holding their own lock while waiting  forever  for the other s  Steps to reproduce one instance of this bug  Use BinderChannelBuilder to create a Channel to an android app Service hosted by the same process Have both the client and server repeatedly send messages to each other around the same time from different threads  What did you expect to see  No deadlock What did you see instead  Example deadlock  via sendAcknowledgeBytes     xxx  prio 5 tid 25 Blocked   group  main  sCount 1 ucsCount 0 flags 1 obj 0x12c40c00 self 0xb400006e9ce43460   sysTid 24211 nice 10 cgrp background sched 0 0 handle 0x6caac7dcb0   state S schedstat   6776198013 56795686488 17245   utm 455 stm 221 core 1 HZ 100   stack 0x6caab7a000 0x6caab7c000 stackSize 1039KB   held mutexes    at io grpc binder internal BinderTransport handleTransaction BinderTransport java 449        waiting to lock  0x04f85343   a io grpc binder internal BinderTransport BinderClientTransport  held by thread 64       locked  0x0f79cc83   a io grpc binder internal BinderTransport BinderServerTransport      at io grpc binder internal LeakSafeOneWayBinder onTransact LeakSafeOneWayBinder java 63      yyy  prio 5 tid 64 Blocked   group  main  sCount 1 ucsCount 0 flags 1 obj 0x12d41100 self 0xb400006e9cf25400   sysTid 16535 nice 10 cgrp background sched 0 0 handle 0x6ca795fcb0   state S schedstat   84994090 239215329 419   utm 5 stm 2 core 1 HZ 100   stack 0x6ca785c000 0x6ca785e000 stackSize 1039KB   held mutexes    at io grpc binder internal BinderTransport handleTransaction BinderTransport java 449        waiting to lock  0x0f79cc83   a io grpc binder internal BinderTransport BinderServerTransport  held by thread 25       locked  0x04f85343   a io grpc binder internal BinderTransport BinderClientTransport      at io grpc binder internal LeakSafeOneWayBinder onTransact LeakSafeOneWayBinder java 63       locked  0x0f7ee7bb   a io grpc binder internal Outbound ServerOutbound