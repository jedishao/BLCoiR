1117	Exception during namespace bundle split  This particular exception was reported  It was during an automatically initiated bundle split  2018 01 26 00 38 08 489   ERROR    pulsar web 61 28 PulsarWebResource 381     null  Failed to validate namespace bundle netflix prod ns1 0x84000000_0x86000000 java lang IllegalArgumentException  Invalid upper boundary for bundle I think that the race condition is happening since all 4 initial bundles are getting split around the same time and the bundle cache gets updated while the new bundle split is being processed  The result doesn t cause any particular issue  just that the bundle split fails with invalid bundle range  but the current bundle will continue to run unaffacted until the next split will take place
2141	Race condition in reconnection after managed ledger is fenced  While checking the intermittent failures for SequenceIdWithErrorTest  I ve seen that the test is failing due to a race condition involving both producer and consumer reconnecting to the broker after the topic was fenced  in the case of the test  the fencing error is purposely inject
3768	pulsar function  intermittent test failure due to deadlock  org apache pulsar functions worker PulsarWorkerAssignmentTest shutdown   has intermittent failure due to deadlock into function worker  Disabling this for now and we have to enable back once deadlock issue is fixed at function worker  Error Message Method org apache pulsar functions worker PulsarWorkerAssignmentTest shutdown   didn t finish within the time out 60000 Stacktrace org testng internal thread ThreadTimeoutException  Method org apache pulsar functions worker PulsarWorkerAssignmentTest shutdown   didn t finish within the time out 60000
4447	Broker gets stuck when getting partitioned stats  Describe the bug Brokers in our cluster get stuck when executing internalGetPartitionedStats   pulsar pulsar broker src main java org apache pulsar broker admin impl PersistentTopicsBase java   Lines 609 to 610 in 3cfa812        TopicStats partitionStats   pulsar   getAdminClient   topics        getStats topicName getPartition i  toString          thread dump  https   gist github com nkurihar 5d90ce683802390cdec2d9fb65cc0297  pulsar web 28 32   531 prio 5 os_prio 0 tid 0x00007f31a4001000 nid 0x3b69 waiting on condition  0x00007f30db1c6000  java lang Thread State  WAITING  parking    parking to wait for   0x00007f512c8e2d80   a java util concurrent CompletableFuture Signaller   To Reproduce We haven t found the way to reproduce yet  Additional context Broker OS  CentOS Linux release 7 6 1810 Broker version  2 2 1 Broker spec  Real server   2 10GHz   2CPU   256GBMEM   SATA SSD 240GB x1   10G Base T 2port
4635	Bookie down causes deadlock in broker  One of multiple bookie servers in our cluster went down due to a hardware failure  At the same time  the broker server went down  Messages that the broker could not connect to ZK were output to its log  I think this is due to a deadlock  19 38 55 846  pulsar zk session watcher 5 1  WARN  o a p z ZooKeeperSessionWatcher        zoo keeper disconnected  waiting to reconnect  time remaining   25 seconds 19 38 57 846  pulsar zk session watcher 5 1  WARN  o a p z ZooKeeperSessionWatcher        zoo keeper disconnected  waiting to reconnect  time remaining   23 seconds 19 38 59 847  pulsar zk session watcher 5 1  WARN  o a p z ZooKeeperSessionWatcher        zoo keeper disconnected  waiting to reconnect  time remaining   21 seconds 19 39 01 847  pulsar zk session watcher 5 1  WARN  o a p z ZooKeeperSessionWatcher        zoo keeper disconnected  waiting to reconnect  time remaining   19 seconds 19 39 03 847  pulsar zk session watcher 5 1  WARN  o a p z ZooKeeperSessionWatcher        zoo keeper disconnected  waiting to reconnect  time remaining   16 seconds 19 39 05 847  pulsar zk session watcher 5 1  WARN  o a p z ZooKeeperSessionWatcher        zoo keeper disconnected  waiting to reconnect  time remaining   14 seconds 19 39 07 848  pulsar zk session watcher 5 1  WARN  o a p z ZooKeeperSessionWatcher        zoo keeper disconnected  waiting to reconnect  time remaining   12 seconds 19 39 09 848  pulsar zk session watcher 5 1  WARN  o a p z ZooKeeperSessionWatcher        zoo keeper disconnected  waiting to reconnect  time remaining   10 seconds 19 39 11 848  pulsar zk session watcher 5 1  WARN  o a p z ZooKeeperSessionWatcher        zoo keeper disconnected  waiting to reconnect  time remaining   8 seconds 19 39 13 849  pulsar zk session watcher 5 1  WARN  o a p z ZooKeeperSessionWatcher        zoo keeper disconnected  waiting to reconnect  time remaining   6 seconds 19 39 15 849  pulsar zk session watcher 5 1  WARN  o a p z ZooKeeperSessionWatcher        zoo keeper disconnected  waiting to reconnect  time remaining   4 seconds 19 39 17 849  pulsar zk session watcher 5 1  WARN  o a p z ZooKeeperSessionWatcher        zoo keeper disconnected  waiting to reconnect  time remaining   2 seconds 19 39 19 849  pulsar zk session watcher 5 1  WARN  o a p z ZooKeeperSessionWatcher        zoo keeper disconnected  waiting to reconnect  time remaining   0 seconds 19 39 21 850  pulsar zk session watcher 5 1  ERROR o a p z ZooKeeperSessionWatcher        timeout expired for reconnecting  invoking shutdown service  Below is a thread dump just before the broker shuts down  broker_threaddump txt This phenomenon is similar to  3566  However the Pulsar version of the broker is 2 3 2  and the previous bug should have already been fixed
4707	PulsarKafkaProducer is not thread safe  Is your feature request related to a problem  Please describe  I replaced kafka client with pulsar client kafka  referring to this document  http   pulsar apache org docs en adaptors kafka  Then  sending messages to multiple topics at the same time caused an exception NullPointerException   This exception does not occur in version 2 2 1  but does occur in version 2 3 2 and later  I think that the change made in this PR is the cause  Each time you send a message to a new topic  the cluster object is regenerated  https   github com apache pulsar blob v2 3 2 pulsar client kafka compat pulsar client kafka src main java org apache kafka clients producer PulsarKafkaProducer java L229 The javadoc states that KafkaProducer is thread safe  https   kafka apache org 20 javadoc org apache kafka clients producer KafkaProducer html I want to use pulsar client kafka without modifying the code that used Kafka client  To Reproduce   build gradle dependencies   testCompile group   junit   name   junit   version   4 12  compile  group   org apache pulsar   name   pulsar client kafka original   version   2 3 2   compile  group   org apache pulsar   name   pulsar client auth athenz   version   2 3 2      public static void main String   args  throws Exception    Properties properties   new Properties    properties setProperty ProducerConfig BOOTSTRAP_SERVERS_CONFIG   pulsar ssl   server 6651    properties setProperty ProducerConfig RETRIES_CONFIG   5    properties setProperty ProducerConfig KEY_SERIALIZER_CLASS_CONFIG  StringSerializer class getName     properties setProperty ProducerConfig VALUE_SERIALIZER_CLASS_CONFIG  StringSerializer class getName      properties setProperty PulsarClientKafkaConfig USE_TLS   true    properties setProperty PulsarClientKafkaConfig TLS_TRUST_CERTS_FILE_PATH  filePath  trust     crt   CERT    properties setProperty PulsarClientKafkaConfig AUTHENTICATION_CLASS  AuthenticationAthenz class getName     properties setProperty PulsarClientKafkaConfig AUTHENTICATION_PARAMS_STRING  authParams      properties setProperty  security protocol    PLAINTEXT     properties setProperty ProducerConfig REQUEST_TIMEOUT_MS_CONFIG   30000    properties setProperty ProducerConfig ACKS_CONFIG   1    properties setProperty ProducerConfig METADATA_MAX_AGE_CONFIG   15000    producer   new PulsarKafkaProducer   properties  new StringSerializer    new StringSerializer      Thread thread1   new Thread         String topic1    persistent   topic1   ProducerRecord String  String  record1   new ProducerRecord   topic1   Hello    producer send record1   recordMetadata  e       System out println recordMetadata            Thread thread2   new Thread         String topic2    persistent   topic2   ProducerRecord String  String  record2   new ProducerRecord   topic2   Hello    producer send record2   recordMetadata  e       System out println recordMetadata            thread1 start    thread2 start       Exception in thread  Thread 2  java lang NullPointerException
5585	Non persistent topic s replication has a deadlock  NonPersistentReplicator disable batching  If there is batch message producer will acquire one but release num in message s meta when process ack     When publishing during replication  we need to set the correct number of message in batch    This is only used in tracking the publish rate stats int numMessages   msg getMessageBuilder   hasNumMessagesInBatch     msg getMessageBuilder   getNumMessagesInBatch     1  ByteBufPair cmd   sendMessage producerId  sequenceId  numMessages  msgMetadata  encryptedPayload   msgMetadataBuilder recycle    msgMetadata recycle     final OpSendMsg op   OpSendMsg create msg  cmd  sequenceId  callback   op setNumMessagesInBatch numMessages   op setBatchSizeByte encryptedPayload readableBytes     pendingMessages put op   lastSendFuture   callback getFuture     So the ProducerImpl s semaphore  no longer has any effect   the ProducerImpl s sendAsync method maybe  blocked in pendingMessages s put method   There may be a deadlock between ackReceived  and sendAsync   The deadlock  is   between  pulsar io 22 8  and pulsar io 22 12 in jstack log  broker jstack log
8050	Replace map with set  In PulsarClientImpl  we use IdentityHashMap to hold the reference of producers consumers like below  producers put producer  Boolean TRUE    It s better to use List  for value is never used but stored  But Pulsar pursues performance  so looks good to use thread safe set    Collections newSetFromMap new ConcurrentHashMap
8293	Race condition in updating ManagedCursorImpl readPosition  Describe the bug  8229 seems to have been caused by a race condition in updating ManagedCursorImpl readPosition To Reproduce Since this is a concurrency issue  it s hard to reproduce and there isn t yet a publicly shared way to reproduce  Expected behavior Updates to ManagedCursorImpl readPosition field should not lead to inconsistent state  It s not clear without understanding the code how concurrent updates should be handled  Additional context Please refer to  8229 for additional context  There s a link to a Slack thread for more discussions  There s a fix for  8229 which prevents the infinite loop   8284   This fix doesn t specifically address the race condition that happens in updating the ManagedCursorImpl readPosition field  There seems to be quite a few past issues where a race condition in updating readPosition has been an issue  For example  1478    3015 &  287   There is also a change  6606 which adds READ_POSITION_UPDATER for ManagedCursorImpl readPosition  Regarding the race condition in  8229  it seems that ManagedCursorImpl readPosition could get out of sync from OpReadEntry readPosition if ManagedCursorImpl readPosition gets updated after the OpReadEntry has been created since OpReadEntry s readPosition gets initialized from ManagedCursorImpl readPosition  The race condition seems to happen in this code in the setAcknowledgePosition method     pulsar managed ledger src main java org apache bookkeeper mledger impl ManagedCursorImpl java   Lines 1512 to 1523 in 825fdd4        if  readPosition compareTo newMarkDeletePosition     0           If the position that is mark deleted is past the read position  it        means that the client has skipped some entries  We need to move        read position forward     PositionImpl oldReadPosition   readPosition      readPosition   ledger getNextValidPosition newMarkDeletePosition            if  log isDebugEnabled          log debug       Moved read position from     to      and new mark delete position      ledger getName        oldReadPosition  readPosition  markDeletePosition                    Clarification  possible solution The problem isn t about synchronization or a missing lock  It s a race condition which cannot be resolved by simply adding a lock or synchronization  It should be possible to detect if another thread has modified the state and then have some code to do  conflict resolution   For example  when readPosition gets updated in setAcknowledgePosition method  it most likely shouldn t move the readPosition  backwards   There s already code in setReadPosition to take the markDeletePosition into account when updating readPosition  Similarly in setAcknowledgePosition  it should most likely take the previous state of readPosition into account when updating the value so that readPosition doesn t  jump backwards  in a race condition
9109	pulsar timer thread blocked at redeliverUnacknowledgedMessages  Describe the bug After broker crash and restart  consumers got blocked  consumer rateOut decrease to 0  can t auto recover without restart consumer process  To Reproduce Steps to reproduce the behavior   Run reproduce code below  public class PulsarConsumerTest    private static final Logger LOG   LoggerFactory getLogger PulsarConsumerTest class    private PulsarClient pulsarClient  private Consumer byte    consumer    Before public void setUp   throws Exception   pulsarClient   PulsarClient builder    serviceUrl  pulsar    broker  6650    build    String topic    persistent   sample ns1 topic1   String subscriptionName    test   DeadLetterPolicy deadLetterPolicy   DeadLetterPolicy builder    deadLetterTopic String format   s  s DLQ   topic  subscriptionName    maxRedeliverCount 3   build    consumer   pulsarClient newConsumer    topic topic   deadLetterPolicy deadLetterPolicy   ackTimeout 5  TimeUnit SECONDS   acknowledgmentGroupTime 0  TimeUnit MILLISECONDS   subscriptionName subscriptionName   subscriptionType SubscriptionType Shared   subscribe        After public void tearDown   throws Exception   pulsarClient close        Test public void test   throws PulsarClientException   while  true    Message byte    message   consumer receive    MessageId messageId   message getMessageId    LOG info  received message with messageId       messageId   try   consume message     catch  Exception e    LOG error  consume message exception with messageId       messageId  e          private void consume Message byte    message    throw new IllegalStateException  mock consume fails         Send 1000 messages to topic persistent   sample ns1 topic1 Wait about 10 15s time to redeliver   kill and restart broker process Look into the thread pulsar timer 4 1 s stack  check whether it s blocked or not If the problem doesn t appear  try step 2 4 a few more times  Expected behavior The thread pulsar timer 4 1 blocked at producer send forever  and the method consumer receive may blocked at UnAckedMessageTracker add method due to acquire a writeLock inside UnAckedMessageTracker   pulsar timer 4 1   37 prio 5 os_prio 0 tid 0x00007efe584c0000 nid 0x62 waiting on condition  0x00007efe30aec000  java lang Thread State  WAITING  parking    parking to wait for   0x00000000af9bbee8   a java util concurrent CompletableFuture Signaller  Locked ownable synchronizers     0x00000000a9dab610   a java util concurrent locks ReentrantReadWriteLock NonfairSync   Screenshots The consumer thread got blocked in our production   Additional context Broker version  2 4 0 pulsar client version  2 5 2
10170	Optimize the lock in AuthenticationAthenz  Describe the bug The use of the lock in AuthenticationAthenz will reduce the efficiency of AuthenticationAthenz  Hope it can be optimized   Error  Medium  Inconsistent synchronization of org apache pulsar client impl auth AuthenticationAthenz roleToken  locked 57  of time  org apache pulsar client impl auth AuthenticationAthenz  org apache pulsar client impl auth AuthenticationAthenz  org apache pulsar client impl auth AuthenticationAthenz  org apache pulsar client impl auth AuthenticationAthenz  org apache pulsar client impl auth AuthenticationAthenz  org apache pulsar client impl auth AuthenticationAthenz  org apache pulsar client impl auth AuthenticationAthenz  Unsynchronized access at AuthenticationAthenz java  line 51 Unsynchronized access at AuthenticationAthenz java  line 51 Unsynchronized access at AuthenticationAthenz java  line 51 Synchronized access at AuthenticationAthenz java  line 88 Synchronized access at AuthenticationAthenz java  line 98 Synchronized access at AuthenticationAthenz java  line 100 Synchronized access at AuthenticationAthenz java  line 107  IS2_INCONSISTENT_SYNC  To Reproduce For more information  please see  8980 CI logs Expected behavior A clear and concise description of what you expected to happen  Screenshots If applicable  add screenshots to help explain your problem  Desktop  please complete the following information    OS   e g  iOS   Additional context Add any other context about the problem here
10235	Deadlock on Monitoring thread     LeaderService isLeader    Describe the bug When checking dashboards  we found that the pulsar summary board showed that 33  of brokers were running  However kubernetes showed that all of the brokers were running  We traced this back to problems with the metrics queries not returning  wget http   localhost 8080 metrics  and this was indeed returning a Gateway timeout  We checked the logs and found no errors in the logs to indicate that a exception occurred during the processing of the metrics query  So we proceeded to take heap dumps and stack traces from the java process  While it continued to process data through many of the queues  Review of the stack traces showed that the prometheus stats thread was hung waiting on a Lock that was held by another thread  pulsar external listener   However the other thread was waiting on additional locks  I suspect that there is a deadlock condition somewhere but I could not find the other lock that it was waiting upon  I believe the problem could be that the scope of what is executed in becameInactive   is too wide  threaddump_itomdipulsar broker 7865b7ff5d zg42s_150421 004920 log threaddump_itomdipulsar broker 7865b7ff5d gcs4n_150421 004920 log threaddump_itomdipulsar broker 7865b7ff5d 8rglm_150421 004920 log itomdipulsar broker 7865b7ff5d zg42s log itomdipulsar broker 7865b7ff5d gcs4n log itomdipulsar broker 7865b7ff5d 8rglm log
10767	Java client  Deadlock in Pulsar Client when running ConsumerBatchReceiveTest  There s a deadlock issue in Pulsar Client in master branch  A PR test run had stalled and the thread dump detected this deadlock issue  Found one Java level deadlock                                 pulsar timer 462 1   waiting to lock monitor 0x00007fce080ad180  object 0x00000000c6094a00  a org apache pulsar client impl ConsumerImpl   which is held by  pulsar client internal 459 1   pulsar client internal 459 1   waiting for ownable synchronizer 0x00000000c6094bf0   a java util concurrent locks ReentrantReadWriteLock NonfairSync   which is held by  pulsar timer 462 1   Java stack information for the threads listed above                                                       pulsar timer 462 1     waiting to lock  0x00000000c6094a00   a org apache pulsar client impl ConsumerImpl   pulsar client internal 459 1   at jdk internal misc Unsafe park java base 11 0 11 Native Method    parking to wait for   0x00000000c6094bf0   a java util concurrent locks ReentrantReadWriteLock NonfairSync    locked  0x00000000c6094a00   a org apache pulsar client impl ConsumerImpl   Found 1 deadlock   Full thread dump  https   gist github com lhotari 1bbcc43e850bd7d62891ba7fe3724b0b thread dump in jstack review UI  https   jstack review  https   gist github com lhotari 1bbcc43e850bd7d62891ba7fe3724b0b tda_1_dump The test that was executing was ConsumerBatchReceiveTest   main   1 prio 5 os_prio 0 cpu 13468 61ms elapsed 6534 24s tid 0x00007fce64027800 nid 0xca9 in Object wait     0x00007fce6a11e000  java lang Thread State  TIMED_WAITING  on object monitor  at java lang Object wait java base 11 0 11 Native Method    waiting on  no object reference available    waiting to re lock in wait    0x00000000c4246738   a io netty util concurrent FastThreadLocalThread   Expected behavior Pulsar Client shouldn t deadlock
11379	Directory facing race condition on NarUnpacker  Describe the bug This issue is an extension for  11340  When users create functions with parallelism larger than the number of functions worker  the directory race condition can be observed  Exception in thread  main  java lang reflect InvocationTargetException Caused by  java io IOException  Unable to delete  tmp pulsar nar pulsar java functions pojo parse 1 0 SNAPSHOT jar unpacked META INF maven     5 more  Above exception from a single node broker   functions worker  with a sample function parallelism 3  The exception shows that it is unable to delete  tmp pulsar nar pulsar java functions pojo parse 1 0 SNAPSHOT jar unpacked META INF maven  which has already been deleted by another instance of the sample function  This exception raised my attention   and lead me to  11340  To Reproduce Steps to reproduce the behavior   Have a single node cluster on K8S by https   github com streamnative charts  I have created a sample values file as gist and can be found here build a sample function  you can take my example https   github com freeznet pulsar java functions pojo parse and build with mvn create function with the sample package with parallelism 1  it is easier to reproduce with larger parallelism  normally you can observe the issue first with pulsar admin functions status with some of the instances are not running  for some instances you can observe the above exception from the broker s log  this is because the race condition happens when multiple threads trying to delete the same file  for some instances you can observe the exception described in Extracted Pulsar Function jar file content in  tmp pulsar nar directory gets corrupted or overridden by different content  11340  this is because the race condition happens when some threads deleted the extracted nar package by the extracting thread  and the extracting thread is still loading the classes
11605	Race condition in concurrent schema deletion  Describe the bug In some scenarios  we deleted topics concurrently  which may trigger some race condition in deleting the same topic schema  For example  in org apache pulsar broker service AbstractTopic deleteSchema we will firstly perform schemaRegistryService getSchema read from bookkeeper  then schemaRegistryService deleteSchemaStorage delete its corresponding ledgers in bookkeeper   Therefore  when we delete a schema concurrently in two threads  it may happen that one thread has already deleted the ledger  and the other thread has just started executing and throws a  no such ledger  exception  To Reproduce Steps to reproduce the behavior   start a cluster with at least two brokers create a partitioned topic  A  with multiple partitions concurrently delete the non partitioned topics within topic  A  with the schema then you will sporadically see  No such ledger exists on Bookies  or  NoNode for  schemas XX YY ZZ   Note that this phenomenon does not happen every time   Expected behavior Broker service can handle these scenarios correctly
11689	Java consumer can block forever on batchReceive  Describe the bug When Consumer batchReceive   is called concurrently by different threads there exists a race condition in ConsumerBase java which when triggered causes a CompletableFuture in the queue pendingBatchReceives to be removed from the queue but not completed  causing the consumer to block forever  The issue is that there are concurrent calls to peek and poll in peekNextBatchReceive and the code is only correct when what is peeked is polled  If another thread calls poll between a peek and poll then this bug occurs  There is an error message when this occurs Bug  Removed entry wasn t the expected one  To Reproduce  Create a consumer On many threads  repeatedly call batchReceive Wait potentially a very long time  but eventually it will block forever   I added a Thread sleep between the peek and poll of peekNextBatchReceive to make it trigger faster  Expected behavior batchReceive should never block forever  I have a fix on the way
11690	Flaky test  LockManagerTest revalidateLockOnDifferentSession  LockManagerTest is flaky  The revalidateLockOnDifferentSession test method fails sporadically  example failure Error   Tests run  12  Failures  1  Errors  0  Skipped  7  Time elapsed  1 52 s     FAILURE   in org apache pulsar metadata LockManagerTest Error   revalidateLockOnDifferentSession org apache pulsar metadata LockManagerTest   Time elapsed  0 264 s      FAILURE java util NoSuchElementException  No value present
11966	Proxy  Proxied  admin endpoint connections might remain blocked forever  Describe the bug Proxied  admin endpoint connections might remain blocked forever  Example stacktrace  pulsar external web 6 7   43 prio 5 os_prio 0 cpu 134724 23ms elapsed 519507 47s tid 0x00007fd7b586a800 nid 0x5a in Object wait     0x00007fd71adef000  java lang Thread State  WAITING  on object monitor  at java lang Object wait java base 11 0 11 Native Method    waiting on  no object reference available    waiting to re lock in wait    0x00000000d3269c28   a java util ArrayDeque   Expected behavior There should be a timeout in place which prevents threads to hang forever in this state  When investigating the issue and code  it can be seen that the timeout isn t set for proxied requests  The call to super setTimeout method is missing in AdminProxyHandler  The base method is overridden and that s why it doesn t get set to any value  this is the code in the base class method which sets the timeout  https   github com eclipse jetty project blob 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8 jetty proxy src main java org eclipse jetty proxy AbstractProxyServlet java L321 L324 Another observation from that code is that it has been copy pasted from the Jetty code  There s no use of using the ServletConfig for getting the configuration values  This code need some cleanup too
12723	Race condition in PersistentTopic addReplicationCluster  Describe the bug In class org apache pulsar broker service persistent PersistentTopic  we have field replicators of class ConcurrentOpenHashMap  There is a race condiftion in method addReplicationCluster  replicators computeIfAbsent remoteCluster  r      try   return new PersistentReplicator PersistentTopic this  cursor  localCluster  remoteCluster  brokerService   PulsarClientImpl  replicationClient     catch  PulsarServerException e    log error       Replicator startup failed      topic  remoteCluster  e     return null          clean up replicator if startup is failed if  replicators containsKey remoteCluster  && replicators get remoteCluster     null    replicators remove remoteCluster      It s clear that there is a race condition if multi threads would run in this code  For example Thread A is just about to execute replicators remove  Thread B inserts a non null PersistentReplicator  Then thread A will delete the PersistentReplicator which thread B just created  And there is no other thread safe measures applied to these code  Expected behavior It should be thead safe  Screenshots Not applicable  Desktop  please complete the following information    OS  mac  Additional context Nop
12885	Bug  Unordered consuming case in  Key_Shared subscription  Describe the bug It s a previous flaky test in org apache pulsar client api KeySharedSubscriptionTest testRemoveFirstConsumer See  11426 There is a race condition in PersistentDispatcherMultipleConsumers when previous client closed with unack messaages  Detailed Explanation  Here is the core steps of this case   produce 10 messages  m0~m9 wait unitl c1 got all the 10 message  create c2 with same setting as c1  produce 10 messages  m10~m19 c2 will not be able to receive any messages until c1 is done processing whatever he got prefetched  c1 close    Call c2 receive   now  It should receive m0  But there is some chance it will return m10   The race condition happens after step6  Just before step6  there is a retry loop going on for this consumer  Thread 1  1 1 PersistentDispatcherMultipleConsumers readMoreEntries   synchronized 1 2     calculateToRead   return  10 xxx  1 3     getMessagesToReplayNow   return m10 m14 1 4     havePendingReplayRead   true 1 5     asyncReplayEntriesInOrder m10 m14      cursor asyncReplayEntries       will callback readEntriesComplete async   Thread 2  2 1 PersistentDispatcherMultipleConsumers readEntriesComplete   synchronized 2 2     havePendingReplayRead   false  2 3     PersistentStickyKeyDispatcherMultipleConsumers sendMessagesToConsumers m10 m14  2 4           DEBUG Log  select consumer c1 with messages num 0  read type is Replay Can not send to c1 because it runs out of permits  2 5           DEBUG Log  select consumer c2 with messages num 0  read type is Replay Can not send to c2 because it should wait c1 to complete unacked messages  2 6         Finally  no consumer to send message  So it schedule call readMoreEntries   in 100ms   After we close c1  PersistentDispatcherMultipleConsumers removeConsumer will be called like this  Thread3 3 1PersistentDispatcherMultipleConsumers removeConsumer 3 2        consumer getPendingAcks   forEach   addMessageToReplay m0 m9 3 3        calls readMoreEntries   If Thread3 happens after Thread2  everything is ok  and this is most of the real cases  As time between Thread1 and Thread2 is quite small  But if it does happens in the order of Thread1     Thread3     Thread2  The disorder occurs  In the readMoreEntries of 3 3    calculateToRead return  1  1 because havePendingReplayRead is true in 1 2 of Thread1  So the readMoreEntries will just returns without further actions  Then in Thread2  previous reading ended  and we got m10 m14 to dispatch  now we can select consumer c2 because we have no more c1 for waiting   So the client c2 got m10 instead of m0  To Reproduce Steps to reproduce the behavior   It easier to reproduce this in local environment by changing the 100ms to 1ms here     pulsar pulsar broker src main java org apache pulsar broker service persistent PersistentStickyKeyDispatcherMultipleConsumers java   Line 294 in 5523604           100  TimeUnit MILLISECONDS        Run Test on org apache pulsar client api KeySharedSubscriptionTest testRemoveFirstConsumer  It will hit the race condition in a few retries   Expected behavior As the case designed in testRemoveFirstConsumer  The consuming order should be right  Screenshots Nop Desktop  please complete the following information    OS  macOS  Additional context NO
12929	Deadlock in internalDeleteSubscription  in metadata store callback thread  Describe the bug When doing certain topic operations  for e g   delete subscription   it can result in a deadlock metadata store callback thread  Please see the stack trace below   metadata store 6 1   43 prio 5 os_prio 0 cpu 380 47ms elapsed 797 76s tid 0x00007f6ffc026800 nid 0x74 waiting on condition   0x00007f7001ce1000  java lang Thread State  WAITING  parking  at jdk internal misc Unsafe park java base 11 0 12 Native Method    parking to wait for   0x00000000ffe95e98   a java util concurrent CompletableFuture Signaller  The issue is similar to    12726 To Reproduce This is a race condition  I have not been able to come up with a reliable way to reproduce this  But it is easily reproducible inin a 3 node broker setup  creating and deleting topic consumers  Expected behavior The metadata store callback thread should not deadlock  Screenshots If applicable  add screenshots to help explain your problem  Desktop  please complete the following information    OS   e g  iOS   Additional context Add any other context about the problem here
13004	metadata  Race condition in ResourceLockImpl revalidate  Describe the bug Current unit test org apache pulsar metadata LockManagerTest updateValueWhenKeyDisappears have a small chance that will fails with following exception   java util concurrent CompletionException  org apache pulsar metadata api MetadataStoreException LockBusyException  Resource at  my path 1 is already locked  Caused by  org apache pulsar metadata api MetadataStoreException LockBusyException  Resource at  my path 1 is already locked     13 more  It fails on the line here     pulsar pulsar metadata src test java org apache pulsar metadata LockManagerTest java   Line 198 in 693a066        lock updateValue  value 2   join         After some digging  I found that it s because there is a race condition of method org apache pulsar metadata coordination impl ResourceLockImpl revalidate  Call stack A   store delete   my path 1   Optional empty    join    Node Delete Event LockManagerImpl handleDataNotification ResourceLockImpl lockWasInvalidated ResourceLockImpl revalidate  Call stack B   lock updateValue  value 2   join    org apache pulsar metadata coordination impl ResourceLockImpl acquire ResourceLockImpl acquireWithNoRevalidation fails with LockBusyException ResourceLockImpl revalidate   See    pulsar pulsar metadata src main java org apache pulsar metadata coordination impl ResourceLockImpl java   Line 130 in 693a066        revalidate newValue        Once the node is deleted and two ResourceLockImpl revalidate are called at the same time  one of them is going to fail  So in the case above lock updateValue is failed  To Reproduce Steps to reproduce the behavior   It s easier to reproduce this when we add a 5ms delay in MetadataStore get or use DelayInjectionMetadataStore in  metadata  add DelayInjectionMetadataStore  13005 Run updateValueWhenKeyDisappears a few times See error   Expected behavior lock updateValue should always success in this case  Screenshots NA Desktop  please complete the following information    OS   e g  iOS   Additional context NA
13923	Proxy  Race condition in Pulsar Proxy that causes UnsupportedOperationExceptions in Proxy logs  Describe the bug It is common that UnsupportedOperationExceptions appear on the Proxy logs  This particular issue was reproduced very often when Geo replication was configured between 2 clusters  16 57 50 305  pulsar proxy io 2 3  INFO  org apache pulsar proxy server ProxyConnection     10 34 1 169 47600  New connection opened 16 57 50 329  pulsar proxy io 2 3  INFO  org apache pulsar proxy server ProxyConnection     10 34 1 169 47600  complete connection  init proxy handler  authenticated with token role superuser  hasProxyToBrokerUrl  false 16 57 50 331  pulsar proxy io 2 3  WARN  io netty channel DefaultChannelPipeline   An exceptionCaught   event was fired  and it reached at the tail of the pipeline  It usually means the last handler in the pipeline did not handle the exception  java lang UnsupportedOperationException  null at io netty channel AbstractChannelHandlerContext invokeChannelRead AbstractChannelHandlerContext java 379   io netty netty transport 4 1 72 Final jar 4 1 72 Fi nal  at io netty channel AbstractChannelHandlerContext invokeChannelRead AbstractChannelHandlerContext java 365   io netty netty transport 4 1 72 Final jar 4 1 72 Fi nal  at io netty channel AbstractChannelHandlerContext fireChannelRead AbstractChannelHandlerContext java 357   io netty netty transport 4 1 72 Final jar 4 1 72 Fina l  at io netty handler codec ByteToMessageDecoder fireChannelRead ByteToMessageDecoder java 324   io netty netty codec 4 1 72 Final jar 4 1 72 Final  at io netty handler codec ByteToMessageDecoder channelRead ByteToMessageDecoder java 296   io netty netty codec 4 1 72 Final jar 4 1 72 Final  at io netty channel AbstractChannelHandlerContext invokeChannelRead AbstractChannelHandlerContext java 379   io netty netty transport 4 1 72 Final jar 4 1 72 Fi nal  at io netty channel AbstractChannelHandlerContext invokeChannelRead AbstractChannelHandlerContext java 365   io netty netty transport 4 1 72 Final jar 4 1 72 Fi nal  at io netty channel AbstractChannelHandlerContext fireChannelRead AbstractChannelHandlerContext java 357   io netty netty transport 4 1 72 Final jar 4 1 72 Fina l  at io netty handler ssl SslHandler unwrap SslHandler java 1371   io netty netty handler 4 1 72 Final jar 4 1 72 Final  at io netty handler ssl SslHandler decodeNonJdkCompatible SslHandler java 1245   io netty netty handler 4 1 72 Final jar 4 1 72 Final  at io netty handler ssl SslHandler decode SslHandler java 1285   io netty netty handler 4 1 72 Final jar 4 1 72 Final  at io netty handler codec ByteToMessageDecoder decodeRemovalReentryProtection ByteToMessageDecoder java 507   io netty netty codec 4 1 72 Final jar 4 1 72 Final   at io netty handler codec ByteToMessageDecoder callDecode ByteToMessageDecoder java 446   io netty netty codec 4 1 72 Final jar 4 1 72 Final  at io netty handler codec ByteToMessageDecoder channelRead ByteToMessageDecoder java 276   io netty netty codec 4 1 72 Final jar 4 1 72 Final  at io netty channel AbstractChannelHandlerContext invokeChannelRead AbstractChannelHandlerContext java 379   io netty netty transport 4 1 72 Final jar 4 1 72 Fi nal  at io netty channel AbstractChannelHandlerContext invokeChannelRead AbstractChannelHandlerContext java 365   io netty netty transport 4 1 72 Final jar 4 1 72 Fi nal  at io netty channel AbstractChannelHandlerContext fireChannelRead AbstractChannelHandlerContext java 357   io netty netty transport 4 1 72 Final jar 4 1 72 Fina l  at io netty channel DefaultChannelPipeline HeadContext channelRead DefaultChannelPipeline java 1410   io netty netty transport 4 1 72 Final jar 4 1 72 Final  at io netty channel AbstractChannelHandlerContext invokeChannelRead AbstractChannelHandlerContext java 379   io netty netty transport 4 1 72 Final jar 4 1 72 Fi nal  at io netty channel AbstractChannelHandlerContext invokeChannelRead AbstractChannelHandlerContext java 365   io netty netty transport 4 1 72 Final jar 4 1 72 Fi nal  at io netty channel DefaultChannelPipeline fireChannelRead DefaultChannelPipeline java 919   io netty netty transport 4 1 72 Final jar 4 1 72 Final  at io netty channel epoll AbstractEpollStreamChannel EpollStreamUnsafe epollInReady AbstractEpollStreamChannel java 795   io netty netty transport classes epoll  4 1 72 Final jar 4 1 72 Final  at io netty channel epoll EpollEventLoop processReady EpollEventLoop java 480   io netty netty transport classes epoll 4 1 72 Final jar 4 1 72 Final  at io netty channel epoll EpollEventLoop run EpollEventLoop java 378   io netty netty transport classes epoll 4 1 72 Final jar 4 1 72 Final  at io netty util concurrent SingleThreadEventExecutor 4 run SingleThreadEventExecutor java 986   io netty netty common 4 1 72 Final jar 4 1 72 Final  at io netty util internal ThreadExecutorMap 2 run ThreadExecutorMap java 74   io netty netty common 4 1 72 Final jar 4 1 72 Final  at io netty util concurrent FastThreadLocalRunnable run FastThreadLocalRunnable java 30   io netty netty common 4 1 72 Final jar 4 1 72 Final  at java lang Thread run Thread java 829         To Reproduce Steps to reproduce the behavior   Setup 2 Pulsar clusters and enable geo replication in a namespace that is shared across the clusters  Create a topic with 200 partitions in the replicated namespace  Restart all brokers in one cluster to get the geo replication connections to re initialize untill you have reproduced the issue in the Pulsar Proxy logs   Expected behavior The race conditions should be handled in Pulsar Proxy
13964	Flaky tests  PulsarClientImpl close hangs forever  PulsarClientImpl close sometimes hangs forever  Example from ClientDeduplicationFailureTest shutdown   example failure  main   1 prio 5 os_prio 0 cpu 287708 03ms elapsed 6747 96s tid 0x00007f46e0028000 nid 0xc9a waiting on condition   0x00007f46e549e000  java lang Thread State  WAITING  parking  at jdk internal misc Unsafe park java base 11 0 13 Native Method    parking to wait for   0x00000000dde38ad0   a java util concurrent CompletableFuture Signaller   pulsar client shutdown threadawaiting notification pulsar client shutdown threadawaiting notification at java lang Object wait java base 11 0 13 Native Method   There s a possible deadlock
13986	Transactiion  Performance bottleneck of TransactionBufferHandler  Is your enhancement request related to a problem  Please describe  The synchronization lock added due to timeout will cause the performance bottleneck of TransactionBufferHandler Describe the solution you d like  Do not use the run method for timeout processing Do not use sync locks
14362	Broker Healthcheck Endpoint Exposes Race Conditions  Describe the bug There appears to be a few race conditions in the  brokers health endpoint  I verified this is in 2 9 1 and it looks like it is probably present in master  There are several issues exposed by this behavior and the resulting stack traces   One call to that endpoint can force another to fail  There is a chance for an NPE here    pulsar pulsar broker src main java org apache pulsar broker service persistent PersistentTopic java   Lines 1051 to 1057 in f7f8619        void removeSubscription String subscriptionName        PersistentSubscription sub   subscriptions remove subscriptionName          preserve accumulative stats form removed subscription     SubscriptionStatsImpl stats   sub getStats false  false  false       bytesOutFromRemovedSubscriptions add stats bytesOutCounter       msgOutFromRemovedSubscriptions add stats msgOutCounter              Also relevant for the above code is that because it is called outside of a lock on the subscription  it could possibly remove the wrong subscription  This method is blocking a pulsar thread without any obvious reason    pulsar managed ledger src main java org apache bookkeeper mledger impl ManagedLedgerImpl java   Lines 1008 to 1037 in f7f8619        public void deleteCursor String name  throws InterruptedException  ManagedLedgerException       final CountDownLatch counter   new CountDownLatch 1       class Result       ManagedLedgerException exception   null            final Result result   new Result             asyncDeleteCursor name  new DeleteCursorCallback          Override     public void deleteCursorComplete Object ctx        counter countDown                    Override     public void deleteCursorFailed ManagedLedgerException exception  Object ctx        result exception   exception      counter countDown                      null            if  counter await AsyncOperationTimeoutSeconds  TimeUnit SECONDS         throw new ManagedLedgerException  Timeout during delete cursors operation                   if  result exception   null        log error  Deleting cursor   result exception       throw result exception                    It s obvious to me how we should fix the endpoint to make it work for concurrent calls  I am going to start by focusing on the other calls  To Reproduce Steps to reproduce the behavior   Deploy a 2 9 1 broker  Run curl  s   fail http   localhost 8080 admin v2 brokers health   &  sleep 0 05   curl  s   fail http   localhost 8080 admin v2 brokers health Observer logs  It s possible that you ll need to rerun step 2 several times  Given that it is a race in the endpoint  it is not guaranteed to trigger every time   Resulting stack trace  2022 02 17T23 29 45 090 0000  pulsar web 36 2  INFO  org apache pulsar broker admin impl BrokersBase   Running healthCheck with topic persistent   pulsar pulsar 172 17 0 13 8080 healthcheck 2022 02 17T23 29 45 094 0000  pulsar io 4 3  INFO  org apache pulsar client impl ProducerImpl    persistent   pulsar pulsar 172 17 0 13 8080 healthcheck   null  Creating producer on cnx  id
14413	ConsumerBuilderImpl subscribeAsync blocks calling thread  Describe the bug When Retry topics are enabled  ConsumerBuilderImpl performs a backwards compatibility check to look for DLQ or Retry topics that were created on previous version of Pulsar using a different naming scheme  It does this by calling getPartitionedTopicMetadata and then using  get   to block while waiting for the results if  client getPartitionedTopicMetadata oldRetryLetterTopic   get client conf getOperationTimeoutMs    TimeUnit MILLISECONDS  partitions   0    retryLetterTopic   oldRetryLetterTopic    if  client getPartitionedTopicMetadata oldDeadLetterTopic   get client conf getOperationTimeoutMs    TimeUnit MILLISECONDS  partitions   0    deadLetterTopic   oldDeadLetterTopic    This was implemented in  10129 A partial fix to add a Timeout was implemented in  11597 However this fix does still block the calling thread during the lookup  This can be an issue for code that attempt to call subscribeAsync on a non blocking Pool  such as when using Netty   The signature of subscribeAsync implies that it s non blocking   And it seems somewhat pointless to have subscribeAsync if it blocks anyway  Note that this is an undocumented breaking change in behavior  Up until 2 9  it was safe to call this from a non blocking pool  In our case  we were running a custom SLF4J log exporter on the same thread pool  which resulted in a deadlock when the number of concurrent subscribeAsync calls exceeded the pool size   This is probably an extreme example and a poor decision on our part  but perhaps a good example of why unexpected blocking can be dangerous  Note that blocking call is still made even if the retry and DLQ names are explicitly specified in DeadLetterPolicy  In that scenario the check should not be needed  Aside from blocking the calling Thread  this also results in unneeded lookup requests  To Reproduce Call  subscribeAsync with enableRetry true   The calling thread will be blocked until the metadata lookup is complete  Expected behavior  subscribeAsync should return a CompletableFuture immediately without performing any IO  Proposed Fix  Restructure subscribeAsync to chain the getPartitionedTopicMetadata calls rather than blocking Don t perform the metadata lookup if the DLQ Retry topic names are already specified Add a flag to disable the compatibility check for projects that don t need it   If the project maintainer are happy with this proposal  I m happy to raise a PR myself with the above change
14438	sometimes  internalDeleteTopicForcefully  will block forever     pulsar pulsar broker src main java org apache pulsar broker admin impl PersistentTopicsBase java   Line 312 in d1fb88a        pulsar   getBrokerService   deleteTopic topicName toString    true  deleteSchema  get          pulsar web 44 4   148 prio 5 os_prio 0 tid 0x00007fcbf7f89000 nid 0x6d97 waiting on condition  0x00007fcabc99e000  java lang Thread State  WAITING  parking    parking to wait for   0x000000073d14e618   a java util concurrent CompletableFuture Signaller   in our scene  when we dump the thread many times   we can see this will wait forever
14633	setupTopicPublishRateLimiterMonitor   can block forever  causing deadlock for metadata store operations  Describe the bug     pulsar pulsar broker src main java org apache pulsar broker service BrokerService java   Line 594 in 2b3e8ae        public synchronized void setupTopicPublishRateLimiterMonitor           This can block forever  causing metadata operations to deadlock   metadata store 6 1   61 prio 5 os_prio 0 cpu 139825 70ms elapsed 81027 01s tid 0x00007f1598001000 nid 0x8b waiting for monitor entry   0x00007f15a7805000  java lang Thread State  BLOCKED  on object monitor    waiting to lock  0x00000003018b1c30   a org apache pulsar broker service BrokerService  To Reproduce No reliable way to reproduce the problem  though we have seen this in production deployment  Expected behavior This method should not block
17446	Bug  Producer may be permanently blocked by chunking messages when blockIfQueueFull is enabled  Search before asking  I searched in the issues and found nothing similar   Version Pulsar 2 11 Minimal reproduce step It can be reproduced simply by following   Test public void testMiniChunkingBlockIfQueueFull   throws Exception   this conf setMaxMessageSize 1000    final String topicName    persistent   my property my ns testChunkingWithOrderingKey    final ExecutorService exec   Executors newFixedThreadPool 1     Cleanup Producer byte    producer   pulsarClient newProducer   topic topicName  enableChunking true   chunkMaxMessageSize 1   blockIfQueueFull true   maxPendingMessages 1000   enableBatching false  create     byte   data   RandomUtils nextBytes 1001   producer newMessage   value data  send       What did you expect to see  The reason for this bug is how the chunk message semaphore is obtained     pulsar pulsar client src main java org apache pulsar client impl ProducerImpl java   Lines 520 to 527 in 359cfa7           chunked message also sent individually so  try to acquire send permits     for  int i   0  i    totalChunks   1   i          if  canEnqueueRequest callback  message getSequenceId    0    The memory was already reserved            client getMemoryLimitController   releaseMemory uncompressedSize       semaphoreRelease i   1       return                   When a large message is split into a large number of chunks  i e  the message is too big or the chunkMaxMessageSize is set too small   all the remaining semaphores will be acquired  The sending  send   and sendAsync    of large message will be blocked by itself forever  By the way  once blockIfQueueFull maxPendingMessages chunking are enabled at the same time  this risk of deadlock exists even if the number of chunks of a single message is not very large  What did you see instead     Anything else  No response Are you willing to submit a PR   I m willing to submit a PR
17913	Bug  Deadlock while reading Schema from BookKeeper  Search before asking  I searched in the issues and found nothing similar   Version 2 10 2rc Minimal reproduce step There is a combination of facts in which you can end up in a stuck broker with the main ZK client thread stuck like this   main EventThread   18 daemon prio 5 os_prio 0 cpu 858 10ms elapsed 2757 17s tid 0x00007f32461ad800 nid 0x1f6db1 waiting on condition   0x00007f3213fb8000  java lang Thread State  WAITING  parking  at jdk internal misc Unsafe park java base 11 0 15 0 1 Native Method    parking to wait for   0x00000007f28a3860   a java util concurrent CompletableFuture Signaller   What did you expect to see  the broker works What did you see instead  the broker is stuck Anything else  It is a consequence of  17762 The main problem here is that with PulsarRegistrationClient even if we use the MetadataCache there is still a chance that we load the value with a blocking call to ZK  https   github com datastax pulsar blob 3738257bd5be07f317aa68c2217aececf28c1761 p … apache pulsar metadata bookkeeper PulsarRegistrationClient java in BookKeeper Zk Registration Driver we never perform reads in that method https   github com datastax bookkeeper blob 034ef8566ad037937a4d58a28f70631175744f … n java org apache bookkeeper discover ZKRegistrationClient java Are you willing to submit a PR   I m willing to submit a PR
18196	Bug  locallyAcquiredLocks leak in OwnershipCache  Search before asking  I searched in the issues and found nothing similar   Version master Minimal reproduce step review the code  34M What did you expect to see  public CompletableFuture Void  removeOwnership NamespaceBundle bundle    ResourceLock NamespaceEphemeralData  lock   locallyAcquiredLocks get bundle   if  lock    null       We don t own the specified bundle anymore return CompletableFuture completedFuture null      return lock release       should be locallyAcquiredLocks remove bundle  What did you see instead  leak Anything else  No response Are you willing to submit a PR   I m willing to submit a PR
18988	Bug  Deadlock pulsar io and metadata store if transactions enabled  Search before asking  I searched in the issues and found nothing similar   Version Seen in 2 10  believe the issue still exists in current master  not able to reproduce Minimal reproduce step Not able to reproduce it programatically What did you expect to see  To not get deadlock What did you see instead  Metadata store         2022 12 19T08 36 33 404 0000   2022 12 19T08 36 30 911356003Z stdout F         locked org apache pulsar broker transaction pendingack impl PendingAckHandleImpl 1289d850 2022 12 19T08 36 33 404 0000   2022 12 19T08 36 30 911309219Z stdout F      owned by pulsar io 12 16 Id 161 2022 12 19T08 36 33 404 0000   2022 12 19T08 36 30 911302363Z stdout F  metadata store 18 1  Id 16 in BLOCKED on lock org apache pulsar broker service persistent PersistentSubscription 981a97c  and pulsar io          2022 12 19T08 36 33 404 0000   2022 12 19T08 36 30 911574831Z stdout F         locked org apache pulsar broker service persistent PersistentSubscription 981a97c 2022 12 19T08 36 33 404 0000   2022 12 19T08 36 30 911536996Z stdout F         locked org apache pulsar broker service persistent PersistentSubscription 981a97c 2022 12 19T08 36 33 404 0000   2022 12 19T08 36 30 911518203Z stdout F      owned by metadata store 18 1 Id 16 2022 12 19T08 36 33 404 0000   2022 12 19T08 36 30 911507882Z stdout F  pulsar io 12 16  Id 161 in BLOCKED on lock org apache pulsar broker transaction pendingack impl PendingAckHandleImpl 1289d850   Anything else  No response Are you willing to submit a PR   I m willing to submit a PR