1614	Query hangs due to missed TaskSource update in Driver  Driver can leave source updates stuck in newSources forever  This hangs the query since the task will never see the  noMoreSplits  message   I believe this is caused by a race where a TaskSource is added between the call to processNewSources and the unlock in DriverLockResult close     The TaskSource gets queued in newSources  but can not get the lock because is is held   The state machine design is flawed and will need to be redesigned
1689	Race condition that fails a query  Stack trace  Caused by  java lang IllegalStateException  Expected a single output buffer for task 20140918_000254_00010_a2utt 0 0  but found     This happens fairly regularly in the CI  same issue  but different test run from the above   com facebook presto client FailureInfo FailureException  Expected response code from http   127 0 0 1 43251 v1 task 20140918_231251_00351_xikpe 1 3 summarize to be 200  but was 500  Request failed
3629	TestStateMachine testSetIf   fails non deterministically  This happens pretty frequently on Travis  so likely is some kind of race  testSetIf com facebook presto execution TestStateMachine   Time elapsed  0 253 sec      FAILURE! java lang AssertionError  expected  true  but found  false
3647	Report a possible race issue  Hi  Developers of facebook presto  I am writing to report a race issue on use of ConcurrentHashMap  The issue is reported by our tool in an automatic way  Although manually confirmed  it would be a false positive  given we do not know the specification of the program  We would very appreciate if you could check below for details and confirm with us whether it is a real problem  For more information  please refer to our website  http   sav sutd edu sg  page_id 2845 File  facebook presto presto blackhole src main java com facebook presto plugin blackhole BlackHoleMetadata java Location  Line  149 150 172 136  Description  The remove then put operations on  tables  in line 149 and 150 are guarded by the lock  tables   If the intention is to guarantee the atomicity of the remove then put operations  then the write operations in line 136  172 may break this atomicity  Relying on the ConcurrentHashMap to ensure exclusive access is dangerous since ConcurrentHashMap has no guarantee of exclusive access
4869	Potential deadlock when using phased scheduler with broadcast joins  SELECT   FROM orders CROSS JOIN  VALUES 1   Fragment 0  SINGLE  Output layout   orderkey  custkey  orderstatus  totalprice  orderdate  orderpriority  clerk  shippriority  comment  field  Output partitioning  SINGLE      Output orderkey  custkey  orderstatus  totalprice  orderdate  orderpriority  clerk  shippriority  comment  _col9      orderkey bigint  custkey bigint  orderstatus varchar  totalprice double  orderdate date  orderpriority  _col9    field   RemoteSource 1      orderkey bigint  custkey bigint  orderstatus varchar  totalprice double  orderdate date  orderpriority varchar  clerk varchar  shippriority bigint  comment varchar  field bigint   Fragment 1  SOURCE  Output layout   orderkey  custkey  orderstatus  totalprice  orderdate  orderpriority  clerk  shippriority  comment  field  Output partitioning  SINGLE      InnerJoin       orderkey bigint  custkey bigint  orderstatus varchar  totalprice double  orderdate date  orderpriority varchar  clerk varchar  shippriority bigint  comment varchar  field bigint    TableScan tpch tpch orders sf1 0  originalConstraint   true      orderkey bigint  custkey bigint  orderstatus varchar  totalprice double  orderdate date  orderpriority varchar  clerk varchar  shippriority bigint  comm orderkey    tpch orderkey custkey    tpch custkey orderstatus    tpch orderstatus totalprice    tpch totalprice orderdate    tpch orderdate orderpriority    tpch orderpriority clerk    tpch clerk shippriority    tpch shippriority comment    tpch comment   RemoteSource 2      field bigint   Fragment 2  SINGLE  Output layout   field  Output partitioning  BROADCAST      Values     field bigint   1   The scheduling dependency graph is missing an edge  2  1   which can cause the join stage to be scheduled independently  and before  the build stage
5005	presto cli  shutdown hook cannot cancel the running query occasionally  The shutdown hook added in  3521 sometimes cannot cancel the query properly  I have tracked down the problem to the following two issues   In StatementClien advance   loop  if an interrupt is received in the httpClient execute   call  in the exception handler I see that we correctly receive an InterruptedIOException  but the interrupt flag is somehow not set  a bug somewhere else    so the query doesn t get canceled and the loop just goes on  In StatementClient close   we do an async http call and there is probably a race where the main thread closes the http client  in QueryRunner close    and the other thread doing the http call fails silently  we are not checking the status of this call    For the first problem I currently handle InterruptedIOException as a special case and close the client  and also interrupt the main thread   For the second one I just do a synch  http call to get rid of the race  With these changes I cannot reproduce this problem anymore   electrum any thoughts
5110	Race condition in array_concat  A query like  select count x  from   select concat a  b  x from t   where a and b are of type array map string  double    fails non deterministically with various exceptions  Presto version 0 144 2   java lang IllegalStateException  closeEntry called before anything is written  Query 20160425_183530_20138_n949w failed  Expected current entry size to be exactly 0 but was 0 java lang IllegalStateException  Expected current entry size to be exactly 0 but was 0  Query 20160425_183539_20170_n949w failed  Expected current entry size to be exactly 0 but was 1 java lang IllegalStateException  Expected current entry size to be exactly 0 but was 1
5116	Race in TestTaskExecutor testTaskHandle  This test fails non deterministically with  java lang AssertionError  expected  1  but found  0  Expected  1 Actual    0  Click to see difference     It s pretty easy to repro if you set the invocation count to 10k
5735	OperatorContext is not ThreadSafe   Contrary to the misleading  ThreadSafe annotation and usage of AtomicLong everywhere in it OperatorContext is not ThreadSafe  I have stumbled on it while implementing some things related to spilling  5142 and revocable memory  5711 There is at least problem with setMemoryReservation  trySetMemoryReservation methods and transferMemoryToTaskContext  private final AtomicLong memoryReservation   new AtomicLong          public void setMemoryReservation long newMemoryReservation    checkArgument newMemoryReservation    0   newMemoryReservation is negative     long delta   newMemoryReservation   memoryReservation get     if  delta   0    reserveMemory delta     else   freeMemory  delta        Example scenario  memoryReservation equals to 10   caller A initiate setMemoryReservation 3   A thread calculates delta    7 and hangs  caller B changes memory reservation by calling setMemoryReservation 3  which finishes before A thread resumes work  This give us memoryReservation   3  A thread resumes work  freeMemory 7  is executed  we end up with memoryReservation    4 which crashes query   This bug s  might haven t shown up so far  because this class is mostly  only    used by single driver s thread  I m not familiar with this part of code and I don t feel whether synchronization here is costly but I would be inclined to drop all AtomicLongs here and switch to synchronized methods  It would be safer and since OperatorContext calls should be rare I don t expect it would have negative performance impact  CC   cberner  sopel39
6196	Deadlock in task info fetching  Hit this one in production  Suddenly all queries get queued and all clients start timing out  Here is the full stack trace  Found one Java level deadlock                                 http worker 2010473   waiting to lock monitor 0x00007f8fed4a6fb8  object 0x00007f9615a7e020  a com facebook presto execution resourceGroups ResourceGroup RootResourceGroup   which is held by  ResourceGroupManager   ResourceGroupManager   waiting to lock monitor 0x00007f91f0cc3598  object 0x00007f9a9f000a48  a com facebook presto execution SqlStageExecution   which is held by  HttpRemoteTask 20160926_222904_08073_w6q27 1 105 2010288   HttpRemoteTask 20160926_222904_08073_w6q27 1 105 2010288   waiting to lock monitor 0x00007f90ed8c3ee8  object 0x00007f9a9f000870  a com facebook presto server remotetask ContinuousTaskStatusFetcher   which is held by  ContinuousTaskStatusFetcher 20160926_222904_08073_w6q27 1 105 2009564   ContinuousTaskStatusFetcher 20160926_222904_08073_w6q27 1 105 2009564   waiting to lock monitor 0x00007f910c0b5948  object 0x00007f9a9f0008e0  a com facebook presto server remotetask HttpRemoteTask   which is held by  HttpRemoteTask 20160926_222904_08073_w6q27 1 105 2010288   Java stack information for the threads listed above                                                       http worker 2010473     waiting to lock  0x00007f9615a7e020   a com facebook presto execution resourceGroups ResourceGroup RootResourceGroup   ResourceGroupManager     waiting to lock  0x00007f9a9f000a48   a com facebook presto execution SqlStageExecution    locked  0x00007f9615a7e020   a com facebook presto execution resourceGroups ResourceGroup RootResourceGroup    locked  0x00007f9615a7e020   a com facebook presto execution resourceGroups ResourceGroup RootResourceGroup    locked  0x00007f9615a7e020   a com facebook presto execution resourceGroups ResourceGroup RootResourceGroup   HttpRemoteTask 20160926_222904_08073_w6q27 1 105 2010288     waiting to lock  0x00007f9a9f000870   a com facebook presto server remotetask ContinuousTaskStatusFetcher    locked  0x00007f9a9f0008e0   a com facebook presto server remotetask HttpRemoteTask    locked  0x00007f9a9f0008e0   a com facebook presto server remotetask HttpRemoteTask    locked  0x00007f9a9f000a48   a com facebook presto execution SqlStageExecution   ContinuousTaskStatusFetcher 20160926_222904_08073_w6q27 1 105 2009564     waiting to lock  0x00007f9a9f0008e0   a com facebook presto server remotetask HttpRemoteTask    locked  0x00007f9a9f000870   a com facebook presto server remotetask ContinuousTaskStatusFetcher   Found 1 deadlock   Deadlock in task info fetching · Issue  6196 · prestodb presto · GitHub Hit this one in production  Suddenly all queries get queued and all clients start timing out  Here is the full stack trace  Found one Java level deadlock                                 http worker 2010473   waiting to lock monitor 0x00007f8fed4a6fb8  object 0x00007f9615a7e020  a com facebook presto execution resourceGroups ResourceGroup RootResourceGroup   which is held by  ResourceGroupManager   ResourceGroupManager   waiting to lock monitor 0x00007f91f0cc3598  object 0x00007f9a9f000a48  a com facebook presto execution SqlStageExecution   which is held by  HttpRemoteTask 20160926_222904_08073_w6q27 1 105 2010288   HttpRemoteTask 20160926_222904_08073_w6q27 1 105 2010288   waiting to lock monitor 0x00007f90ed8c3ee8  object 0x00007f9a9f000870  a com facebook presto server remotetask ContinuousTaskStatusFetcher   which is held by  ContinuousTaskStatusFetcher 20160926_222904_08073_w6q27 1 105 2009564   ContinuousTaskStatusFetcher 20160926_222904_08073_w6q27 1 105 2009564   waiting to lock monitor 0x00007f910c0b5948  object 0x00007f9a9f0008e0  a com facebook presto server remotetask HttpRemoteTask   which is held by  HttpRemoteTask 20160926_222904_08073_w6q27 1 105 2010288   Java stack information for the threads listed above                                                       http worker 2010473     waiting to lock  0x00007f9615a7e020   a com facebook presto execution resourceGroups ResourceGroup RootResourceGroup   ResourceGroupManager     waiting to lock  0x00007f9a9f000a48   a com facebook presto execution SqlStageExecution    locked  0x00007f9615a7e020   a com facebook presto execution resourceGroups ResourceGroup RootResourceGroup    locked  0x00007f9615a7e020   a com facebook presto execution resourceGroups ResourceGroup RootResourceGroup    locked  0x00007f9615a7e020   a com facebook presto execution resourceGroups ResourceGroup RootResourceGroup   HttpRemoteTask 20160926_222904_08073_w6q27 1 105 2010288     waiting to lock  0x00007f9a9f000870   a com facebook presto server remotetask ContinuousTaskStatusFetcher    locked  0x00007f9a9f0008e0   a com facebook presto server remotetask HttpRemoteTask    locked  0x00007f9a9f0008e0   a com facebook presto server remotetask HttpRemoteTask    locked  0x00007f9a9f000a48   a com facebook presto execution SqlStageExecution   ContinuousTaskStatusFetcher 20160926_222904_08073_w6q27 1 105 2009564     waiting to lock  0x00007f9a9f0008e0   a com facebook presto server remotetask HttpRemoteTask    locked  0x00007f9a9f000870   a com facebook presto server remotetask ContinuousTaskStatusFetcher   Found 1 deadlock
6319	Fix thread safety of IndexLookup in AccumuloClient AccumuloClient is a singleton and contains an instance of IndexLookup  It then calls setAuths   from getTabletSplits   which is called from different threads  Additionally  IndexLookup uses ColumnCardinalityCache which has a normal HashMap without any synchronization  This code path is also called from multiple threads  This should probably be done after  6318  since several of the classes involved store an AccumuloConfig
6755	Potential deadlock in db backed resource group config  manager  While I was working on the reloadable file backed resource group config manager I hit a deadlock situation  which I think also applies to the db backed config manager unless I am missing something   cc  elonazoulay  cberner Assume we have these two threads  T1   DbResourceGroupConfigurationManager           T2  pool thread in ClusterMemoryManager   configureChangedGroups                             AbstractResourceConfigurationManager  memory_change_listener_lambda   lock root group                                    lock generalPoolMemoryFraction  configureGroup                                     InternalResourceGroup  setSoftMemoryLimit   lock generalPoolMemoryFraction                     lock root group
6923	Performance regression for OUTER JOINs  Following simple query  SELECT COUNT    FROM store_sales s FULL OUTER JOIN web_sales w ON s ss_item_sk   w ws_item_sk AND s ss_sold_date_sk   w ws_sold_date_sk   has slowed down between versions 154 and 157 by the factor of 10  On 8 nodes cluster it was completing in ~60 seconds now it completes in over 540 seconds  with very low CPU usage  constant ~20  of 24 cores  regardless of the task_concurrency  It doesn t seem to be the problem with hash partitioning   6864   From profiling it seems like the problem is with OuterLookupSource OuterPositionTracker positionVisited method  called from PartitionedLookupSource appendTo  in which code spends ~90  of the time  This code was changed recently by  dain here 4f0f9fa Previous version of PartitionedLookupSource didn t synchronize visitedPositions     array  but OuterLookupSource did synchronize it  Currently  both are synchronized  From profiling it looks like synchronization locking prevents query from utilizing more CPU  I didn t have time to look deeper into this code  but is this synchronization absolutely required  Has something else changed recently in this area  CC  KBP TDC
7667	db TestQueues testBasic is nondeterministic  db TestQueues testBasic sometimes fails due to a race condition  Tests that have the similar wait assert pattern can have the same issue   In db TestQueues testBasic we call waitForQueryState   to wait until a query is running  This function directly checks the state machine of a query  However  a following assertion checks a counter maintains by the state change listener  It s possible that after a state change happens  the listener has not been triggered or has not finished at the point we are checking the assert  Such a race condition causes the related test  not only the TestQueues testBasic  to be nondeterministic  see error log here    To reproduce the issue  one can comment out this line to increase the possibility of the failure
7689	Flaky tests that rely on async counters variables  Test cases like AbstractTestDistributedQueries testQueryLoggingCount assert on a counter that is updated by a query state change listener  There is a race condition between the assertion on the counter and the execution of the listener  similar issue here  7667   To reproduce the problem  one can manually add a sleep in the listener callback  e g  sleep 10ms here
8716	Hang in AWS SDK with parallel uploads under load  As PrestoS3FileSystem uploadObject is using aws s3 sdk 1 10 30  a race condition in aws sdk waitForCompletion method affects it  A proposal to update AWS SDK to a newer stable version  A race condition that can occur with multiple calls to CopyMonitor setFuture and UploadMonitor setFuture that can cause an infinite loop in AbstractTransfer waitForCompletion
9881	Race condition in OuterPositionTracker  We saw an error in the OuterPositionTracker  where the outer position iterator is being fetched while there are still reference counts  I think this might be caused because the query was tearing down due to a limit   Here is the stack  com google common base VerifyException
11253	Split scheduling is broken for colocated join  There are two bugs that affect scheduling for fragment stage with more than one scan nodes  Only colocated join can create such fragment stage at this time  The combined outcome is that there is no back pressure for split scheduling for all but the first scan node  in source scheduling order  in a stage  bug 1 When a fragment stage contains multiple scan nodes  None of the scan nodes will receive a TaskSource where TaskSource noMoreSplits   true until scheduling for splits for all the scan nodes in the stage finishes  In SqlStageExecution  there are 2 places where completeSources variable is updated  One in schedulingComplete  which happens after the entire stage finishes scheduling  The other in addExchangeLocations  which isn t relevant for scan nodes  In SqlStageExecution  there are 3 places where task noMoreSplits is invoked  It s invoked in scheduleTask  where task noMoreSplits is invoked for each element in completeSources  The other two are schedulingComplete and addExchangeLocation  This bug seems unlikely because the bug should have caused frequent query deadlock  That leads me to bug 2  bug 2 In PipelineContext getPipelineStatus  queuedDrivers is computed by looking at DriverContexts  However  note that in SqlTaskExecution schedulePartitionedSource  there is this concept of pendingSplitsByPlanNode  pendingSplitsByPlanNode buffers splits for scan nodes that aren t yet eligible to schedule because another scan node who is ahead in term of source scheduling order hasn t finished scheduling  Specifically  that  another scan node  is the first scan node  in source scheduling order   Due to bug 1  it will not finish scheduling until all splits for the stage are delivered to workers  queuedDrivers should include splits in pendingSplitsByPlanNode although a DriverSplitRunner is yet to be created for those splits  The fact that the worker chose to defer the creation of those drivers is an implementation detail  Conceptually  those splits have been delivered to the worker  and the worker has created the drivers for them although those drivers are  blocked   not runnable   Now look at NodeScheduler selectDistributionNodes  It depends on NodeAssignmentStats getTotalSplitCount  It is effectively queuedDrivers   recent assignment  While recent assignment would increment  it is reset to zero whenever the split is delivered to workers  queuedDrivers would be a small number  and eventually hit zero  due to this bug  This bug also leads to misleading unituitive client stats  mutual effect If bug 2 gets fixed alone  it would lead to scheduling deadlocks  If bug 1 gets fixed alone  it will restore back pressure and somewhat mitigate bug 2
13142	Backport  Handle race in QueryStateTimer   backport trinodb trino 841   This should fix failures like  ERROR  Tests run  2240  Failures  1  Errors  0  Skipped  2  Time elapsed  2 808 075 s     FAILURE!   in TestSuite  ERROR  tearDown com facebook presto execution resourceGroups db TestQueuesDb   Time elapsed  5 906 s      FAILURE! java lang IllegalArgumentException  value is negative