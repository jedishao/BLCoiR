582	Race in jar upload during hadoop indexing  Hadoop indexing  through io druid indexer JobHelper  does something like  if  !fs exists jar     upload jar using fs create   to workingPath classpath jarName jar    The same jar path is used for all druid jobs running on the same cluster  If two jobs start at the same time  both can find that the file doesn t exist  and the second one can overwrite the first one  This will cause the AM to fail because the hadoop AM is pretty particular about the mtime of its jar files  Possible solutions   a  Better locking  or  b  have each job upload its jars to a separate directory
812	RealtimePlumber  Race between server view callback and merge push thread  If you have redundant realtime tasks  a historical node can load a segment pushed by task A while task B is still performing final merging of the segment  In this case  task B s server view callback will see the loaded segment and call abandonSegment  This will unannounce the segment  delete the local files  and release locks in the indexing service  if you are running inside the indexing service   This will cause havoc in the merge push thread that is still trying to merge the segment
1014	real time node lead to wrong result empty result  for the request with regexp filter when real time node is ingesting kafka msg  increase the kafka message send speed gradually  real time query result will be wrong for the request with regex filter  request is   filter      type    regex    dimension    sub5    pattern    B         orderBy      type    default    columns        dimension    clicks    direction    DESCENDING        limit   3    result is       version     v1    timestamp     2015 01 06T05 37 00 000Z    event       clicks    28         version     v1    timestamp     2015 01 06T05 37 00 000Z    event       clicks    3   sub5     BB1            Note  I did not send empty value for the dimension I queried  But result show one null value for the sub5 dimension  query later  the empty result disappear  It seems thread race condition leads to this error  If not solve the problem  druid will not be really real time system  both tested on 0 6 121 1 and 0 6 160
1210	race condition in coordinator  the coordniator node stopped responding  and so did the broker   The coordinator logs say  Crazy race condition   Druid 0 6 160 logs of the coordinator    2015 03 17 05 35 19 607 INFO  Master PeonExec  0  com metamx emitter core LoggingEmitter   Event    feed   alerts   timestamp   2015 03 17T05 35 19 588Z   service   coordinator   host   191 237 157 96 8080   severity   component failure   description   Crazy race condition! server  druidprod base loadQueue 158 85 8 18 8080    data    class   io druid server coordinator LoadQueuePeon        2015 03 17 05 35 19 580 ERROR  Master PeonExec  0  io druid server coordinator LoadQueuePeon   Crazy race condition! server  druidprod base loadQueue 158 85 8 18 8080    class io druid server coordinator LoadQueuePeon     2015 03 17 05 35 18 594 INFO  CoordinatorLeader 0  io druid curator discovery CuratorServiceAnnouncer   Unannouncing service DruidNode serviceName  coordinator   host  191 237 157 96 8080   port 8080      2015 03 17 05 35 18 554 INFO  main EventThread  org apache curator framework state ConnectionStateManager   State change  RECONNECTED    2015 03 17 05 35 18 553 INFO  main SendThread 104 130 162 16 2181   org apache zookeeper ClientCnxn   Session establishment complete on server 104 130 162 16 104 130 162 16 2181  sessionid   0x24bde5fbb1f0007  negotiated timeout   30000    2015 03 17 05 35 18 541 INFO  main SendThread 104 130 162 16 2181   org apache zookeeper ClientCnxn   Socket connection established to 104 130 162 16 104 130 162 16 2181  initiating session    2015 03 17 05 35 18 532 INFO  main SendThread 104 130 162 16 2181   org apache zookeeper ClientCnxn   Opening socket connection to server 104 130 162 16 104 130 162 16 2181  Will not attempt to authenticate using SASL  unknown error     2015 03 17 05 35 18 480 INFO  CoordinatorLeader 0  io druid server coordinator DruidCoordinator   I am no longer the leader       2015 03 17 05 35 18 472 INFO  main EventThread  org apache curator framework state ConnectionStateManager   State change  SUSPENDED    2015 03 17 05 35 18 356 INFO  main SendThread 104 130 16 203 2181   org apache zookeeper ClientCnxn   Client session timed out  have not heard from server in 20008ms for sessionid 0x24bde5fbb1f0007  closing socket connection and attempting reconnect    2015 03 17 05 35 07 878 INFO  Coordinator Exec  0  com metamx emitter core LoggingEmitter   Event    feed   metrics   timestamp   2015 03 17T05 35 07 878Z   service   coordinator   host   191 237 157 96 8080   metric   coordinator segment count   value  2852  user1   d
1360	Race condition in autoscaling terminates nodes that were just assigned tasks  It is possible for the indexing service auto scaling to terminate nodes that have just been assigned a task  but that have not updated the task status to running yet  Autoscaling currently only relies on task status provided by the workers  but does not take into account tasks that are known to be running or about to run on a given worker  If you are unlucky and autoscaling checks right after the task has been assigned  but right before the worker announces the task status  and the worker is not running any other tasks  your tasks will get killed  Running task replicas may not help in this situations  If the indexing service was recently updated and spawned new middle managers that aren t running anything yet  it is possible for multiple nodes to be killed that were about to run tasks
1715	Zombie tasks able to acquire locks after failure  We had an issue where disconnect somewhere occurred long enough for the overlord to log a task as failed  but the task was still running  After 45 mins or so the task submitted a lock acquire request and it was granted  but since the task had already been failed  it never was properly cleaned up  These are from the overlord 2015 09 09T07 33 58 102 INFO  RemoteTaskRunner Scheduled Cleanup  0  io druid indexing overlord RemoteTaskRunner   Running scheduled cleanup for Worker REDACTED 8080  2015 09 09T07 33 58 106 INFO  RemoteTaskRunner Scheduled Cleanup  0  io druid indexing overlord RemoteTaskRunner   Failing task index_realtime_REDACTED  2015 09 09T07 33 58 106 INFO  RemoteTaskRunner Scheduled Cleanup  0  io druid indexing overlord TaskQueue   Received FAILED status for task  index_realtime_REDACTED 2015 09 09T07 33 58 109 INFO  RemoteTaskRunner Scheduled Cleanup  0  io druid indexing overlord RemoteTaskRunner   Can t shutdown! No worker running task index_realtime_REDACTED 2015 09 09T07 33 58 110 INFO  RemoteTaskRunner Scheduled Cleanup  0  io druid indexing overlord MetadataTaskStorage   Updating task index_realtime_REDACTED to status  TaskStatus id index_realtime_REDACTED  status FAILED  duration  1  2015 09 09T07 33 58 132 INFO  RemoteTaskRunner Scheduled Cleanup  0  io druid indexing overlord TaskLockbox   Removing task index_realtime_REDACTED  from TaskLock index_realtime_REDACTED   2015 09 09T08 15 15 428 INFO  qtpREDACTED  io druid indexing common actions LocalTaskActionClient   Performing action for task index_realtime_REDACTED   LockAcquireAction interval 2015 09 09T08 00 00 000Z 2015 09 09T09 00 00 000Z  2015 09 09T08 15 15 428 INFO  qtpREDACTED  io druid indexing overlord TaskLockbox   Created new TaskLockPosse  TaskLockPosse taskLock TaskLock groupId index_realtime_REDACTED  dataSource REDACTED  interval 2015 09 09T08 00 00 000Z 2015 09 09T09 00 00 000Z  version 2015 09 09T08 15 15 428Z   taskIds     2015 09 09T08 15 15 428 INFO  qtpREDACTED  io druid indexing overlord TaskLockbox   Added task index_realtime_REDACTED  to TaskLock index_realtime_REDACTED   Suffice to say  this is a long running task that acquires multiple 1 hr locks during the course of its execution
2793	Duplicate primary key errors cause TaskQueue big lock to be held for way longer than it should   1896 introduced an addition of e4e5f03 diff e677e1ba7e3cf3b5b97660cfc17749beR144 which causes errors on duplicate primary key entries to be retried way more than they should  2016 04 05T20 52 57 269 WARN  qtp726762476 61  com metamx common RetryUtils   Failed on try 1  retrying in 1 976ms  org skife jdbi v2 exceptions CallbackFailedException  org skife jdbi v2 exceptions UnableToExecuteStatementException  com mysql jdbc exceptions jdbc4 MySQLIntegrityConstraintViolationException  Duplicate entry  index_realtime_REDACTED_2016 04 05T20 00 0  for key  PRIMARY   statement  INSERT INTO REDACTED  id  created_date  datasource  payload  active  status_payload  VALUES   id   created_date   datasource   payload   active   status_payload    located  INSERT INTO REDACTED  id  created_date  datasource  payload  active  status_payload  VALUES   id   created_date   datasource   payload   active   status_payload    rewritten  INSERT INTO REDACTED  id  created_date  datasource  payload  active  status_payload  VALUES                      arguments   positional     named  payload       at org skife jdbi v2 DBI withHandle DBI java 284  ~ druid selfcontained 0 9 0 rc3 mmx2 jar 0 9 0 rc3 mmx2  at io druid metadata SQLMetadataConnector 2 call SQLMetadataConnector java 110  ~ druid selfcontained 0 9 0 rc3 mmx2 jar 0 9 0 rc3 mmx2  at com metamx common RetryUtils retry RetryUtils java 38   druid selfcontained 0 9 0 rc3 mmx2 jar 0 9 0 rc3 mmx2  at io druid metadata SQLMetadataConnector retryWithHandle SQLMetadataConnector java 115   druid selfcontained 0 9 0 rc3 mmx2 jar 0 9 0 rc3 mmx2  at io druid metadata SQLMetadataStorageActionHandler insert SQLMetadataStorageActionHandler java 97   druid selfcontained 0 9 0 rc3 mmx2 jar 0 9 0 rc3 mmx2  at io druid indexing overlord MetadataTaskStorage insert MetadataTaskStorage java 134   druid selfcontained 0 9 0 rc3 mmx2 jar 0 9 0 rc3 mmx2  at io druid indexing overlord TaskQueue add TaskQueue java 321   druid selfcontained 0 9 0 rc3 mmx2 jar 0 9 0 rc3 mmx2  at io druid indexing overlord http OverlordResource 1 apply OverlordResource java 124   druid selfcontained 0 9 0 rc3 mmx2 jar 0 9 0 rc3 mmx2  at io druid indexing overlord http OverlordResource 1 apply OverlordResource java 119   druid selfcontained 0 9 0 rc3 mmx2 jar 0 9 0 rc3 mmx2  at io druid indexing overlord http OverlordResource asLeaderWith OverlordResource java 518   druid selfcontained 0 9 0 rc3 mmx2 jar 0 9 0 rc3 mmx2  at io druid indexing overlord http OverlordResource taskPost OverlordResource java 116   druid selfcontained 0 9 0 rc3 mmx2 jar 0 9 0 rc3 mmx2  at sun reflect NativeMethodAccessorImpl invoke0 Native Method  ~   1 8 0_60  at sun reflect NativeMethodAccessorImpl invoke NativeMethodAccessorImpl java 62  ~   1 8 0_60  at sun reflect DelegatingMethodAccessorImpl invoke DelegatingMethodAccessorImpl java 43  ~   1 8 0_60  at java lang reflect Method invoke Method java 497  ~   1 8 0_60  at com sun jersey spi container JavaMethodInvokerFactory 1 invoke JavaMethodInvokerFactory java 60   druid selfcontained 0 9 0 rc3 mmx2 jar 0 9 0 rc3 mmx2  at com sun jersey server impl model method dispatch AbstractResourceMethodDispatchProvider ResponseOutInvoker
2842	NPE in RTR with multiple threads for task assignment  Multiple threads in task assignment causes NPE  Seems to have been introduced by  2521 2016 04 15T09 56 11 335 ERROR  rtr pending tasks runner 2  io druid indexing overlord RemoteTaskRunner   Exception while trying to assign task   class io druid indexing overlord RemoteTask Runner  exceptionType class java lang NullPointerException  exceptionMessage task  taskId  TASKID   java lang NullPointerException  task at com google common base Preconditions checkNotNull Preconditions java 229  ~ druid selfcontained 7aaf2b3 jar 7aaf2b3  at io druid indexing overlord RemoteTaskRunner tryAssignTask RemoteTaskRunner java 638  ~ druid selfcontained 7aaf2b3 jar 7aaf2b3  at io druid indexing overlord RemoteTaskRunner access 1400 RemoteTaskRunner java 116  ~ druid selfcontained 7aaf2b3 jar 7aaf2b3  at io druid indexing overlord RemoteTaskRunner 3 call RemoteTaskRunner java 568   druid selfcontained 7aaf2b3 jar 7aaf2b3  at io druid indexing overlord RemoteTaskRunner 3 call RemoteTaskRunner java 556   druid selfcontained 7aaf2b3 jar 7aaf2b3  at java util concurrent FutureTask run FutureTask java 266     1 8 0_60  at java util concurrent ThreadPoolExecutor runWorker ThreadPoolExecutor java 1142     1 8 0_60  at java util concurrent ThreadPoolExecutor Worker run ThreadPoolExecutor java 617     1 8 0_60  at java lang Thread run Thread java 745     1 8 0_60  2016 04 15T09 56 11 336 ERROR  rtr pending tasks runner 2  io druid indexing overlord RemoteTaskRunner   Exception in running pending tasks   class io druid indexing overlord RemoteTaskRun ner  exceptionType class java lang NullPointerException  exceptionMessage taskRunnerWorkItem  java lang NullPointerException  taskRunnerWorkItem at com google common base Preconditions checkNotNull Preconditions java 229  ~ druid selfcontained 7aaf2b3 jar 7aaf2b3  at io druid indexing overlord RemoteTaskRunner taskComplete RemoteTaskRunner java 1048  ~ druid selfcontained 7aaf2b3 jar 7aaf2b3  at io druid indexing overlord RemoteTaskRunner access 1500 RemoteTaskRunner java 116  ~ druid selfcontained 7aaf2b3 jar 7aaf2b3  at io druid indexing overlord RemoteTaskRunner 3 call RemoteTaskRunner java 577   druid selfcontained 7aaf2b3 jar 7aaf2b3  at io druid indexing overlord RemoteTaskRunner 3 call RemoteTaskRunner java 556   druid selfcontained 7aaf2b3 jar 7aaf2b3  at java util concurrent FutureTask run FutureTask java 266     1 8 0_60  at java util concurrent ThreadPoolExecutor runWorker ThreadPoolExecutor java 1142     1 8 0_60  at java util concurrent ThreadPoolExecutor Worker run ThreadPoolExecutor java 617     1 8 0_60  at java lang Thread run Thread java 745     1 8 0_60
2991	Race condition in IncrementalIndex s facts  In OnheapIncrementalIndex  addToFacts       when we get a new fact  we will first add the mapping from key to fact s index  then we do aggregate  It will cause query saw some unaggregated metrics with initial value  and sometimes may cause some weird situations  Like if we use LongMax for build but use LongSum for query  then the result may add LongMax s initial value which is LONG MIN_VALUE  BTW  do we need to deal with this  final Integer prev   facts putIfAbsent key  rowIndex   if  null    prev    numEntries incrementAndGet      else      We lost a race aggs   concurrentGet prev      Free up the misfire concurrentRemove rowIndex      This is expected to occur ~80  of the time in the worst scenarios    I don t find any multi thread adds for IncremetalIndex
3063	Unloadable lookups can tie up all query threads  In 0 9 1 rc1  if the coordinator post to  druid listen v1 involves a lookup that can t be loaded  that will cause the request to block until the lookup load times out  After 10s the coordinator will time out this request  Later on it will issue a new one  but if the original request s thread is still active  which it totally could be   the new request s thread will just block while trying to acquire the lock held by the first request thread  Fast forward a few minutes  and all jetty threads are blocked on lookup related stuff and no queries can be made  Possible solutions   Make  druid listen v1 requests nonblocking  by having them insert into a queue rather than actually do the work  The work would be done by another thread  The request thread would have to fail when the queue is full rather than block on full  Or if the requests really need to be blocking  limit the number of concurrent  druid listen v1 requests  by failing any requests if there are more than X in flight   X   1 seems reasonable to me
3393	should handle channel disconnected better in NettyHttpClient or Druid  I m facing such a race condition  Broker runs a query  finally calls the httpClient go   in DirectDruidClient  the httpClient is instance of NettyHttpClient  Inside the NettyHttpClient go   method  after the channel is taken from the pool and verified in good shape and ready to fire the query  then the historical side reaches the idle time and disconnect this channel  Then handler channelDisconnected callback is called  and finally the query is failed in broker  This exception occurs hundreds of times  in our druid cluster  it is a busy cluster   This channel never has a chance to actually connect to the historical to get a single byte back  thus  I think  the NettyHttpClient go   should retry internally one time to get a new channel to finish the  go  instead of set the retVal future to fail  The same thing can happen when channel write httpRequest  addListener      in NettyHttpClient  the channel to write can be closed by historical server after it is taken from the pool channel  thus it also should be retried  here is the trace  2016 08 24T14 03 29 029 ERROR  qtp1369854401 202 timeseries_eternal_olap_click_da76287c 4eec 4c84 a8b8 2e12ea92aa05   io druid server QueryResource   Exception handling request   class io druid server QueryResource  exceptionType class com metamx common RE  exceptionMessage Failure getting results from http   hdppic0101 et2 tbsite net 8083 druid v2   because of  org jboss netty channel ChannelException  Channel disconnected   exception com metamx common RE  Failure getting results from http   hdppic0101 et2 tbsite net 8083 druid v2   because of  org jboss netty channel ChannelException  Channel disconnected   query TimeseriesQuery dataSource  eternal_olap_click   querySegmentSpec LegacySegmentSpec intervals  2016 08 24T00 00 00 000 08 00 2016 08 24T13 54 00 000 08 00    descending false  dimFilter  scene_tag   sort_type C_coefp && bu_src   taobao_topic && item_id   tm6048738 && exper_token_str   wl_topic_sort && action_type   click   granularity  PeriodGranularity period PT1M  timeZone  08 00  origin null    aggregatorSpecs  DoubleSumAggregatorFactory fieldName  ipv   name  ipv     postAggregatorSpecs     context  queryId da76287c 4eec 4c84 a8b8 2e12ea92aa05  timeout 30000    peer 10 197 16 234  com metamx common RE  Failure getting results from http   hdppic0101 et2 tbsite net 8083 druid v2   because of  org jboss netty channel ChannelException  Channel disconnected  at io druid client DirectDruidClient JsonParserIterator init DirectDruidClient java 498  ~ druid server 0 9 0 jar 0 9 0  at io druid client DirectDruidClient JsonParserIterator hasNext DirectDruidClient java 442  ~ druid server 0 9 0 jar 0 9 0  at com metamx common guava BaseSequence makeYielder BaseSequence java 103  ~ java util 0 27 7 jar    at com metamx common guava BaseSequence toYielder BaseSequence java 81  ~ java util 0 27 7 jar    at com metamx common guava MappedSequence toYielder MappedSequence java 46
3459	Deadlock in global lookup cache  Had an issue with a query node which ended up reporting a deadlock on kill  3 I m still investigating and intend to make sure  3071 avoids this case  Found one Java level deadlock                                 NamespaceExtractionCacheManager 0   waiting for ownable synchronizer 0x00000004c42eaba8   a java util concurrent locks ReentrantLock NonfairSync   which is held by  topN_REDACTED_ 2016 09 01T15 00 00 000Z 2016 09 01T16 00 00 000Z    topN_REDACTED_ 2016 09 01T15 00 00 000Z 2016 09 01T16 00 00 000Z    waiting for ownable synchronizer 0x00000004c42eacc8   a java util concurrent locks ReentrantLock NonfairSync   which is held by  topN_REDACTED_ 2016 09 01T15 00 00 000Z 2016 09 01T16 00 00 000Z    topN_REDACTED_ 2016 09 01T15 00 00 000Z 2016 09 01T16 00 00 000Z    waiting for ownable synchronizer 0x00000004c42eaba8   a java util concurrent locks ReentrantLock NonfairSync   which is held by  topN_REDACTED_ 2016 09 01T15 00 00 000Z 2016 09 01T16 00 00 000Z    Java stack information for the threads listed above                                                       NamespaceExtractionCacheManager 0     parking to wait for   0x00000004c42eaba8   a java util concurrent locks ReentrantLock NonfairSync    locked  0x00000004c42c11a8   a java util concurrent atomic AtomicBoolean   topN_REDACTED_ 2016 09 01T15 00 00 000Z 2016 09 01T16 00 00 000Z      parking to wait for   0x00000004c42eacc8   a java util concurrent locks ReentrantLock NonfairSync    locked  0x00000005f4d39248   a java lang Object   topN_REDACTED_ 2016 09 01T15 00 00 000Z 2016 09 01T16 00 00 000Z      parking to wait for   0x00000004c42eaba8   a java util concurrent locks ReentrantLock NonfairSync    locked  0x00000005f4d35518   a java lang Object   Found 1 deadlock
3593	QTL  Deadlock in offheap cache                                 NamespaceExtractionCacheManager 0   waiting for ownable synchronizer 0x00000004f0550030   a java util concurrent locks ReentrantLock NonfairSync   which is held by  qtp1930842682 69   qtp1930842682 69   waiting to lock monitor 0x00007fa3d4009bd8  object 0x00000004c38b04a8  a java util concurrent atomic AtomicBoolean   which is held by  NamespaceExtractionCacheManager 0   Java stack information for the threads listed above                                                       NamespaceExtractionCacheManager 0     parking to wait for   0x00000004f0550030   a java util concurrent locks ReentrantLock NonfairSync    locked  0x00000004c38b04a8   a java util concurrent atomic AtomicBoolean   qtp1930842682 69     waiting to lock  0x00000004c38b04a8   a java util concurrent atomic AtomicBoolean   Found 1 deadlock
3600	Possible race condition while updating DataSource Metadata  Kafka Indexing Service   We have Kafka indexing service running currently with no replication and task count equivalent to number of kafka partitions  One of the tasks failed with this exception   com metamx common ISE  Transaction failure publishing segments  aborting at io druid indexing kafka KafkaIndexTask run KafkaIndexTask java 524  ~ druid kafka indexing service 0 9 3 1476736930 8d59341 1004 jar 0 9 3 1476736930 8d59341 1004  at io druid indexing overlord ThreadPoolTaskRunner ThreadPoolTaskRunnerCallable call ThreadPoolTaskRunner java 436   druid indexing service 0 9 3 1476736930 8d59341 1004 jar 0 9 3 1476736930 8d59341 1004  at io druid indexing overlord ThreadPoolTaskRunner ThreadPoolTaskRunnerCallable call ThreadPoolTaskRunner java 408   druid indexing service 0 9 3 1476736930 8d59341 1004 jar 0 9 3 1476736930 8d59341 1004  at java util concurrent FutureTask run FutureTask java 266     1 8 0_60  at java util concurrent ThreadPoolExecutor runWorker ThreadPoolExecutor java 1142     1 8 0_60  at java util concurrent ThreadPoolExecutor Worker run ThreadPoolExecutor java 617     1 8 0_60  at java lang Thread run Thread java 745     1 8 0_60  2016 10 19T08 23 19 572 INFO  task runner 0 priority 0  io druid indexing overlord TaskRunnerUtils   Task  index_kafka_v2_metrics_cluster_966f1c1b7a001cf_menkjigj  status changed to  FAILED   2016 10 19T08 23 19 575 INFO  task runner 0 priority 0  io druid indexing worker executor ExecutorLifecycle   Task completed with status     id     index_kafka_v2_metrics_cluster_966f1c1b7a001cf_menkjigj    status     FAILED    duration    11637165    Earlier in the task log I see this   2016 10 19T08 23 19 300 INFO  task runner 0 priority 0  io druid indexing common actions RemoteTaskActionClient   Submitting action for task index_kafka_v2_metrics_cluster_966f1c1b7a001cf_menkjigj  to overlord  druid indexer v1 action   SegmentInsertAction segments  DataSegment size 357264367  shardSpec NumberedShardSpec partitionNum 36  partitions 0   metrics     dimensions     version  2016 10 19T05 00 01 567Z   loadSpec  type hdfs  path    interval 2016 10 19T05 00 00 000Z 2016 10 19T06 00 00 000Z  dataSource  v2_metrics_cluster   binaryVersion  9    DataSegment size 328688502  shardSpec NumberedShardSpec partitionNum 5  partitions 0   metrics     dimensions     version  2016 10 19T07 00 00 975Z   loadSpec  type hdfs  path    interval 2016 10 19T07 00 00 000Z 2016 10 19T08 00 00 000Z  dataSource  v2_metrics_cluster   binaryVersion  9    DataSegment size 55419587  shardSpec NumberedShardSpec partitionNum 3  partitions 0   metrics     dimensions     version  2016 10 19T08 00 00 443Z   loadSpec  type hdfs  path    interval 2016 10 19T08 00 00 000Z 2016 10 19T09 00 00 000Z  dataSource  v2_metrics_cluster   binaryVersion  9    DataSegment size 396175234  shardSpec NumberedShardSpec partitionNum 1  partitions 0   metrics     dimensions     version  2016 10 19T06 00 00 545Z   loadSpec  type hdfs  path    interval 2016 10 19T06 00 00 000Z 2016 10 19T07 00 00 000Z  dataSource  v2_metrics_cluster   binaryVersion  9     startMetadata KafkaDataSourceMetadata kafkaPartitions KafkaPartitions topic  mx cluster production metrics   partitionOffsetMap  8 158229668     endMetadata KafkaDataSourceMetadata kafkaPartitions KafkaPartitions topic  mx cluster production metrics   partitionOffsetMap
3608	IngestSegmentFirehoseFactory race between tasks  IngestSegmentFirehoseFactory uses a fixed task id  reindex  when creating the toolbox factory that it uses to get segments  This creates a race condition where two tasks could actually download the same segment at the same time into the same directory  and one will get clobbered and fail  One situation where this can happen easily is if you re reindexing a datasource into two datasources  maybe reducing to two different levels of grain   The tasks will proceed simultaneously  since there s no cross locking  but they ll be downloading the same segments for the input datasource  One fix is having it actually use the real task id  which would need to get plumbed in   This has the advantage of putting the task files all together  and using the existing mechanisms for cleaning up task work directories
3772	Race condition in KafkaLookupExtractorFactory   3403  comment  FYI  gvsmirnov  just creating an issue in order not to forget about it
4226	druid lookups cached global race condition when realtime node starts  When we spawn a new realtime node from tranquility  from storm   it takes a bit to load all the lookups  aprox 36 seconds   and during that time  the node already published the segment and queries using lookups obviously fail This can be seen in the log of the realtime task  2017 04 28T08 00 25 880 ERROR  timeseries_stats_ 2017 04 28T08 00 00 000Z 2017 04 28T08 00 25 000Z   io druid query ChainedExecutionQueryRunner   Exception with one of the sequences! java lang NullPointerException  Lookup  advertiser_account_manager_id  not found at com google common base Preconditions checkNotNull Preconditions java 253  ~ guava 16 0 1 jar    at io druid query lookup RegisteredLookupExtractionFn ensureDelegate RegisteredLookupExtractionFn java 143  ~ druid processing 0 9 2 jar 0 9 2  at io druid query lookup RegisteredLookupExtractionFn apply RegisteredLookupExtractionFn java 115  ~ druid processing 0 9 2 jar 0 9 2  at io druid segment filter InFilter 2 1 apply InFilter java 110  ~ druid processing 0 9 2 jar 0 9 2  at io druid segment filter InFilter 2 1 apply InFilter java 106  ~ druid processing 0 9 2 jar 0 9 2  at io druid segment StringDimensionIndexer makeIndexingValueMatcher StringDimensionIndexer java 532  ~ druid processing 0 9 2 jar 0 9 2  at io druid segment incremental IncrementalIndexStorageAdapter CursorAndEntryHolderValueMatcherFactory makeValueMatcher IncrementalIndexStorageAdapter java 647  ~ druid processing 0 9 2 jar 0 9 2  at io druid segment filter InFilter makeMatcher InFilter java 88  ~ druid processing 0 9 2 jar 0 9 2  at io druid segment filter AndFilter makeMatcher AndFilter java 75  ~ druid processing 0 9 2 jar 0 9 2  at io druid segment incremental IncrementalIndexStorageAdapter makeFilterMatcher IncrementalIndexStorageAdapter java
4296	HLL BufferUnderflowException querying realtime indexing tasks  I already posted in the druid user google group about this  https   groups google com forum  !topic druid user X71sEX1Ia_8 but I ve decided to also post here since I think it may be an actual issue  I believe I m seeing an issue related to  3560  even though I m running the 0 9 2 final release downloaded from http   static druid io artifacts releases druid 0 9 2 bin tar gz  which should contain the fix   3578   A little context   I m running indexing realtime tasks on middleManagers that have events pushed to them from applications using tranquility   When I run timeseries queries for intervals that are being indexed on an indexing node  although the queries may initially return data  after a time  if I repeatedly run the queries I ll eventually begin receiving BufferUnderflowException errors from peon nodes  Here is the relevant query and stack trace pulled from the peon task logs Query    queryType    timeseries    dataSource    memcached_v1    intervals    2017 05 18T17 20Z 2017 05 18T18 20Z    granularity    all    context      timeout   120000     aggregations        name    count    type    doubleSum    fieldName    count        name    requests    type    hyperUnique    fieldName    requests_hll        name    unique_keys    type    hyperUnique    fieldName    unique_keys_hll
4984	Callback race on start in cachingCost balancer  CachingCostBalancerStrategyFactory registers callbacks in its start   method  which is problematic since any callbacks that happen between start of ServerInventoryView and BalancerStrategyFactory will be missed  Inventory view callbacks should generally be registered in constructors  not in start methods  I discovered this when trying to test the cachingCost balancer on a cluster that has a small number of segments  and found that it missed the initialization callback  happened too fast  so it refused to create itself  cc   dgolitsyn  leventov
5338	Potential bug in HttpPostEmitter causing high CPU usage  After upgrading to 0 11 0  some of the deployments are facing high CPU usage issue  After taking a thread dump  it was a suspicion that emitter thread might be causing it  Thread 65688   state   IN_JAVA    com metamx emitter core HttpPostEmitter emitAndReturnBatch com metamx emitter core Event   bci 111  line 249  Compiled frame  information may be imprecise    com metamx emitter core HttpPostEmitter emit com metamx emitter core Event   bci 2  line 214  Compiled frame    com metamx emitter core ComposingEmitter emit com metamx emitter core Event   bci 31  line 57  Compiled frame    com metamx emitter service ServiceEmitter emit com metamx emitter core Event   bci 5  line 72  Compiled frame    com metamx emitter service ServiceEmitter emit com metamx emitter service ServiceEventBuilder   bci 9  line 77  Compiled frame   To verify the issue  we added an executor in DruidCoordinator class which just keeps on emitting events in while true  loop like this   while  true    emitter emit ServiceMetricEvent builder   setDimension  dataSource    try   build  test   10       We found that after some time  batch tryAddEvents method always return false and the reference in concurrentBatch never changes and the while true  loop just keeps on spinning without sending anything or creating new batch  Still not sure why it is happening as its not happening for all deployments  might be some concurrency issue   leventov
6020	Race in taskMaster when the overlord becomes the leader  TaskMaster has the interfaces to return the variables  taskRunner  taskQueue  etc  which are initialized only when the overlord becomes the leader  The code of the interfaces is like this  public Optional TaskRunner  getTaskRunner     if  overlordLeaderSelector isLeader      return Optional of taskRunner     else   return Optional absent        However  taskRunner is initialized in DruidLeaderSelector Listener becomeLeader   which is called after the overlord becomes the leader  and thus Optional of   throws an NPE  The full stack trace is as follows  java lang NullPointerException at com google common base Preconditions checkNotNull Preconditions java 213  ~ guava 16 0 1 jar    at com google common base Optional of Optional java 85  ~ guava 16 0 1 jar    at io druid indexing overlord TaskMaster getTaskRunner TaskMaster java 214  ~ druid indexing service 0 12 1 iap8 jar 0 12 1 iap8  at io druid indexing overlord http OverlordResource getWorkers OverlordResource java 810  ~ druid indexing service 0 12 1 iap8 jar 0 12 1 iap8  at sun reflect NativeMethodAccessorImpl invoke0 Native Method  ~   1 8 0_163  at sun reflect NativeMethodAccessorImpl invoke NativeMethodAccessorImpl java 62  ~   1 8 0_163  at sun reflect DelegatingMethodAccessorImpl invoke DelegatingMethodAccessorImpl java 43  ~   1 8 0_163  at java lang reflect Method invoke Method java 498  ~   1 8 0_163  at com sun jersey spi container JavaMethodInvokerFactory 1 invoke JavaMethodInvokerFactory java 60  ~ jersey server 1 19 3 jar 1 19 3  at com sun jersey server impl model method dispatch AbstractResourceMethodDispatchProvider ResponseOutInvoker _dispatch AbstractResourceMethodDispatchProvider java 205  ~ jersey server 1 19 3 jar 1 19 3  at com sun jersey server impl model method dispatch ResourceJavaMethodDispatcher dispatch ResourceJavaMethodDispatcher java 75  ~ jersey server 1 19 3 jar 1 19 3  at com sun jersey server impl uri rules HttpMethodRule accept HttpMethodRule java 302  ~ jersey server 1 19 3 jar 1 19 3  at com sun jersey server impl uri rules RightHandPathRule accept RightHandPathRule java 147  ~ jersey server 1 19 3 jar 1 19 3  at com sun jersey server impl uri rules ResourceClassRule accept ResourceClassRule java 108  ~ jersey server 1 19 3 jar 1 19 3  at com sun jersey server impl uri rules RightHandPathRule accept RightHandPathRule java 147  ~ jersey server 1 19 3 jar 1 19 3  at com sun jersey server impl uri rules RootResourceClassesRule accept RootResourceClassesRule java 84  ~ jersey server 1 19 3 jar 1 19 3  at com sun jersey server impl application WebApplicationImpl _handleRequest WebApplicationImpl
6028	Error in SqlMetadataRuleManagerTest  Tests run  4  Failures  0  Errors  1  Skipped  0  Time elapsed  22 619 sec     FAILURE!   in io druid metadata SQLMetadataRuleManagerTest testMultipleStopAndStart io druid metadata SQLMetadataRuleManagerTest   Time elapsed  20 698 sec      ERROR! org skife jdbi v2 exceptions CallbackFailedException  org skife jdbi v2 exceptions UnableToExecuteStatementException  java sql SQLTransactionRollbackException  A lock could not be obtained due to a deadlock  cycle of locks and waiters is  Lock   ROW  SYSCOLUMNS   5 12  Waiting XID    237  X    APP  DROP TABLE druidTestc0dd9b47c4df453daeba2aca4613783e_rules Granted XID    217  S  Lock   TABLE  DRUIDTESTC0DD9B47C4DF453DAEBA2ACA4613783E_RULES  Tablelock Waiting XID    217  IS    APP  SELECT r dataSource  r payload FROM druidTestc0dd9b47c4df453daeba2aca4613783e_rules r INNER JOIN SELECT dataSource  max version  as version FROM druidTestc0dd9b47c4df453daeba2aca4613783e_rules GROUP BY dataSource  ds ON r datasource   ds datasource and r version   ds version Granted XID    237  X    The selected victim is XID   237   statement  DROP TABLE druidTestc0dd9b47c4df453daeba2aca4613783e_rules   located  DROP TABLE druidTestc0dd9b47c4df453daeba2aca4613783e_rules   rewritten  DROP TABLE druidTestc0dd9b47c4df453daeba2aca4613783e_rules   arguments   positional     named     finder      Caused by  org skife jdbi v2 exceptions UnableToExecuteStatementException  java sql SQLTransactionRollbackException  A lock could not be obtained due to a deadlock  cycle of locks and waiters is  Lock   ROW  SYSCOLUMNS   5 12  Waiting XID    237  X    APP  DROP TABLE druidTestc0dd9b47c4df453daeba2aca4613783e_rules Granted XID    217  S  Lock   TABLE  DRUIDTESTC0DD9B47C4DF453DAEBA2ACA4613783E_RULES  Tablelock Waiting XID    217  IS    APP  SELECT r dataSource  r payload FROM druidTestc0dd9b47c4df453daeba2aca4613783e_rules r INNER JOIN SELECT dataSource  max version  as version FROM druidTestc0dd9b47c4df453daeba2aca4613783e_rules GROUP BY dataSource  ds ON r datasource   ds datasource and r version   ds version Granted XID    237  X    The selected victim is XID   237   statement  DROP TABLE druidTestc0dd9b47c4df453daeba2aca4613783e_rules   located  DROP TABLE druidTestc0dd9b47c4df453daeba2aca4613783e_rules   rewritten  DROP TABLE druidTestc0dd9b47c4df453daeba2aca4613783e_rules   arguments   positional     named     finder      Caused by  java sql SQLTransactionRollbackException  A lock could not be obtained due to a deadlock  cycle of locks and waiters is  Lock   ROW  SYSCOLUMNS   5 12  Waiting XID    237  X    APP  DROP TABLE druidTestc0dd9b47c4df453daeba2aca4613783e_rules Granted XID    217  S  Lock   TABLE  DRUIDTESTC0DD9B47C4DF453DAEBA2ACA4613783E_RULES  Tablelock Waiting XID    217  IS    APP  SELECT r dataSource  r payload FROM druidTestc0dd9b47c4df453daeba2aca4613783e_rules r INNER JOIN SELECT dataSource  max version  as version FROM druidTestc0dd9b47c4df453daeba2aca4613783e_rules GROUP BY dataSource  ds ON r datasource   ds datasource and r version   ds version Granted XID    237  X    The selected victim is XID   237  Caused by  org apache derby iapi error StandardException  A lock could not be obtained due to a deadlock  cycle of locks and waiters is  Lock   ROW  SYSCOLUMNS   5 12  Waiting XID    237  X    APP  DROP TABLE druidTestc0dd9b47c4df453daeba2aca4613783e_rules Granted XID    217  S  Lock   TABLE  DRUIDTESTC0DD9B47C4DF453DAEBA2ACA4613783E_RULES  Tablelock Waiting XID    217  IS    APP  SELECT r dataSource  r payload FROM druidTestc0dd9b47c4df453daeba2aca4613783e_rules r INNER JOIN SELECT dataSource  max version  as version FROM druidTestc0dd9b47c4df453daeba2aca4613783e_rules GROUP BY dataSource  ds ON r datasource   ds datasource and r version   ds version Granted XID    237  X    The selected victim is XID   237
6139	Race in testCheckpointForUnknownTaskGroup   of KafkaSupervisorTest  The stacktrace is  2018 08 10T00 03 11 557 ERROR  KafkaSupervisor testDS  io druid indexing kafka supervisor KafkaSupervisor   KafkaSupervisor testDS  failed to handle notice   class io druid indexing kafka supervisor KafkaSupervisor  exceptionType class io druid java util common ISE  exceptionMessage WTH ! cannot find taskGroup  0  among all taskGroups       noticeClass CheckpointNotice  io druid java util common ISE  WTH ! cannot find taskGroup  0  among all taskGroups      at io druid indexing kafka supervisor KafkaSupervisor CheckpointNotice isValidTaskGroup KafkaSupervisor java 686  ~ classes     at io druid indexing kafka supervisor KafkaSupervisor CheckpointNotice handle KafkaSupervisor java 638  ~ classes     at io druid indexing kafka supervisor KafkaSupervisor 2 run KafkaSupervisor java 363   classes     at java util concurrent Executors RunnableAdapter call Executors java 511     1 8 0_161  at java util concurrent FutureTask run FutureTask java 266     1 8 0_161  at java util concurrent ThreadPoolExecutor runWorker ThreadPoolExecutor java 1149     1 8 0_161  at java util concurrent ThreadPoolExecutor Worker run ThreadPoolExecutor java 624     1 8 0_161  at java lang Thread run Thread java 748     1 8 0_161   The corresponding code is  Assert assertNotNull serviceEmitter getStackTrace     So  the issue is  serviceEmitter getStackTrace   can return null when this check is called  so the test should wait for the stacktrace to be set
6201	Deadlock on overlord with HttpRemoteTaskRunner  We noticed deadlocks on the overlord in our test cluster in HttpRemoteTaskRunner   hrtr pending tasks runner 0   193 daemon prio 5 os_prio 0 tid 0x00007fc5a8020000 nid 0x72ec waiting for monitor entry  0x00007fc5644b4000  java lang Thread State  BLOCKED  on object monitor    waiting to lock  0x00000000c1544c10   a java lang Object   java lang Thread State  BLOCKED  on object monitor    waiting to lock  0x00000000c1544c10   a java lang Object    HttpRemoteTaskRunner worker sync 4   116 daemon prio 5 os_prio 0 tid 0x00007fc588018800 nid 0x724c waiting for monitor entry  0x00007fc569801000  java lang Thread State  BLOCKED  on object monitor    waiting to lock  0x00000000c1544980   a io druid concurrent LifecycleLock    locked  0x00000000c1540300   a java util concurrent ConcurrentHashMap    locked  0x00000000c1540300   a java util concurrent ConcurrentHashMap   java lang Thread State  BLOCKED  on object monitor    waiting to lock  0x00000000c1544c10   a java lang Object    locked  0x00000000c1543b50   a io druid concurrent LifecycleLock
6287	Race condition in Kafka Indexing Service  verifyAndMergeCheckpoints in KafkaSupervisor has an issue where Futures addCallback      may not have been completed before its results are used in taskSequences sort  o1  o2     o2 rhs firstKey   compareTo o1 rhs firstKey
6826	Concurrency bug in DruidSchema refreshSegmentsForDataSource    segmentMap is created outside of lock  so by the time of SegmentMetadataHolder holder   dataSourceSegments get segment   segment could be already removed  It could cause NPE below  FYI  surekhasaharan  gianm
6867	deadlock found in DruidStatement  We found deadlock in DruidStatement and DruidConnection  this will cause exhaust connections  Found one Java level deadlock                                 DruidMeta 4e50ae56 ScheduledExecutor   waiting to lock monitor 0x00007fa5084802b8  object 0x00000000d6389830  a java lang Object   which is held by  qtp1871838170 210   qtp1871838170 210   waiting to lock monitor 0x00007fa5084916b8  object 0x00000000d603e4b8  a java util HashMap   which is held by  DruidMeta 4e50ae56 ScheduledExecutor   Java stack information for the threads listed above                                                       DruidMeta 4e50ae56 ScheduledExecutor     waiting to lock  0x00000000d6389830   a java lang Object    locked  0x00000000d603e4b8   a java util HashMap   qtp1871838170 210     waiting to lock  0x00000000d603e4b8   a java util HashMap    locked  0x00000000d6389830   a java lang Object    locked  0x00000000d6389830   a java lang Object   Found one Java level deadlock                                 qtp1871838170 283   waiting to lock monitor 0x00007fa50b53c918  object 0x00000007712471d8  a java lang Object   which is held by  qtp1871838170 199   qtp1871838170 199   waiting to lock monitor 0x00007fa508d0ae68  object 0x00000007766ce418  a java util HashMap   which is held by  qtp1871838170 283   Java stack information for the threads listed above                                                       qtp1871838170 283     waiting to lock  0x00000007712471d8   a java lang Object    locked  0x00000007766ce418   a java util HashMap   qtp1871838170 199     waiting to lock  0x00000007766ce418   a java util HashMap    locked  0x00000007712471d8   a java lang Object    locked  0x00000007712471d8   a java lang Object
7400	Allow users to use a provider for metadata username  Description Currently  we can use a PasswordProvider to provide credentials for connecting to the metadata store without hardcoding them  However  we still need to hardcode the username  and we need to restart Druid if we want to change the username  Motivation We would like to be able to change the user Druid uses to interact with the metadata database without needing to restart Druid  To do so  we need to be able to change the druid metadata storage connector user property during operation in the same way we can change druid metadata storage connector password while Druid is running  In our specific case  we would like to be able to use a single provider to furnish both the user name and the password  and such an implementation would also avoid introducing a race condition a la  6666  If users wanted to use separate providers for the username and the password  we could build a PasswordProvider that yoked together two other PasswordProviders to provide each field
8622	Proposal  concept of supervisor type task slots for resolving parallel task deadlocks  Motivation ParallelIndexSupervisorTask is a supervisor style task that delegates work to one or more spawned subtasks  Since both  supervisor task and subtasks  are using same task slot pool  there is possibility of a deadlock  For example  say a druid cluster has 4 task slots and 4 ParallelIndexSupervisorTask tasks are submitted simultaneously and started running on available 4 task slots  Subtasks spawned by the supervisor tasks would never be able to run and supervisor tasks would keep on waiting  It is also discussed in  8061  comment    Proposed changes Add a method boolean Task isSupervisor   to Task interface which returns true if the task is a supervisor task that spawns subtasks to delegate work  ParallelIndexSupervisorTask would return true while all other current task impls would return false  Add a druid worker supervisorCapacity configuration on middleManagers  which designates available slots to run supervisor tasks  This config is similar to druid worker capacity which designates available non supervisor task slots   Http RemoteTaskRunner code would be updated to recognize that supervisor tasks consume slot from supervisorCapacity and not capacity   Rationale A potential alternative is to use  7066 to send all supervisor tasks to a dedicated set of middleManagers which only get supervisor tasks  Operational impact User of ParallelIndexSupervisorTask would need to set property druid worker supervisorCapacity on middleManagers  Test plan  optional  will run it on a staging cluster  Future work  optional  for reliability  supervisor tasks could be treated further specially be imposing a  always restartable  restriction on them and also not failing them if middleManager running them crashed
9292	CoordinatorRuleManager rules doesn t need to store ConcurrentHashMap  CoordinatorRuleManager didn t have a race condition prior to  6898  actually  because the ConcurrentHashMap is never modified after assigning into AtomicReference  So the proper fix was  and the proper action now is  to replace ConcurrentHashMap with simple HashMap
10005	API to verify that published segments are loaded and available for a datasource  Description This new API will be use to verify that published segments are loaded and available for a datasource  The new API will be able to do the following    new api takes in datasource  This will returns false if any used segment  of the past 2 weeks  of the given datasource are not available to be query  i e  not loaded onto historical yet   Return true otherwise     same  new api takes in datasource and a time interval  start   end   This will returns false if any used segment  between the given start and given end time  of the given datasource are not available to be query  i e  not loaded onto historical yet   Return true otherwise    Note that the above are both the same API  The time interval is an optional parameter  The time interval referred above is the timestamp of the data in the segment  nothing to do with when the segment is ingested   This can be the same time interval as the time interval the user want to query data from  Basically if the user wants to query from x to y then they can call this new api with the datasource and time interval x to y  This will ensure that all segments of the datasource for the timestamp from x to y is ready to be query  loaded onto historical   Important differences between this API from the existing coordinator loadstatus API   Takes datasource  required  to be able to check faster  iterate smaller number of segments  Takes interval  optional  to be able to check faster  iterate smaller number of segments  Important  Takes boolean forceMetadataRefresh  If this is true  this will force poll the metadata source to get latest published segment information   API Path   druid coordinator v1 datasources  dataSourceName  loadstatus Request   GET  Path    dataSourceName  loadstatus    Produces MediaType APPLICATION_JSON   ResourceFilters DatasourceResourceFilter class  public Response getDatasourceLoadstatus   PathParam  dataSourceName   String dataSourceName   QueryParam  interval    Nullable final String interval   QueryParam  forceMetadataRefresh    Nullable final Boolean forceMetadataRefresh  QueryParam  simple    Nullabl final String simple   QueryParam  full    Nullabl final String full     Response  Default  No simple full given   Returns the percentage of segments actually loaded in the cluster versus segments that should be loaded in the cluster for the given datasource over the given interval  or last 2 weeks if not given   value in response is percentage         GIVEN_DATASOURCE  95 0    Simple  Returns the number of segments left to load until segments that should be loaded in the cluster are available for queries  This does not include replication  value in response is number of segments         GIVEN_DATASOURCE  5    full  Returns the number of segments left to load in each tier until segments that should be loaded in the cluster are all available  This includes replication  value in response is number of segments         _default_tier     GIVEN_DATASOURCE  1      interval can be null   default to last 2 weeks forceMetadataRefresh can be null   default to true Motivation This is to address  5721 The existing loadstatus API on the Coordinator reads segments from SqlSegmentsMetadataManager of the Coordinator which caches segments in memory and periodically updates them  Hence  there can be a race condition as this API implementation compares segments metadata from the mentioned cache with published segments in historical  Particularly  when there is a new ingestion after the initial load of the datasource  the cache still only contains the metadata of old segments  The API would compares list of old segments with what is published by historical and returns that everything is available when the new segments are not actually available yet  The workflow will be    submit ingestion task poll task api until task succeeded poll the new api with datasource  interval  and forceMetadataRefresh true once  If false  go to step 4  otherwise the data is available and user can query  poll the new api with datasource  interval  and forceMetadataRefresh false until return true  After true  data is available and user can query