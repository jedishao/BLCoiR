2962	Possible Deadlock Thead starvation between Hive loadPartitionByName and getTable  We ve seen an issue a couple times now where the Presto coordinator continues to queue queries  but none get executed  I believe I ve traced it back to a race condition that can cause a deadlock  In a thread dump of the coordinator we saw that all 40 of our Hive Metastore refresh threads were waiting on a getTable call from within the loadPartitionByName method  hive metastore hive 15188   2599434 daemon prio 5 os_prio 0 cpu 2 14ms elapsed 8545 48s tid 0x00007fb098048000 nid 0x27b55f waiting on condition   0x00007fb06c40d000  java lang Thread State  WAITING  parking  at jdk internal misc Unsafe park java base 11 0 6 Native Method    parking to wait for   0x00007fb9d2a1e350   a com google common util concurrent SettableFuture   I believe the issue is that all the executors in the thread pool are being used  leaving no open executors to complete the loadTable call needed to fulfill getTable  This behaviour seems to have been introduced by  1921  When loadPartitionByName was switched from using delete directly to calling getTable
6202	Include full stacktraces from LogTestDurationListener test hang monitor  See https   github com prestosql presto runs 1497472672  tests apparently hanged  one test thread is here   2020 12 04T07 53 43 1535741Z  pool 2 thread 2  prio 5 Id 16 BLOCKED on org testng TestClass 2a78dd6a owned by  pool 2 thread 1  Id 15 2020 12 04T07 53 43 1539747Z 	   blocked on org testng TestClass 2a78dd6a 2020 12 04T07 53 43 1548037Z 2020 12 04T07 53 43 1548486Z 	Number of locked synchronizers   1 2020 12 04T07 53 43 1549747Z 	  java util concurrent ThreadPoolExecutor Worker 7cedfa63  the information who owns TestClass 2a78dd6a is not present  probably because every stacktrace is limited to 8 frames   see full thread dump    https   github com prestosql presto runs 1497472672  or  https   gist github com findepi 2f87bad4047976bee6af9223f5d3ff6c
6389	Sporadic query failures due to a deadlock in SqlServerClient getTableProperties   Transaction was deadlocked     and has been chosen as the deadlock victim   https   github com prestosql presto runs 1581864249 2020 12 19T15 49 54 7256766Z  ERROR  testStddevPushdown io prestosql plugin sqlserver TestSqlServerIntegrationSmokeTest   Time elapsed  13 253 s      FAILURE! 2020 12 19T15 49 54 7271020Z java lang RuntimeException  Unable to advance result set  statement  SELECT data_compression_desc FROM sys partitions p INNER JOIN sys tables t ON p object_id   t object_id INNER JOIN sys indexes i ON t object_id   i object_id WHERE SCHEMA_NAME t schema_id     schema AND t name    table_name AND i type IN  0 1  AND i data_space_id NOT IN  SELECT data_space_id FROM sys partition_schemes    arguments  positional     named  schema dbo table_name test_stddev_pushdown_gn0z4   finder      2020 12 19T15 49 54 7363757Z Caused by  org jdbi v3 core result ResultSetException  Unable to advance result set  statement  SELECT data_compression_desc FROM sys partitions p INNER JOIN sys tables t ON p object_id   t object_id INNER JOIN sys indexes i ON t object_id   i object_id WHERE SCHEMA_NAME t schema_id     schema AND t name    table_name AND i type IN  0 1  AND i data_space_id NOT IN  SELECT data_space_id FROM sys partition_schemes    arguments  positional     named  schema dbo table_name test_stddev_pushdown_gn0z4   finder      2020 12 19T15 49 54 7471276Z Caused by  com microsoft sqlserver jdbc SQLServerException  Transaction  Process ID 52  was deadlocked on lock resources with another process and has been chosen as the deadlock victim  Rerun the transaction  2020 12 19T15 49 54 7488169Z 	    31 more
7454	Join spill to disk race condition  Join spill  from join side  operates as follows  Last join operator instance calls io trino operator PartitionedLookupSourceFactory finishProbeOperator  New io trino operator PartitionedConsumption is created with spilledPartitions  Spilled partitions are released by join in order using io trino operator PartitionedConsumption Partition release  Each SpilledLookupSourceHandle is disposed io trino operator SpilledLookupSourceHandle dispose when all joins call PartitionedConsumption Partition release on particular partition Dispose sets io trino operator SpilledLookupSourceHandle disposeRequested  which was also returned in io trino operator HashBuilderOperator isBlocked  This unblocks HashBuilderOperator which now can finish gracefully  Join spill  from build side  operates as follows   io trino operator HashBuilderOperator startMemoryRevoke is called on HashBuilderOperator  Revoke future finishes HashBuilderOperator gets blocked on io trino operator SpilledLookupSourceHandle disposeRequested or unspillingRequested  SpilledLookupSourceHandle is disposed when it s no longer used by join  This sets io trino operator SpilledLookupSourceHandle disposeRequested which unblocks HashBuilderOperator  HashBuilderOperator finishes gracefully  Race condition   io trino operator PartitionedLookupSourceFactory finishProbeOperator  New io trino operator PartitionedConsumption is created with spilledPartitions  io trino operator HashBuilderOperator startMemoryRevoke is called on HashBuilderOperator  It calls io trino operator PartitionedLookupSourceFactory setPartitionSpilledLookupSourceHandle which registers spilled partition in io trino operator PartitionedLookupSourceFactory spilledPartitions  However  PartitionedConsumption is already created  so newly spilled partition will never be released by join operators  HashBuilderOperator isBlocked never finishes because dispose is never called on spilled partition
7872	Flaky TestSqlServerTypeMapping   Transaction was deadlocked     and has been chosen as the deadlock victim   2021 05 09T11 29 42 9038391Z  ERROR  init io trino plugin sqlserver TestSqlServerTypeMapping   Time elapsed  2 698 s      FAILURE! 2021 05 09T11 29 42 9040757Z java lang RuntimeException  Failed to execute statement  ALTER DATABASE database_6dfc467e1fca49b3b1e222cf5b22d97b SET READ_COMMITTED_SNAPSHOT ON 2021 05 09T11 29 42 9086391Z Caused by  com microsoft sqlserver jdbc SQLServerException  Transaction  Process ID 51  was deadlocked on lock resources with another process and has been chosen as the deadlock victim  Rerun the transaction  2021 05 09T11 29 42 9114812Z 	    17 more 2021 05 09T11 29 42 9115059Z 2021 05 09T11 29 43 3184530  Similar issue   6389
9741	TestSqlServerConnectorTest testSelectInformationSchemaColumns fail in 2019 CU13 ubuntu 20 04  The test succeeds in 2017 image  but it fails in 2019 image  We need to find the cause of deadlock  Error   Tests run  274  Failures  1  Errors  0  Skipped  32  Time elapsed  694 821 s     FAILURE!   in TestSuite Error   io trino plugin sqlserver TestSqlServerConnectorTest testSelectInformationSchemaColumns  Time elapsed  5 331 s      FAILURE! java lang AssertionError  Expected query to succeed  SELECT   FROM information_schema columns Caused by  java lang RuntimeException  Transaction  Process ID 60  was deadlocked on lock resources with another process and has been chosen as the deadlock victim  Rerun the transaction      16 more Suppressed  java lang Exception  SQL  SELECT   FROM information_schema columns     17 more Caused by  io trino spi TrinoException  Transaction  Process ID 60  was deadlocked on lock resources with another process and has been chosen as the deadlock victim  Rerun the transaction  Caused by  com microsoft sqlserver jdbc SQLServerException  Transaction  Process ID 60  was deadlocked on lock resources with another process and has been chosen as the deadlock victim  Rerun the transaction      46 more
11798	Hive locks in waiting state still held indefinitely  Hive locks in waiting state  timestamp acquired_at would have ‘NULL’ value   and then locks are held indefinitely  The missing timestamp might be the cause  Steps to reproduce   There is an exclusive lock on the hive table  perhaps by writing data with hive spark  Run select query in Trino  Trino shared_read lock in waiting state due to the exclusive lock in step 1 If we cancel the Trino query  the Trino lock stays there forever in waiting state    50026627 1   finance_dh   booking_f   NULL   ACQUIRED     SHARED_READ   51469930   0   1646976632000   somenaths   coordinator 58d7bfd46d thq4w   20220311_053014_06047_rerfi     49750772 1   finance_dh   booking_f   NULL   WAITING   49749303 1   SHARED_READ   51197595   0   NULL   sghildiyal   coordinator 74bc6dfc86 48dkj   20220309_054310_07520_29emh     49751175 1   finance_dh   booking_f   NULL   WAITING   49749303 1   SHARED_READ   51197962   0   NULL   beherasa   coordinator 74bc6dfc86 48dkj   20220309_054825_07702_29emh     10401 released the locks in ACQUIRED state but didn’t release the locks in WAITING state  The possible workaround is a script to delete the locks in Hive  CC   findepi  losipiuk
13212	Lock congestion in DirectExchangeClient  io trino operator DirectExchangeClient addPages and io trino operator DirectExchangeClient pollPage can cause significant lock congestion  This is most likely caused by 5832a59 which moved setBytes to critical section
14132	Missing manifest file when releasing Hive Metastore s the lock has failed  HiveMetastoreTableOperations using aquireLock and releaseLock while committing to iceberg existing table  releaseLock happens in the finally block and that step can also fail  and that result into that missing manifest file  Steps To Reproduce  CREATE TABLE tbl   id integer  name varchar  part varchar   WITH   location     path   format    PARQUET      INSERT INTO tbl VALUES  1  product    part1      if HiveMetastoreTableOperations s line failed to execute at Metastore level  It can fail for Metastore timeout or if lock is missing from HMS side it can throw NoSuchLockException which is an Exception and not RuntimeException Then after the failure  insert select or any query on this table will fail  and table becomes unusable  INSERT INTO tbl VALUES  2  product    part1     Query 20220829_214404_00026_j9tvf  FAILED  3 nodes Splits  68 total  67 done  98 53   0 47  0 rows  0B   0 rows s  0B s   Query 20220829_214404_00026_j9tvf failed  Failed to get status for file    path tbl metadata snap 5290494244823571353 1 34a427a1 054c 4ecf a49b f3e3ef9203a0 avro   Reason would be when the release lock step was failed  the Iceberg s commit handles only RuntimeException so in case of NoSuchLockException being thrown then it has not done any step  So  looks like the new snapshot is created and pointing to a new manifest file in the previous step  but that file is missing not created  related issues   14104  12581
14605	Queries are occasionally getting stuck  Manifests on the CI  for example https   pipelines actions githubusercontent com serviceHosts 127fb2d0 0ed8 42e9 b060 7c4f23b9a844 _apis pipelines 1 runs 133247 signedlogcontent 85 urlExpires 2022 10 12T22 3A16 3A33 7329214Z&urlSigningMethod HMACV1&urlSignature BdhvMIRPxrk2hYFLkx7OmL9rPSLVFyQgNbLYFg8rbMo 3D   The test that got stuck in this specific run is io trino faulttolerant iceberg TestIcebergTaskFailureRecoveryTest testExplainAnalyze  Stack trace  2022 10 12T20 36 49 8702056Z  pool 3 thread 2  prio 5 Id 17 RUNNABLE  in native   While the test is built to validate failure recovery capabilities in fault tolerant execution the actual query that is getting stuck is not run with fault tolerant execution  see FailureRecoveryAssert failsWithoutRetries in the stack trace   There s a chance that something is broken in the query failure path in pipelined execution